+++
#+OPTIONS: \n:t
categories = ["技术文章"]
date = "2022-11-22T22:42:55+08:00"
description = ""
keywords = ["K8S","项目实践"]
tags = ["K8S","项目实践"]
title = "K8S项目实践(02): 动手实现minicni"
url = "/2022/11/22/k8s-practice-minicni/"

+++

* 简介
  不管是容器网络还是 Kubernetes 网络都需要解决以下两个核心问题：

  - 容器/Pod IP 地址的管理
  - 容器/Pod 之间的相互通信
  容器/Pod IP 地址的管理包括容器 IP 地址的分配与回收，而容器/Pod 之间的相互通信包括同一主机的容器/Pod 之间和跨主机的容器/Pod 之间通信两种场景。这两个问题也不能完全分开来看，因为不同的解决方案往往要同时考虑以上两点。对于同一主机的容器/Pod 之间的通信来说实现相对容易，实际的挑战在于，不同容器/Pod 完全可能分布在不同的集群节点上，如何实现跨主机节点的通信不是一件容易的事情。

如果不采用 SDN(Software define networking) 方式来修改底层网络设备的配置，主流方案是在主机节点的 underlay 网络平面构建新的 overlay 网络负责传输容器/Pod 之间通信数据。这种网络方案在如何复用原有的 underlay 网络平面也有不同的实现方式：

  - 将容器的数据包封装到原主机网络（underlay 网络平面）的三层或四层数据包中，然后使用主机网络的三层或者四层协议传输到目标主机，目标主机拆包后再转发给目标容器；
  - 把容器网络加到主机路由表中，把主机网络（underlay 网络平面）设备当作容器网关，通过路由规则转发到指定的主机，实现容器的三层互通；

* CNI原理
  CNI 规范相对于 CNM(Container Network Model) 对开发者的约束更少、更开放，不依赖于容器运行时，因此也更简单。关于 CNI 规范的详情请查看[[https://github.com/containernetworking/cni/blob/master/SPEC.md][官方文档]]。

    [[http://blog.mospan.cn/post/img/k8s/minicni/cni-standard.png]]

    详见: [[https://blog.csdn.net/elihe2011/article/details/122926399][K8S 网络CNI]]


  实现一个 CNI 网络插件只需要一个配置文件和一个可执行文件：

  - 配置文件描述插件的版本、名称、描述等基本信息；
  - 可执行文件会被上层的容器管理平台调用，一个 CNI 可执行文件需要实现将容器加入到网络的 ADD 操作以及将容器从网络中删除的 DEL 操作等；

  Kubernetes 使用 CNI 网络插件的基本工作流程是：

  - kubelet 先创建 pause 容器创建对应的网络命名空间；
  - 根据配置调用具体的 CNI 插件，可以配置成 CNI 插件链来进行链式调用；
  - 当 CNI 插件被调用时，它根据环境变量以及命令行参数来获得网络命名空间、容器的网络设备等必要信息，然后执行 ADD 或者其他操作；
  - CNI 插件给 pause 容器配置正确的网络，pod 中其他的容器都是复用 pause 容器的网络；

* CNI配置文件
  现在我们来到关键的部分。一般来说，CNI 插件需要在集群的每个节点上运行，在 CNI 的规范里面，实现一个 CNI 插件首先需要一个 JSON 格式的配置文件，配置文件需要放到每个节点的 /etc/cni/net.d/ 目录，一般命名为 <数字>-<CNI-plugin>.conf，而且配置文件至少需要以下几个必须的字段：

 1. cniVersion: CNI 插件的字符串版本号，要求符合 Semantic Version 2.0 规范；
 2. name: 字符串形式的网络名；
 3. type: 字符串表示的 CNI 插件的可运行文件；
  除此之外，我们也可以增加一些自定义的配置字段，用于传递参数给 CNI 插件，这些配置会在运行时传递给 CNI 插件。在我们的例子里面，需要配置每个宿主机网桥的设备名、网络设备的最大传输单元(MTU)以及每个节点分配的24位子网地址，因此，我们的 CNI 插件的配置看起来会像下面这样：
 
  #+BEGIN_SRC json
  {
    "cniVersion": "0.1.0",
    "name": "minicni",
    "type": "minicni",
    "bridge": "minicni0",
    "mtu": 1500,
    "subnet": __NODE_SUBNET__
}
  
  #+END_SRC

 #+BEGIN_QUOTE
 Note: 确保配置文件放到 /etc/cni/net.d/ 目录，kubelet 默认此目录寻找 CNI 插件配置；并且，插件的配置可以分为多个插件链的形式来运行，但是为了简单起见，在我们的例子中，只配置一个独立的 CNI 插件，因为配置文件的后缀名为 .conf。
 #+END_QUOTE 

* CNI核心实现
  接下来就开始看怎么实现 CNI 插件来管理 pod IP 地址以及配置容器网络设备。在此之前，我们需要明确的是，CNI 介入的时机是 kubelet 创建 pause 容器创建对应的网络命名空间之后，同时当 CNI 插件被调用的时候，kubelet 会将相关操作命令以及参数通过环境变量的形式传递给它。这些环境变量包括：
  - CNI_COMMAND: CNI 操作命令，包括 ADD, DEL, CHECK 以及 VERSION
  - CNI_CONTAINERID: 容器 ID
  - CNI_NETNS: pod 网络命名空间
  - CNI_IFNAME: pod 网络设备名称
  - CNI_PATH: CNI 插件可执行文件的搜索路径
  - CNI_ARGS: 可选的其他参数，形式类似于 key1=value1,key2=value2...
  在运行时，kubelet 通过 CNI 配置文件寻找 CNI 可执行文件，然后基于上述几个环境变量来执行相关的操作。CNI 插件必须支持的操作包括：

  - ADD: 将 pod 加入到 pod 网络中
  - DEL: 将 pod 从 pod 网络中删除
  - CHECK: 检查 pod 网络配置正常
  - VERSION: 返回可选 CNI 插件的版本信息
  让我们直接跳到 CNI 插件的入口函数：
  #+BEGIN_SRC go
    func main() {
  	    cmd, cmdArgs, err := args.GetArgsFromEnv()
  	    if err != nil {
  	    	fmt.Fprintf(os.Stderr, "getting cmd arguments with error: %v", err)
  	    }
  
  	    fh := handler.NewFileHandler(IPStore)
  
  	    switch cmd {
  	    case "ADD":
  	    	err = fh.HandleAdd(cmdArgs)
  	    case "DEL":
  	    	err = fh.HandleDel(cmdArgs)
  	    case "CHECK":
  	    	err = fh.HandleCheck(cmdArgs)
  	    case "VERSION":
  	    	err = fh.HandleVersion(cmdArgs)
  	    default:
  	    	err = fmt.Errorf("unknown CNI_COMMAND: %s", cmd)
  	    }
  	    if err != nil {
  	    	fmt.Fprintf(os.Stderr, "Failed to handle CNI_COMMAND %q: %v", cmd, err)
  	    	os.Exit(1)
  	    }
     }
  #+END_SRC
  可以看到，我们首先调用 GetArgsFromEnv() 函数将 CNI 插件的操作命令以及相关参数通过环境变量读入，同时从标准输入获取 CNI 插件的 JSON 配置，然后基于不同的 CNI 操作命令执行不同的处理函数。

  需要注意的是，我们将处理函数的集合实现为一个接口，这样就可以很容易的扩展不同的接口实现。在最基础的版本实现中，我们基本文件存储分配的 IP 信息。但是，这种实现方式存在很多问题，例如，文件存储不可靠，读写可能会发生冲突等，在后续的版本中，我们会实现基于 kubernetes 存储的接口实现，将子网信息以及 IP 信息存储到 apiserver 中，从而实现可靠存储。

  接下来，我们就看看基于文件的接口实现是怎么处理这些 CNI 操作命令的。
  对于 ADD 命令：

  - 从标准输入获取 CNI 插件的配置信息，最重要的是当前宿主机网桥的设备名、网络设备的最大传输单元(MTU)以及当前节点分配的24位子网地址；
  - 然后从环境变量中找到对应的 CNI 操作参数，包括 pod 容器网络命名空间以及 pod 网络设备名等；
  - 接下来创建或者更新节点宿主机网桥，从当前节点分配的24位子网地址中抽取子网的网关地址，准备分配给节点宿主机网桥；
  - 接着将从文件读取已经分配的 IP 地址列表，遍历24位子网地址并从中取出第一个没有被分配的 IP 地址信息，准备分配给 pod 网络设备；pod 网络设备是 veth 设备对，一端在 pod 网络命名空间中，另外一端连接着宿主机上的网桥设备，同时所有的 pod 网络设备将宿主机上的网桥设备当作默认网关；
  - 最终成功后需要将新的 pod IP 写入到文件中。

看起来很简单对吧？其实作为最简单的方式，这种方案可以实现最基础的 ADD 功能：
   
  
* 编译部署
** 1、编译
   #+BEGIN_SRC sh
   make build
   make image
   #+END_SRC
    [[http://blog.mospan.cn/post/img/k8s/minicni/make-image-minicni.png]]

    在docker hub里已看到了minicni镜像：
    
    [[http://blog.mospan.cn/post/img/k8s/minicni/docker-hub-minicni.png]]

** 部署
   1、登录已安装好的k8s集群，把之前已存在的cni如(calico)卸载掉再安装minici:
   #+BEGIN_SRC sh
   [root@k8s-master minicni]# kubectl  apply -f minicni.yaml
   clusterrole.rbac.authorization.k8s.io/minicni created
   serviceaccount/minicni created
   clusterrolebinding.rbac.authorization.k8s.io/minicni created
   configmap/minicni-config created
   Warning: spec.template.spec.nodeSelector[beta.kubernetes.io/os]: deprecated since v1.14; use "kubernetes.io/os" instead
   Warning: spec.template.metadata.annotations[scheduler.alpha.kubernetes.io/critical-pod]: non-functional in v1.16+; use the "priorityClassName" field instead
   daemonset.apps/minicni-node created
   [root@k8s-master minicni]#
   
   #+END_SRC

   2、查看minicni部署状态：
   #+BEGIN_SRC sh
   [root@k8s-master minicni]# kubectl get pod -A -o wide
    NAMESPACE     NAME                                       READY   STATUS        RESTARTS        AGE     IP               NODE         NOMINATED NODE   READINESS GATES
    default       nginx-85b98978db-qkd6h                     0/1     Completed     5               4d21h   <none>           k8s-work1    <none>           <none>
    kube-system   calico-kube-controllers-7b8458594b-p2fqj   0/1     Terminating   2               4d12h   <none>           k8s-work2    <none>           <none>
    kube-system   coredns-6d8c4cb4d-ck2x5                    0/1     Completed     7               4d22h   <none>           k8s-master   <none>           <none>
    kube-system   coredns-6d8c4cb4d-mbctj                    0/1     Completed     7               4d22h   <none>           k8s-master   <none>           <none>
    kube-system   etcd-k8s-master                            1/1     Running       8 (8m14s ago)   4d22h   172.25.140.216   k8s-master   <none>           <none>
    kube-system   kube-apiserver-k8s-master                  1/1     Running       8 (8m4s ago)    4d22h   172.25.140.216   k8s-master   <none>           <none>
    kube-system   kube-controller-manager-k8s-master         1/1     Running       8 (8m14s ago)   4d22h   172.25.140.216   k8s-master   <none>           <none>
    kube-system   kube-proxy-dnsjg                           1/1     Running       8 (8m14s ago)   4d21h   172.25.140.215   k8s-work1    <none>           <none>
    kube-system   kube-proxy-r84lg                           1/1     Running       8 (8m14s ago)   4d22h   172.25.140.216   k8s-master   <none>           <none>
    kube-system   kube-proxy-tbkx2                           1/1     Running       7 (8m14s ago)   4d21h   172.25.140.214   k8s-work2    <none>           <none>
    kube-system   kube-scheduler-k8s-master                  1/1     Running       8 (8m14s ago)   4d22h   172.25.140.216   k8s-master   <none>           <none>
    kube-system   minicni-node-8lmxc                         1/1     Running       0               5m17s   172.25.140.214   k8s-work2    <none>           <none>
    kube-system   minicni-node-sgjmg                         1/1     Running       0               5m17s   172.25.140.216   k8s-master   <none>           <none>
    kube-system   minicni-node-xslgx                         1/1     Running       0               5m17s   172.25.140.215   k8s-work1    <none>           <none> 
   #+END_SRC
   可以看出minicni处于Running状态。

   
* 测试
  1、环境准备, 分别给node打上相应的标签
  #+BEGIN_SRC sh
  [root@k8s-master minicni]# k label nodes k8s-master role=master
  node/k8s-master labeled
  [root@k8s-master minicni]# k label nodes k8s-work1 role=worker
  node/k8s-work1 labeled
  [root@k8s-master minicni]# k label nodes k8s-work2 role=worker
  node/k8s-work2 labeled
  
  #+END_SRC
   
  2、分别在 master 与 worker 节点部署 netshoot 与 httpbin：
  #+BEGIN_SRC sh
  [root@k8s-master minicni]# k apply -f test-pods.yaml
  pod/httpbin-master created
  pod/netshoot-master created
  pod/httpbin-worker created
  pod/netshoot-worker created
  [root@k8s-master minicni]#
  
  #+END_SRC

  3、确保所有 pod 都启动并开始运行：
  #+BEGIN_SRC sh
  [root@k8s-master minicni]# k get pod
  NAME                     READY   STATUS              RESTARTS   AGE
  httpbin-master           0/1     ContainerCreating   0          8s
  httpbin-worker           0/1     ContainerCreating   0          8s
  netshoot-master          0/1     ContainerCreating   0          8s
  netshoot-worker          0/1     ContainerCreating   0          8s
  [root@k8s-master minicni]# 
  
  #+END_SRC
  发现状态为ContainerCreating, 查看pod描述报错为:
  #+BEGIN_SRC sh
  Warning  FailedCreatePodSandBox  36s                kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container "7cb17eace85729539db7d7af1a4955a353a14eb7ffa96f84c117ee6633b3e2b5" network for pod "httpbin-master": networkPlugin cni failed to set up pod "httpbin-master_default" network: error getting ClusterInformation: connection is unauthorized: Unauthorized, failed to clean up sandbox container "7cb17eace85729539db7d7af1a4955a353a14eb7ffa96f84c117ee6633b3e2b5" network for pod "httpbin-master": networkPlugin cni failed to teardown pod "httpbin-master_default" network: error getting ClusterInformation: connection is unauthorized: Unauthorized]
  
  #+END_SRC 
  如果安装了calico网络插件，需要删除calico:
  #+BEGIN_SRC sh
  kubectl delete -f  <yaml>
  #+END_SRC
  还要去所有节点etc/cni/net.d/目录下 删掉与calico相关的所有配置文件, 然后重启机器。 不然pod起不来，会报错 network: error getting ClusterInformation: connection is unauthorized: Unauthorized . 

  4、最后再查看POD都变成Running状态了
  #+BEGIN_SRC sh
  [root@k8s-master minicni]# k get pod -A -o wide
  NAMESPACE     NAME                                 READY   STATUS    RESTARTS       AGE     IP               NODE         NOMINATED NODE   READINESS GATES
  default       httpbin-master                       1/1     Running   0              20m     10.244.0.4       k8s-master   <none>           <none>
  default       httpbin-worker                       1/1     Running   0              20m     10.244.2.2       k8s-work2    <none>           <none>
  default       netshoot-master                      1/1     Running   0              20m     10.244.0.5       k8s-master   <none>           <none>
  default       netshoot-worker                      1/1     Running   0              20m     10.244.2.3       k8s-work2    <none>           <none>
  default       nginx-85b98978db-2hzc9               1/1     Running   0              50m     10.244.1.2       k8s-work1    <none>           <none>
  kube-system   coredns-6d8c4cb4d-ck2x5              1/1     Running   9 (24h ago)    6d23h   10.244.0.2       k8s-master   <none>           <none>
  kube-system   coredns-6d8c4cb4d-mbctj              1/1     Running   9 (24h ago)    6d23h   10.244.0.3       k8s-master   <none>           <none>
  kube-system   etcd-k8s-master                      1/1     Running   10 (24h ago)   6d23h   172.25.140.216   k8s-master   <none>           <none>
  kube-system   kube-apiserver-k8s-master            1/1     Running   12 (24h ago)   6d23h   172.25.140.216   k8s-master   <none>           <none>
  kube-system   kube-controller-manager-k8s-master   1/1     Running   10 (24h ago)   6d23h   172.25.140.216   k8s-master   <none>           <none>
  kube-system   kube-proxy-dnsjg                     1/1     Running   10 (24h ago)   6d22h   172.25.140.215   k8s-work1    <none>           <none>
  kube-system   kube-proxy-r84lg                     1/1     Running   10 (24h ago)   6d23h   172.25.140.216   k8s-master   <none>           <none>
  kube-system   kube-proxy-tbkx2                     1/1     Running   9 (24h ago)    6d22h   172.25.140.214   k8s-work2    <none>           <none>
  kube-system   kube-scheduler-k8s-master            1/1     Running   10 (24h ago)   6d23h   172.25.140.216   k8s-master   <none>           <none>
  kube-system   minicni-node-5w9fn                   1/1     Running   0              55m     172.25.140.215   k8s-work1    <none>           <none>
  kube-system   minicni-node-jb5cj                   1/1     Running   0              55m     172.25.140.214   k8s-work2    <none>           <none>
  kube-system   minicni-node-kp25h                   1/1     Running   0              55m     172.25.140.216   k8s-master   <none>           <none>
  [root@k8s-master minicni]#
  #+END_SRC

  5、之后测试以下四种网络通信是否正常：

  - pod 到宿主机的通信

    #+BEGIN_SRC sh 

    [root@k8s-master minicni]# kubectl exec -ti netshoot-master -- /bin/bash
    bash-5.1# ping 172.25.140.216
    PING 172.25.140.216 (172.25.140.216) 56(84) bytes of data.
    64 bytes from 172.25.140.216: icmp_seq=1 ttl=64 time=0.074 ms
    64 bytes from 172.25.140.216: icmp_seq=2 ttl=64 time=0.035 ms
    64 bytes from 172.25.140.216: icmp_seq=3 ttl=64 time=0.033 ms
    64 bytes from 172.25.140.216: icmp_seq=4 ttl=64 time=0.043 ms
    64 bytes from 172.25.140.216: icmp_seq=5 ttl=64 time=0.048 ms
    ^C
    --- 172.25.140.216 ping statistics ---
    5 packets transmitted, 5 received, 0% packet loss, time 3999ms
    rtt min/avg/max/mdev = 0.033/0.046/0.074/0.014 ms
    bash-5.1# 
    
    #+END_SRC   

  - 同一个节点 pod-to-pod 通信
    默认情况下，同一台主机上的 pod-to-pod 网络包默认会被 Linux 内核丢弃，原因是 Linux 默认会把非 default 网络命名空间的网络包看作是外部数据包，关于这个问题的具体细节，请查看 stackoverflow 上的讨论。目前，我们需要在每个集群结点上使用以下命令手动添加以下 iptables 规则来让 pod-to-pod 网络数据包顺利转发：

    #+BEGIN_SRC sh
    [root@k8s-master ~]# iptables -t filter -A FORWARD -s 10.244.0.0/24  -j ACCEPT
    [root@k8s-master ~]# iptables -t filter -A FORWARD -d 10.244.0.0/24  -j ACCEPT
    #+END_SRC

    再进行测试:
  
    #+BEGIN_SRC sh
    [root@k8s-master ~]# kubectl  exec -ti netshoot-master -- /bin/bash
    bash-5.1# ping 10.244.0.25
    PING 10.244.0.25 (10.244.0.25) 56(84) bytes of data.
    64 bytes from 10.244.0.25: icmp_seq=1 ttl=64 time=0.079 ms
    64 bytes from 10.244.0.25: icmp_seq=2 ttl=64 time=0.061 ms 
  
    #+END_SRC
    测试通信正常。


  - pod 到其他主机的通信

    #+BEGIN_SRC sh
    [root@k8s-master ~]# kubectl  exec -ti netshoot-master -- /bin/bash
    bash-5.1# ping 172.25.140.214
    PING 172.25.140.214 (172.25.140.214) 56(84) bytes of data.

    ^C
    --- 172.25.140.214 ping statistics ---
    4 packets transmitted, 0 received, 100% packet loss, time 2999ms
  
    #+END_SRC
    Ping不通，原因是阿里云底层网络对非法IP限制，不是随便配个IP就可以通，如需通可以考虑overlay如calico或本地虚拟机搭建方式。

  - 跨一个节点 pod-to-pod 通信
    对于跨节点的 pod-to-pod 网络包，需要像 Calico 那样添加宿主机的路由表，保证发往各个节点上的 pod 流量经过节点的转发。目前这些路由表需要手动添加：
    #+BEGIN_SRC 
    ip route add 10.244.2.0/24  via 172.25.140.214 dev eth0 #run on master 
    ip route add 10.244.0.0/24  via 172.25.140.216 dev eth0 #run on worker
    #+END_SRC
    再测试：
    #+BEGIN_SRC sh
    [root@k8s-master ~]# kubectl  exec -ti netshoot-master -- /bin/bash
    bash-5.1# ping 10.244.2.11
    PING 10.244.2.11 (10.244.2.11) 56(84) bytes of data.

    ^C
    --- 10.244.2.11 ping statistics ---
    108 packets transmitted, 0 received, 100% packet loss, time 106996ms    
    #+END_SRC
    现象与原理同上。

* FAQ
** cannot find package 编译不过
   当出现如下错误时：
   #+BEGIN_SRC sh
   $ make build
   Building the minicni on amd64...
   cmd/main.go:8:2: cannot find package "github.com/morvencao/minicni/pkg/args" in any of:
   	/usr/local/go/src/github.com/morvencao/minicni/pkg/args (from $GOROOT)
   	/Users/mosp/goget/src/github.com/morvencao/minicni/pkg/args (from $GOPATH)
   cmd/main.go:9:2: cannot find package "github.com/morvencao/minicni/pkg/handler" in any of:
   	/usr/local/go/src/github.com/morvencao/minicni/pkg/handler (from $GOROOT)
   	/Users/mosp/goget/src/github.com/morvencao/minicni/pkg/handler (from $GOPATH)
   make: *** [build] Error 1
   #+END_SRC
   需要把GO111MODULE=on或auto，才能使用Go module功能，可在.bashrc或.zshrc里加上：export GO111MODULE=auto
   详见[[http://www.ay1.cc/article/18635.html][go自动下载所有的依赖包go module使用详解_Golang]]：
   
** build constraints exclude all Go files in
   当出现如下错误时:
   #+BEGIN_SRC sh
   $ make build
   Building the minicni on amd64...
   go build github.com/containernetworking/plugins/pkg/ns: build constraints exclude all Go files in /Users/mosp/goget/pkg/mod/github.com/containernetworking/plugins@v1.1.1/pkg/ns
   make: *** [build] Error 1
      
   #+END_SRC
   需要设置如下两个变量：
   export GOOS=“linux”。即：不能为darwin
   export CGO_ENABLED=“1”。 
   详见：[[https://blog.csdn.net/weixin_42845682/article/details/124568715][build constraints exclude all Go files in xxx/xxx/xxx]]


* 参考资料
  【01】 [[https://blog.csdn.net/u012772803/article/details/113703029][find -print0和xargs -0原理及用法]]]
  【02】 [[https://zhuanlan.zhihu.com/p/411181637][Go语言import分组管理利器: goimports-reviser]]
  【03】 [[https://morven.life/posts/create-your-own-cni-with-golang/][使用 Go 从零开始实现 CNI]]
  【04】 [[https://blog.csdn.net/a5534789/article/details/112848404][centOS内网安装kubernetes集群]] 
  【05】 [[https://zhuanlan.zhihu.com/p/415032187][Linux 路由表(RIB表、FIB表)、ARP表、MAC表整理]]] 
  【06】 [[https://www.jianshu.com/p/75704eb30eff][查看CNI中的veth pair]]
  【07】 [[https://mp.weixin.qq.com/s/7t_MoZ0quJF50VoIwqvKyQ][一文吃透 K8S 网络模型]] 
  【08】 [[https://blog.csdn.net/elihe2011/article/details/122926399][K8S 网络CNI]] 
  
