<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>墨斯潘園 on 墨斯潘園</title>
        <link>http://mospany.github.io/</link>
        <language>zh-CN</language>
        <author>Mospan</author>
        <rights>Copyright (c) 2016, mospan; all rights reserved.</rights>
        <updated>Sat, 15 Nov 2025 12:03:56 CST</updated>
        
        <item>
            <title>大模型从0到1训练推理入门</title>
            <link>http://mospany.github.io/2025/11/15/llm-0-to-1/</link>
            <pubDate>Sat, 15 Nov 2025 12:03:56 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2025/11/15/llm-0-to-1/</guid>
            <description>

&lt;h1 id=&#34;数据准备&#34;&gt;数据准备&lt;/h1&gt;

&lt;h1 id=&#34;模型微调&#34;&gt;模型微调&lt;/h1&gt;

&lt;p&gt;如果您希望模型在特定领域表现更好，可以通过在该领域的数据集上对模型微调来实现。本文以如下场景为例为您介绍模型微调的作用和步骤。&lt;/p&gt;

&lt;h2 id=&#34;场景示例&#34;&gt;场景示例&lt;/h2&gt;

&lt;p&gt;在物流领域，常需要从自然语言中提取结构化信息（如收件人、地址、电话）。直接使用大参数模型（如：Qwen3-235B-A22B）效果好，但成本高、响应慢。为兼顾效果与成本，可先用大参数模型标注数据，再用这些数据微调小参数模型（如 Qwen3-0.6B），使其在相同任务上达到相近表现。此过程也被称为模型蒸馏。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;相同结构化信息提取任务，使用原始的Qwen3-0.6B模型准确率14%，微调后准确率可达90%以上。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;例如：收件人地址信息示例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;长沙市岳麓区桃花岭路189号润丰园B座1202室 | 电话021-17613435 | 联系人江雨桐
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;结构化信息提取示例&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;province&amp;quot;: &amp;quot;湖南省&amp;quot;,
  &amp;quot;city&amp;quot;: &amp;quot;长沙市&amp;quot;,
  &amp;quot;district&amp;quot;: &amp;quot;岳麓区&amp;quot;,
  &amp;quot;specific_location&amp;quot;: &amp;quot;桃花岭路189号润丰园B座1202室&amp;quot;,
  &amp;quot;name&amp;quot;: &amp;quot;江雨桐&amp;quot;,
  &amp;quot;phone&amp;quot;: &amp;quot;021-17613435&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;数据准备-1&#34;&gt;数据准备&lt;/h2&gt;

&lt;p&gt;为将教师模型（Qwen3-235B-A22B）在该任务中的知识蒸馏到 Qwen3-0.6B，需先通过教师模型的 API，将收件人的地址信息抽取为结构化 JSON 数据。模型生成这些 JSON 数据可能需要较长时间，因此，本文已为您准备好了示例训练集&lt;a href=&#34;https://help-static-aliyun-doc.aliyuncs.com/file-manage-files/zh-CN/20250612/ojiiaq/train_qwen3.json?spm=a2c4g.11186623.0.0.327f71a1RknQE0&amp;amp;file=train_qwen3.json&#34;&gt;train_qwen3.json&lt;/a&gt;和验证集&lt;a href=&#34;https://help-static-aliyun-doc.aliyuncs.com/file-manage-files/zh-CN/20250619/cxusce/eval_qwen3.json?spm=a2c4g.11186623.0.0.327f71a1RknQE0&amp;amp;file=eval_qwen3.json&#34;&gt;eval_qwen3.json&lt;/a&gt;，您可以直接下载使用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;在模型蒸馏中大参数量模型也被称为教师模型。本文所用数据均为大模型模拟生成，不涉及用户敏感信息。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;微调模型&#34;&gt;微调模型&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;在左侧导航栏单击&lt;strong&gt;Model Gallery&lt;/strong&gt;，搜索并找到&lt;strong&gt;Qwen3-0.6B&lt;/strong&gt;选项卡，然后单击&lt;strong&gt;训练&lt;/strong&gt;。
&lt;img src=&#34;post/2025/images/2025-11-15-llm-0-to-1/IMG_20251115-211530190.png&#34; alt=&#34;图 1&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置训练任务参数。只需配置如下关键参数，其他参数默认即可。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;训练方式&lt;/strong&gt;：默认选择SFT监督微调，LoRA微调方法。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;LoRA是一种高效的模型微调技术，仅修改模型的部分参数，以节省训练使用的资源。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;训练数据集&lt;/strong&gt;：先单击下载示例训练集train_qwen3.json，然后配置页面选择OSS文件或目录，单击image图标选择Bucket，单击上传文件将下载训练集上传到OSS中，并选择该文件。
   &lt;img src=&#34;post/2025/images/2025-11-15-llm-0-to-1/IMG_20251115-214930421.png&#34; alt=&#34;图 2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;验证数据集&lt;/strong&gt;：先单击下载验证集&lt;a href=&#34;https://help-static-aliyun-doc.aliyuncs.com/file-manage-files/zh-CN/20250619/cxusce/eval_qwen3.json?spm=a2c4g.11186623.0.0.7f6b14a8bKIWvn&amp;amp;file=eval_qwen3.json&#34;&gt;eval_qwen3.json&lt;/a&gt;，然后单击&lt;strong&gt;添加验证数据集&lt;/strong&gt;，按照和配置训练集相同的操作，上传并选择该文件。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;验证集用于在训练过程中判断模型的性能，帮助评估模型在未见过的数据上的表现。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;模型输出路径&lt;/strong&gt;：默认会将微调后的模型存储到OSS中，如果OSS目录为空，请您新建目录并指定该目录。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;资源组类型&lt;/strong&gt;：选择使用公共资源组，本次微调大约需要5GB显存，控制台中已经为您筛选出满足要求的规格，选择如：ecs.gn7i-c16g1.4xlarge。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; 后续部署其他模型时，您可参考估算大模型所需显存，计算模型训练需要的显存大小。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;超参数配置&lt;/strong&gt;：
    - learning_rate: 设置为0.0005
    - num_train_epochs：设置为4
    - per_device_train_batch_size：设置为8
    - seq_length：设置为512&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;此超参数配置下模型在本文的测试数据上表现较好。当您使用模型微调解决您自己的业务问题时，如果准确率不高，也可以尝试调整超参数。您可以通过学习&lt;a href=&#34;https://edu.aliyun.com/course/3130200/lesson/343270950?spm=a2c4g.11186623.0.0.7f6b14a8bKIWvn&#34;&gt;阿里云大模型 ACP 课程&lt;/a&gt;，深入了解超参数的作用，以及如何通过损失曲线来判断超参数的调整方向。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;然后单击&lt;strong&gt;训练&lt;/strong&gt; &amp;gt; &lt;strong&gt;确定&lt;/strong&gt;，训练任务进入&lt;strong&gt;创建中&lt;/strong&gt;状态，当处于&lt;strong&gt;运行中&lt;/strong&gt;时，开始微调模型。
&lt;img src=&#34;post/2025/images/2025-11-15-llm-0-to-1/IMG_20251116-213024083.png&#34; alt=&#34;图 3&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;post/2025/images/2025-11-15-llm-0-to-1/IMG_20251116-213100548.png&#34; alt=&#34;图 4&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;post/2025/images/2025-11-15-llm-0-to-1/IMG_20251116-213128668.png&#34; alt=&#34;图 5&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;post/2025/images/2025-11-15-llm-0-to-1/IMG_20251116-213157667.png&#34; alt=&#34;图 6&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;post/2025/images/2025-11-15-llm-0-to-1/IMG_20251116-213239359.png&#34; alt=&#34;图 7&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;post/2025/images/2025-11-15-llm-0-to-1/IMG_20251116-213301657.png&#34; alt=&#34;图 8&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;post/2025/images/2025-11-15-llm-0-to-1/IMG_20251116-213343666.png&#34; alt=&#34;图 9&#34; /&gt;&lt;br /&gt;
模型开始训练：
&lt;img src=&#34;post/2025/images/2025-11-15-llm-0-to-1/IMG_20251116-214244338.png&#34; alt=&#34;图 11&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;post/2025/images/2025-11-15-llm-0-to-1/IMG_20251116-214334001.png&#34; alt=&#34;图 12&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;post/2025/images/2025-11-15-llm-0-to-1/IMG_20251116-214439911.png&#34; alt=&#34;图 13&#34; /&gt;&lt;/p&gt;

&lt;p&gt;训练日志如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;TrainingJob launch starting
PAI_HPS={&amp;quot;apply_chat_template&amp;quot;:&amp;quot;true&amp;quot;,&amp;quot;dpo_beta&amp;quot;:&amp;quot;0.1&amp;quot;,&amp;quot;gradient_accumulation_steps&amp;quot;:&amp;quot;8&amp;quot;,&amp;quot;learning_rate&amp;quot;:&amp;quot;5e-5&amp;quot;,&amp;quot;load_in_4bit&amp;quot;:&amp;quot;false&amp;quot;,&amp;quot;load_in_8bit&amp;quot;:&amp;quot;false&amp;quot;,&amp;quot;lora_alpha&amp;quot;:&amp;quot;32&amp;quot;,&amp;quot;lora_dim&amp;quot;:&amp;quot;32&amp;quot;,&amp;quot;num_train_epochs&amp;quot;:&amp;quot;4&amp;quot;,&amp;quot;per_device_train_batch_size&amp;quot;:&amp;quot;8&amp;quot;,&amp;quot;seq_length&amp;quot;:&amp;quot;512&amp;quot;,&amp;quot;split_ratio&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;system_prompt&amp;quot;:&amp;quot;You are a helpful assistant&amp;quot;,&amp;quot;training_strategy&amp;quot;:&amp;quot;sft&amp;quot;}
PET_NNODES=1
NV_LIBCUBLAS_VERSION=12.4.2.65-1
NVIDIA_VISIBLE_DEVICES=0
KUBERNETES_SERVICE_PORT_HTTPS=6443
NCCL_IB_TC=136
REGION=cn-shanghai
NV_NVML_DEV_VERSION=12.4.99-1
KUBERNETES_SERVICE_PORT=6443
PYTHONUNBUFFERED=1
NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.20.5-1+cuda12.4
NV_LIBNCCL_DEV_PACKAGE_VERSION=2.20.5-1
CLUSTER_NAME=asi_cn-shanghai_pai_g01
KUBERNETES_CONTAINER_RESOURCE_GPU=1
HOSTNAME=dlc1lm8dmxok7cfj-master-0
PYTHON_VERSION=3.11.11
LANGUAGE=zh_CN.UTF-8
PET_NODE_RANK=0
ACCL_IB_GID_INDEX_FIX=1
MASTER_PORT=23456
NVIDIA_REQUIRE_CUDA=cuda&amp;gt;=12.4 brand=tesla,driver&amp;gt;=470,driver&amp;lt;471 brand=unknown,driver&amp;gt;=470,driver&amp;lt;471 brand=nvidia,driver&amp;gt;=470,driver&amp;lt;471 brand=nvidiartx,driver&amp;gt;=470,driver&amp;lt;471 brand=geforce,driver&amp;gt;=470,driver&amp;lt;471 brand=geforcertx,driver&amp;gt;=470,driver&amp;lt;471 brand=quadro,driver&amp;gt;=470,driver&amp;lt;471 brand=quadrortx,driver&amp;gt;=470,driver&amp;lt;471 brand=titan,driver&amp;gt;=470,driver&amp;lt;471 brand=titanrtx,driver&amp;gt;=470,driver&amp;lt;471 brand=tesla,driver&amp;gt;=525,driver&amp;lt;526 brand=unknown,driver&amp;gt;=525,driver&amp;lt;526 brand=nvidia,driver&amp;gt;=525,driver&amp;lt;526 brand=nvidiartx,driver&amp;gt;=525,driver&amp;lt;526 brand=geforce,driver&amp;gt;=525,driver&amp;lt;526 brand=geforcertx,driver&amp;gt;=525,driver&amp;lt;526 brand=quadro,driver&amp;gt;=525,driver&amp;lt;526 brand=quadrortx,driver&amp;gt;=525,driver&amp;lt;526 brand=titan,driver&amp;gt;=525,driver&amp;lt;526 brand=titanrtx,driver&amp;gt;=525,driver&amp;lt;526 brand=tesla,driver&amp;gt;=535,driver&amp;lt;536 brand=unknown,driver&amp;gt;=535,driver&amp;lt;536 brand=nvidia,driver&amp;gt;=535,driver&amp;lt;536 brand=nvidiartx,driver&amp;gt;=535,driver&amp;lt;536 brand=geforce,driver&amp;gt;=535,driver&amp;lt;536 brand=geforcertx,driver&amp;gt;=535,driver&amp;lt;536 brand=quadro,driver&amp;gt;=535,driver&amp;lt;536 brand=quadrortx,driver&amp;gt;=535,driver&amp;lt;536 brand=titan,driver&amp;gt;=535,driver&amp;lt;536 brand=titanrtx,driver&amp;gt;=535,driver&amp;lt;536
SCRAPE_PROMETHEUS_METRICS=yes
ACCL_IB_SPLIT_DATA_NUM=4
NV_LIBCUBLAS_DEV_PACKAGE=libcublas-dev-12-4=12.4.2.65-1
NV_NVTX_VERSION=12.4.99-1
PAI_HPS_NUM_TRAIN_EPOCHS=4
NCCL_MIN_NCHANNELS=16
NCCL_NET_PLUGIN=none
POD_NAME=dlc1lm8dmxok7cfj-master-0
NV_CUDA_CUDART_DEV_VERSION=12.4.99-1
NV_LIBCUSPARSE_VERSION=12.3.0.142-1
PAI_OUTPUT_MODEL=/ml/output/model/
NV_LIBNPP_VERSION=12.2.5.2-1
POD_NAMESPACE=t1218357510451250
NCCL_VERSION=2.20.5-1
ACCL_IB_QPS_LOAD_BALANCE=1
NCCL_SOCKET_IFNAME=eth0
REGION_ID=cn-shanghai
PWD=/root
PAI_HPS_APPLY_CHAT_TEMPLATE=true
NVIDIA_DRIVER_CAPABILITIES=compute,utility
NV_NVPROF_DEV_PACKAGE=cuda-nvprof-12-4=12.4.99-1
PAI_USER_ID=204828862223716692
NV_LIBNPP_PACKAGE=libnpp-12-4=12.2.5.2-1
NCCL_DEBUG=INFO
PYTHON_SETUPTOOLS_VERSION=65.5.1
NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
ECI_CONTAINER_TYPE=normal
TZ=Asia/Shanghai
PET_MASTER_PORT=23456
NV_LIBCUBLAS_DEV_VERSION=12.4.2.65-1
VLLM_USE_MODELSCOPE=True
NCCL_IB_HCA=erdma
WORLD_SIZE=1
PAI_HPS_SYSTEM_PROMPT=You are a helpful assistant
NVIDIA_PRODUCT_NAME=CUDA
ACCL_C4_STATS_MODE=CONN
NV_LIBCUBLAS_DEV_PACKAGE_NAME=libcublas-dev-12-4
ACCL_LOG_TIME=1
NV_CUDA_CUDART_VERSION=12.4.99-1
HOME=/root
LANG=C.UTF-8
PAI_HPS_DPO_BETA=0.1
KUBERNETES_PORT_443_TCP=tcp://10.192.0.1:443
NCCL_IB_GID_INDEX=1
PAI_TRAINING_JOB_ID=train1lc8s95geb9
PET_MASTER_ADDR=dlc1lm8dmxok7cfj-master-0
CUDA_VERSION=12.4.0
NV_LIBCUBLAS_PACKAGE=libcublas-12-4=12.4.2.65-1
PIP_TRUSTED_HOST=mirrors.cloud.aliyuncs.com
PAI_INPUT_TRAIN=/ml/input/data/train/train_qwen3.json
NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=cuda-nsight-compute-12-4=12.4.0-1
GPG_KEY=A035C8C19219BA821ECEA86B64E628F8D684696D
PAI_HPS_TRAINING_STRATEGY=sft
NV_LIBNPP_DEV_PACKAGE=libnpp-dev-12-4=12.2.5.2-1
PAI_HPS_LEARNING_RATE=5e-5
NV_LIBCUBLAS_PACKAGE_NAME=libcublas-12-4
PAI_INPUT_MODEL=/ml/input/data/model
NV_LIBNPP_DEV_VERSION=12.2.5.2-1
PAI_CONFIG_DIR=/ml/input/config/
MASTER_ADDR=dlc1lm8dmxok7cfj-master-0
PAI_INPUT_VALIDATION=/ml/input/data/validation/train_qwen3.json
NCCL_IB_QPS_PER_CONNECTION=8
PAI_WORKING_DIR=/ml/code/
PYTHONPATH=/ml/code
SETUPTOOLS_USE_DISTUTILS=stdlib
NV_LIBCUSPARSE_DEV_VERSION=12.3.0.142-1
MODELSCOPE_CACHE=/mnt/workspace/.cache/modelscope/hub
LIBRARY_PATH=/usr/local/cuda/lib64/stubs
PYTHONIOENCODING=utf-8
PAI_ODPS_CREDENTIAL=/ml/input/credential/odps.json
PIP_INDEX_URL=https://mirrors.cloud.aliyuncs.com/pypi/simple
NCCL_IB_TIMEOUT=22
SHLVL=1
PAI_HPS_SPLIT_RATIO=0
NV_CUDA_LIB_VERSION=12.4.0-1
NVARCH=x86_64
KUBERNETES_PORT_443_TCP_PROTO=tcp
PAI_HPS_GRADIENT_ACCUMULATION_STEPS=8
PAI_HPS_LORA_DIM=32
PAI_HPS_PER_DEVICE_TRAIN_BATCH_SIZE=8
DLC_JOB_ID=dlc1lm8dmxok7cfj
PYTHON_PIP_VERSION=23.0.1
NCCL_IB_SL=5
KUBERNETES_PORT_443_TCP_ADDR=10.192.0.1
TENANT_API_SERVER_URL=https://10.224.176.2:6443
NV_CUDA_COMPAT_PACKAGE=cuda-compat-12-4
ECI_CONTAINER_ENABLE_ERDMA=true
NV_LIBNCCL_PACKAGE=libnccl2=2.20.5-1+cuda12.4
LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64
RANK=0
PAI_HPS_LORA_ALPHA=32
LMDEPLOY_USE_MODELSCOPE=True
PYTHON_GET_PIP_SHA256=dfe9fd5c28dc98b5ac17979a953ea550cec37ae1b47a5116007395bfacff2ab9
NV_CUDA_NSIGHT_COMPUTE_VERSION=12.4.0-1
PAI_USER_ARGS=--seq_length 512 --lora_dim 32 --lora_alpha 32 --load_in_8bit false --learning_rate 5e-5 --per_device_train_batch_size 8 --apply_chat_template true --split_ratio 0 --load_in_4bit false --gradient_accumulation_steps 8 --training_strategy sft --dpo_beta 0.1 --num_train_epochs 4 --system_prompt &#39;You are a helpful assistant&#39;
PAI_WORKSPACE_ID=1026689
ALIBABA_CLOUD_CREDENTIALS_URI=http://localhost:7002/api/v1/credentials/0
KUBERNETES_SERVICE_HOST=10.224.176.2
NV_NVPROF_VERSION=12.4.99-1
LC_ALL=zh_CN.UTF-8
PET_NPROC_PER_NODE=1
KUBERNETES_PORT=tcp://10.192.0.1:443
KUBERNETES_PORT_443_TCP_PORT=443
arch=x86_64
PYTHON_GET_PIP_URL=https://github.com/pypa/get-pip/raw/dbf0c85f76fb6e1ab42aa672ffca6f0a675d9ee4/public/get-pip.py
NPROC_PER_NODE=1
PATH=/usr/local/bin:/usr/local/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
NV_LIBNCCL_PACKAGE_NAME=libnccl2
NV_LIBNCCL_PACKAGE_VERSION=2.20.5-1
PAI_HPS_SEQ_LENGTH=512
PAI_HPS_LOAD_IN_8BIT=false
PAI_HPS_LOAD_IN_4BIT=false
_=/usr/bin/env
Change to Working Directory, /ml/code/
User program launching
-----------------------------------------------------------------
2025/11/16 21:38:24 INFO: training_utils version: 1.0.4
2025/11/16 21:38:24 INFO: Env PAI_INPUT_MODEL=/ml/input/data/model
2025/11/16 21:38:24 INFO: Env LIBRARY_PATH=/usr/local/cuda/lib64/stubs
2025/11/16 21:38:24 INFO: Env PAI_HPS_PER_DEVICE_TRAIN_BATCH_SIZE=8
2025/11/16 21:38:24 INFO: Env NV_LIBCUBLAS_VERSION=12.4.2.65-1
2025/11/16 21:38:24 INFO: Env KUBERNETES_SERVICE_PORT=6443
2025/11/16 21:38:24 INFO: Env ACCL_IB_GID_INDEX_FIX=1
2025/11/16 21:38:24 INFO: Env NV_NVPROF_DEV_PACKAGE=cuda-nvprof-12-4=12.4.99-1
2025/11/16 21:38:24 INFO: Env NCCL_DEBUG=INFO
2025/11/16 21:38:24 INFO: Env PYTHONIOENCODING=utf-8
2025/11/16 21:38:24 INFO: Env NV_CUDA_COMPAT_PACKAGE=cuda-compat-12-4
2025/11/16 21:38:24 INFO: Env LMDEPLOY_USE_MODELSCOPE=True
2025/11/16 21:38:24 INFO: Env KUBERNETES_PORT=tcp://10.192.0.1:443
2025/11/16 21:38:24 INFO: Env LANGUAGE=zh_CN.UTF-8
2025/11/16 21:38:24 INFO: Env NCCL_NET_PLUGIN=none
2025/11/16 21:38:24 INFO: Env NCCL_SOCKET_IFNAME=eth0
2025/11/16 21:38:24 INFO: Env PAI_HPS_APPLY_CHAT_TEMPLATE=true
2025/11/16 21:38:24 INFO: Env NCCL_IB_HCA=erdma
2025/11/16 21:38:24 INFO: Env PIP_TRUSTED_HOST=mirrors.cloud.aliyuncs.com
2025/11/16 21:38:24 INFO: Env NV_CUDA_NSIGHT_COMPUTE_VERSION=12.4.0-1
2025/11/16 21:38:24 INFO: Env SCRAPE_PROMETHEUS_METRICS=yes
2025/11/16 21:38:24 INFO: Env PAI_USER_ID=204828862223716692
2025/11/16 21:38:24 INFO: Env MASTER_ADDR=dlc1lm8dmxok7cfj-master-0
2025/11/16 21:38:24 INFO: Env PAI_HPS_LORA_DIM=32
2025/11/16 21:38:24 INFO: Env HOSTNAME=dlc1lm8dmxok7cfj-master-0
2025/11/16 21:38:24 INFO: Env ECI_CONTAINER_TYPE=normal
2025/11/16 21:38:24 INFO: Env ACCL_IB_QPS_LOAD_BALANCE=1
2025/11/16 21:38:24 INFO: Env SHLVL=0
2025/11/16 21:38:24 INFO: Env PYTHON_PIP_VERSION=23.0.1
2025/11/16 21:38:24 INFO: Env LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2025/11/16 21:38:24 INFO: Env NV_LIBNCCL_PACKAGE_VERSION=2.20.5-1
2025/11/16 21:38:24 INFO: Env MASTER_PORT=23456
2025/11/16 21:38:24 INFO: Env ACCL_LOG_TIME=1
2025/11/16 21:38:24 INFO: Env HOME=/root
2025/11/16 21:38:24 INFO: Env NCCL_IB_QPS_PER_CONNECTION=8
2025/11/16 21:38:24 INFO: Env OLDPWD=/root
2025/11/16 21:38:24 INFO: Env NCCL_MIN_NCHANNELS=16
2025/11/16 21:38:24 INFO: Env NV_LIBCUBLAS_DEV_VERSION=12.4.2.65-1
2025/11/16 21:38:24 INFO: Env PAI_USER_ARGS=--seq_length 512 --lora_dim 32 --lora_alpha 32 --load_in_8bit false --learning_rate 5e-5 --per_device_train_batch_size 8 --apply_chat_template true --split_ratio 0 --load_in_4bit false --gradient_accumulation_steps 8 --training_strategy sft --dpo_beta 0.1 --num_train_epochs 4 --system_prompt &#39;You are a helpful assistant&#39;
2025/11/16 21:38:24 INFO: Env PYTHONUNBUFFERED=1
2025/11/16 21:38:24 INFO: Env REGION=cn-shanghai
2025/11/16 21:38:24 INFO: Env GPG_KEY=A035C8C19219BA821ECEA86B64E628F8D684696D
2025/11/16 21:38:24 INFO: Env PAI_CONFIG_DIR=/ml/input/config/
2025/11/16 21:38:24 INFO: Env NPROC_PER_NODE=1
2025/11/16 21:38:24 INFO: Env WORLD_SIZE=1
2025/11/16 21:38:24 INFO: Env PAI_HPS_SEQ_LENGTH=512
2025/11/16 21:38:24 INFO: Env NV_LIBNCCL_DEV_PACKAGE_VERSION=2.20.5-1
2025/11/16 21:38:24 INFO: Env REGION_ID=cn-shanghai
2025/11/16 21:38:24 INFO: Env NV_LIBNPP_PACKAGE=libnpp-12-4=12.2.5.2-1
2025/11/16 21:38:24 INFO: Env PAI_HPS_TRAINING_STRATEGY=sft
2025/11/16 21:38:24 INFO: Env CLUSTER_NAME=asi_cn-shanghai_pai_g01
2025/11/16 21:38:24 INFO: Env NCCL_IB_GID_INDEX=1
2025/11/16 21:38:24 INFO: Env CUDA_VERSION=12.4.0
2025/11/16 21:38:24 INFO: Env PAI_INPUT_TRAIN=/ml/input/data/train/train_qwen3.json
2025/11/16 21:38:24 INFO: Env RANK=0
2025/11/16 21:38:24 INFO: Env NV_NVPROF_VERSION=12.4.99-1
2025/11/16 21:38:24 INFO: Env NV_LIBCUBLAS_PACKAGE_NAME=libcublas-12-4
2025/11/16 21:38:24 INFO: Env ECI_CONTAINER_ENABLE_ERDMA=true
2025/11/16 21:38:24 INFO: Env NVIDIA_REQUIRE_CUDA=cuda&amp;gt;=12.4 brand=tesla,driver&amp;gt;=470,driver&amp;lt;471 brand=unknown,driver&amp;gt;=470,driver&amp;lt;471 brand=nvidia,driver&amp;gt;=470,driver&amp;lt;471 brand=nvidiartx,driver&amp;gt;=470,driver&amp;lt;471 brand=geforce,driver&amp;gt;=470,driver&amp;lt;471 brand=geforcertx,driver&amp;gt;=470,driver&amp;lt;471 brand=quadro,driver&amp;gt;=470,driver&amp;lt;471 brand=quadrortx,driver&amp;gt;=470,driver&amp;lt;471 brand=titan,driver&amp;gt;=470,driver&amp;lt;471 brand=titanrtx,driver&amp;gt;=470,driver&amp;lt;471 brand=tesla,driver&amp;gt;=525,driver&amp;lt;526 brand=unknown,driver&amp;gt;=525,driver&amp;lt;526 brand=nvidia,driver&amp;gt;=525,driver&amp;lt;526 brand=nvidiartx,driver&amp;gt;=525,driver&amp;lt;526 brand=geforce,driver&amp;gt;=525,driver&amp;lt;526 brand=geforcertx,driver&amp;gt;=525,driver&amp;lt;526 brand=quadro,driver&amp;gt;=525,driver&amp;lt;526 brand=quadrortx,driver&amp;gt;=525,driver&amp;lt;526 brand=titan,driver&amp;gt;=525,driver&amp;lt;526 brand=titanrtx,driver&amp;gt;=525,driver&amp;lt;526 brand=tesla,driver&amp;gt;=535,driver&amp;lt;536 brand=unknown,driver&amp;gt;=535,driver&amp;lt;536 brand=nvidia,driver&amp;gt;=535,driver&amp;lt;536 brand=nvidiartx,driver&amp;gt;=535,driver&amp;lt;536 brand=geforce,driver&amp;gt;=535,driver&amp;lt;536 brand=geforcertx,driver&amp;gt;=535,driver&amp;lt;536 brand=quadro,driver&amp;gt;=535,driver&amp;lt;536 brand=quadrortx,driver&amp;gt;=535,driver&amp;lt;536 brand=titan,driver&amp;gt;=535,driver&amp;lt;536 brand=titanrtx,driver&amp;gt;=535,driver&amp;lt;536
2025/11/16 21:38:24 INFO: Env DLC_JOB_ID=dlc1lm8dmxok7cfj
2025/11/16 21:38:24 INFO: Env TENANT_API_SERVER_URL=https://10.224.176.2:6443
2025/11/16 21:38:24 INFO: Env arch=x86_64
2025/11/16 21:38:24 INFO: Env _=/usr/bin/sh
2025/11/16 21:38:24 INFO: Env NV_LIBCUSPARSE_VERSION=12.3.0.142-1
2025/11/16 21:38:24 INFO: Env NVIDIA_DRIVER_CAPABILITIES=compute,utility
2025/11/16 21:38:24 INFO: Env PAI_TRAINING_JOB_ID=train1lc8s95geb9
2025/11/16 21:38:24 INFO: Env NV_CUDA_LIB_VERSION=12.4.0-1
2025/11/16 21:38:24 INFO: Env PYTHON_GET_PIP_URL=https://github.com/pypa/get-pip/raw/dbf0c85f76fb6e1ab42aa672ffca6f0a675d9ee4/public/get-pip.py
2025/11/16 21:38:24 INFO: Env NV_LIBNCCL_PACKAGE_NAME=libnccl2
2025/11/16 21:38:24 INFO: Env NV_NVML_DEV_VERSION=12.4.99-1
2025/11/16 21:38:24 INFO: Env VLLM_USE_MODELSCOPE=True
2025/11/16 21:38:24 INFO: Env NV_LIBNPP_DEV_PACKAGE=libnpp-dev-12-4=12.2.5.2-1
2025/11/16 21:38:24 INFO: Env PET_NPROC_PER_NODE=1
2025/11/16 21:38:24 INFO: Env POD_NAME=dlc1lm8dmxok7cfj-master-0
2025/11/16 21:38:24 INFO: Env PAI_OUTPUT_MODEL=/ml/output/model/
2025/11/16 21:38:24 INFO: Env NV_CUDA_CUDART_VERSION=12.4.99-1
2025/11/16 21:38:24 INFO: Env KUBERNETES_PORT_443_TCP_ADDR=10.192.0.1
2025/11/16 21:38:24 INFO: Env PAI_HPS_LOAD_IN_4BIT=false
2025/11/16 21:38:24 INFO: Env PAI_WORKSPACE_ID=1026689
2025/11/16 21:38:24 INFO: Env PATH=/usr/local/bin:/usr/local/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
2025/11/16 21:38:24 INFO: Env PAI_HPS_SYSTEM_PROMPT=You are a helpful assistant
2025/11/16 21:38:24 INFO: Env NV_LIBCUBLAS_DEV_PACKAGE_NAME=libcublas-dev-12-4
2025/11/16 21:38:24 INFO: Env NV_LIBCUBLAS_PACKAGE=libcublas-12-4=12.4.2.65-1
2025/11/16 21:38:24 INFO: Env PIP_INDEX_URL=https://mirrors.cloud.aliyuncs.com/pypi/simple
2025/11/16 21:38:24 INFO: Env NVARCH=x86_64
2025/11/16 21:38:24 INFO: Env ACCL_IB_SPLIT_DATA_NUM=4
2025/11/16 21:38:24 INFO: Env KUBERNETES_PORT_443_TCP_PORT=443
2025/11/16 21:38:24 INFO: Env NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
2025/11/16 21:38:24 INFO: Env NV_LIBCUSPARSE_DEV_VERSION=12.3.0.142-1
2025/11/16 21:38:24 INFO: Env KUBERNETES_PORT_443_TCP_PROTO=tcp
2025/11/16 21:38:24 INFO: Env NV_LIBNCCL_PACKAGE=libnccl2=2.20.5-1+cuda12.4
2025/11/16 21:38:24 INFO: Env PAI_HPS_LOAD_IN_8BIT=false
2025/11/16 21:38:24 INFO: Env NVIDIA_PRODUCT_NAME=CUDA
2025/11/16 21:38:24 INFO: Env LANG=C.UTF-8
2025/11/16 21:38:24 INFO: Env PAI_HPS_LEARNING_RATE=5e-5
2025/11/16 21:38:24 INFO: Env NV_CUDA_CUDART_DEV_VERSION=12.4.99-1
2025/11/16 21:38:24 INFO: Env PAI_HPS_GRADIENT_ACCUMULATION_STEPS=8
2025/11/16 21:38:24 INFO: Env PAI_HPS_NUM_TRAIN_EPOCHS=4
2025/11/16 21:38:24 INFO: Env PAI_HPS_LORA_ALPHA=32
2025/11/16 21:38:24 INFO: Env NCCL_IB_TC=136
2025/11/16 21:38:24 INFO: Env NV_LIBCUBLAS_DEV_PACKAGE=libcublas-dev-12-4=12.4.2.65-1
2025/11/16 21:38:24 INFO: Env PAI_INPUT_VALIDATION=/ml/input/data/validation/train_qwen3.json
2025/11/16 21:38:24 INFO: Env KUBERNETES_CONTAINER_RESOURCE_GPU=1
2025/11/16 21:38:24 INFO: Env PYTHON_VERSION=3.11.11
2025/11/16 21:38:24 INFO: Env PYTHON_SETUPTOOLS_VERSION=65.5.1
2025/11/16 21:38:24 INFO: Env PET_MASTER_ADDR=dlc1lm8dmxok7cfj-master-0
2025/11/16 21:38:24 INFO: Env NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=cuda-nsight-compute-12-4=12.4.0-1
2025/11/16 21:38:24 INFO: Env PAI_HPS_SPLIT_RATIO=0
2025/11/16 21:38:24 INFO: Env NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.20.5-1+cuda12.4
2025/11/16 21:38:24 INFO: Env SETUPTOOLS_USE_DISTUTILS=stdlib
2025/11/16 21:38:24 INFO: Env KUBERNETES_SERVICE_PORT_HTTPS=6443
2025/11/16 21:38:24 INFO: Env NV_NVTX_VERSION=12.4.99-1
2025/11/16 21:38:24 INFO: Env NV_LIBNPP_VERSION=12.2.5.2-1
2025/11/16 21:38:24 INFO: Env PET_MASTER_PORT=23456
2025/11/16 21:38:24 INFO: Env KUBERNETES_PORT_443_TCP=tcp://10.192.0.1:443
2025/11/16 21:38:24 INFO: Env POD_NAMESPACE=t1218357510451250
2025/11/16 21:38:24 INFO: Env ACCL_C4_STATS_MODE=CONN
2025/11/16 21:38:24 INFO: Env ALIBABA_CLOUD_CREDENTIALS_URI=http://localhost:7002/api/v1/credentials/0
2025/11/16 21:38:24 INFO: Env PWD=/ml/code
2025/11/16 21:38:24 INFO: Env PAI_HPS_DPO_BETA=0.1
2025/11/16 21:38:24 INFO: Env KUBERNETES_SERVICE_HOST=10.224.176.2
2025/11/16 21:38:24 INFO: Env LC_ALL=zh_CN.UTF-8
2025/11/16 21:38:24 INFO: Env PAI_HPS={&amp;quot;apply_chat_template&amp;quot;:&amp;quot;true&amp;quot;,&amp;quot;dpo_beta&amp;quot;:&amp;quot;0.1&amp;quot;,&amp;quot;gradient_accumulation_steps&amp;quot;:&amp;quot;8&amp;quot;,&amp;quot;learning_rate&amp;quot;:&amp;quot;5e-5&amp;quot;,&amp;quot;load_in_4bit&amp;quot;:&amp;quot;false&amp;quot;,&amp;quot;load_in_8bit&amp;quot;:&amp;quot;false&amp;quot;,&amp;quot;lora_alpha&amp;quot;:&amp;quot;32&amp;quot;,&amp;quot;lora_dim&amp;quot;:&amp;quot;32&amp;quot;,&amp;quot;num_train_epochs&amp;quot;:&amp;quot;4&amp;quot;,&amp;quot;per_device_train_batch_size&amp;quot;:&amp;quot;8&amp;quot;,&amp;quot;seq_length&amp;quot;:&amp;quot;512&amp;quot;,&amp;quot;split_ratio&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;system_prompt&amp;quot;:&amp;quot;You are a helpful assistant&amp;quot;,&amp;quot;training_strategy&amp;quot;:&amp;quot;sft&amp;quot;}
2025/11/16 21:38:24 INFO: Env PYTHON_GET_PIP_SHA256=dfe9fd5c28dc98b5ac17979a953ea550cec37ae1b47a5116007395bfacff2ab9
2025/11/16 21:38:24 INFO: Env PET_NNODES=1
2025/11/16 21:38:24 INFO: Env PET_NODE_RANK=0
2025/11/16 21:38:24 INFO: Env PYTHONPATH=/ml/code
2025/11/16 21:38:24 INFO: Env MODELSCOPE_CACHE=/mnt/workspace/.cache/modelscope/hub
2025/11/16 21:38:24 INFO: Env NCCL_IB_SL=5
2025/11/16 21:38:24 INFO: Env NVIDIA_VISIBLE_DEVICES=0
2025/11/16 21:38:24 INFO: Env NCCL_VERSION=2.20.5-1
2025/11/16 21:38:24 INFO: Env TZ=Asia/Shanghai
2025/11/16 21:38:24 INFO: Env NV_LIBNPP_DEV_VERSION=12.2.5.2-1
2025/11/16 21:38:24 INFO: Env NCCL_IB_TIMEOUT=22
2025/11/16 21:38:24 INFO: Env PAI_WORKING_DIR=/ml/code/
2025/11/16 21:38:24 INFO: Env PAI_ODPS_CREDENTIAL=/ml/input/credential/odps.json
2025/11/16 21:38:24 INFO: launch: launcherType=PythonLauncher, cmd=[&#39;/usr/local/bin/python&#39;, &#39;-m&#39;, &#39;train&#39;, &#39;--apply_chat_template&#39;, &#39;true&#39;, &#39;--dpo_beta&#39;, &#39;0.1&#39;, &#39;--gradient_accumulation_steps&#39;, &#39;8&#39;, &#39;--learning_rate&#39;, &#39;5e-5&#39;, &#39;--load_in_4bit&#39;, &#39;false&#39;, &#39;--load_in_8bit&#39;, &#39;false&#39;, &#39;--lora_alpha&#39;, &#39;32&#39;, &#39;--lora_dim&#39;, &#39;32&#39;, &#39;--num_train_epochs&#39;, &#39;4&#39;, &#39;--per_device_train_batch_size&#39;, &#39;8&#39;, &#39;--seq_length&#39;, &#39;512&#39;, &#39;--split_ratio&#39;, &#39;0&#39;, &#39;--system_prompt&#39;, &#39;You are a helpful assistant&#39;, &#39;--training_strategy&#39;, &#39;sft&#39;]
2025/11/16 21:38:24 INFO: launch: parameters={&#39;apply_chat_template&#39;: &#39;true&#39;, &#39;dpo_beta&#39;: &#39;0.1&#39;, &#39;gradient_accumulation_steps&#39;: &#39;8&#39;, &#39;learning_rate&#39;: &#39;5e-5&#39;, &#39;load_in_4bit&#39;: &#39;false&#39;, &#39;load_in_8bit&#39;: &#39;false&#39;, &#39;lora_alpha&#39;: &#39;32&#39;, &#39;lora_dim&#39;: &#39;32&#39;, &#39;num_train_epochs&#39;: &#39;4&#39;, &#39;per_device_train_batch_size&#39;: &#39;8&#39;, &#39;seq_length&#39;: &#39;512&#39;, &#39;split_ratio&#39;: &#39;0&#39;, &#39;system_prompt&#39;: &#39;You are a helpful assistant&#39;, &#39;training_strategy&#39;: &#39;sft&#39;}
2025/11/16 21:38:24 INFO: hyper_params: {&#39;apply_chat_template&#39;: &#39;true&#39;, &#39;dpo_beta&#39;: 0.1, &#39;gradient_accumulation_steps&#39;: 8, &#39;learning_rate&#39;: 5e-05, &#39;load_in_4bit&#39;: &#39;false&#39;, &#39;load_in_8bit&#39;: &#39;false&#39;, &#39;lora_alpha&#39;: 32, &#39;lora_dim&#39;: 32, &#39;num_train_epochs&#39;: 4, &#39;per_device_train_batch_size&#39;: 8, &#39;seq_length&#39;: 512, &#39;split_ratio&#39;: 0, &#39;system_prompt&#39;: &#39;You are a helpful assistant&#39;, &#39;training_strategy&#39;: &#39;sft&#39;}
2025/11/16 21:38:24 INFO: Get hostname dlc1lm8dmxok7cfj-master-0 succeed. retry times: 0
2025/11/16 21:38:24 INFO: execute command: mkdir -p /tmp/input_model/
2025/11/16 21:38:24 INFO: execute command succeed
2025/11/16 21:38:24 INFO: execute command: cp -R /ml/input/data/model/* /tmp/input_model/
2025/11/16 21:38:27 INFO: execute command succeed
2025/11/16 21:38:27 INFO: Training command: [&#39;accelerate&#39;, &#39;launch&#39;, &#39;--num_processes&#39;, &#39;1&#39;, &#39;--config_file&#39;, &#39;multi_gpu.yaml&#39;, &#39;sft.py&#39;, &#39;--model_name&#39;, &#39;/tmp/input_model/&#39;, &#39;--model_type&#39;, &#39;qwen3&#39;, &#39;--train_dataset_name&#39;, &#39;/ml/input/data/train/train_qwen3.json&#39;, &#39;--num_train_epochs&#39;, &#39;4&#39;, &#39;--batch_size&#39;, &#39;8&#39;, &#39;--gradient_accumulation_steps&#39;, &#39;8&#39;, &#39;--seq_length&#39;, &#39;512&#39;, &#39;--learning_rate&#39;, &#39;5e-05&#39;, &#39;--system_prompt&#39;, &#39;You are a helpful assistant&#39;, &#39;--eval_dataset_name&#39;, &#39;/ml/input/data/validation/train_qwen3.json&#39;, &#39;--apply_chat_template&#39;, &#39;--use_peft&#39;, &#39;--target_modules&#39;, &#39;k_proj&#39;, &#39;o_proj&#39;, &#39;q_proj&#39;, &#39;v_proj&#39;, &#39;--peft_lora_r&#39;, &#39;32&#39;, &#39;--peft_lora_alpha&#39;, &#39;32&#39;, &#39;--output_dir&#39;, &#39;/tmp/adapter/&#39;]
2025/11/16 21:38:27 INFO: execute command: mkdir -p /tmp/model/
2025/11/16 21:38:27 INFO: execute command succeed
2025/11/16 21:38:27 INFO: execute command: mkdir -p /tmp/adapter/
2025/11/16 21:38:27 INFO: execute command succeed
[2025-11-16 21:38:33,007] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
df: /root/.triton/autotune: 没有那个文件或目录
[2025-11-16 21:38:37,576] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 1600 examples [00:00, 16821.96 examples/s]

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 1600 examples [00:00, 18845.93 examples/s]
[2025-11-16 21:38:42,362] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-11-16 21:38:42,362] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl

Applying formatting function to train dataset:   0%|          | 0/1600 [00:00&amp;lt;?, ? examples/s]
Applying formatting function to train dataset: 100%|██████████| 1600/1600 [00:00&amp;lt;00:00, 39825.56 examples/s]

Converting train dataset to ChatML:   0%|          | 0/1600 [00:00&amp;lt;?, ? examples/s]
Converting train dataset to ChatML: 100%|██████████| 1600/1600 [00:00&amp;lt;00:00, 43505.43 examples/s]

Adding EOS to train dataset:   0%|          | 0/1600 [00:00&amp;lt;?, ? examples/s]
Adding EOS to train dataset: 100%|██████████| 1600/1600 [00:00&amp;lt;00:00, 34724.11 examples/s]

Tokenizing train dataset:   0%|          | 0/1600 [00:00&amp;lt;?, ? examples/s]
Tokenizing train dataset:  22%|██▏       | 351/1600 [00:00&amp;lt;00:00, 3489.79 examples/s]
Tokenizing train dataset:  45%|████▍     | 718/1600 [00:00&amp;lt;00:00, 3587.32 examples/s]
Tokenizing train dataset:  74%|███████▎  | 1176/1600 [00:00&amp;lt;00:00, 3101.77 examples/s]
Tokenizing train dataset:  96%|█████████▌| 1535/1600 [00:00&amp;lt;00:00, 3260.76 examples/s]
Tokenizing train dataset: 100%|██████████| 1600/1600 [00:00&amp;lt;00:00, 3141.33 examples/s]

Truncating train dataset:   0%|          | 0/1600 [00:00&amp;lt;?, ? examples/s]
Truncating train dataset: 100%|██████████| 1600/1600 [00:00&amp;lt;00:00, 336874.98 examples/s]
[rank0]:[W1116 21:38:43.428342223 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
dlc1lm8dmxok7cfj-master-0:133:133 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
dlc1lm8dmxok7cfj-master-0:133:133 [0] NCCL INFO Bootstrap : Using eth0:10.224.160.180&amp;lt;0&amp;gt;
dlc1lm8dmxok7cfj-master-0:133:133 [0] NCCL INFO NET/Plugin: No plugin found (none)
dlc1lm8dmxok7cfj-master-0:133:133 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net-none.so)
dlc1lm8dmxok7cfj-master-0:133:133 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net-none.so: cannot open shared object file: No such file or directory : when loading none
dlc1lm8dmxok7cfj-master-0:133:133 [0] NCCL INFO NET/Plugin: Using internal network plugin.
dlc1lm8dmxok7cfj-master-0:133:133 [0] NCCL INFO cudaDriverVersion 12040
NCCL version 2.21.5+cuda12.4
dlc1lm8dmxok7cfj-master-0:133:133 [0] NCCL INFO Comm config Blocking set to 1
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO NCCL_IB_HCA set to erdma
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO NET/IB : No device found.
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO NET/Socket : Using [0]eth0:10.224.160.180&amp;lt;0&amp;gt;
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Using non-device net plugin version 0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Using network Socket
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO ncclCommInitRank comm 0x5603c0703fe0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 30 commId 0x431046bfc5d31cfe - Init START
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO comm 0x5603c0703fe0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO NCCL_MIN_NCHANNELS set by environment to 16.
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 00/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 01/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 02/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 03/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 04/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 05/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 06/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 07/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 08/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 09/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 10/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 11/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 12/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 13/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 14/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 15/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 16/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 17/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 18/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 19/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 20/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 21/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 22/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 23/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 24/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 25/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 26/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 27/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 28/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 29/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 30/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Channel 31/32 :    0
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Trees [0] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [1] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [2] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [3] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [4] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [5] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [6] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [7] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [8] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [9] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [10] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [11] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [12] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [13] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [14] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [15] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [16] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [17] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [18] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [19] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [20] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [21] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [22] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [23] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [24] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [25] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [26] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [27] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [28] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [29] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [30] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [31] -1/-1/-1-&amp;gt;0-&amp;gt;-1
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO P2P Chunksize set to 131072
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Connected all rings
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO Connected all trees
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO TUNER/Plugin: Most recent plugin load returned 2 : libnccl-net-none.so: cannot open shared object file: No such file or directory. All attempts to load &#39;libnccl-tuner.so none&#39; also failed.
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
dlc1lm8dmxok7cfj-master-0:133:393 [0] NCCL INFO ncclCommInitRank comm 0x5603c0703fe0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 30 commId 0x431046bfc5d31cfe - Init COMPLETE

Applying formatting function to eval dataset:   0%|          | 0/1600 [00:00&amp;lt;?, ? examples/s]
Applying formatting function to eval dataset: 100%|██████████| 1600/1600 [00:00&amp;lt;00:00, 40597.73 examples/s]

Converting eval dataset to ChatML:   0%|          | 0/1600 [00:00&amp;lt;?, ? examples/s]
Converting eval dataset to ChatML: 100%|██████████| 1600/1600 [00:00&amp;lt;00:00, 44469.76 examples/s]

Adding EOS to eval dataset:   0%|          | 0/1600 [00:00&amp;lt;?, ? examples/s]
Adding EOS to eval dataset: 100%|██████████| 1600/1600 [00:00&amp;lt;00:00, 37549.30 examples/s]

Tokenizing eval dataset:   0%|          | 0/1600 [00:00&amp;lt;?, ? examples/s]
Tokenizing eval dataset:  25%|██▍       | 397/1600 [00:00&amp;lt;00:00, 3954.38 examples/s]
Tokenizing eval dataset:  61%|██████    | 975/1600 [00:00&amp;lt;00:00, 3881.21 examples/s]
Tokenizing eval dataset:  89%|████████▉ | 1422/1600 [00:00&amp;lt;00:00, 3432.48 examples/s]
Tokenizing eval dataset: 100%|██████████| 1600/1600 [00:00&amp;lt;00:00, 3409.79 examples/s]

Truncating eval dataset:   0%|          | 0/1600 [00:00&amp;lt;?, ? examples/s]
Truncating eval dataset: 100%|██████████| 1600/1600 [00:00&amp;lt;00:00, 380889.18 examples/s]
Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...
Creating extension directory /root/.cache/torch_extensions/py311_cu124/cpu_adam...
Emitting ninja build file /root/.cache/torch_extensions/py311_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\&amp;quot;_gcc\&amp;quot; -DPYBIND11_STDLIB=\&amp;quot;_libstdcpp\&amp;quot; -DPYBIND11_BUILD_ABI=\&amp;quot;_cxxabi1011\&amp;quot; -I/usr/local/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /usr/local/lib/python3.11/site-packages/torch/include -isystem /usr/local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/site-packages/torch/include/TH -isystem /usr/local/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /usr/local/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o 
[2/3] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\&amp;quot;_gcc\&amp;quot; -DPYBIND11_STDLIB=\&amp;quot;_libstdcpp\&amp;quot; -DPYBIND11_BUILD_ABI=\&amp;quot;_cxxabi1011\&amp;quot; -I/usr/local/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /usr/local/lib/python3.11/site-packages/torch/include -isystem /usr/local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/site-packages/torch/include/TH -isystem /usr/local/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /usr/local/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o 
[3/3] c++ cpu_adam.o cpu_adam_impl.o -shared -lcurand -L/usr/local/cuda/lib64 -L/usr/local/lib/python3.11/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o cpu_adam.so
Loading extension module cpu_adam...
Time to load cpu_adam op: 25.448318243026733 seconds
dlc1lm8dmxok7cfj-master-0:133:133 [0] NCCL INFO Comm config Blocking set to 1
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Using non-device net plugin version 0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Using network Socket
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO ncclCommInitRank comm 0x5603c0bfdac0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 30 commId 0x9cbcac870fd5ec63 - Init START
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO comm 0x5603c0bfdac0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 00/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 01/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 02/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 03/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 04/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 05/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 06/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 07/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 08/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 09/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 10/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 11/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 12/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 13/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 14/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 15/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 16/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 17/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 18/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 19/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 20/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 21/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 22/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 23/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 24/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 25/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 26/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 27/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 28/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 29/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 30/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Channel 31/32 :    0
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Trees [0] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [1] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [2] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [3] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [4] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [5] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [6] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [7] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [8] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [9] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [10] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [11] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [12] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [13] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [14] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [15] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [16] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [17] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [18] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [19] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [20] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [21] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [22] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [23] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [24] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [25] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [26] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [27] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [28] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [29] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [30] -1/-1/-1-&amp;gt;0-&amp;gt;-1 [31] -1/-1/-1-&amp;gt;0-&amp;gt;-1
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO P2P Chunksize set to 131072
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Connected all rings
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO Connected all trees
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
dlc1lm8dmxok7cfj-master-0:133:444 [0] NCCL INFO ncclCommInitRank comm 0x5603c0bfdac0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 30 commId 0x9cbcac870fd5ec63 - Init COMPLETE

  0%|          | 0/100 [00:00&amp;lt;?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.

  1%|          | 1/100 [00:01&amp;lt;02:53,  1.76s/it]
                                               
{&#39;loss&#39;: 2.9969, &#39;grad_norm&#39;: 0.6156104803085327, &#39;learning_rate&#39;: 0.0, &#39;num_tokens&#39;: 12395.0, &#39;mean_token_accuracy&#39;: 0.557265505194664, &#39;epoch&#39;: 0.04}

  1%|          | 1/100 [00:01&amp;lt;02:53,  1.76s/it]
  2%|▏         | 2/100 [00:03&amp;lt;02:25,  1.49s/it]
                                               
{&#39;loss&#39;: 3.0022, &#39;grad_norm&#39;: 0.6304222345352173, &#39;learning_rate&#39;: 5e-05, &#39;num_tokens&#39;: 24663.0, &#39;mean_token_accuracy&#39;: 0.5569408759474754, &#39;epoch&#39;: 0.08}

  2%|▏         | 2/100 [00:03&amp;lt;02:25,  1.49s/it]
  3%|▎         | 3/100 [00:04&amp;lt;02:15,  1.40s/it]
                                               
{&#39;loss&#39;: 2.9518, &#39;grad_norm&#39;: 0.5929722189903259, &#39;learning_rate&#39;: 4.9987413559579636e-05, &#39;num_tokens&#39;: 36927.0, &#39;mean_token_accuracy&#39;: 0.5601871758699417, &#39;epoch&#39;: 0.12}

  3%|▎         | 3/100 [00:04&amp;lt;02:15,  1.40s/it]
  4%|▍         | 4/100 [00:05&amp;lt;02:10,  1.35s/it]
                                               
{&#39;loss&#39;: 2.8384, &#39;grad_norm&#39;: 0.5077946186065674, &#39;learning_rate&#39;: 4.994966691179711e-05, &#39;num_tokens&#39;: 49324.0, &#39;mean_token_accuracy&#39;: 0.5842191278934479, &#39;epoch&#39;: 0.16}

  4%|▍         | 4/100 [00:05&amp;lt;02:10,  1.35s/it]
  5%|▌         | 5/100 [00:06&amp;lt;02:06,  1.33s/it]
                                               
{&#39;loss&#39;: 2.7403, &#39;grad_norm&#39;: 0.47692668437957764, &#39;learning_rate&#39;: 4.988679806432712e-05, &#39;num_tokens&#39;: 61656.0, &#39;mean_token_accuracy&#39;: 0.5848015174269676, &#39;epoch&#39;: 0.2}

  5%|▌         | 5/100 [00:06&amp;lt;02:06,  1.33s/it]
  6%|▌         | 6/100 [00:08&amp;lt;02:03,  1.32s/it]
                                               
{&#39;loss&#39;: 2.6408, &#39;grad_norm&#39;: 0.4504166841506958, &#39;learning_rate&#39;: 4.9798870320769886e-05, &#39;num_tokens&#39;: 74040.0, &#39;mean_token_accuracy&#39;: 0.5917089208960533, &#39;epoch&#39;: 0.24}

  6%|▌         | 6/100 [00:08&amp;lt;02:03,  1.32s/it]
  7%|▋         | 7/100 [00:09&amp;lt;02:02,  1.32s/it]
                                               
{&#39;loss&#39;: 2.5569, &#39;grad_norm&#39;: 0.43109777569770813, &#39;learning_rate&#39;: 4.968597221690986e-05, &#39;num_tokens&#39;: 86402.0, &#39;mean_token_accuracy&#39;: 0.6047331020236015, &#39;epoch&#39;: 0.28}

  7%|▋         | 7/100 [00:09&amp;lt;02:02,  1.32s/it]
  8%|▊         | 8/100 [00:10&amp;lt;02:00,  1.31s/it]
                                               
{&#39;loss&#39;: 2.4599, &#39;grad_norm&#39;: 0.4197584092617035, &#39;learning_rate&#39;: 4.9548217431567665e-05, &#39;num_tokens&#39;: 98815.0, &#39;mean_token_accuracy&#39;: 0.6172997951507568, &#39;epoch&#39;: 0.32}

  8%|▊         | 8/100 [00:10&amp;lt;02:00,  1.31s/it]
  9%|▉         | 9/100 [00:12&amp;lt;01:58,  1.30s/it]
                                               
{&#39;loss&#39;: 2.3896, &#39;grad_norm&#39;: 0.4105101525783539, &#39;learning_rate&#39;: 4.938574467213518e-05, &#39;num_tokens&#39;: 111181.0, &#39;mean_token_accuracy&#39;: 0.6319251507520676, &#39;epoch&#39;: 0.36}

  9%|▉         | 9/100 [00:12&amp;lt;01:58,  1.30s/it]
 10%|█         | 10/100 [00:13&amp;lt;01:56,  1.30s/it]
                                                
{&#39;loss&#39;: 2.295, &#39;grad_norm&#39;: 0.4044440984725952, &#39;learning_rate&#39;: 4.919871753490891e-05, &#39;num_tokens&#39;: 123497.0, &#39;mean_token_accuracy&#39;: 0.6286465525627136, &#39;epoch&#39;: 0.4}

 10%|█         | 10/100 [00:13&amp;lt;01:56,  1.30s/it]
 11%|█         | 11/100 [00:14&amp;lt;01:55,  1.30s/it]
                                                
{&#39;loss&#39;: 2.1899, &#39;grad_norm&#39;: 0.4107915163040161, &#39;learning_rate&#39;: 4.898732434036244e-05, &#39;num_tokens&#39;: 135987.0, &#39;mean_token_accuracy&#39;: 0.6360785737633705, &#39;epoch&#39;: 0.44}

 11%|█         | 11/100 [00:14&amp;lt;01:55,  1.30s/it]
 12%|█▏        | 12/100 [00:15&amp;lt;01:53,  1.30s/it]
                                                
{&#39;loss&#39;: 2.1242, &#39;grad_norm&#39;: 0.4469277262687683, &#39;learning_rate&#39;: 4.8751777943523634e-05, &#39;num_tokens&#39;: 148455.0, &#39;mean_token_accuracy&#39;: 0.649301253259182, &#39;epoch&#39;: 0.48}

 12%|█▏        | 12/100 [00:15&amp;lt;01:53,  1.30s/it]
 13%|█▎        | 13/100 [00:17&amp;lt;01:52,  1.29s/it]
                                                
{&#39;loss&#39;: 2.0296, &#39;grad_norm&#39;: 0.3584776222705841, &#39;learning_rate&#39;: 4.849231551964771e-05, &#39;num_tokens&#39;: 160848.0, &#39;mean_token_accuracy&#39;: 0.654139406979084, &#39;epoch&#39;: 0.52}

 13%|█▎        | 13/100 [00:17&amp;lt;01:52,  1.29s/it]
 14%|█▍        | 14/100 [00:18&amp;lt;01:51,  1.29s/it]
                                                
{&#39;loss&#39;: 1.9926, &#39;grad_norm&#39;: 0.3459338843822479, &#39;learning_rate&#39;: 4.8209198325401815e-05, &#39;num_tokens&#39;: 173176.0, &#39;mean_token_accuracy&#39;: 0.6589227616786957, &#39;epoch&#39;: 0.56}

 14%|█▍        | 14/100 [00:18&amp;lt;01:51,  1.29s/it]
 15%|█▌        | 15/100 [00:19&amp;lt;01:49,  1.29s/it]
                                                
{&#39;loss&#39;: 1.9241, &#39;grad_norm&#39;: 0.3389168679714203, &#39;learning_rate&#39;: 4.790271143580174e-05, &#39;num_tokens&#39;: 185528.0, &#39;mean_token_accuracy&#39;: 0.6655279397964478, &#39;epoch&#39;: 0.6}

 15%|█▌        | 15/100 [00:19&amp;lt;01:49,  1.29s/it]
 16%|█▌        | 16/100 [00:21&amp;lt;01:48,  1.29s/it]
                                                
{&#39;loss&#39;: 1.8466, &#39;grad_norm&#39;: 0.3481423258781433, &#39;learning_rate&#39;: 4.7573163457165534e-05, &#39;num_tokens&#39;: 197848.0, &#39;mean_token_accuracy&#39;: 0.6780158132314682, &#39;epoch&#39;: 0.64}

 16%|█▌        | 16/100 [00:21&amp;lt;01:48,  1.29s/it]
 17%|█▋        | 17/100 [00:22&amp;lt;01:47,  1.29s/it]
                                                
{&#39;loss&#39;: 1.7937, &#39;grad_norm&#39;: 0.350722074508667, &#39;learning_rate&#39;: 4.722088621637309e-05, &#39;num_tokens&#39;: 210208.0, &#39;mean_token_accuracy&#39;: 0.6687768772244453, &#39;epoch&#39;: 0.68}

 17%|█▋        | 17/100 [00:22&amp;lt;01:47,  1.29s/it]
 18%|█▊        | 18/100 [00:23&amp;lt;01:45,  1.29s/it]
                                                
{&#39;loss&#39;: 1.7131, &#39;grad_norm&#39;: 0.3526684641838074, &#39;learning_rate&#39;: 4.684623442674463e-05, &#39;num_tokens&#39;: 222596.0, &#39;mean_token_accuracy&#39;: 0.669261246919632, &#39;epoch&#39;: 0.72}

 18%|█▊        | 18/100 [00:23&amp;lt;01:45,  1.29s/it]
 19%|█▉        | 19/100 [00:25&amp;lt;01:44,  1.29s/it]
                                                
{&#39;loss&#39;: 1.6793, &#39;grad_norm&#39;: 0.3442957401275635, &#39;learning_rate&#39;: 4.644958533087443e-05, &#39;num_tokens&#39;: 235010.0, &#39;mean_token_accuracy&#39;: 0.6730949878692627, &#39;epoch&#39;: 0.76}

 19%|█▉        | 19/100 [00:25&amp;lt;01:44,  1.29s/it]
 20%|██        | 20/100 [00:26&amp;lt;01:43,  1.29s/it]
                                                
{&#39;loss&#39;: 1.608, &#39;grad_norm&#39;: 0.31706002354621887, &#39;learning_rate&#39;: 4.6031338320779534e-05, &#39;num_tokens&#39;: 247478.0, &#39;mean_token_accuracy&#39;: 0.6819775551557541, &#39;epoch&#39;: 0.8}

 20%|██        | 20/100 [00:26&amp;lt;01:43,  1.29s/it]
 21%|██        | 21/100 [00:27&amp;lt;01:42,  1.29s/it]
                                                
{&#39;loss&#39;: 1.5786, &#39;grad_norm&#39;: 0.3038071393966675, &#39;learning_rate&#39;: 4.559191453574582e-05, &#39;num_tokens&#39;: 259806.0, &#39;mean_token_accuracy&#39;: 0.6907401010394096, &#39;epoch&#39;: 0.84}

 21%|██        | 21/100 [00:27&amp;lt;01:42,  1.29s/it]
 22%|██▏       | 22/100 [00:28&amp;lt;01:40,  1.29s/it]
                                                
{&#39;loss&#39;: 1.5251, &#39;grad_norm&#39;: 0.28096795082092285, &#39;learning_rate&#39;: 4.513175643827647e-05, &#39;num_tokens&#39;: 272161.0, &#39;mean_token_accuracy&#39;: 0.7046292722225189, &#39;epoch&#39;: 0.88}

 22%|██▏       | 22/100 [00:28&amp;lt;01:40,  1.29s/it]
 23%|██▎       | 23/100 [00:30&amp;lt;01:38,  1.29s/it]
                                                
{&#39;loss&#39;: 1.4769, &#39;grad_norm&#39;: 0.276657372713089, &#39;learning_rate&#39;: 4.465132736856969e-05, &#39;num_tokens&#39;: 284510.0, &#39;mean_token_accuracy&#39;: 0.7159407809376717, &#39;epoch&#39;: 0.92}

 23%|██▎       | 23/100 [00:30&amp;lt;01:38,  1.29s/it]
 24%|██▍       | 24/100 [00:31&amp;lt;01:37,  1.29s/it]
                                                
{&#39;loss&#39;: 1.4424, &#39;grad_norm&#39;: 0.25698068737983704, &#39;learning_rate&#39;: 4.415111107797445e-05, &#39;num_tokens&#39;: 296882.0, &#39;mean_token_accuracy&#39;: 0.7303404361009598, &#39;epoch&#39;: 0.96}

 24%|██▍       | 24/100 [00:31&amp;lt;01:37,  1.29s/it]
 25%|██▌       | 25/100 [00:32&amp;lt;01:36,  1.28s/it]
                                                
{&#39;loss&#39;: 1.4216, &#39;grad_norm&#39;: 0.25088033080101013, &#39;learning_rate&#39;: 4.3631611241893874e-05, &#39;num_tokens&#39;: 309282.0, &#39;mean_token_accuracy&#39;: 0.7428581342101097, &#39;epoch&#39;: 1.0}

 25%|██▌       | 25/100 [00:32&amp;lt;01:36,  1.28s/it]

  0%|          | 0/200 [00:00&amp;lt;?, ?it/s][A

  2%|▏         | 4/200 [00:00&amp;lt;00:06, 30.36it/s][A

  4%|▍         | 8/200 [00:00&amp;lt;00:07, 25.48it/s][A

  6%|▌         | 11/200 [00:00&amp;lt;00:07, 24.52it/s][A

  7%|▋         | 14/200 [00:00&amp;lt;00:07, 23.83it/s][A

  8%|▊         | 17/200 [00:00&amp;lt;00:07, 23.52it/s][A

 10%|█         | 20/200 [00:00&amp;lt;00:07, 23.25it/s][A

 12%|█▏        | 23/200 [00:00&amp;lt;00:07, 22.90it/s][A

 13%|█▎        | 26/200 [00:01&amp;lt;00:07, 22.99it/s][A

 14%|█▍        | 29/200 [00:01&amp;lt;00:07, 22.96it/s][A

 16%|█▌        | 32/200 [00:01&amp;lt;00:07, 22.84it/s][A

 18%|█▊        | 35/200 [00:01&amp;lt;00:07, 22.72it/s][A

 19%|█▉        | 38/200 [00:01&amp;lt;00:07, 22.70it/s][A

 20%|██        | 41/200 [00:01&amp;lt;00:06, 22.75it/s][A

 22%|██▏       | 44/200 [00:01&amp;lt;00:06, 22.75it/s][A

 24%|██▎       | 47/200 [00:02&amp;lt;00:06, 22.72it/s][A

 25%|██▌       | 50/200 [00:02&amp;lt;00:06, 22.67it/s][A

 26%|██▋       | 53/200 [00:02&amp;lt;00:06, 22.72it/s][A

 28%|██▊       | 56/200 [00:02&amp;lt;00:06, 22.68it/s][A

 30%|██▉       | 59/200 [00:02&amp;lt;00:06, 22.70it/s][A

 31%|███       | 62/200 [00:02&amp;lt;00:06, 22.80it/s][A

 32%|███▎      | 65/200 [00:02&amp;lt;00:05, 22.86it/s][A

 34%|███▍      | 68/200 [00:02&amp;lt;00:05, 22.73it/s][A

 36%|███▌      | 71/200 [00:03&amp;lt;00:05, 22.83it/s][A

 37%|███▋      | 74/200 [00:03&amp;lt;00:05, 22.90it/s][A

 38%|███▊      | 77/200 [00:03&amp;lt;00:05, 22.81it/s][A

 40%|████      | 80/200 [00:03&amp;lt;00:05, 22.72it/s][A

 42%|████▏     | 83/200 [00:03&amp;lt;00:05, 22.84it/s][A

 43%|████▎     | 86/200 [00:03&amp;lt;00:05, 22.77it/s][A

 44%|████▍     | 89/200 [00:03&amp;lt;00:04, 22.88it/s][A

 46%|████▌     | 92/200 [00:03&amp;lt;00:04, 22.90it/s][A

 48%|████▊     | 95/200 [00:04&amp;lt;00:04, 22.92it/s][A

 49%|████▉     | 98/200 [00:04&amp;lt;00:04, 22.96it/s][A

 50%|█████     | 101/200 [00:04&amp;lt;00:04, 22.84it/s][A

 52%|█████▏    | 104/200 [00:04&amp;lt;00:04, 22.67it/s][A

 54%|█████▎    | 107/200 [00:04&amp;lt;00:04, 22.77it/s][A

 55%|█████▌    | 110/200 [00:04&amp;lt;00:03, 22.75it/s][A

 56%|█████▋    | 113/200 [00:04&amp;lt;00:03, 22.64it/s][A

 58%|█████▊    | 116/200 [00:05&amp;lt;00:03, 22.76it/s][A

 60%|█████▉    | 119/200 [00:05&amp;lt;00:03, 22.87it/s][A

 61%|██████    | 122/200 [00:05&amp;lt;00:03, 22.89it/s][A

 62%|██████▎   | 125/200 [00:05&amp;lt;00:03, 22.71it/s][A

 64%|██████▍   | 128/200 [00:05&amp;lt;00:03, 22.69it/s][A

 66%|██████▌   | 131/200 [00:05&amp;lt;00:03, 22.68it/s][A

 67%|██████▋   | 134/200 [00:05&amp;lt;00:02, 22.69it/s][A

 68%|██████▊   | 137/200 [00:05&amp;lt;00:02, 22.65it/s][A

 70%|███████   | 140/200 [00:06&amp;lt;00:02, 22.79it/s][A

 72%|███████▏  | 143/200 [00:06&amp;lt;00:02, 22.83it/s][A

 73%|███████▎  | 146/200 [00:06&amp;lt;00:02, 22.77it/s][A

 74%|███████▍  | 149/200 [00:06&amp;lt;00:02, 22.73it/s][A

 76%|███████▌  | 152/200 [00:06&amp;lt;00:02, 22.81it/s][A

 78%|███████▊  | 155/200 [00:06&amp;lt;00:01, 22.78it/s][A

 79%|███████▉  | 158/200 [00:06&amp;lt;00:01, 22.89it/s][A

 80%|████████  | 161/200 [00:07&amp;lt;00:01, 22.93it/s][A

 82%|████████▏ | 164/200 [00:07&amp;lt;00:01, 22.88it/s][A

 84%|████████▎ | 167/200 [00:07&amp;lt;00:01, 22.95it/s][A

 85%|████████▌ | 170/200 [00:07&amp;lt;00:01, 22.92it/s][A

 86%|████████▋ | 173/200 [00:07&amp;lt;00:01, 22.75it/s][A

 88%|████████▊ | 176/200 [00:07&amp;lt;00:01, 22.84it/s][A

 90%|████████▉ | 179/200 [00:07&amp;lt;00:00, 22.77it/s][A

 91%|█████████ | 182/200 [00:07&amp;lt;00:00, 22.83it/s][A

 92%|█████████▎| 185/200 [00:08&amp;lt;00:00, 22.77it/s][A

 94%|█████████▍| 188/200 [00:08&amp;lt;00:00, 22.64it/s][A

 96%|█████████▌| 191/200 [00:08&amp;lt;00:00, 22.67it/s][A

 97%|█████████▋| 194/200 [00:08&amp;lt;00:00, 22.68it/s][A

 98%|█████████▊| 197/200 [00:08&amp;lt;00:00, 22.57it/s][A

100%|██████████| 200/200 [00:08&amp;lt;00:00, 22.78it/s][A
                                                

                                                 
[A{&#39;eval_loss&#39;: 1.380300760269165, &#39;eval_runtime&#39;: 8.7868, &#39;eval_samples_per_second&#39;: 182.091, &#39;eval_steps_per_second&#39;: 22.761, &#39;eval_num_tokens&#39;: 309282.0, &#39;eval_mean_token_accuracy&#39;: 0.7480635163187981, &#39;epoch&#39;: 1.0}

 25%|██▌       | 25/100 [00:41&amp;lt;01:36,  1.28s/it]

100%|██████████| 200/200 [00:08&amp;lt;00:00, 22.78it/s][A

                                                 [A
 26%|██▌       | 26/100 [00:42&amp;lt;04:50,  3.92s/it]
                                                
{&#39;loss&#39;: 1.3642, &#39;grad_norm&#39;: 0.23684436082839966, &#39;learning_rate&#39;: 4.309335095262676e-05, &#39;num_tokens&#39;: 321611.0, &#39;mean_token_accuracy&#39;: 0.7474109008908272, &#39;epoch&#39;: 1.04}

 26%|██▌       | 26/100 [00:42&amp;lt;04:50,  3.92s/it]
 27%|██▋       | 27/100 [00:44&amp;lt;03:48,  3.13s/it]
                                                
{&#39;loss&#39;: 1.3367, &#39;grad_norm&#39;: 0.24372604489326477, &#39;learning_rate&#39;: 4.2536872192658036e-05, &#39;num_tokens&#39;: 333958.0, &#39;mean_token_accuracy&#39;: 0.7538925781846046, &#39;epoch&#39;: 1.08}

 27%|██▋       | 27/100 [00:44&amp;lt;03:48,  3.13s/it]
 28%|██▊       | 28/100 [00:45&amp;lt;03:05,  2.58s/it]
                                                
{&#39;loss&#39;: 1.3164, &#39;grad_norm&#39;: 0.22672152519226074, &#39;learning_rate&#39;: 4.1962735288928305e-05, &#39;num_tokens&#39;: 346363.0, &#39;mean_token_accuracy&#39;: 0.7542992457747459, &#39;epoch&#39;: 1.12}

 28%|██▊       | 28/100 [00:45&amp;lt;03:05,  2.58s/it]
 29%|██▉       | 29/100 [00:46&amp;lt;02:35,  2.19s/it]
                                                
{&#39;loss&#39;: 1.2815, &#39;grad_norm&#39;: 0.22313237190246582, &#39;learning_rate&#39;: 4.137151834863213e-05, &#39;num_tokens&#39;: 358740.0, &#39;mean_token_accuracy&#39;: 0.7578952461481094, &#39;epoch&#39;: 1.16}

 29%|██▉       | 29/100 [00:46&amp;lt;02:35,  2.19s/it]
 30%|███       | 30/100 [00:47&amp;lt;02:14,  1.92s/it]
                                                
{&#39;loss&#39;: 1.272, &#39;grad_norm&#39;: 0.21239323914051056, &#39;learning_rate&#39;: 4.0763816677113064e-05, &#39;num_tokens&#39;: 371163.0, &#39;mean_token_accuracy&#39;: 0.7672104388475418, &#39;epoch&#39;: 1.2}

 30%|███       | 30/100 [00:47&amp;lt;02:14,  1.92s/it]
 31%|███       | 31/100 [00:49&amp;lt;01:59,  1.73s/it]
                                                
{&#39;loss&#39;: 1.2363, &#39;grad_norm&#39;: 0.21683549880981445, &#39;learning_rate&#39;: 4.014024217844167e-05, &#39;num_tokens&#39;: 383524.0, &#39;mean_token_accuracy&#39;: 0.7794611379504204, &#39;epoch&#39;: 1.24}

 31%|███       | 31/100 [00:49&amp;lt;01:59,  1.73s/it]
 32%|███▏      | 32/100 [00:50&amp;lt;01:48,  1.59s/it]
                                                
{&#39;loss&#39;: 1.2006, &#39;grad_norm&#39;: 0.21469290554523468, &#39;learning_rate&#39;: 3.9501422739279956e-05, &#39;num_tokens&#39;: 395924.0, &#39;mean_token_accuracy&#39;: 0.7862255200743675, &#39;epoch&#39;: 1.28}

 32%|███▏      | 32/100 [00:50&amp;lt;01:48,  1.59s/it]
 33%|███▎      | 33/100 [00:51&amp;lt;01:41,  1.51s/it]
                                                
{&#39;loss&#39;: 1.1771, &#39;grad_norm&#39;: 0.21210847795009613, &#39;learning_rate&#39;: 3.884800159665276e-05, &#39;num_tokens&#39;: 408322.0, &#39;mean_token_accuracy&#39;: 0.7854267209768295, &#39;epoch&#39;: 1.32}

 33%|███▎      | 33/100 [00:51&amp;lt;01:41,  1.51s/it]
 34%|███▍      | 34/100 [00:53&amp;lt;01:35,  1.45s/it]
                                                
{&#39;loss&#39;: 1.1582, &#39;grad_norm&#39;: 0.20535743236541748, &#39;learning_rate&#39;: 3.818063669026256e-05, &#39;num_tokens&#39;: 420776.0, &#39;mean_token_accuracy&#39;: 0.7891717627644539, &#39;epoch&#39;: 1.36}

 34%|███▍      | 34/100 [00:53&amp;lt;01:35,  1.45s/it]
 35%|███▌      | 35/100 [00:54&amp;lt;01:30,  1.39s/it]
                                                
{&#39;loss&#39;: 1.1326, &#39;grad_norm&#39;: 0.20486287772655487, &#39;learning_rate&#39;: 3.7500000000000003e-05, &#39;num_tokens&#39;: 433218.0, &#39;mean_token_accuracy&#39;: 0.7905180305242538, &#39;epoch&#39;: 1.4}

 35%|███▌      | 35/100 [00:54&amp;lt;01:30,  1.39s/it]
 36%|███▌      | 36/100 [00:55&amp;lt;01:26,  1.36s/it]
                                                
{&#39;loss&#39;: 1.1095, &#39;grad_norm&#39;: 0.20110926032066345, &#39;learning_rate&#39;: 3.680677686931707e-05, &#39;num_tokens&#39;: 445548.0, &#39;mean_token_accuracy&#39;: 0.7904055789113045, &#39;epoch&#39;: 1.44}

 36%|███▌      | 36/100 [00:55&amp;lt;01:26,  1.36s/it]
 37%|███▋      | 37/100 [00:56&amp;lt;01:24,  1.33s/it]
                                                
{&#39;loss&#39;: 1.0912, &#39;grad_norm&#39;: 0.20082473754882812, &#39;learning_rate&#39;: 3.610166531514436e-05, &#39;num_tokens&#39;: 457982.0, &#39;mean_token_accuracy&#39;: 0.7945679351687431, &#39;epoch&#39;: 1.48}

 37%|███▋      | 37/100 [00:56&amp;lt;01:24,  1.33s/it]
 38%|███▊      | 38/100 [00:58&amp;lt;01:22,  1.32s/it]
                                                
{&#39;loss&#39;: 1.0757, &#39;grad_norm&#39;: 0.20655913650989532, &#39;learning_rate&#39;: 3.5385375325047166e-05, &#39;num_tokens&#39;: 470302.0, &#39;mean_token_accuracy&#39;: 0.8008103594183922, &#39;epoch&#39;: 1.52}

 38%|███▊      | 38/100 [00:58&amp;lt;01:22,  1.32s/it]
 39%|███▉      | 39/100 [00:59&amp;lt;01:20,  1.31s/it]
                                                
{&#39;loss&#39;: 1.0475, &#39;grad_norm&#39;: 0.21213574707508087, &#39;learning_rate&#39;: 3.465862814232822e-05, &#39;num_tokens&#39;: 482652.0, &#39;mean_token_accuracy&#39;: 0.8052932694554329, &#39;epoch&#39;: 1.56}

 39%|███▉      | 39/100 [00:59&amp;lt;01:20,  1.31s/it]
 40%|████      | 40/100 [01:00&amp;lt;01:18,  1.30s/it]
                                                
{&#39;loss&#39;: 1.0193, &#39;grad_norm&#39;: 0.2088337391614914, &#39;learning_rate&#39;: 3.392215553979679e-05, &#39;num_tokens&#39;: 494935.0, &#39;mean_token_accuracy&#39;: 0.8083378374576569, &#39;epoch&#39;: 1.6}

 40%|████      | 40/100 [01:00&amp;lt;01:18,  1.30s/it]
 41%|████      | 41/100 [01:02&amp;lt;01:16,  1.30s/it]
                                                
{&#39;loss&#39;: 0.9896, &#39;grad_norm&#39;: 0.20944200456142426, &#39;learning_rate&#39;: 3.3176699082935545e-05, &#39;num_tokens&#39;: 507306.0, &#39;mean_token_accuracy&#39;: 0.8125813230872154, &#39;epoch&#39;: 1.64}

 41%|████      | 41/100 [01:02&amp;lt;01:16,  1.30s/it]
 42%|████▏     | 42/100 [01:03&amp;lt;01:15,  1.30s/it]
                                                
{&#39;loss&#39;: 0.9567, &#39;grad_norm&#39;: 0.208340585231781, &#39;learning_rate&#39;: 3.2423009383206876e-05, &#39;num_tokens&#39;: 519659.0, &#39;mean_token_accuracy&#39;: 0.8086286038160324, &#39;epoch&#39;: 1.68}

 42%|████▏     | 42/100 [01:03&amp;lt;01:15,  1.30s/it]
 43%|████▎     | 43/100 [01:04&amp;lt;01:13,  1.29s/it]
                                                
{&#39;loss&#39;: 0.9799, &#39;grad_norm&#39;: 0.21747039258480072, &#39;learning_rate&#39;: 3.166184534225087e-05, &#39;num_tokens&#39;: 532016.0, &#39;mean_token_accuracy&#39;: 0.8070342764258385, &#39;epoch&#39;: 1.72}

 43%|████▎     | 43/100 [01:04&amp;lt;01:13,  1.29s/it]
 44%|████▍     | 44/100 [01:05&amp;lt;01:12,  1.29s/it]
                                                
{&#39;loss&#39;: 0.9411, &#39;grad_norm&#39;: 0.21131806075572968, &#39;learning_rate&#39;: 3.0893973387735687e-05, &#39;num_tokens&#39;: 544451.0, &#39;mean_token_accuracy&#39;: 0.8081119135022163, &#39;epoch&#39;: 1.76}

 44%|████▍     | 44/100 [01:05&amp;lt;01:12,  1.29s/it]
 45%|████▌     | 45/100 [01:07&amp;lt;01:10,  1.29s/it]
                                                
{&#39;loss&#39;: 0.9102, &#39;grad_norm&#39;: 0.20553453266620636, &#39;learning_rate&#39;: 3.012016670162977e-05, &#39;num_tokens&#39;: 556764.0, &#39;mean_token_accuracy&#39;: 0.8155807703733444, &#39;epoch&#39;: 1.8}

 45%|████▌     | 45/100 [01:07&amp;lt;01:10,  1.29s/it]
 46%|████▌     | 46/100 [01:08&amp;lt;01:09,  1.29s/it]
                                                
{&#39;loss&#39;: 0.8995, &#39;grad_norm&#39;: 0.20625217258930206, &#39;learning_rate&#39;: 2.9341204441673266e-05, &#39;num_tokens&#39;: 569095.0, &#39;mean_token_accuracy&#39;: 0.8212151899933815, &#39;epoch&#39;: 1.84}

 46%|████▌     | 46/100 [01:08&amp;lt;01:09,  1.29s/it]
 47%|████▋     | 47/100 [01:09&amp;lt;01:08,  1.29s/it]
                                                
{&#39;loss&#39;: 0.8756, &#39;grad_norm&#39;: 0.20689339935779572, &#39;learning_rate&#39;: 2.8557870956832132e-05, &#39;num_tokens&#39;: 581369.0, &#39;mean_token_accuracy&#39;: 0.824812762439251, &#39;epoch&#39;: 1.88}

 47%|████▋     | 47/100 [01:09&amp;lt;01:08,  1.29s/it]
 48%|████▊     | 48/100 [01:11&amp;lt;01:06,  1.28s/it]
                                                
{&#39;loss&#39;: 0.8715, &#39;grad_norm&#39;: 0.19971652328968048, &#39;learning_rate&#39;: 2.7770954997525277e-05, &#39;num_tokens&#39;: 593861.0, &#39;mean_token_accuracy&#39;: 0.8196996673941612, &#39;epoch&#39;: 1.92}

 48%|████▊     | 48/100 [01:11&amp;lt;01:06,  1.28s/it]
 49%|████▉     | 49/100 [01:12&amp;lt;01:05,  1.28s/it]
                                                
{&#39;loss&#39;: 0.8449, &#39;grad_norm&#39;: 0.19617316126823425, &#39;learning_rate&#39;: 2.698124892141971e-05, &#39;num_tokens&#39;: 606239.0, &#39;mean_token_accuracy&#39;: 0.8340854272246361, &#39;epoch&#39;: 1.96}

 49%|████▉     | 49/100 [01:12&amp;lt;01:05,  1.28s/it]
 50%|█████     | 50/100 [01:13&amp;lt;01:04,  1.28s/it]
                                                
{&#39;loss&#39;: 0.8372, &#39;grad_norm&#39;: 0.2008523941040039, &#39;learning_rate&#39;: 2.6189547895593562e-05, &#39;num_tokens&#39;: 618564.0, &#39;mean_token_accuracy&#39;: 0.8334235697984695, &#39;epoch&#39;: 2.0}

 50%|█████     | 50/100 [01:13&amp;lt;01:04,  1.28s/it]

  0%|          | 0/200 [00:00&amp;lt;?, ?it/s][A

  2%|▏         | 4/200 [00:00&amp;lt;00:06, 30.52it/s][A

  4%|▍         | 8/200 [00:00&amp;lt;00:07, 25.60it/s][A

  6%|▌         | 11/200 [00:00&amp;lt;00:07, 24.64it/s][A

  7%|▋         | 14/200 [00:00&amp;lt;00:07, 23.94it/s][A

  8%|▊         | 17/200 [00:00&amp;lt;00:07, 23.61it/s][A

 10%|█         | 20/200 [00:00&amp;lt;00:07, 23.33it/s][A

 12%|█▏        | 23/200 [00:00&amp;lt;00:07, 22.98it/s][A

 13%|█▎        | 26/200 [00:01&amp;lt;00:07, 23.08it/s][A

 14%|█▍        | 29/200 [00:01&amp;lt;00:07, 23.05it/s][A

 16%|█▌        | 32/200 [00:01&amp;lt;00:07, 22.93it/s][A

 18%|█▊        | 35/200 [00:01&amp;lt;00:07, 22.83it/s][A

 19%|█▉        | 38/200 [00:01&amp;lt;00:07, 22.79it/s][A

 20%|██        | 41/200 [00:01&amp;lt;00:06, 22.85it/s][A

 22%|██▏       | 44/200 [00:01&amp;lt;00:06, 22.88it/s][A

 24%|██▎       | 47/200 [00:02&amp;lt;00:06, 22.82it/s][A

 25%|██▌       | 50/200 [00:02&amp;lt;00:06, 22.77it/s][A

 26%|██▋       | 53/200 [00:02&amp;lt;00:06, 22.83it/s][A

 28%|██▊       | 56/200 [00:02&amp;lt;00:06, 22.81it/s][A

 30%|██▉       | 59/200 [00:02&amp;lt;00:06, 22.80it/s][A

 31%|███       | 62/200 [00:02&amp;lt;00:06, 22.84it/s][A

 32%|███▎      | 65/200 [00:02&amp;lt;00:05, 22.89it/s][A

 34%|███▍      | 68/200 [00:02&amp;lt;00:05, 22.75it/s][A

 36%|███▌      | 71/200 [00:03&amp;lt;00:05, 22.85it/s][A

 37%|███▋      | 74/200 [00:03&amp;lt;00:05, 22.93it/s][A

 38%|███▊      | 77/200 [00:03&amp;lt;00:05, 22.85it/s][A

 40%|████      | 80/200 [00:03&amp;lt;00:05, 22.80it/s][A

 42%|████▏     | 83/200 [00:03&amp;lt;00:05, 22.85it/s][A

 43%|████▎     | 86/200 [00:03&amp;lt;00:05, 22.78it/s][A

 44%|████▍     | 89/200 [00:03&amp;lt;00:04, 22.89it/s][A

 46%|████▌     | 92/200 [00:03&amp;lt;00:04, 22.91it/s][A

 48%|████▊     | 95/200 [00:04&amp;lt;00:04, 22.93it/s][A

 49%|████▉     | 98/200 [00:04&amp;lt;00:04, 22.98it/s][A

 50%|█████     | 101/200 [00:04&amp;lt;00:04, 22.85it/s][A

 52%|█████▏    | 104/200 [00:04&amp;lt;00:04, 22.68it/s][A

 54%|█████▎    | 107/200 [00:04&amp;lt;00:04, 22.80it/s][A

 55%|█████▌    | 110/200 [00:04&amp;lt;00:03, 22.76it/s][A

 56%|█████▋    | 113/200 [00:04&amp;lt;00:03, 22.66it/s][A

 58%|█████▊    | 116/200 [00:05&amp;lt;00:03, 22.78it/s][A

 60%|█████▉    | 119/200 [00:05&amp;lt;00:03, 22.88it/s][A

 61%|██████    | 122/200 [00:05&amp;lt;00:03, 22.91it/s][A

 62%|██████▎   | 125/200 [00:05&amp;lt;00:03, 22.75it/s][A

 64%|██████▍   | 128/200 [00:05&amp;lt;00:03, 22.75it/s][A

 66%|██████▌   | 131/200 [00:05&amp;lt;00:03, 22.70it/s][A

 67%|██████▋   | 134/200 [00:05&amp;lt;00:02, 22.73it/s][A

 68%|██████▊   | 137/200 [00:05&amp;lt;00:02, 22.72it/s][A

 70%|███████   | 140/200 [00:06&amp;lt;00:02, 22.83it/s][A

 72%|███████▏  | 143/200 [00:06&amp;lt;00:02, 22.82it/s][A

 73%|███████▎  | 146/200 [00:06&amp;lt;00:02, 22.77it/s][A

 74%|███████▍  | 149/200 [00:06&amp;lt;00:02, 22.73it/s][A

 76%|███████▌  | 152/200 [00:06&amp;lt;00:02, 22.82it/s][A

 78%|███████▊  | 155/200 [00:06&amp;lt;00:01, 22.77it/s][A

 79%|███████▉  | 158/200 [00:06&amp;lt;00:01, 22.90it/s][A

 80%|████████  | 161/200 [00:07&amp;lt;00:01, 22.97it/s][A

 82%|████████▏ | 164/200 [00:07&amp;lt;00:01, 22.87it/s][A

 84%|████████▎ | 167/200 [00:07&amp;lt;00:01, 22.94it/s][A

 85%|████████▌ | 170/200 [00:07&amp;lt;00:01, 22.91it/s][A

 86%|████████▋ | 173/200 [00:07&amp;lt;00:01, 22.74it/s][A

 88%|████████▊ | 176/200 [00:07&amp;lt;00:01, 22.83it/s][A

 90%|████████▉ | 179/200 [00:07&amp;lt;00:00, 22.77it/s][A

 91%|█████████ | 182/200 [00:07&amp;lt;00:00, 22.81it/s][A

 92%|█████████▎| 185/200 [00:08&amp;lt;00:00, 22.77it/s][A

 94%|█████████▍| 188/200 [00:08&amp;lt;00:00, 22.64it/s][A

 96%|█████████▌| 191/200 [00:08&amp;lt;00:00, 22.68it/s][A

 97%|█████████▋| 194/200 [00:08&amp;lt;00:00, 22.68it/s][A

 98%|█████████▊| 197/200 [00:08&amp;lt;00:00, 22.56it/s][A

100%|██████████| 200/200 [00:08&amp;lt;00:00, 22.78it/s][A
                                                

                                                 
[A{&#39;eval_loss&#39;: 0.832106351852417, &#39;eval_runtime&#39;: 8.7725, &#39;eval_samples_per_second&#39;: 182.388, &#39;eval_steps_per_second&#39;: 22.798, &#39;eval_num_tokens&#39;: 618564.0, &#39;eval_mean_token_accuracy&#39;: 0.8328079205751419, &#39;epoch&#39;: 2.0}

 50%|█████     | 50/100 [01:22&amp;lt;01:04,  1.28s/it]

100%|██████████| 200/200 [00:08&amp;lt;00:00, 22.78it/s][A

                                                 [A
 51%|█████     | 51/100 [01:23&amp;lt;03:11,  3.91s/it]
                                                
{&#39;loss&#39;: 0.8343, &#39;grad_norm&#39;: 0.19672958552837372, &#39;learning_rate&#39;: 2.5396649095870202e-05, &#39;num_tokens&#39;: 630924.0, &#39;mean_token_accuracy&#39;: 0.8327541872859001, &#39;epoch&#39;: 2.04}

 51%|█████     | 51/100 [01:23&amp;lt;03:11,  3.91s/it]
 52%|█████▏    | 52/100 [01:25&amp;lt;02:30,  3.13s/it]
                                                
{&#39;loss&#39;: 0.8031, &#39;grad_norm&#39;: 0.19217002391815186, &#39;learning_rate&#39;: 2.46033509041298e-05, &#39;num_tokens&#39;: 643260.0, &#39;mean_token_accuracy&#39;: 0.8392890691757202, &#39;epoch&#39;: 2.08}

 52%|█████▏    | 52/100 [01:25&amp;lt;02:30,  3.13s/it]
 53%|█████▎    | 53/100 [01:26&amp;lt;02:01,  2.58s/it]
                                                
{&#39;loss&#39;: 0.8031, &#39;grad_norm&#39;: 0.19064681231975555, &#39;learning_rate&#39;: 2.3810452104406444e-05, &#39;num_tokens&#39;: 655664.0, &#39;mean_token_accuracy&#39;: 0.8399832472205162, &#39;epoch&#39;: 2.12}

 53%|█████▎    | 53/100 [01:26&amp;lt;02:01,  2.58s/it]
 54%|█████▍    | 54/100 [01:27&amp;lt;01:40,  2.20s/it]
                                                
{&#39;loss&#39;: 0.7971, &#39;grad_norm&#39;: 0.18536289036273956, &#39;learning_rate&#39;: 2.3018751078580287e-05, &#39;num_tokens&#39;: 668139.0, &#39;mean_token_accuracy&#39;: 0.8374352976679802, &#39;epoch&#39;: 2.16}

 54%|█████▍    | 54/100 [01:27&amp;lt;01:40,  2.20s/it]
 55%|█████▌    | 55/100 [01:28&amp;lt;01:26,  1.92s/it]
                                                
{&#39;loss&#39;: 0.7709, &#39;grad_norm&#39;: 0.1924009621143341, &#39;learning_rate&#39;: 2.222904500247473e-05, &#39;num_tokens&#39;: 680479.0, &#39;mean_token_accuracy&#39;: 0.840285949409008, &#39;epoch&#39;: 2.2}

 55%|█████▌    | 55/100 [01:28&amp;lt;01:26,  1.92s/it]
 56%|█████▌    | 56/100 [01:30&amp;lt;01:16,  1.74s/it]
                                                
{&#39;loss&#39;: 0.7821, &#39;grad_norm&#39;: 0.2057674378156662, &#39;learning_rate&#39;: 2.1442129043167874e-05, &#39;num_tokens&#39;: 692908.0, &#39;mean_token_accuracy&#39;: 0.8382995650172234, &#39;epoch&#39;: 2.24}

 56%|█████▌    | 56/100 [01:30&amp;lt;01:16,  1.74s/it]
 57%|█████▋    | 57/100 [01:31&amp;lt;01:08,  1.60s/it]
                                                
{&#39;loss&#39;: 0.7624, &#39;grad_norm&#39;: 0.18855980038642883, &#39;learning_rate&#39;: 2.0658795558326743e-05, &#39;num_tokens&#39;: 705288.0, &#39;mean_token_accuracy&#39;: 0.8444176241755486, &#39;epoch&#39;: 2.28}

 57%|█████▋    | 57/100 [01:31&amp;lt;01:08,  1.60s/it]
 58%|█████▊    | 58/100 [01:32&amp;lt;01:03,  1.51s/it]
                                                
{&#39;loss&#39;: 0.7663, &#39;grad_norm&#39;: 0.1674283891916275, &#39;learning_rate&#39;: 1.9879833298370238e-05, &#39;num_tokens&#39;: 717657.0, &#39;mean_token_accuracy&#39;: 0.8422146812081337, &#39;epoch&#39;: 2.32}

 58%|█████▊    | 58/100 [01:32&amp;lt;01:03,  1.51s/it]
 59%|█████▉    | 59/100 [01:34&amp;lt;00:59,  1.46s/it]
                                                
{&#39;loss&#39;: 0.727, &#39;grad_norm&#39;: 0.14157968759536743, &#39;learning_rate&#39;: 1.9106026612264316e-05, &#39;num_tokens&#39;: 729966.0, &#39;mean_token_accuracy&#39;: 0.8482123836874962, &#39;epoch&#39;: 2.36}

 59%|█████▉    | 59/100 [01:34&amp;lt;00:59,  1.46s/it]
 60%|██████    | 60/100 [01:35&amp;lt;00:56,  1.41s/it]
                                                
{&#39;loss&#39;: 0.7415, &#39;grad_norm&#39;: 0.12936082482337952, &#39;learning_rate&#39;: 1.8338154657749128e-05, &#39;num_tokens&#39;: 742278.0, &#39;mean_token_accuracy&#39;: 0.8452928811311722, &#39;epoch&#39;: 2.4}

 60%|██████    | 60/100 [01:35&amp;lt;00:56,  1.41s/it]
 61%|██████    | 61/100 [01:36&amp;lt;00:53,  1.37s/it]
                                                
{&#39;loss&#39;: 0.7487, &#39;grad_norm&#39;: 0.12007677555084229, &#39;learning_rate&#39;: 1.7576990616793137e-05, &#39;num_tokens&#39;: 754708.0, &#39;mean_token_accuracy&#39;: 0.8431602641940117, &#39;epoch&#39;: 2.44}

 61%|██████    | 61/100 [01:36&amp;lt;00:53,  1.37s/it]
 62%|██████▏   | 62/100 [01:37&amp;lt;00:51,  1.34s/it]
                                                
{&#39;loss&#39;: 0.7237, &#39;grad_norm&#39;: 0.12370007485151291, &#39;learning_rate&#39;: 1.682330091706446e-05, &#39;num_tokens&#39;: 767047.0, &#39;mean_token_accuracy&#39;: 0.8453052863478661, &#39;epoch&#39;: 2.48}

 62%|██████▏   | 62/100 [01:37&amp;lt;00:51,  1.34s/it]
 63%|██████▎   | 63/100 [01:39&amp;lt;00:49,  1.33s/it]
                                                
{&#39;loss&#39;: 0.7118, &#39;grad_norm&#39;: 0.11554520577192307, &#39;learning_rate&#39;: 1.6077844460203206e-05, &#39;num_tokens&#39;: 779363.0, &#39;mean_token_accuracy&#39;: 0.8485653474926949, &#39;epoch&#39;: 2.52}

 63%|██████▎   | 63/100 [01:39&amp;lt;00:49,  1.33s/it]
 64%|██████▍   | 64/100 [01:40&amp;lt;00:47,  1.32s/it]
                                                
{&#39;loss&#39;: 0.7284, &#39;grad_norm&#39;: 0.12923835217952728, &#39;learning_rate&#39;: 1.5341371857671782e-05, &#39;num_tokens&#39;: 791688.0, &#39;mean_token_accuracy&#39;: 0.8447374626994133, &#39;epoch&#39;: 2.56}

 64%|██████▍   | 64/100 [01:40&amp;lt;00:47,  1.32s/it]
 65%|██████▌   | 65/100 [01:41&amp;lt;00:45,  1.31s/it]
                                                
{&#39;loss&#39;: 0.7302, &#39;grad_norm&#39;: 0.11769213527441025, &#39;learning_rate&#39;: 1.4614624674952842e-05, &#39;num_tokens&#39;: 804034.0, &#39;mean_token_accuracy&#39;: 0.8439726531505585, &#39;epoch&#39;: 2.6}

 65%|██████▌   | 65/100 [01:41&amp;lt;00:45,  1.31s/it]
 66%|██████▌   | 66/100 [01:43&amp;lt;00:44,  1.30s/it]
                                                
{&#39;loss&#39;: 0.7408, &#39;grad_norm&#39;: 0.12309098243713379, &#39;learning_rate&#39;: 1.3898334684855647e-05, &#39;num_tokens&#39;: 816457.0, &#39;mean_token_accuracy&#39;: 0.8421422615647316, &#39;epoch&#39;: 2.64}

 66%|██████▌   | 66/100 [01:43&amp;lt;00:44,  1.30s/it]
 67%|██████▋   | 67/100 [01:44&amp;lt;00:42,  1.29s/it]
                                                
{&#39;loss&#39;: 0.7206, &#39;grad_norm&#39;: 0.11818303912878036, &#39;learning_rate&#39;: 1.3193223130682936e-05, &#39;num_tokens&#39;: 828875.0, &#39;mean_token_accuracy&#39;: 0.8433116152882576, &#39;epoch&#39;: 2.68}

 67%|██████▋   | 67/100 [01:44&amp;lt;00:42,  1.29s/it]
 68%|██████▊   | 68/100 [01:45&amp;lt;00:41,  1.29s/it]
                                                
{&#39;loss&#39;: 0.7203, &#39;grad_norm&#39;: 0.10981553792953491, &#39;learning_rate&#39;: 1.2500000000000006e-05, &#39;num_tokens&#39;: 841193.0, &#39;mean_token_accuracy&#39;: 0.8448885753750801, &#39;epoch&#39;: 2.72}

 68%|██████▊   | 68/100 [01:45&amp;lt;00:41,  1.29s/it]
 69%|██████▉   | 69/100 [01:46&amp;lt;00:39,  1.29s/it]
                                                
{&#39;loss&#39;: 0.7169, &#39;grad_norm&#39;: 0.11943168938159943, &#39;learning_rate&#39;: 1.181936330973744e-05, &#39;num_tokens&#39;: 853487.0, &#39;mean_token_accuracy&#39;: 0.8476847633719444, &#39;epoch&#39;: 2.76}

 69%|██████▉   | 69/100 [01:46&amp;lt;00:39,  1.29s/it]
 70%|███████   | 70/100 [01:48&amp;lt;00:38,  1.29s/it]
                                                
{&#39;loss&#39;: 0.7179, &#39;grad_norm&#39;: 0.10694629698991776, &#39;learning_rate&#39;: 1.1151998403347244e-05, &#39;num_tokens&#39;: 865933.0, &#39;mean_token_accuracy&#39;: 0.8449734374880791, &#39;epoch&#39;: 2.8}

 70%|███████   | 70/100 [01:48&amp;lt;00:38,  1.29s/it]
 71%|███████   | 71/100 [01:49&amp;lt;00:37,  1.29s/it]
                                                
{&#39;loss&#39;: 0.7184, &#39;grad_norm&#39;: 0.11949411779642105, &#39;learning_rate&#39;: 1.049857726072005e-05, &#39;num_tokens&#39;: 878273.0, &#39;mean_token_accuracy&#39;: 0.8473017513751984, &#39;epoch&#39;: 2.84}

 71%|███████   | 71/100 [01:49&amp;lt;00:37,  1.29s/it]
 72%|███████▏  | 72/100 [01:50&amp;lt;00:36,  1.29s/it]
                                                
{&#39;loss&#39;: 0.7053, &#39;grad_norm&#39;: 0.10539913177490234, &#39;learning_rate&#39;: 9.859757821558337e-06, &#39;num_tokens&#39;: 890669.0, &#39;mean_token_accuracy&#39;: 0.8475821688771248, &#39;epoch&#39;: 2.88}

 72%|███████▏  | 72/100 [01:50&amp;lt;00:36,  1.29s/it]
 73%|███████▎  | 73/100 [01:52&amp;lt;00:35,  1.30s/it]
                                                
{&#39;loss&#39;: 0.7228, &#39;grad_norm&#39;: 0.11216222494840622, &#39;learning_rate&#39;: 9.236183322886945e-06, &#39;num_tokens&#39;: 903150.0, &#39;mean_token_accuracy&#39;: 0.8438336625695229, &#39;epoch&#39;: 2.92}

 73%|███████▎  | 73/100 [01:52&amp;lt;00:35,  1.30s/it]
 74%|███████▍  | 74/100 [01:53&amp;lt;00:33,  1.30s/it]
                                                
{&#39;loss&#39;: 0.7107, &#39;grad_norm&#39;: 0.11380290240049362, &#39;learning_rate&#39;: 8.628481651367876e-06, &#39;num_tokens&#39;: 915529.0, &#39;mean_token_accuracy&#39;: 0.8479492291808128, &#39;epoch&#39;: 2.96}

 74%|███████▍  | 74/100 [01:53&amp;lt;00:33,  1.30s/it]
 75%|███████▌  | 75/100 [01:54&amp;lt;00:32,  1.29s/it]
                                                
{&#39;loss&#39;: 0.7052, &#39;grad_norm&#39;: 0.10310018062591553, &#39;learning_rate&#39;: 8.0372647110717e-06, &#39;num_tokens&#39;: 927846.0, &#39;mean_token_accuracy&#39;: 0.845212772488594, &#39;epoch&#39;: 3.0}

 75%|███████▌  | 75/100 [01:54&amp;lt;00:32,  1.29s/it]

  0%|          | 0/200 [00:00&amp;lt;?, ?it/s][A

  2%|▏         | 4/200 [00:00&amp;lt;00:06, 30.59it/s][A

  4%|▍         | 8/200 [00:00&amp;lt;00:07, 25.65it/s][A

  6%|▌         | 11/200 [00:00&amp;lt;00:07, 24.67it/s][A

  7%|▋         | 14/200 [00:00&amp;lt;00:07, 23.95it/s][A

  8%|▊         | 17/200 [00:00&amp;lt;00:07, 23.62it/s][A

 10%|█         | 20/200 [00:00&amp;lt;00:07, 23.33it/s][A

 12%|█▏        | 23/200 [00:00&amp;lt;00:07, 22.97it/s][A

 13%|█▎        | 26/200 [00:01&amp;lt;00:07, 23.07it/s][A

 14%|█▍        | 29/200 [00:01&amp;lt;00:07, 23.05it/s][A

 16%|█▌        | 32/200 [00:01&amp;lt;00:07, 22.93it/s][A

 18%|█▊        | 35/200 [00:01&amp;lt;00:07, 22.83it/s][A

 19%|█▉        | 38/200 [00:01&amp;lt;00:07, 22.76it/s][A

 20%|██        | 41/200 [00:01&amp;lt;00:06, 22.81it/s][A

 22%|██▏       | 44/200 [00:01&amp;lt;00:06, 22.87it/s][A

 24%|██▎       | 47/200 [00:02&amp;lt;00:06, 22.82it/s][A

 25%|██▌       | 50/200 [00:02&amp;lt;00:06, 22.77it/s][A

 26%|██▋       | 53/200 [00:02&amp;lt;00:06, 22.79it/s][A

 28%|██▊       | 56/200 [00:02&amp;lt;00:06, 22.78it/s][A

 30%|██▉       | 59/200 [00:02&amp;lt;00:06, 22.74it/s][A

 31%|███       | 62/200 [00:02&amp;lt;00:06, 22.79it/s][A

 32%|███▎      | 65/200 [00:02&amp;lt;00:05, 22.86it/s][A

 34%|███▍      | 68/200 [00:02&amp;lt;00:05, 22.73it/s][A

 36%|███▌      | 71/200 [00:03&amp;lt;00:05, 22.83it/s][A

 37%|███▋      | 74/200 [00:03&amp;lt;00:05, 22.90it/s][A

 38%|███▊      | 77/200 [00:03&amp;lt;00:05, 22.82it/s][A

 40%|████      | 80/200 [00:03&amp;lt;00:05, 22.77it/s][A

 42%|████▏     | 83/200 [00:03&amp;lt;00:05, 22.86it/s][A

 43%|████▎     | 86/200 [00:03&amp;lt;00:05, 22.79it/s][A

 44%|████▍     | 89/200 [00:03&amp;lt;00:04, 22.90it/s][A

 46%|████▌     | 92/200 [00:03&amp;lt;00:04, 22.90it/s][A

 48%|████▊     | 95/200 [00:04&amp;lt;00:04, 22.89it/s][A

 49%|████▉     | 98/200 [00:04&amp;lt;00:04, 22.88it/s][A

 50%|█████     | 101/200 [00:04&amp;lt;00:04, 22.77it/s][A

 52%|█████▏    | 104/200 [00:04&amp;lt;00:04, 22.60it/s][A

 54%|█████▎    | 107/200 [00:04&amp;lt;00:04, 22.73it/s][A

 55%|█████▌    | 110/200 [00:04&amp;lt;00:03, 22.70it/s][A

 56%|█████▋    | 113/200 [00:04&amp;lt;00:03, 22.60it/s][A

 58%|█████▊    | 116/200 [00:05&amp;lt;00:03, 22.71it/s][A

 60%|█████▉    | 119/200 [00:05&amp;lt;00:03, 22.80it/s][A

 61%|██████    | 122/200 [00:05&amp;lt;00:03, 22.85it/s][A

 62%|██████▎   | 125/200 [00:05&amp;lt;00:03, 22.69it/s][A

 64%|██████▍   | 128/200 [00:05&amp;lt;00:03, 22.70it/s][A

 66%|██████▌   | 131/200 [00:05&amp;lt;00:03, 22.67it/s][A

 67%|██████▋   | 134/200 [00:05&amp;lt;00:02, 22.71it/s][A

 68%|██████▊   | 137/200 [00:05&amp;lt;00:02, 22.70it/s][A

 70%|███████   | 140/200 [00:06&amp;lt;00:02, 22.82it/s][A

 72%|███████▏  | 143/200 [00:06&amp;lt;00:02, 22.81it/s][A

 73%|███████▎  | 146/200 [00:06&amp;lt;00:02, 22.77it/s][A

 74%|███████▍  | 149/200 [00:06&amp;lt;00:02, 22.72it/s][A

 76%|███████▌  | 152/200 [00:06&amp;lt;00:02, 22.81it/s][A

 78%|███████▊  | 155/200 [00:06&amp;lt;00:01, 22.78it/s][A

 79%|███████▉  | 158/200 [00:06&amp;lt;00:01, 22.90it/s][A

 80%|████████  | 161/200 [00:07&amp;lt;00:01, 22.97it/s][A

 82%|████████▏ | 164/200 [00:07&amp;lt;00:01, 22.87it/s][A

 84%|████████▎ | 167/200 [00:07&amp;lt;00:01, 22.95it/s][A

 85%|████████▌ | 170/200 [00:07&amp;lt;00:01, 22.91it/s][A

 86%|████████▋ | 173/200 [00:07&amp;lt;00:01, 22.74it/s][A

 88%|████████▊ | 176/200 [00:07&amp;lt;00:01, 22.83it/s][A

 90%|████████▉ | 179/200 [00:07&amp;lt;00:00, 22.76it/s][A

 91%|█████████ | 182/200 [00:07&amp;lt;00:00, 22.81it/s][A

 92%|█████████▎| 185/200 [00:08&amp;lt;00:00, 22.74it/s][A

 94%|█████████▍| 188/200 [00:08&amp;lt;00:00, 22.59it/s][A

 96%|█████████▌| 191/200 [00:08&amp;lt;00:00, 22.65it/s][A

 97%|█████████▋| 194/200 [00:08&amp;lt;00:00, 22.66it/s][A

 98%|█████████▊| 197/200 [00:08&amp;lt;00:00, 22.54it/s][A

100%|██████████| 200/200 [00:08&amp;lt;00:00, 22.75it/s][A
                                                

                                                 
[A{&#39;eval_loss&#39;: 0.7123968601226807, &#39;eval_runtime&#39;: 8.7811, &#39;eval_samples_per_second&#39;: 182.209, &#39;eval_steps_per_second&#39;: 22.776, &#39;eval_num_tokens&#39;: 927846.0, &#39;eval_mean_token_accuracy&#39;: 0.84687136054039, &#39;epoch&#39;: 3.0}

 75%|███████▌  | 75/100 [02:03&amp;lt;00:32,  1.29s/it]

100%|██████████| 200/200 [00:08&amp;lt;00:00, 22.75it/s][A

                                                 [A
 76%|███████▌  | 76/100 [02:04&amp;lt;01:34,  3.93s/it]
                                                
{&#39;loss&#39;: 0.7017, &#39;grad_norm&#39;: 0.11179792135953903, &#39;learning_rate&#39;: 7.463127807341966e-06, &#39;num_tokens&#39;: 940209.0, &#39;mean_token_accuracy&#39;: 0.8499162197113037, &#39;epoch&#39;: 3.04}

 76%|███████▌  | 76/100 [02:04&amp;lt;01:34,  3.93s/it]
 77%|███████▋  | 77/100 [02:06&amp;lt;01:12,  3.14s/it]
                                                
{&#39;loss&#39;: 0.705, &#39;grad_norm&#39;: 0.10462603718042374, &#39;learning_rate&#39;: 6.906649047373246e-06, &#39;num_tokens&#39;: 952553.0, &#39;mean_token_accuracy&#39;: 0.8468690812587738, &#39;epoch&#39;: 3.08}

 77%|███████▋  | 77/100 [02:06&amp;lt;01:12,  3.14s/it]
 78%|███████▊  | 78/100 [02:07&amp;lt;00:56,  2.58s/it]
                                                
{&#39;loss&#39;: 0.7063, &#39;grad_norm&#39;: 0.10219854861497879, &#39;learning_rate&#39;: 6.368388758106133e-06, &#39;num_tokens&#39;: 964949.0, &#39;mean_token_accuracy&#39;: 0.8474109619855881, &#39;epoch&#39;: 3.12}

 78%|███████▊  | 78/100 [02:07&amp;lt;00:56,  2.58s/it]
 79%|███████▉  | 79/100 [02:08&amp;lt;00:46,  2.20s/it]
                                                
{&#39;loss&#39;: 0.7133, &#39;grad_norm&#39;: 0.10548188537359238, &#39;learning_rate&#39;: 5.848888922025553e-06, &#39;num_tokens&#39;: 977387.0, &#39;mean_token_accuracy&#39;: 0.846750982105732, &#39;epoch&#39;: 3.16}

 79%|███████▉  | 79/100 [02:08&amp;lt;00:46,  2.20s/it]
 80%|████████  | 80/100 [02:09&amp;lt;00:38,  1.92s/it]
                                                
{&#39;loss&#39;: 0.7017, &#39;grad_norm&#39;: 0.11375600844621658, &#39;learning_rate&#39;: 5.348672631430318e-06, &#39;num_tokens&#39;: 989763.0, &#39;mean_token_accuracy&#39;: 0.84575305134058, &#39;epoch&#39;: 3.2}

 80%|████████  | 80/100 [02:09&amp;lt;00:38,  1.92s/it]
 81%|████████  | 81/100 [02:11&amp;lt;00:32,  1.73s/it]
                                                
{&#39;loss&#39;: 0.7071, &#39;grad_norm&#39;: 0.10798640549182892, &#39;learning_rate&#39;: 4.868243561723535e-06, &#39;num_tokens&#39;: 1002130.0, &#39;mean_token_accuracy&#39;: 0.8462942466139793, &#39;epoch&#39;: 3.24}

 81%|████████  | 81/100 [02:11&amp;lt;00:32,  1.73s/it]
 82%|████████▏ | 82/100 [02:12&amp;lt;00:28,  1.60s/it]
                                                
{&#39;loss&#39;: 0.6985, &#39;grad_norm&#39;: 0.1166706532239914, &#39;learning_rate&#39;: 4.408085464254183e-06, &#39;num_tokens&#39;: 1014499.0, &#39;mean_token_accuracy&#39;: 0.8511309772729874, &#39;epoch&#39;: 3.28}

 82%|████████▏ | 82/100 [02:12&amp;lt;00:28,  1.60s/it]
 83%|████████▎ | 83/100 [02:13&amp;lt;00:25,  1.50s/it]
                                                
{&#39;loss&#39;: 0.7077, &#39;grad_norm&#39;: 0.09920177608728409, &#39;learning_rate&#39;: 3.968661679220468e-06, &#39;num_tokens&#39;: 1026907.0, &#39;mean_token_accuracy&#39;: 0.846180722117424, &#39;epoch&#39;: 3.32}

 83%|████████▎ | 83/100 [02:13&amp;lt;00:25,  1.50s/it]
 84%|████████▍ | 84/100 [02:15&amp;lt;00:22,  1.43s/it]
                                                
{&#39;loss&#39;: 0.6979, &#39;grad_norm&#39;: 0.10761214792728424, &#39;learning_rate&#39;: 3.5504146691255736e-06, &#39;num_tokens&#39;: 1039315.0, &#39;mean_token_accuracy&#39;: 0.8476870656013489, &#39;epoch&#39;: 3.36}

 84%|████████▍ | 84/100 [02:15&amp;lt;00:22,  1.43s/it]
 85%|████████▌ | 85/100 [02:16&amp;lt;00:20,  1.38s/it]
                                                
{&#39;loss&#39;: 0.7012, &#39;grad_norm&#39;: 0.09792429208755493, &#39;learning_rate&#39;: 3.1537655732553768e-06, &#39;num_tokens&#39;: 1051706.0, &#39;mean_token_accuracy&#39;: 0.8467400893568993, &#39;epoch&#39;: 3.4}

 85%|████████▌ | 85/100 [02:16&amp;lt;00:20,  1.38s/it]
 86%|████████▌ | 86/100 [02:17&amp;lt;00:19,  1.36s/it]
                                                
{&#39;loss&#39;: 0.6794, &#39;grad_norm&#39;: 0.10248725116252899, &#39;learning_rate&#39;: 2.779113783626916e-06, &#39;num_tokens&#39;: 1063972.0, &#39;mean_token_accuracy&#39;: 0.8509822189807892, &#39;epoch&#39;: 3.44}

 86%|████████▌ | 86/100 [02:17&amp;lt;00:19,  1.36s/it]
 87%|████████▋ | 87/100 [02:18&amp;lt;00:17,  1.34s/it]
                                                
{&#39;loss&#39;: 0.7087, &#39;grad_norm&#39;: 0.10411231964826584, &#39;learning_rate&#39;: 2.4268365428344736e-06, &#39;num_tokens&#39;: 1076372.0, &#39;mean_token_accuracy&#39;: 0.8476828411221504, &#39;epoch&#39;: 3.48}

 87%|████████▋ | 87/100 [02:18&amp;lt;00:17,  1.34s/it]
 88%|████████▊ | 88/100 [02:20&amp;lt;00:15,  1.32s/it]
                                                
{&#39;loss&#39;: 0.695, &#39;grad_norm&#39;: 0.09845604747533798, &#39;learning_rate&#39;: 2.09728856419826e-06, &#39;num_tokens&#39;: 1088667.0, &#39;mean_token_accuracy&#39;: 0.8476302772760391, &#39;epoch&#39;: 3.52}

 88%|████████▊ | 88/100 [02:20&amp;lt;00:15,  1.32s/it]
 89%|████████▉ | 89/100 [02:21&amp;lt;00:14,  1.31s/it]
                                                
{&#39;loss&#39;: 0.6951, &#39;grad_norm&#39;: 0.10748502612113953, &#39;learning_rate&#39;: 1.790801674598186e-06, &#39;num_tokens&#39;: 1101068.0, &#39;mean_token_accuracy&#39;: 0.8479525744915009, &#39;epoch&#39;: 3.56}

 89%|████████▉ | 89/100 [02:21&amp;lt;00:14,  1.31s/it]
 90%|█████████ | 90/100 [02:22&amp;lt;00:13,  1.30s/it]
                                                
{&#39;loss&#39;: 0.6911, &#39;grad_norm&#39;: 0.10413651913404465, &#39;learning_rate&#39;: 1.5076844803522922e-06, &#39;num_tokens&#39;: 1113361.0, &#39;mean_token_accuracy&#39;: 0.8491488546133041, &#39;epoch&#39;: 3.6}

 90%|█████████ | 90/100 [02:22&amp;lt;00:13,  1.30s/it]
 91%|█████████ | 91/100 [02:24&amp;lt;00:11,  1.30s/it]
                                                
{&#39;loss&#39;: 0.7179, &#39;grad_norm&#39;: 0.10528236627578735, &#39;learning_rate&#39;: 1.248222056476367e-06, &#39;num_tokens&#39;: 1125801.0, &#39;mean_token_accuracy&#39;: 0.845989890396595, &#39;epoch&#39;: 3.64}

 91%|█████████ | 91/100 [02:24&amp;lt;00:11,  1.30s/it]
 92%|█████████▏| 92/100 [02:25&amp;lt;00:10,  1.30s/it]
                                                
{&#39;loss&#39;: 0.7221, &#39;grad_norm&#39;: 0.09716542065143585, &#39;learning_rate&#39;: 1.0126756596375686e-06, &#39;num_tokens&#39;: 1138163.0, &#39;mean_token_accuracy&#39;: 0.8444727882742882, &#39;epoch&#39;: 3.68}

 92%|█████████▏| 92/100 [02:25&amp;lt;00:10,  1.30s/it]
 93%|█████████▎| 93/100 [02:26&amp;lt;00:09,  1.29s/it]
                                                
{&#39;loss&#39;: 0.6987, &#39;grad_norm&#39;: 0.1005888283252716, &#39;learning_rate&#39;: 8.012824650910938e-07, &#39;num_tokens&#39;: 1150607.0, &#39;mean_token_accuracy&#39;: 0.8478176221251488, &#39;epoch&#39;: 3.72}

 93%|█████████▎| 93/100 [02:26&amp;lt;00:09,  1.29s/it]
 94%|█████████▍| 94/100 [02:27&amp;lt;00:07,  1.29s/it]
                                                
{&#39;loss&#39;: 0.7198, &#39;grad_norm&#39;: 0.09514491260051727, &#39;learning_rate&#39;: 6.142553278648239e-07, &#39;num_tokens&#39;: 1162963.0, &#39;mean_token_accuracy&#39;: 0.8451517522335052, &#39;epoch&#39;: 3.76}

 94%|█████████▍| 94/100 [02:27&amp;lt;00:07,  1.29s/it]
 95%|█████████▌| 95/100 [02:29&amp;lt;00:06,  1.29s/it]
                                                
{&#39;loss&#39;: 0.704, &#39;grad_norm&#39;: 0.10339541733264923, &#39;learning_rate&#39;: 4.517825684323324e-07, &#39;num_tokens&#39;: 1175339.0, &#39;mean_token_accuracy&#39;: 0.8451466336846352, &#39;epoch&#39;: 3.8}

 95%|█████████▌| 95/100 [02:29&amp;lt;00:06,  1.29s/it]
 96%|█████████▌| 96/100 [02:30&amp;lt;00:05,  1.29s/it]
                                                
{&#39;loss&#39;: 0.7149, &#39;grad_norm&#39;: 0.10203055292367935, &#39;learning_rate&#39;: 3.140277830901428e-07, &#39;num_tokens&#39;: 1187665.0, &#39;mean_token_accuracy&#39;: 0.8476828783750534, &#39;epoch&#39;: 3.84}

 96%|█████████▌| 96/100 [02:30&amp;lt;00:05,  1.29s/it]
 97%|█████████▋| 97/100 [02:31&amp;lt;00:03,  1.29s/it]
                                                
{&#39;loss&#39;: 0.7048, &#39;grad_norm&#39;: 0.0932934507727623, &#39;learning_rate&#39;: 2.011296792301165e-07, &#39;num_tokens&#39;: 1200128.0, &#39;mean_token_accuracy&#39;: 0.8435055613517761, &#39;epoch&#39;: 3.88}

 97%|█████████▋| 97/100 [02:31&amp;lt;00:03,  1.29s/it]
 98%|█████████▊| 98/100 [02:33&amp;lt;00:02,  1.29s/it]
                                                
{&#39;loss&#39;: 0.703, &#39;grad_norm&#39;: 0.10292600840330124, &#39;learning_rate&#39;: 1.1320193567288529e-07, &#39;num_tokens&#39;: 1212457.0, &#39;mean_token_accuracy&#39;: 0.8458232879638672, &#39;epoch&#39;: 3.92}

 98%|█████████▊| 98/100 [02:33&amp;lt;00:02,  1.29s/it]
 99%|█████████▉| 99/100 [02:34&amp;lt;00:01,  1.29s/it]
                                                
{&#39;loss&#39;: 0.6991, &#39;grad_norm&#39;: 0.10709496587514877, &#39;learning_rate&#39;: 5.033308820289184e-08, &#39;num_tokens&#39;: 1224822.0, &#39;mean_token_accuracy&#39;: 0.8467075452208519, &#39;epoch&#39;: 3.96}

 99%|█████████▉| 99/100 [02:34&amp;lt;00:01,  1.29s/it]
100%|██████████| 100/100 [02:35&amp;lt;00:00,  1.29s/it]
                                                 
{&#39;loss&#39;: 0.6888, &#39;grad_norm&#39;: 0.12091183662414551, &#39;learning_rate&#39;: 1.2586440420372936e-08, &#39;num_tokens&#39;: 1237128.0, &#39;mean_token_accuracy&#39;: 0.8510910719633102, &#39;epoch&#39;: 4.0}

100%|██████████| 100/100 [02:35&amp;lt;00:00,  1.29s/it]

  0%|          | 0/200 [00:00&amp;lt;?, ?it/s][A

  2%|▏         | 4/200 [00:00&amp;lt;00:06, 30.62it/s][A

  4%|▍         | 8/200 [00:00&amp;lt;00:07, 25.67it/s][A

  6%|▌         | 11/200 [00:00&amp;lt;00:07, 24.69it/s][A

  7%|▋         | 14/200 [00:00&amp;lt;00:07, 23.97it/s][A

  8%|▊         | 17/200 [00:00&amp;lt;00:07, 23.64it/s][A

 10%|█         | 20/200 [00:00&amp;lt;00:07, 23.36it/s][A

 12%|█▏        | 23/200 [00:00&amp;lt;00:07, 23.00it/s][A

 13%|█▎        | 26/200 [00:01&amp;lt;00:07, 23.09it/s][A

 14%|█▍        | 29/200 [00:01&amp;lt;00:07, 23.08it/s][A

 16%|█▌        | 32/200 [00:01&amp;lt;00:07, 22.95it/s][A

 18%|█▊        | 35/200 [00:01&amp;lt;00:07, 22.86it/s][A

 19%|█▉        | 38/200 [00:01&amp;lt;00:07, 22.82it/s][A

 20%|██        | 41/200 [00:01&amp;lt;00:06, 22.87it/s][A

 22%|██▏       | 44/200 [00:01&amp;lt;00:06, 22.90it/s][A

 24%|██▎       | 47/200 [00:02&amp;lt;00:06, 22.83it/s][A

 25%|██▌       | 50/200 [00:02&amp;lt;00:06, 22.77it/s][A

 26%|██▋       | 53/200 [00:02&amp;lt;00:06, 22.84it/s][A

 28%|██▊       | 56/200 [00:02&amp;lt;00:06, 22.83it/s][A

 30%|██▉       | 59/200 [00:02&amp;lt;00:06, 22.82it/s][A

 31%|███       | 62/200 [00:02&amp;lt;00:06, 22.88it/s][A

 32%|███▎      | 65/200 [00:02&amp;lt;00:05, 22.94it/s][A

 34%|███▍      | 68/200 [00:02&amp;lt;00:05, 22.79it/s][A

 36%|███▌      | 71/200 [00:03&amp;lt;00:05, 22.88it/s][A

 37%|███▋      | 74/200 [00:03&amp;lt;00:05, 22.94it/s][A

 38%|███▊      | 77/200 [00:03&amp;lt;00:05, 22.87it/s][A

 40%|████      | 80/200 [00:03&amp;lt;00:05, 22.82it/s][A

 42%|████▏     | 83/200 [00:03&amp;lt;00:05, 22.89it/s][A

 43%|████▎     | 86/200 [00:03&amp;lt;00:04, 22.82it/s][A

 44%|████▍     | 89/200 [00:03&amp;lt;00:04, 22.93it/s][A

 46%|████▌     | 92/200 [00:03&amp;lt;00:04, 22.95it/s][A

 48%|████▊     | 95/200 [00:04&amp;lt;00:04, 22.96it/s][A

 49%|████▉     | 98/200 [00:04&amp;lt;00:04, 23.02it/s][A

 50%|█████     | 101/200 [00:04&amp;lt;00:04, 22.89it/s][A

 52%|█████▏    | 104/200 [00:04&amp;lt;00:04, 22.70it/s][A

 54%|█████▎    | 107/200 [00:04&amp;lt;00:04, 22.79it/s][A

 55%|█████▌    | 110/200 [00:04&amp;lt;00:03, 22.76it/s][A

 56%|█████▋    | 113/200 [00:04&amp;lt;00:03, 22.65it/s][A

 58%|█████▊    | 116/200 [00:05&amp;lt;00:03, 22.78it/s][A

 60%|█████▉    | 119/200 [00:05&amp;lt;00:03, 22.87it/s][A

 61%|██████    | 122/200 [00:05&amp;lt;00:03, 22.90it/s][A

 62%|██████▎   | 125/200 [00:05&amp;lt;00:03, 22.76it/s][A

 64%|██████▍   | 128/200 [00:05&amp;lt;00:03, 22.76it/s][A

 66%|██████▌   | 131/200 [00:05&amp;lt;00:03, 22.69it/s][A

 67%|██████▋   | 134/200 [00:05&amp;lt;00:02, 22.72it/s][A

 68%|██████▊   | 137/200 [00:05&amp;lt;00:02, 22.71it/s][A

 70%|███████   | 140/200 [00:06&amp;lt;00:02, 22.84it/s][A

 72%|███████▏  | 143/200 [00:06&amp;lt;00:02, 22.87it/s][A

 73%|███████▎  | 146/200 [00:06&amp;lt;00:02, 22.80it/s][A

 74%|███████▍  | 149/200 [00:06&amp;lt;00:02, 22.76it/s][A

 76%|███████▌  | 152/200 [00:06&amp;lt;00:02, 22.84it/s][A

 78%|███████▊  | 155/200 [00:06&amp;lt;00:01, 22.80it/s][A

 79%|███████▉  | 158/200 [00:06&amp;lt;00:01, 22.91it/s][A

 80%|████████  | 161/200 [00:07&amp;lt;00:01, 22.98it/s][A

 82%|████████▏ | 164/200 [00:07&amp;lt;00:01, 22.90it/s][A

 84%|████████▎ | 167/200 [00:07&amp;lt;00:01, 22.95it/s][A

 85%|████████▌ | 170/200 [00:07&amp;lt;00:01, 22.92it/s][A

 86%|████████▋ | 173/200 [00:07&amp;lt;00:01, 22.75it/s][A

 88%|████████▊ | 176/200 [00:07&amp;lt;00:01, 22.84it/s][A

 90%|████████▉ | 179/200 [00:07&amp;lt;00:00, 22.77it/s][A

 91%|█████████ | 182/200 [00:07&amp;lt;00:00, 22.84it/s][A

 92%|█████████▎| 185/200 [00:08&amp;lt;00:00, 22.79it/s][A

 94%|█████████▍| 188/200 [00:08&amp;lt;00:00, 22.62it/s][A

 96%|█████████▌| 191/200 [00:08&amp;lt;00:00, 22.68it/s][A

 97%|█████████▋| 194/200 [00:08&amp;lt;00:00, 22.69it/s][A

 98%|█████████▊| 197/200 [00:08&amp;lt;00:00, 22.57it/s][A

100%|██████████| 200/200 [00:08&amp;lt;00:00, 22.79it/s][A
                                                 

                                                 
[A{&#39;eval_loss&#39;: 0.7050034999847412, &#39;eval_runtime&#39;: 8.7663, &#39;eval_samples_per_second&#39;: 182.518, &#39;eval_steps_per_second&#39;: 22.815, &#39;eval_num_tokens&#39;: 1237128.0, &#39;eval_mean_token_accuracy&#39;: 0.8474910864233971, &#39;epoch&#39;: 4.0}

100%|██████████| 100/100 [02:45&amp;lt;00:00,  1.29s/it]

100%|██████████| 200/200 [00:08&amp;lt;00:00, 22.79it/s][A

                                                 [A
                                                 
{&#39;train_runtime&#39;: 165.094, &#39;train_samples_per_second&#39;: 38.766, &#39;train_steps_per_second&#39;: 0.606, &#39;train_loss&#39;: 1.1633441585302353, &#39;epoch&#39;: 4.0}

100%|██████████| 100/100 [02:45&amp;lt;00:00,  1.29s/it]
100%|██████████| 100/100 [02:45&amp;lt;00:00,  1.65s/it]
[rank0]:[W1116 21:41:58.014854057 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
dlc1lm8dmxok7cfj-master-0:133:394 [0] NCCL INFO [Service thread] Connection closed by localRank 0
dlc1lm8dmxok7cfj-master-0:133:454 [0] NCCL INFO comm 0x5603c0703fe0 rank 0 nranks 1 cudaDev 0 busId 30 - Abort COMPLETE
2025/11/16 21:42:00 INFO: Covert command: [&#39;python&#39;, &#39;convert.py&#39;, &#39;--model_name&#39;, &#39;/tmp/input_model/&#39;, &#39;--model_type&#39;, &#39;qwen3&#39;, &#39;--output_dir&#39;, &#39;/tmp/model/&#39;, &#39;--adapter_dir&#39;, &#39;/tmp/adapter/&#39;]
[2025-11-16 21:42:03,497] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2025/11/16 21:42:11 INFO: execute command: mkdir -p  /ml/output/model/adapter/
2025/11/16 21:42:12 INFO: execute command succeed
2025/11/16 21:42:12 INFO: execute command: cp -R /tmp/adapter/* /ml/output/model/adapter/
2025/11/16 21:42:15 INFO: execute command succeed
2025/11/16 21:42:15 INFO: execute command: cp -r /tmp/model/ /ml/output/
2025/11/16 21:42:17 INFO: execute command succeed
User program finished!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;训练监控：
&lt;img src=&#34;post/2025/images/2025-11-15-llm-0-to-1/IMG_20251116-222654776.png&#34; alt=&#34;图 14&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;模型评测&#34;&gt;模型评测&lt;/h1&gt;

&lt;h1 id=&#34;模型部署&#34;&gt;模型部署&lt;/h1&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;p&gt;【01】&lt;a href=&#34;https://help.aliyun.com/zh/pai/getting-started/model-gallery-quick-start?spm=a2c4g.11186623.0.0.689d14a8kHCMo0#4c930eb6efvjf&#34;&gt;Model Gallery 快速入门&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>大模型权重文件解析</title>
            <link>http://mospany.github.io/2025/11/09/hugging-face/</link>
            <pubDate>Sun, 09 Nov 2025 11:24:40 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2025/11/09/hugging-face/</guid>
            <description>

&lt;h1 id=&#34;1-模型文件&#34;&gt;1、模型文件&lt;/h1&gt;

&lt;p&gt;以Qwen1.5-32B-Chat-AWQ模型文件列表为例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;shengping.mo@y108p30:/data2/huggingface/Qwen/Qwen1.5-32B-Chat-AWQ$ ls -lh
total 20G
-rw-r--r-- 1 xd xd  839 Jun 27 22:30 config.json
-rw-r--r-- 1 xd xd  243 Jun 27 22:30 generation_config.json
-rw-r--r-- 1 xd xd 6.8K Jun 27 22:30 LICENSE
-rw-r--r-- 1 xd xd 1.6M Jun 27 22:30 merges.txt
-rw-r--r-- 1 xd xd 4.7G Jun 27 22:28 model-00001-of-00005.safetensors
-rw-r--r-- 1 xd xd 4.7G Jun 27 22:29 model-00002-of-00005.safetensors
-rw-r--r-- 1 xd xd 4.7G Jun 27 22:29 model-00003-of-00005.safetensors
-rw-r--r-- 1 xd xd 4.3G Jun 27 22:30 model-00004-of-00005.safetensors
-rw-r--r-- 1 xd xd 1.5G Jun 27 22:30 model-00005-of-00005.safetensors
-rw-r--r-- 1 xd xd 140K Jun 27 22:30 model.safetensors.index.json
-rw-r--r-- 1 xd xd 4.0K Jun 27 22:30 README.md
-rw-r--r-- 1 xd xd 1.3K Jun 27 22:30 tokenizer_config.json
-rw-r--r-- 1 xd xd 6.8M Jun 27 22:30 tokenizer.json
-rw-r--r-- 1 xd xd 2.7M Jun 27 22:30 vocab.json
shengping.mo@y108p30:/data2/huggingface/Qwen/Qwen1.5-32B-Chat-AWQ$ 
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;2-整体分析&#34;&gt;2、整体分析&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;config.json&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;存储模型架构元数据，包括网络层数，隐藏维度，注意力头数，词表大小，激活函数类型等参数
&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;模型权重文件：torch 和 hf 权重文件&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;pytroch_model.pth: 用于保存模型权重参数的二进制文件，以字典的形式保存，键为层名称，值为对应的权重张量数据。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;model.safetensor：和pth权重文件的功能相同，区别是采用了安全序列化格式，避免反序列化漏洞风险。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;分片权重文件：对于超大型的模型权重，会被分割成多个文件，解决单个文件体积限制问题。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;分词器相关&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;tokenizer_config.json: 分词器类型、特殊标记的定义&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;tokenizer.json: 将分词器的所有配置信息，包括词汇表，合并规则，归一化，预分词，后处理等打包在一起，目的是让训练和推理过程中的分词行为完全一致。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;文本生成参数配置文件 &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;generation_config.json：定义文本生成策略，包括温度，top-p等超参数。
&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;tokenizer-json&#34;&gt;tokenizer.json&lt;/h3&gt;

&lt;p&gt;文件定义了分词器的所有配置&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;added_tokens: 原有词汇表基础上额外添加的特殊token([PAD] [CLS])&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;post_processor: 后处理模板，设置单句，双句对输入时输入时自动添加[cls] 和 [sep]&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;decoder：解码器类型&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;model：底层分词器模型 的类型及其参数。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Vocab: 定义了词汇表，其中key为token的字符串，value为对应的id。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;merges：定义了子token合并成更长token的规则。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;special-tokens-map-json&#34;&gt;special_tokens_map.json&lt;/h3&gt;

&lt;p&gt;文件定义了与文本序列处理相关的特殊token&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;bos_token (Beginning of Sentence Token): 序列开始标记，文本序列的起始位置&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;eos_token (End of Sentence Token)： 序列结束标记，表示文本序列的结束位置。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;eop_token (End of Paragraph Token): 段落结束标记，表示段落结束的特殊标记&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;pad_token (Padding Token): 填充标记，它将用于文本序列填充到相同长度时使用的特殊token。tokenizer.json 文件解析。 &lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;3-具体分析&#34;&gt;3、具体分析&lt;/h1&gt;

&lt;h2 id=&#34;3-1-config-json&#34;&gt;3.1 config.json&lt;/h2&gt;

&lt;p&gt;作用：用于&lt;strong&gt;描述模型的结构、超参数、训练方式、特殊行为等信息&lt;/strong&gt;，在加载模型时自动解析并构建模型实例。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  // 使用的模型架构名称
  &amp;quot;architectures&amp;quot;: [
    &amp;quot;Qwen2ForCausalLM&amp;quot;
  ],
  // 注意力层的dropout比例
  &amp;quot;attention_dropout&amp;quot;: 0.0,
  // 句子起始（BOS）token的ID
  &amp;quot;bos_token_id&amp;quot;: 151643,
  // 句子结束（EOS）token的ID
  &amp;quot;eos_token_id&amp;quot;: 151645,
  // 隐藏层激活函数
  &amp;quot;hidden_act&amp;quot;: &amp;quot;silu&amp;quot;,
  // 隐藏层维度
  &amp;quot;hidden_size&amp;quot;: 5120,
  // 参数初始化范围
  &amp;quot;initializer_range&amp;quot;: 0.02,
  // 前馈网络中间层的维度
  &amp;quot;intermediate_size&amp;quot;: 27392,
  // 最大位置编码长度
  &amp;quot;max_position_embeddings&amp;quot;: 32768,
  // 支持滑动窗口的最大层数
  &amp;quot;max_window_layers&amp;quot;: 35,
  // 模型类型标识
  &amp;quot;model_type&amp;quot;: &amp;quot;qwen2&amp;quot;,
  // 注意力头的数量
  &amp;quot;num_attention_heads&amp;quot;: 40,
  // Transformer层的数量
  &amp;quot;num_hidden_layers&amp;quot;: 64,
  // key-value注意力头的数量（用于分组注意力等）
  &amp;quot;num_key_value_heads&amp;quot;: 8,
  // 量化相关配置
  &amp;quot;quantization_config&amp;quot;: {
    // 量化位数
    &amp;quot;bits&amp;quot;: 4,
    // 量化分组大小
    &amp;quot;group_size&amp;quot;: 32,
    // 不进行量化的模块
    &amp;quot;modules_to_not_convert&amp;quot;: null,
    // 量化方法
    &amp;quot;quant_method&amp;quot;: &amp;quot;awq&amp;quot;,
    // 量化版本
    &amp;quot;version&amp;quot;: &amp;quot;gemm&amp;quot;,
    // 是否使用零点
    &amp;quot;zero_point&amp;quot;: true
  },
  // RMSNorm的epsilon值
  &amp;quot;rms_norm_eps&amp;quot;: 1e-06,
  // ROPE位置编码的theta参数
  &amp;quot;rope_theta&amp;quot;: 1000000.0,
  // 滑动窗口长度
  &amp;quot;sliding_window&amp;quot;: 32768,
  // 是否共享词嵌入权重
  &amp;quot;tie_word_embeddings&amp;quot;: false,
  // PyTorch中使用的数据类型
  &amp;quot;torch_dtype&amp;quot;: &amp;quot;float16&amp;quot;,
  // transformers库的版本
  &amp;quot;transformers_version&amp;quot;: &amp;quot;4.38.2&amp;quot;,
  // 是否使用缓存（加速推理）
  &amp;quot;use_cache&amp;quot;: true,
  // 是否启用滑动窗口机制
  &amp;quot;use_sliding_window&amp;quot;: false,
  // 词表大小
  &amp;quot;vocab_size&amp;quot;: 152064
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;一-基本模型规格&#34;&gt;🧠 一、基本模型规格&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;参数&lt;/th&gt;
&lt;th&gt;含义&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;model_type&lt;/code&gt;: &lt;code&gt;&amp;quot;qwen2&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;模型类型&lt;/td&gt;
&lt;td&gt;来自阿里达摩院的 Qwen2 系列&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;architectures&lt;/code&gt;: &lt;code&gt;&amp;quot;Qwen2ForCausalLM&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;架构&lt;/td&gt;
&lt;td&gt;自回归语言模型（Causal Language Model）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;hidden_size&lt;/code&gt;: &lt;code&gt;5120&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;每层维度&lt;/td&gt;
&lt;td&gt;大型模型（估算 20B 参数级别）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;num_hidden_layers&lt;/code&gt;: &lt;code&gt;64&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Transformer 层数&lt;/td&gt;
&lt;td&gt;层数非常深&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;num_attention_heads&lt;/code&gt;: &lt;code&gt;40&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;注意力头数&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;num_key_value_heads&lt;/code&gt;: &lt;code&gt;8&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;KV 分组数量&lt;/td&gt;
&lt;td&gt;表明采用了 &lt;strong&gt;Grouped Query Attention (GQA)&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;intermediate_size&lt;/code&gt;: &lt;code&gt;27392&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;FFN 层维度&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;vocab_size&lt;/code&gt;: &lt;code&gt;152064&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;词表大小&lt;/td&gt;
&lt;td&gt;包含中文/英文等多语言词表&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;二-上下文和位置处理&#34;&gt;🧠 二、上下文和位置处理&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;参数&lt;/th&gt;
&lt;th&gt;含义&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;max_position_embeddings&lt;/code&gt;: &lt;code&gt;32768&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;最大位置嵌入（理论最大上下文长度）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;sliding_window&lt;/code&gt;: &lt;code&gt;32768&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;实际窗口大小&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;max_window_layers&lt;/code&gt;: &lt;code&gt;35&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;可支持滑动窗口的层数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;rope_theta&lt;/code&gt;: &lt;code&gt;1e6&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;用于 RoPE（旋转位置编码）的频率参数（适配长上下文）&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;📌 &lt;strong&gt;说明&lt;/strong&gt;：这是一个&lt;strong&gt;支持超长上下文&lt;/strong&gt;（32K token）的模型，适合：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;读大文档（PDF/合同/代码）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;长对话历史保持上下文连贯&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;文本审校、知识提取&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;三-优化与量化配置&#34;&gt;⚙️ 三、优化与量化配置&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;参数&lt;/th&gt;
&lt;th&gt;含义&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;quantization_config.bits&lt;/code&gt;: &lt;code&gt;4&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;4-bit 量化&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;quant_method&lt;/code&gt;: &lt;code&gt;awq&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;使用 AWQ（Activation-aware Weight Quantization）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;group_size&lt;/code&gt;: &lt;code&gt;32&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;每组参数量化块大小&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;version&lt;/code&gt;: &lt;code&gt;gemm&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;量化内核类型，基于 GEMM&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;zero_point&lt;/code&gt;: true&lt;/td&gt;
&lt;td&gt;使用 Zero-point 对称量化&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;torch_dtype&lt;/code&gt;: &lt;code&gt;&amp;quot;float16&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;半精度 float16 运算&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;📌 &lt;strong&gt;说明&lt;/strong&gt;：这说明这个模型是经过 &lt;strong&gt;4bit AWQ 量化优化的版本&lt;/strong&gt;，意味着你可以：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;在消费级显卡（比如 24G 显存）运行&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;延迟更低，推理速度快&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;部署成本降低&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;3-2-generation-config-json&#34;&gt;3.2 generation_config.json&lt;/h2&gt;

&lt;p&gt;推理参数（生成策略）配置文件,  用于控制模型生成文本时的行为（例如采样策略、终止符号、重复惩罚等）， 这段 JSON 是模型的“生成策略说明书”，控制模型如何从输入中“说出”下一句话。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  // 起始标记的token id
  &amp;quot;bos_token_id&amp;quot;: 151643,
  // 是否使用采样生成
  &amp;quot;do_sample&amp;quot;: true,
  // 结束标记的token id列表
  &amp;quot;eos_token_id&amp;quot;: [
    151645,
    151643
  ],
  // 填充token的id
  &amp;quot;pad_token_id&amp;quot;: 151643,
  // 重复惩罚系数
  &amp;quot;repetition_penalty&amp;quot;: 1.05,
  // 采样温度
  &amp;quot;temperature&amp;quot;: 0.7,
  // top-k采样参数
  &amp;quot;top_k&amp;quot;: 20,
  // top-p采样参数
  &amp;quot;top_p&amp;quot;: 0.8,
  // transformers库的版本
  &amp;quot;transformers_version&amp;quot;: &amp;quot;4.38.2&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;字段解析与作用&#34;&gt;🔍 字段解析与作用&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;参数名&lt;/th&gt;
&lt;th&gt;示例值&lt;/th&gt;
&lt;th&gt;含义与作用&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;bos_token_id&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;151643&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Begin Of Sequence，生成起始标记 token ID&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;eos_token_id&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[151645, 151643]&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;End Of Sequence，遇到这些 token ID 时停止生成&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pad_token_id&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;151643&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;用于 padding（对齐序列）时使用的 token ID&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;do_sample&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;启用采样（否则是贪婪/beam search）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;temperature&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0.7&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;控制采样随机性（值越低越确定）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;top_k&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;20&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;限制每一步采样时最多考虑前 20 个概率最高的 token&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;top_p&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0.8&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;nucleus sampling（只考虑累积概率 ≥ 0.8 的 token）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;repetition_penalty&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1.05&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;惩罚重复词语（&amp;gt;1 会降低重复出现 token 的概率）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;transformers_version&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;4.38.2&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;指明此配置兼容的 HuggingFace Transformers 版本&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;实际生成中它怎么用&#34;&gt;🧠 实际生成中它怎么用？&lt;/h3&gt;

&lt;p&gt;你加载模型后，如果该文件存在，Transformers 会自动读取这些参数并应用到 &lt;code&gt;generate()&lt;/code&gt; 调用中：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(&amp;quot;Qwen/Qwen2-7B&amp;quot;)
tokenizer = AutoTokenizer.from_pretrained(&amp;quot;Qwen/Qwen2-7B&amp;quot;)

input_ids = tokenizer(&amp;quot;你好，请帮我总结一下这段话：&amp;quot;, return_tensors=&amp;quot;pt&amp;quot;).input_ids
outputs = model.generate(input_ids)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;如果目录下存在 &lt;code&gt;generation_config.json&lt;/code&gt;，&lt;code&gt;model.generate()&lt;/code&gt; 会默认读取其中的设置（如 &lt;code&gt;top_k&lt;/code&gt;, &lt;code&gt;temperature&lt;/code&gt; 等）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;你也可以手动覆盖它：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;outputs = model.generate(
    input_ids,
    temperature=0.7,
    top_k=20,
    top_p=0.8,
    repetition_penalty=1.05,
    do_sample=True,
    max_new_tokens=256
)

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;你可以用它做什么&#34;&gt;✅ 你可以用它做什么？&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;自定义生成风格&lt;/strong&gt;：更随机或更确定、控制重复&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;优化对话流畅性&lt;/strong&gt;：结合 &lt;code&gt;eos_token_id&lt;/code&gt;，能更自然地中止输出&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;自动化部署&lt;/strong&gt;：多个模型可统一读取生成策略文件&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;3-3-merges-txt&#34;&gt;3.3 merges.txt&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;merges.txt&lt;/code&gt; 是 Hugging Face 模型中的一个 &lt;strong&gt;分词器（Tokenizer）文件&lt;/strong&gt;，它是 &lt;strong&gt;Byte Pair Encoding（BPE）&lt;/strong&gt; 分词算法的核心组成部分之一，配合 &lt;code&gt;vocab.json&lt;/code&gt; 使用，决定了如何将文本分割成 token。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/4jKqm0b8gmBmbnw1/img/ad459646-9cf9-4b50-9321-5f022f5f71d6.png&#34; alt=&#34;image.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;merges.txt&lt;/code&gt; 是 Hugging Face 模型中的一个 &lt;strong&gt;分词器（Tokenizer）文件&lt;/strong&gt;，它是 &lt;strong&gt;Byte Pair Encoding（BPE）&lt;/strong&gt; 分词算法的核心组成部分之一，配合 &lt;code&gt;vocab.json&lt;/code&gt; 使用，决定了如何将文本分割成 token。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;bpe-分词算法背景简述&#34;&gt;🧠 BPE 分词算法背景简述&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Byte Pair Encoding (BPE)&lt;/strong&gt; 是一种基于频率的子词（subword）分词方法，常用于：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;GPT 系列（包括 OpenAI 模型）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;RoBERTa、DistilBERT、Qwen、LLaMA、BLOOM 等 HuggingFace 模型&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;适用于多语言、中文、代码等&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;merges-txt-文件作用&#34;&gt;📂 &lt;code&gt;**merges.txt**&lt;/code&gt; 文件作用&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;定义每一步合并哪些子词，形成新的词片段（token）&lt;/strong&gt;，即：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;从原始字符逐步构建更大的常见子词&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;保留高频组合、减少词表大小、提升表示能力&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;文件内容格式示例&#34;&gt;🧾 文件内容格式示例&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;#version: 0.2
t h
th e
he r
re s
es t
...

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每一行是一个 &lt;strong&gt;合并规则（merge rule）&lt;/strong&gt;，格式为两个空格分隔的子词（symbol）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-plaintext&#34;&gt;A B

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;表示 &lt;code&gt;A B → AB&lt;/code&gt; 的合并操作。模型在分词时会按这些规则顺序地将字符对合并成 token，直到无法合并为止。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;与-vocab-json-的关系&#34;&gt;🔄 与 vocab.json 的关系&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;文件&lt;/th&gt;
&lt;th&gt;作用&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;vocab.json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;保存最终的 token → id 映射（所有可用 token）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;merges.txt&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;保存 BPE 合并规则（从字符构造这些 token 的路径）&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
&lt;p&gt;二者共同定义了分词器的行为，缺一不可。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;应用举例-gpt-分词过程&#34;&gt;📦 应用举例：GPT 分词过程&lt;/h3&gt;

&lt;p&gt;假设原始文本是 &lt;code&gt;&amp;quot;there&amp;quot;&lt;/code&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;初始字符序列：&lt;code&gt;[&amp;quot;t&amp;quot;, &amp;quot;h&amp;quot;, &amp;quot;e&amp;quot;, &amp;quot;r&amp;quot;, &amp;quot;e&amp;quot;]&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;第一次匹配合并规则：&lt;code&gt;t h → th&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;变为：&lt;code&gt;[&amp;quot;th&amp;quot;, &amp;quot;e&amp;quot;, &amp;quot;r&amp;quot;, &amp;quot;e&amp;quot;]&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;然后：&lt;code&gt;th e → the&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;然后：&lt;code&gt;the r → ther&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;然后：&lt;code&gt;ther e → there&lt;/code&gt; → ✅ 完整 token&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;最终就是一个 token：&lt;code&gt;there&lt;/code&gt;，对应 &lt;code&gt;vocab.json&lt;/code&gt; 中的某个 ID。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;怎么查看&#34;&gt;🔍 怎么查看？&lt;/h3&gt;

&lt;p&gt;使用 HuggingFace Tokenizer API 查看分词器：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&amp;quot;Qwen/Qwen2-7B&amp;quot;)
print(tokenizer.tokenize(&amp;quot;there&amp;quot;))

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;常见用途&#34;&gt;🛠️ 常见用途&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;作用&lt;/th&gt;
&lt;th&gt;示例&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;✅ 自定义分词器&lt;/td&gt;
&lt;td&gt;用你自己的 &lt;code&gt;merges.txt&lt;/code&gt; + &lt;code&gt;vocab.json&lt;/code&gt; 创建分词器&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;🧪 分析模型分词习惯&lt;/td&gt;
&lt;td&gt;看哪些词被拆了，比如中文 &lt;code&gt;数据处理&lt;/code&gt; → &lt;code&gt;数&lt;/code&gt;, &lt;code&gt;据&lt;/code&gt;, &lt;code&gt;处理&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;📦 微调 tokenizer&lt;/td&gt;
&lt;td&gt;用 HuggingFace &lt;code&gt;tokenizers&lt;/code&gt; 工具重新训练 tokenizer&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;实战示例-创建自定义-bpe-分词器&#34;&gt;🧰 实战示例：创建自定义 BPE 分词器&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tokenizers import Tokenizer, models, trainers, pre_tokenizers

tokenizer = Tokenizer(models.BPE())
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

trainer = trainers.BpeTrainer(vocab_size=10000, special_tokens=[&amp;quot;&amp;lt;pad&amp;gt;&amp;quot;, &amp;quot;&amp;lt;unk&amp;gt;&amp;quot;])
tokenizer.train([&amp;quot;your_corpus.txt&amp;quot;], trainer)

tokenizer.save(&amp;quot;tokenizer.json&amp;quot;)  # 里面包含 vocab 和 merges

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;总结一句话&#34;&gt;✅ 总结一句话&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;merges.txt&lt;/code&gt; 是 HuggingFace BPE 分词器的“合并规则表”，告诉模型如何从字符组合成 token，它是 &lt;code&gt;vocab.json&lt;/code&gt; 的构造过程记录，二者配合用于模型输入的分词处理。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;3-4-model-00001-of-000xx-safetensors&#34;&gt;3.4 model-00001-of-000XX.safetensors&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;model-00001-of-000XX.safetensors&lt;/code&gt; 是 Hugging Face 或类似框架在保存大型模型权重时生成的文件之一。其中XX值代表有多少个类似的权重文件，这个文件的 &lt;strong&gt;作用是保存模型的实际权重（参数）数据&lt;/strong&gt;，是推理或训练时真正加载到模型中的核心内容。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;一句话概括&#34;&gt;📦 一句话概括&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;model-00001-of-000XX.safetensors&lt;/code&gt; 是保存 Transformer 模型参数的&lt;strong&gt;分片化 safetensors 文件&lt;/strong&gt;，它是 &lt;code&gt;.bin&lt;/code&gt; 的安全高效替代，用于多文件分布式加载大型模型。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;文件组成与命名结构&#34;&gt;🔍 文件组成与命名结构&lt;/h3&gt;

&lt;p&gt;比如你看到这样的目录结构：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;model-00001-of-00007.safetensors
model-00002-of-00007.safetensors
...
model-00007-of-00007.safetensors
model.safetensors.index.json
config.json
tokenizer.json
generation_config.json
...

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;含义如下：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;文件名&lt;/th&gt;
&lt;th&gt;含义&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;model-00001-of-00007.safetensors&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;模型参数的第 1 个分片，共 7 个&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;model.safetensors.index.json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;记录每个权重张量存储在哪个分片中（索引文件）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;config.json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;模型结构描述&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tokenizer.json&lt;/code&gt; / &lt;code&gt;merges.txt&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;分词器文件&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;generation_config.json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;文本生成策略配置&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;safetensors-格式简析&#34;&gt;🧠 safetensors 格式简析&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;safetensors&lt;/code&gt; 是一种由 Hugging Face 社区主导开发的&lt;strong&gt;安全、快速、零拷贝的模型权重保存格式&lt;/strong&gt;，用于替代早期的 &lt;code&gt;.bin&lt;/code&gt;（PyTorch pickle）格式。&lt;/p&gt;

&lt;h4 id=&#34;相比-bin-格式的优势&#34;&gt;✅ 相比 &lt;code&gt;**.bin**&lt;/code&gt; 格式的优势：&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;特性&lt;/th&gt;
&lt;th&gt;&lt;code&gt;.bin&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;&lt;code&gt;.safetensors&lt;/code&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;安全性（避免执行 pickle）&lt;/td&gt;
&lt;td&gt;❌ 可能执行恶意代码&lt;/td&gt;
&lt;td&gt;✅ 完全静态数据&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;加载速度（零拷贝 mmap）&lt;/td&gt;
&lt;td&gt;较慢&lt;/td&gt;
&lt;td&gt;✅ 极快&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;分片支持&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;通用性（兼容 PyTorch）&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;✅（HuggingFace 专用）&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;文件内部结构&#34;&gt;🔍 文件内部结构&lt;/h3&gt;

&lt;p&gt;你不能直接打开 &lt;code&gt;.safetensors&lt;/code&gt; 文件，但可以用 &lt;code&gt;safetensors&lt;/code&gt; 官方库读取其中内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from safetensors import safe_open

with safe_open(&amp;quot;model-00001-of-00007.safetensors&amp;quot;, framework=&amp;quot;pt&amp;quot;) as f:
    for name in f.keys():
        tensor = f.get_tensor(name)
        print(name, tensor.shape)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些 &lt;code&gt;name&lt;/code&gt; 就是模型的权重名（如：&lt;code&gt;transformer.h.0.attn.q_proj.weight&lt;/code&gt;），对应模型层。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;model-safetensors-index-json-作用&#34;&gt;📦 &lt;code&gt;**model.safetensors.index.json**&lt;/code&gt; 作用&lt;/h3&gt;

&lt;p&gt;这个 JSON 文件告诉框架：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;所有参数都有哪些&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;每个参数在哪个 &lt;code&gt;.safetensors&lt;/code&gt; 分片中&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;每个 tensor 的形状与数据类型&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;示例结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;metadata&amp;quot;: {
    &amp;quot;total_size&amp;quot;: 54123342123
  },
  &amp;quot;weight_map&amp;quot;: {
    &amp;quot;transformer.h.0.attn.q_proj.weight&amp;quot;: &amp;quot;model-00001-of-00007.safetensors&amp;quot;,
    &amp;quot;transformer.h.0.attn.k_proj.weight&amp;quot;: &amp;quot;model-00001-of-00007.safetensors&amp;quot;,
    ...
  }
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样框架就能在多个分片中找到每个参数的位置。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;如何加载含-safetensors-的模型&#34;&gt;🛠️ 如何加载含 &lt;code&gt;**.safetensors**&lt;/code&gt; 的模型？&lt;/h3&gt;

&lt;p&gt;使用 Hugging Face Transformers 会自动识别：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(&amp;quot;your_model_dir&amp;quot;, trust_remote_code=True)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;框架会：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;加载 &lt;code&gt;config.json&lt;/code&gt; 构建模型结构&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;加载 &lt;code&gt;safetensors.index.json&lt;/code&gt; 映射文件&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;从多个 &lt;code&gt;.safetensors&lt;/code&gt; 分片中加载权重&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;如果你要保存为-safetensors&#34;&gt;📤 如果你要保存为 &lt;code&gt;**.safetensors**&lt;/code&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM

model.save_pretrained(&amp;quot;your_dir&amp;quot;, safe_serialization=True)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它会自动生成：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;分片的 &lt;code&gt;model-xxxx-of-yyyy.safetensors&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;model.safetensors.index.json&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;你也可以设置 &lt;code&gt;max_shard_size&lt;/code&gt; 控制分片大小（如 &lt;code&gt;&amp;quot;2GB&amp;quot;&lt;/code&gt;）。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;总结&#34;&gt;✅ 总结&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;项目&lt;/th&gt;
&lt;th&gt;内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;文件名&lt;/td&gt;
&lt;td&gt;&lt;code&gt;model-00001-of-000XX.safetensors&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;作用&lt;/td&gt;
&lt;td&gt;存储 Transformer 模型参数的分片（权重）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;搭配使用&lt;/td&gt;
&lt;td&gt;&lt;code&gt;model.safetensors.index.json&lt;/code&gt; 提供索引&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;加载方式&lt;/td&gt;
&lt;td&gt;自动由 Hugging Face Transformers 识别&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;替代格式&lt;/td&gt;
&lt;td&gt;更快、更安全的 &lt;code&gt;.bin&lt;/code&gt; 替代品&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;应用场景&lt;/td&gt;
&lt;td&gt;大模型分布式权重管理，私有部署，安全模型分发&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;3-5-model-safetensors-index-json&#34;&gt;3.5 model.safetensors.index.json&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;model.safetensors.index.json&lt;/code&gt; 是 Hugging Face Transformers 在使用 &lt;code&gt;.safetensors&lt;/code&gt; 格式保存大模型权重时生成的 &lt;strong&gt;索引文件&lt;/strong&gt;，用于记录 &lt;strong&gt;每个模型参数（tensor）在哪个分片文件中&lt;/strong&gt;，让模型加载器能正确分布式加载多个 &lt;code&gt;.safetensors&lt;/code&gt; 文件。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;一句话总结&#34;&gt;🧠 一句话总结&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;model.safetensors.index.json&lt;/code&gt; 是多分片 &lt;code&gt;.safetensors&lt;/code&gt; 权重文件的&lt;strong&gt;目录映射表&lt;/strong&gt;，记录了每个权重张量的文件位置。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;为什么需要它&#34;&gt;📦 为什么需要它？&lt;/h3&gt;

&lt;p&gt;当模型很大（例如 7B/14B/70B），所有参数无法放进一个 &lt;code&gt;.safetensors&lt;/code&gt; 文件时，会被拆成多片，比如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-plaintext&#34;&gt;model-00001-of-00007.safetensors
model-00002-of-00007.safetensors
...
model-00007-of-00007.safetensors

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时就需要一个索引文件告诉你：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;每个 tensor 存在哪个分片文件里&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;所有权重一共多大&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;文件结构详解&#34;&gt;📄 文件结构详解&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
    &amp;quot;metadata&amp;quot;: {
        // 总大小（字节）
        &amp;quot;total_size&amp;quot;: 21011769344
    },
    &amp;quot;weight_map&amp;quot;: {
        // 嵌入层权重文件
        &amp;quot;model.embed_tokens.weight&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,
        // 第0层自注意力q_proj的权重文件
        &amp;quot;model.layers.0.self_attn.q_proj.qweight&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,
        &amp;quot;model.layers.0.self_attn.q_proj.qzeros&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,
        &amp;quot;model.layers.0.self_attn.q_proj.scales&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,
        &amp;quot;model.layers.0.self_attn.q_proj.bias&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个典型的 &lt;code&gt;model.safetensors.index.json&lt;/code&gt; 文件内容如下（已格式化）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;metadata&amp;quot;: {
    &amp;quot;total_size&amp;quot;: 13789162534  // 所有分片的总大小（字节数）
  },
  &amp;quot;weight_map&amp;quot;: {
    &amp;quot;model.embed_tokens.weight&amp;quot;: &amp;quot;model-00001-of-00007.safetensors&amp;quot;,
    &amp;quot;model.layers.0.self_attn.q_proj.weight&amp;quot;: &amp;quot;model-00001-of-00007.safetensors&amp;quot;,
    &amp;quot;model.layers.0.self_attn.k_proj.weight&amp;quot;: &amp;quot;model-00001-of-00007.safetensors&amp;quot;,
    ...
    &amp;quot;lm_head.weight&amp;quot;: &amp;quot;model-00007-of-00007.safetensors&amp;quot;
  }
}

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;字段解释&#34;&gt;🧩 字段解释&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;字段&lt;/th&gt;
&lt;th&gt;类型&lt;/th&gt;
&lt;th&gt;含义&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metadata.total_size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;所有权重总大小（字节数）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;weight_map&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;dict&lt;/td&gt;
&lt;td&gt;key 为参数名，value 为对应的分片文件名&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;&amp;quot;model.layers.42.mlp.down_proj.weight&amp;quot;: &amp;quot;model-00006-of-00007.safetensors&amp;quot;

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;表示这一层的参数在第 6 个权重分片中。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;使用方式&#34;&gt;🛠️ 使用方式&lt;/h3&gt;

&lt;p&gt;当你用 Transformers 的 &lt;code&gt;.from_pretrained()&lt;/code&gt; 加载 safetensors 权重时，它会：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;读取 &lt;code&gt;config.json&lt;/code&gt; 建好模型结构&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;检查是否有 &lt;code&gt;model.safetensors.index.json&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;根据 &lt;code&gt;weight_map&lt;/code&gt; 去每个分片 &lt;code&gt;.safetensors&lt;/code&gt; 文件加载参数&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;你无需手动操作。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(&amp;quot;path/to/model&amp;quot;, trust_remote_code=True)

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;自定义用法-高级&#34;&gt;✏️ 自定义用法（高级）&lt;/h3&gt;

&lt;p&gt;你可以修改或生成自己的 &lt;code&gt;model.safetensors.index.json&lt;/code&gt;，例如：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;加载部分权重&lt;/strong&gt;：比如只加载 encoder 层用于微调&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;重新分片&lt;/strong&gt;：将一个大文件拆成多个分片&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;工具包支持：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from safetensors.torch import load_file
from transformers import PreTrainedModel

# 可读取特定 safetensors 文件
tensors = load_file(&amp;quot;model-00001-of-00007.safetensors&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;补充说明&#34;&gt;📌 补充说明&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;.index.json&lt;/code&gt; 是 &lt;strong&gt;只读辅助文件&lt;/strong&gt;，不包含参数值本身&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;不可缺失&lt;/strong&gt;：如果有多个分片，必须存在此文件，否则加载会报错&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对应 &lt;code&gt;.bin&lt;/code&gt; 的同类文件叫 &lt;code&gt;pytorch_model.bin.index.json&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;总结-1&#34;&gt;✅ 总结&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;项目&lt;/th&gt;
&lt;th&gt;内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;文件名&lt;/td&gt;
&lt;td&gt;&lt;code&gt;model.safetensors.index.json&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;作用&lt;/td&gt;
&lt;td&gt;记录每个模型参数在哪个 &lt;code&gt;.safetensors&lt;/code&gt; 分片中&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;用于&lt;/td&gt;
&lt;td&gt;自动多分片加载、分布式部署、节省内存&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;典型配套文件&lt;/td&gt;
&lt;td&gt;&lt;code&gt;model-00001-of-00007.safetensors&lt;/code&gt; 等多个分片&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;自动处理&lt;/td&gt;
&lt;td&gt;Transformers 框架会自动解析并加载&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;3-6-readme-md&#34;&gt;3.6 README.md&lt;/h2&gt;

&lt;p&gt;在 Hugging Face 模型目录中（如 &lt;code&gt;huggingface.co/你的模型名/&lt;/code&gt;），&lt;code&gt;README.md&lt;/code&gt; 的作用非常重要，它不仅是模型介绍文档，还直接影响你在 Hugging Face 网页上看到的模型主页内容。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;一句话总结-1&#34;&gt;🧠 一句话总结&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;README.md&lt;/code&gt; 是 Hugging Face 模型仓库的&lt;strong&gt;可视化主页说明文档&lt;/strong&gt;，用于展示模型的介绍、用法、性能、许可证等信息。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;典型结构与作用&#34;&gt;📄 典型结构与作用&lt;/h3&gt;

&lt;p&gt;一个标准的 &lt;code&gt;README.md&lt;/code&gt; 通常包含如下部分：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;---
license: other
license_name: tongyi-qianwen
license_link: &amp;gt;-
  https://huggingface.co/Qwen/Qwen1.5-32B-Chat-AWQ/blob/main/LICENSE
language:
- en
pipeline_tag: text-generation
tags:
- chat
---

# Qwen1.5-32B-Chat-AWQ


## Introduction

Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen, the improvements include: 

* 8 model sizes, including 0.5B, 1.8B, 4B, 7B, 14B, 32B and 72B dense models, and an MoE model of 14B with 2.7B activated;
* Significant performance improvement in human preference for chat models;
* Multilingual support of both base and chat models;
* Stable support of 32K context length for models of all sizes
* No need of `trust_remote_code`.

For more details, please refer to our [blog post](https://qwenlm.github.io/blog/qwen1.5/) and [GitHub repo](https://github.com/QwenLM/Qwen1.5).
&amp;lt;br&amp;gt;

## Model Details
Qwen1.5 is a language model series including decoder language models of different model sizes. For each size, we release the base language model and the aligned chat model. It is based on the Transformer architecture with SwiGLU activation, attention QKV bias, group query attention, mixture of sliding window attention and full attention, etc. Additionally, we have an improved tokenizer adaptive to multiple natural languages and codes. For the beta version, temporarily we did not include GQA (except for 32B) and the mixture of SWA and full attention.

## Training details
We pretrained the models with a large amount of data, and we post-trained the models with both supervised finetuning and direct preference optimization.

## Requirements
The code of Qwen1.5 has been in the latest Hugging face transformers and we advise you to install `transformers&amp;gt;=4.37.0`, or you might encounter the following error:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;KeyError: &amp;lsquo;qwen2&amp;rsquo;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
## Quickstart

Here provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
device = &amp;quot;cuda&amp;quot; # the device to load the model onto

model = AutoModelForCausalLM.from_pretrained(
    &amp;quot;Qwen/Qwen1.5-32B-Chat-AWQ&amp;quot;,
    torch_dtype=&amp;quot;auto&amp;quot;,
    device_map=&amp;quot;auto&amp;quot;
)
tokenizer = AutoTokenizer.from_pretrained(&amp;quot;Qwen/Qwen1.5-32B-Chat-AWQ&amp;quot;)

prompt = &amp;quot;Give me a short introduction to large language model.&amp;quot;
messages = [
    {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;You are a helpful assistant.&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors=&amp;quot;pt&amp;quot;).to(device)

generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;tips&#34;&gt;Tips&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;If you encounter code switching or other bad cases, we advise you to use our provided hyper-parameters in &lt;code&gt;generation_config.json&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;If you find our work helpful, feel free to give us a cite.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@article{qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;
#### 1. 模型简介

```markdown
# Qwen2-7B-Chat

Qwen2 是阿里达摩院发布的新一代大语言模型，支持中英文，具备对话能力。

&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;简明扼要介绍模型名称、来源、能力、适用场景。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;2-模型卡元数据-yaml-header&#34;&gt;2. 模型卡元数据（YAML Header）&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;---
license: apache-2.0
language:
  - zh
  - en
tags:
  - chat
  - qwen2
pipeline_tag: text-generation
---

&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;这段 YAML 元数据是 Hugging Face “模型卡”规范的一部分，供网页渲染和搜索使用。常见字段包括：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;字段&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;license&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;模型许可证类型（如 apache-2.0、mit、openrail）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;language&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;支持的语言&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tags&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;模型标签，影响推荐和搜索&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pipeline_tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;用于展示推理 Widget 的模型类型（如 &lt;code&gt;text-generation&lt;/code&gt;, &lt;code&gt;text-classification&lt;/code&gt;）&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;3-模型使用方式-代码示例&#34;&gt;3. 模型使用方式（代码示例）&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;## 使用方法

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(&amp;quot;Qwen/Qwen2-7B-Chat&amp;quot;)
model = AutoModelForCausalLM.from_pretrained(&amp;quot;Qwen/Qwen2-7B-Chat&amp;quot;)

inputs = tokenizer(&amp;quot;你好，请介绍一下你自己。&amp;quot;, return_tensors=&amp;quot;pt&amp;quot;)
outputs = model.generate(**inputs)
print(tokenizer.decode(outputs[0]))

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-plaintext&#34;&gt;
&amp;gt; 展示如何加载和使用模型，通常使用 `transformers` 接口，帮助用户快速上手。

---

### 4. 模型细节（结构、训练语料、推理设置）

```markdown
## 模型结构

- 参数量：7B
- 层数：32
- 支持上下文长度：32K
- 训练数据：中英混合、网络文本、代码等

&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;有助于用户了解模型能力与边界。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;5-性能评估-benchmark-可选&#34;&gt;5. 性能评估 / Benchmark（可选）&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;## Benchmark

| Dataset | Score |
|---------|-------|
| MMLU    | 68.3  |
| CMMLU   | 56.1  |

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;6-许可与责任声明&#34;&gt;6. 许可与责任声明&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;## 许可证

本模型采用 Apache 2.0 协议，可商用使用。

## 注意事项

本模型为研究用途，不对生成内容负责。请勿用于违法行为。

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;如何影响-hugging-face-页面&#34;&gt;💻 如何影响 Hugging Face 页面？&lt;/h3&gt;

&lt;p&gt;上传的 &lt;code&gt;README.md&lt;/code&gt; 会被渲染为 Hugging Face 模型页的主要内容部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;位于模型卡最上方&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;显示推理示例、标签、语言支持、下载入口等&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;决定了用户是否信任、理解、使用你的模型&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;👉 可以理解为：&lt;strong&gt;它既是文档，又是营销页！&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;如何编辑-上传&#34;&gt;🛠️ 如何编辑 / 上传？&lt;/h3&gt;

&lt;h4 id=&#34;方法-1-手动创建&#34;&gt;方法 1：手动创建&lt;/h4&gt;

&lt;p&gt;将 &lt;code&gt;README.md&lt;/code&gt; 放入模型目录中，调用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli upload ./your_model --repo-id yourname/yourmodel

&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;方法-2-在线编辑&#34;&gt;方法 2：在线编辑&lt;/h4&gt;

&lt;p&gt;在模型页面右上角点击 ✏️「Edit model card」，在线修改 &lt;code&gt;README.md&lt;/code&gt;。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;总结-2&#34;&gt;✅ 总结&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;项目&lt;/th&gt;
&lt;th&gt;内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;文件名&lt;/td&gt;
&lt;td&gt;&lt;code&gt;README.md&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;位置&lt;/td&gt;
&lt;td&gt;模型目录根路径&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;作用&lt;/td&gt;
&lt;td&gt;显示在 Hugging Face 模型主页，提供模型介绍、使用方法、元数据&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;类型&lt;/td&gt;
&lt;td&gt;Markdown 文本，支持代码块、表格、图片等&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;与谁配套&lt;/td&gt;
&lt;td&gt;&lt;code&gt;config.json&lt;/code&gt;（结构）、&lt;code&gt;safetensors&lt;/code&gt;（权重）、&lt;code&gt;tokenizer.json&lt;/code&gt;（分词器）&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;3-7-tokenizer-config-json&#34;&gt;3.7 tokenizer_config.json&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;tokenizer_config.json&lt;/code&gt; 是 Hugging Face Transformers 模型目录中的重要配置文件，它用于&lt;strong&gt;描述 tokenizer 的运行时行为和元信息&lt;/strong&gt;，对加载、兼容性、推理行为等都有影响。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;一句话理解&#34;&gt;🧠 一句话理解&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;tokenizer_config.json&lt;/code&gt; 是告诉 &lt;code&gt;AutoTokenizer&lt;/code&gt;（或其它 Tokenizer 类）&lt;strong&gt;如何正确加载和使用分词器&lt;/strong&gt;的说明书。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;文件位置&#34;&gt;📦 文件位置&lt;/h3&gt;

&lt;p&gt;在 Hugging Face 模型目录结构中，通常与下列文件位于同一目录下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-plaintext&#34;&gt;- config.json
- tokenizer_config.json
- tokenizer.json
- merges.txt / vocab.json
- special_tokens_map.json
- model.safetensors
- README.md

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;典型内容示例&#34;&gt;🧾 典型内容示例&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  // 是否在分词时添加前缀空格
  &amp;quot;add_prefix_space&amp;quot;: false,
  // 特殊token的解码器
  &amp;quot;added_tokens_decoder&amp;quot;: {
    // &amp;lt;|endoftext|&amp;gt; 结束文本的特殊token
    &amp;quot;151643&amp;quot;: {
      &amp;quot;content&amp;quot;: &amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;,
      &amp;quot;lstrip&amp;quot;: false,
      &amp;quot;normalized&amp;quot;: false,
      &amp;quot;rstrip&amp;quot;: false,
      &amp;quot;single_word&amp;quot;: false,
      &amp;quot;special&amp;quot;: true
    },
    // &amp;lt;|im_start|&amp;gt; 对话开始的特殊token
    &amp;quot;151644&amp;quot;: {
      &amp;quot;content&amp;quot;: &amp;quot;&amp;lt;|im_start|&amp;gt;&amp;quot;,
      &amp;quot;lstrip&amp;quot;: false,
      &amp;quot;normalized&amp;quot;: false,
      &amp;quot;rstrip&amp;quot;: false,
      &amp;quot;single_word&amp;quot;: false,
      &amp;quot;special&amp;quot;: true
    },
    // &amp;lt;|im_end|&amp;gt; 对话结束的特殊token
    &amp;quot;151645&amp;quot;: {
      &amp;quot;content&amp;quot;: &amp;quot;&amp;lt;|im_end|&amp;gt;&amp;quot;,
      &amp;quot;lstrip&amp;quot;: false,
      &amp;quot;normalized&amp;quot;: false,
      &amp;quot;rstrip&amp;quot;: false,
      &amp;quot;single_word&amp;quot;: false,
      &amp;quot;special&amp;quot;: true
    }
  },
  // 额外的特殊token列表
  &amp;quot;additional_special_tokens&amp;quot;: [],
  // 句子起始token（未设置）
  &amp;quot;bos_token&amp;quot;: null,
  // 聊天模板，定义消息格式
  &amp;quot;chat_template&amp;quot;: &amp;quot;{% for message in messages %}{% if loop.first and messages[0][&#39;role&#39;] != &#39;system&#39; %}{{ &#39;&amp;lt;|im_start|&amp;gt;system\nYou are a helpful assistant.&amp;lt;|im_end|&amp;gt;\n&#39; }}{% endif %}{{&#39;&amp;lt;|im_start|&amp;gt;&#39; + message[&#39;role&#39;] + &#39;\n&#39; + message[&#39;content&#39;] + &#39;&amp;lt;|im_end|&amp;gt;&#39; + &#39;\n&#39;}}{% endfor %}{% if add_generation_prompt %}{{ &#39;&amp;lt;|im_start|&amp;gt;assistant\n&#39; }}{% endif %}&amp;quot;,
  // 是否清理分词空格
  &amp;quot;clean_up_tokenization_spaces&amp;quot;: false,
  // 句子结束token
  &amp;quot;eos_token&amp;quot;: &amp;quot;&amp;lt;|im_end|&amp;gt;&amp;quot;,
  // 错误处理方式
  &amp;quot;errors&amp;quot;: &amp;quot;replace&amp;quot;,
  // 模型最大长度
  &amp;quot;model_max_length&amp;quot;: 32768,
  // 填充token
  &amp;quot;pad_token&amp;quot;: &amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;,
  // 是否分割特殊token
  &amp;quot;split_special_tokens&amp;quot;: false,
  // 分词器类名
  &amp;quot;tokenizer_class&amp;quot;: &amp;quot;Qwen2Tokenizer&amp;quot;,
  // 未知token（未设置）
  &amp;quot;unk_token&amp;quot;: null
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以一个用于文本生成的 tokenizer 为例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;add_bos_token&amp;quot;: true,
  &amp;quot;add_eos_token&amp;quot;: false,
  &amp;quot;bos_token&amp;quot;: &amp;quot;&amp;lt;s&amp;gt;&amp;quot;,
  &amp;quot;eos_token&amp;quot;: &amp;quot;&amp;lt;/s&amp;gt;&amp;quot;,
  &amp;quot;unk_token&amp;quot;: &amp;quot;&amp;lt;unk&amp;gt;&amp;quot;,
  &amp;quot;pad_token&amp;quot;: &amp;quot;&amp;lt;pad&amp;gt;&amp;quot;,
  &amp;quot;cls_token&amp;quot;: &amp;quot;&amp;lt;s&amp;gt;&amp;quot;,
  &amp;quot;sep_token&amp;quot;: &amp;quot;&amp;lt;/s&amp;gt;&amp;quot;,
  &amp;quot;do_lower_case&amp;quot;: false,
  &amp;quot;model_max_length&amp;quot;: 2048,
  &amp;quot;tokenizer_class&amp;quot;: &amp;quot;LlamaTokenizer&amp;quot;,
  &amp;quot;auto_map&amp;quot;: {
    &amp;quot;AutoTokenizer&amp;quot;: [&amp;quot;tokenization_llama.LlamaTokenizer&amp;quot;, null]
  },
  &amp;quot;use_fast&amp;quot;: false
}

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;字段说明&#34;&gt;🔍 字段说明&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;字段名&lt;/th&gt;
&lt;th&gt;类型&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;add_bos_token&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;是否在输入前添加 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 开头 token（通常用于 decoder-only 模型）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;add_eos_token&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;是否自动在输入后添加 &lt;code&gt;&amp;lt;/s&amp;gt;&lt;/code&gt;（终止符）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;bos_token&lt;/code&gt;, &lt;code&gt;eos_token&lt;/code&gt;, &lt;code&gt;unk_token&lt;/code&gt;, &lt;code&gt;pad_token&lt;/code&gt;, &lt;code&gt;cls_token&lt;/code&gt;, &lt;code&gt;sep_token&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;特殊标记符定义，影响解码和处理&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;do_lower_case&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;是否将输入强制小写（常用于 BERT）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;model_max_length&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;默认支持的最大序列长度（超出时会报错或截断）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tokenizer_class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;指定用哪个 Tokenizer 类加载（如 &lt;code&gt;LlamaTokenizer&lt;/code&gt;, &lt;code&gt;GPT2Tokenizer&lt;/code&gt;）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;auto_map&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;自定义 Tokenizer 类映射（用于私有 tokenizer 的注册）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;use_fast&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;是否默认启用 &lt;code&gt;FastTokenizer&lt;/code&gt;（Rust 实现，更快）&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;和其它文件的关系&#34;&gt;🚀 和其它文件的关系&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;文件&lt;/th&gt;
&lt;th&gt;作用&lt;/th&gt;
&lt;th&gt;相互关系&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tokenizer.json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;主要的序列化分词器模型&lt;/td&gt;
&lt;td&gt;&lt;code&gt;tokenizer_config.json&lt;/code&gt; 指定如何加载它&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;vocab.json&lt;/code&gt;, &lt;code&gt;merges.txt&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;适用于 BPE tokenizer（如 GPT-2）&lt;/td&gt;
&lt;td&gt;低层词表配置文件&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;special_tokens_map.json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;映射特殊 token 到实际字符串（例如 &lt;code&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 对应 ID）&lt;/td&gt;
&lt;td&gt;可与本文件内容重叠，但也可独立覆盖&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;config.json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;模型结构（层数、头数等）&lt;/td&gt;
&lt;td&gt;和 tokenizer 配套使用，但用途不同&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;示例-加载方式受-tokenizer-config-json-影响&#34;&gt;✅ 示例：加载方式受 &lt;code&gt;**tokenizer_config.json**&lt;/code&gt; 影响&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&amp;quot;Qwen/Qwen2-7B-Chat&amp;quot;)

# 会优先读取 tokenizer_config.json 中的信息
# 决定是否加载 fast tokenizer、默认特殊符号、是否添加 bos_token 等

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;编辑或自定义使用场景&#34;&gt;🛠️ 编辑或自定义使用场景&lt;/h3&gt;

&lt;p&gt;你可能会想修改它的场景包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;为 tokenizer 添加 pad_token 或 eos_token&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;调整 &lt;code&gt;model_max_length&lt;/code&gt; 以适配 longer context&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;替换 tokenizer 类名（如加载自定义的分词器逻辑）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;禁用 fast tokenizer（当 fast 模型存在 bug）&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;pad_token&amp;quot;: &amp;quot;&amp;lt;|pad|&amp;gt;&amp;quot;,
  &amp;quot;model_max_length&amp;quot;: 4096,
  &amp;quot;tokenizer_class&amp;quot;: &amp;quot;MyCustomTokenizer&amp;quot;,
  &amp;quot;use_fast&amp;quot;: false
}

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;总结-3&#34;&gt;总结 ✅&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;功能&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;核心作用&lt;/td&gt;
&lt;td&gt;告诉 &lt;code&gt;AutoTokenizer&lt;/code&gt; 如何加载、配置、初始化&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;关键字段&lt;/td&gt;
&lt;td&gt;特殊符号、是否 lower-case、最大长度、fast/slow 模式&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;和其它文件关系&lt;/td&gt;
&lt;td&gt;配合 &lt;code&gt;tokenizer.json&lt;/code&gt; 和 &lt;code&gt;special_tokens_map.json&lt;/code&gt; 等文件共同使用&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;是否可选&lt;/td&gt;
&lt;td&gt;是，但缺失时会采用默认推断逻辑，可能导致不一致行为&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;3-8-tokenizer-json&#34;&gt;3.8 tokenizer.json&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;tokenizer.json&lt;/code&gt; 是 Hugging Face Transformers 分词器目录中最核心的文件之一，它是&lt;strong&gt;Tokenizer 的完整序列化文件&lt;/strong&gt;，包括词表、预处理规则、正则、特殊 token 映射、分词算法逻辑等。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;一句话理解-1&#34;&gt;🧠 一句话理解&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;tokenizer.json&lt;/code&gt; 是 &lt;strong&gt;tokenizer 的所有行为和词表信息的统一打包版本&lt;/strong&gt;，用于快速加载和复现 tokenizer 行为，尤其在使用 &lt;code&gt;fast tokenizer&lt;/code&gt; 时。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;它和其它文件有何区别&#34;&gt;🔍 它和其它文件有何区别？&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;文件&lt;/th&gt;
&lt;th&gt;作用&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tokenizer_config.json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;参数设置&lt;/td&gt;
&lt;td&gt;控制 tokenizer 运行行为（如是否加 bos_token）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tokenizer.json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;主体&lt;/td&gt;
&lt;td&gt;保存完整 fast tokenizer 定义，包括词表、merge、pretokenizer 等&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;vocab.json&lt;/code&gt; + &lt;code&gt;merges.txt&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;GPT/BPE 模型词表&lt;/td&gt;
&lt;td&gt;用于 slow tokenizer 的构建&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;special_tokens_map.json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;映射&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;lt;pad&amp;gt;&lt;/code&gt;、&lt;code&gt;&amp;lt;eos&amp;gt;&lt;/code&gt; 这些符号的实际字符串/ID&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;code&gt;tokenizer.json&lt;/code&gt; 是 Fast Tokenizer（Rust 实现）专用文件，可以&lt;strong&gt;替代 vocab.json 和 merges.txt&lt;/strong&gt;，因此更快、更便携。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;典型结构内容&#34;&gt;📦 典型结构内容&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;shengping.mo@y108p30:/data2/huggingface/Qwen/Qwen1.5-32B-Chat-AWQ$ head  -n 100 tokenizer.json 
{
  &amp;quot;version&amp;quot;: &amp;quot;1.0&amp;quot;,
  &amp;quot;truncation&amp;quot;: null, // 截断设置
  &amp;quot;padding&amp;quot;: null,    // 填充设置
  &amp;quot;added_tokens&amp;quot;: [   // 新增的特殊token
    {
      &amp;quot;id&amp;quot;: 151643,
      &amp;quot;content&amp;quot;: &amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;, // 文本结束标记
      &amp;quot;single_word&amp;quot;: false,
      &amp;quot;lstrip&amp;quot;: false,
      &amp;quot;rstrip&amp;quot;: false,
      &amp;quot;normalized&amp;quot;: false,
      &amp;quot;special&amp;quot;: true
    },
    {
      &amp;quot;id&amp;quot;: 151644,
      &amp;quot;content&amp;quot;: &amp;quot;&amp;lt;|im_start|&amp;gt;&amp;quot;, // 对话开始标记
      &amp;quot;single_word&amp;quot;: false,
      &amp;quot;lstrip&amp;quot;: false,
      &amp;quot;rstrip&amp;quot;: false,
      &amp;quot;normalized&amp;quot;: false,
      &amp;quot;special&amp;quot;: true
    },
    {
      &amp;quot;id&amp;quot;: 151645,
      &amp;quot;content&amp;quot;: &amp;quot;&amp;lt;|im_end|&amp;gt;&amp;quot;, // 对话结束标记
      &amp;quot;single_word&amp;quot;: false,
      &amp;quot;lstrip&amp;quot;: false,
      &amp;quot;rstrip&amp;quot;: false,
      &amp;quot;normalized&amp;quot;: false,
      &amp;quot;special&amp;quot;: true
    }
  ],
  &amp;quot;normalizer&amp;quot;: {
    &amp;quot;type&amp;quot;: &amp;quot;NFC&amp;quot; // 归一化类型
  },
  &amp;quot;pre_tokenizer&amp;quot;: { // 分词前处理
    &amp;quot;type&amp;quot;: &amp;quot;Sequence&amp;quot;,
    &amp;quot;pretokenizers&amp;quot;: [
      {
        &amp;quot;type&amp;quot;: &amp;quot;Split&amp;quot;,
        &amp;quot;pattern&amp;quot;: {
          &amp;quot;Regex&amp;quot;: &amp;quot;(?i:&#39;s|&#39;t|&#39;re|&#39;ve|&#39;m|&#39;ll|&#39;d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+&amp;quot;
        },
        &amp;quot;behavior&amp;quot;: &amp;quot;Isolated&amp;quot;,
        &amp;quot;invert&amp;quot;: false
      },
      {
        &amp;quot;type&amp;quot;: &amp;quot;ByteLevel&amp;quot;,
        &amp;quot;add_prefix_space&amp;quot;: false,
        &amp;quot;trim_offsets&amp;quot;: false,
        &amp;quot;use_regex&amp;quot;: false
      }
    ]
  },
  &amp;quot;post_processor&amp;quot;: { // 分词后处理
    &amp;quot;type&amp;quot;: &amp;quot;ByteLevel&amp;quot;,
    &amp;quot;add_prefix_space&amp;quot;: false,
    &amp;quot;trim_offsets&amp;quot;: false,
    &amp;quot;use_regex&amp;quot;: false
  },
  &amp;quot;decoder&amp;quot;: { // 解码器
    &amp;quot;type&amp;quot;: &amp;quot;ByteLevel&amp;quot;,
    &amp;quot;add_prefix_space&amp;quot;: false,
    &amp;quot;trim_offsets&amp;quot;: false,
    &amp;quot;use_regex&amp;quot;: false
  },
  &amp;quot;model&amp;quot;: { // 模型相关配置
    &amp;quot;type&amp;quot;: &amp;quot;BPE&amp;quot;,
    &amp;quot;dropout&amp;quot;: null,
    &amp;quot;unk_token&amp;quot;: null,
    &amp;quot;continuing_subword_prefix&amp;quot;: &amp;quot;&amp;quot;,
    &amp;quot;end_of_word_suffix&amp;quot;: &amp;quot;&amp;quot;,
    &amp;quot;fuse_unk&amp;quot;: false,
    &amp;quot;byte_fallback&amp;quot;: false,
    &amp;quot;vocab&amp;quot;: { // 词表
      &amp;quot;!&amp;quot;: 0,
      &amp;quot;\&amp;quot;&amp;quot;: 1,
      &amp;quot;#&amp;quot;: 2,
      &amp;quot;$&amp;quot;: 3,
      &amp;quot;%&amp;quot;: 4,
      &amp;quot;&amp;amp;&amp;quot;: 5,
      &amp;quot;&#39;&amp;quot;: 6,
      &amp;quot;(&amp;quot;: 7,
      &amp;quot;)&amp;quot;: 8,
      &amp;quot;*&amp;quot;: 9,
      &amp;quot;+&amp;quot;: 10,
      &amp;quot;,&amp;quot;: 11,
      &amp;quot;-&amp;quot;: 12,
      &amp;quot;.&amp;quot;: 13,
      &amp;quot;/&amp;quot;: 14,
      &amp;quot;0&amp;quot;: 15,
      &amp;quot;1&amp;quot;: 16,
      &amp;quot;2&amp;quot;: 17,
      &amp;quot;3&amp;quot;: 18,
      &amp;quot;4&amp;quot;: 19,
      &amp;quot;5&amp;quot;: 20,
      &amp;quot;6&amp;quot;: 21,
      &amp;quot;7&amp;quot;: 22,
      &amp;quot;8&amp;quot;: 23,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个 &lt;code&gt;tokenizer.json&lt;/code&gt; 是一个大型 JSON 文件，通常包含以下字段：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-jsonc&#34;&gt;{
  &amp;quot;version&amp;quot;: &amp;quot;1.0&amp;quot;,
  &amp;quot;truncation&amp;quot;: { ... },
  &amp;quot;padding&amp;quot;: { ... },
  &amp;quot;added_tokens&amp;quot;: [ ... ],
  &amp;quot;normalizer&amp;quot;: { ... },
  &amp;quot;pre_tokenizer&amp;quot;: { ... },
  &amp;quot;post_processor&amp;quot;: { ... },
  &amp;quot;decoder&amp;quot;: { ... },
  &amp;quot;model&amp;quot;: {
    &amp;quot;type&amp;quot;: &amp;quot;BPE&amp;quot;,
    &amp;quot;vocab&amp;quot;: {
      &amp;quot;the&amp;quot;: 0,
      &amp;quot;a&amp;quot;: 1,
      ...
    },
    &amp;quot;merges&amp;quot;: [
      [&amp;quot;t&amp;quot;, &amp;quot;h&amp;quot;],
      [&amp;quot;th&amp;quot;, &amp;quot;e&amp;quot;],
      ...
    ]
  }
}

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;关键字段解释&#34;&gt;🔧 关键字段解释&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;字段名&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;model&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;分词模型结构，如 &lt;code&gt;BPE&lt;/code&gt; / &lt;code&gt;WordPiece&lt;/code&gt;，含词表和合并规则&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;vocab&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;单词到 token ID 的映射&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;merges&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;合并规则（仅用于 BPE）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;normalizer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;正规化规则，如 Unicode 规范化、lowercasing 等&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pre_tokenizer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;分词前的处理，如空格拆分、正则&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;post_processor&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;分词后的处理，如添加 &lt;code&gt;&amp;lt;bos&amp;gt;&lt;/code&gt;、&lt;code&gt;&amp;lt;/s&amp;gt;&lt;/code&gt; 等特殊 token&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;decoder&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;将 token ID 序列还原为字符串&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;added_tokens&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;用户添加的特殊 token，比如 &lt;code&gt;&amp;lt;user&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;bot&amp;gt;&lt;/code&gt; 等&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;padding&lt;/code&gt; / &lt;code&gt;truncation&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;padding 和截断规则，决定输入对齐策略&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;加载示例&#34;&gt;✅ 加载示例&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import PreTrainedTokenizerFast

tokenizer = PreTrainedTokenizerFast(tokenizer_file=&amp;quot;path/to/tokenizer.json&amp;quot;)
print(tokenizer.tokenize(&amp;quot;你好，世界！&amp;quot;))

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以使用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tokenizer = AutoTokenizer.from_pretrained(&amp;quot;模型路径或Hub名&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;背后就是调用 &lt;code&gt;tokenizer.json&lt;/code&gt; 加载逻辑。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;应用场景&#34;&gt;🚀 应用场景&lt;/h3&gt;

&lt;h4 id=&#34;使用-rust-实现的-fast-tokenizer-默认-hugging-face-会优先加载-tokenizer-json&#34;&gt;✅ 使用 Rust 实现的 fast tokenizer（默认 Hugging Face 会优先加载 &lt;code&gt;**tokenizer.json**&lt;/code&gt;）&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;更快的批量 tokenization&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;支持 pretokenizer + postprocessor 全流程定制&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;更容易部署（打包成 ONNX 或导出）&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;自定义-tokenizer-时生成这个文件&#34;&gt;✅ 自定义 tokenizer 时生成这个文件：&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tokenizers import Tokenizer, trainers, models

# 训练 BPE 模型
tokenizer = Tokenizer(models.BPE())
# ...
tokenizer.train(...)  # 自定义训练
tokenizer.save(&amp;quot;tokenizer.json&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;和-fast-tokenizer-的关系&#34;&gt;📝 和 fast tokenizer 的关系&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;特性&lt;/th&gt;
&lt;th&gt;slow tokenizer&lt;/th&gt;
&lt;th&gt;fast tokenizer (&lt;code&gt;tokenizer.json&lt;/code&gt;)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;加载速度&lt;/td&gt;
&lt;td&gt;慢&lt;/td&gt;
&lt;td&gt;快（Rust 实现）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;是否需要 vocab/merges&lt;/td&gt;
&lt;td&gt;是&lt;/td&gt;
&lt;td&gt;不需要（已合并到 tokenizer.json）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;多语言支持&lt;/td&gt;
&lt;td&gt;有&lt;/td&gt;
&lt;td&gt;更强&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;正则/pipeline 细节可配置性&lt;/td&gt;
&lt;td&gt;较弱&lt;/td&gt;
&lt;td&gt;很强（支持 pretokenizer/normalizer/postprocessor）&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;总结-4&#34;&gt;总结 ✅&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;属性&lt;/th&gt;
&lt;th&gt;内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;文件类型&lt;/td&gt;
&lt;td&gt;JSON&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;用途&lt;/td&gt;
&lt;td&gt;描述 tokenizer 全部行为，用于 fast tokenizer 加载&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;是否必须&lt;/td&gt;
&lt;td&gt;不是必须，但存在时优先使用&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;和 vocab/merges 的关系&lt;/td&gt;
&lt;td&gt;可以替代，功能更完整&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;修改方式&lt;/td&gt;
&lt;td&gt;可通过 &lt;code&gt;tokenizers&lt;/code&gt; 库创建或修改&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;建议&lt;/td&gt;
&lt;td&gt;模型训练或部署推荐使用 &lt;code&gt;tokenizer.json&lt;/code&gt;，尤其在需要高性能时&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;3-9-vocab-json&#34;&gt;3.9 vocab.json&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;vocab.json&lt;/code&gt; 是 Hugging Face Tokenizer 模型中用于存储**词汇表（vocabulary）**的文件，常见于基于 BPE（Byte Pair Encoding）或 WordPiece 等子词分词算法的模型，例如 GPT、BERT、Roberta、LLaMA 等。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;一句话理解-2&#34;&gt;✅ 一句话理解&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;vocab.json&lt;/code&gt; 定义了「&lt;strong&gt;token 到 ID 的映射关系&lt;/strong&gt;」，是分词器的核心文件之一。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;文件结构示例&#34;&gt;📦 文件结构示例&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;the&amp;quot;: 0,
  &amp;quot;Ġthe&amp;quot;: 1,
  &amp;quot;a&amp;quot;: 2,
  &amp;quot;Ġa&amp;quot;: 3,
  &amp;quot;apple&amp;quot;: 1000,
  &amp;quot;##ple&amp;quot;: 1001,
  &amp;quot;&amp;lt;pad&amp;gt;&amp;quot;: 50256,
  &amp;quot;&amp;lt;eos&amp;gt;&amp;quot;: 50257
}

&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;key：&lt;strong&gt;token（字符串）&lt;/strong&gt;，可以是单词、子词、符号或特殊 token&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;value：&lt;strong&gt;整型 ID&lt;/strong&gt;，用于将 token 转为模型可用的输入&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;Ġ&lt;/code&gt; 表示前面有空格（在 Roberta/BPE 中常见） &lt;code&gt;##&lt;/code&gt; 表示这个词是接在前一个词后面的（WordPiece 标记）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;它与其它文件的区别&#34;&gt;🔍 它与其它文件的区别&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;文件&lt;/th&gt;
&lt;th&gt;作用&lt;/th&gt;
&lt;th&gt;是否必须&lt;/th&gt;
&lt;th&gt;快速说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;vocab.json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;token ➜ ID 映射&lt;/td&gt;
&lt;td&gt;是（slow tokenizer）&lt;/td&gt;
&lt;td&gt;分词后映射成整数 token ID&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;merges.txt&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;BPE 合并规则&lt;/td&gt;
&lt;td&gt;是（BPE 模型）&lt;/td&gt;
&lt;td&gt;指定如何从字符对合并出 token&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tokenizer.json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Fast tokenizer 全部行为&lt;/td&gt;
&lt;td&gt;否&lt;/td&gt;
&lt;td&gt;打包了 vocab 和 merges，也更快&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tokenizer_config.json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;加载 tokenizer 的配置项&lt;/td&gt;
&lt;td&gt;否&lt;/td&gt;
&lt;td&gt;控制是否添加特殊 token 等行为&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;special_tokens_map.json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;特殊 token 的 ID/映射&lt;/td&gt;
&lt;td&gt;否&lt;/td&gt;
&lt;td&gt;控制 ,  等的定义&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;应用方式&#34;&gt;🔧 应用方式&lt;/h3&gt;

&lt;p&gt;加载模型 tokenizer 时会自动用到：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&amp;quot;gpt2&amp;quot;)  # 会读取 vocab.json + merges.txt
tokens = tokenizer.tokenize(&amp;quot;The apple is red.&amp;quot;)
ids = tokenizer.convert_tokens_to_ids(tokens)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以手动加载：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import json

with open(&amp;quot;vocab.json&amp;quot;) as f:
    vocab = json.load(f)
print(vocab[&amp;quot;apple&amp;quot;])  # 输出对应的 token ID

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;如何生成-vocab-json&#34;&gt;🏗 如何生成 vocab.json？&lt;/h3&gt;

&lt;p&gt;通常是通过训练分词器：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tokenizers import Tokenizer, models, trainers, pre_tokenizers

tokenizer = Tokenizer(models.BPE())
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

trainer = trainers.BpeTrainer(vocab_size=10000, special_tokens=[&amp;quot;&amp;lt;pad&amp;gt;&amp;quot;, &amp;quot;&amp;lt;eos&amp;gt;&amp;quot;])
tokenizer.train([&amp;quot;corpus.txt&amp;quot;], trainer)

tokenizer.model.save(&amp;quot;.&amp;quot;, &amp;quot;my-tokenizer&amp;quot;)  # 输出 vocab.json + merges.txt

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;在推理中的作用&#34;&gt;🚀 在推理中的作用&lt;/h3&gt;

&lt;p&gt;在推理时，文本输入流程如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-plaintext&#34;&gt;文本输入 → 分词器 → token 序列 → vocab.json 查 ID → 模型输入

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;反向输出流程：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-plaintext&#34;&gt;模型输出 token ID → vocab.json 反查 token → 拼接成文本

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以它是编码与解码的基础。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;总结-5&#34;&gt;✅ 总结&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;属性&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;文件类型&lt;/td&gt;
&lt;td&gt;JSON（键为字符串 token，值为整数 ID）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;用途&lt;/td&gt;
&lt;td&gt;供 slow tokenizer 使用；token ↔ ID 映射&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;依赖&lt;/td&gt;
&lt;td&gt;通常与 merges.txt 配合使用&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;被谁用&lt;/td&gt;
&lt;td&gt;&lt;code&gt;AutoTokenizer&lt;/code&gt;, &lt;code&gt;PreTrainedTokenizer&lt;/code&gt; 等&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;是否可替代&lt;/td&gt;
&lt;td&gt;若有 &lt;code&gt;tokenizer.json&lt;/code&gt;，则可不再使用它&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;4-总结&#34;&gt;4、总结&lt;/h1&gt;

&lt;p&gt;Hugging Face 模型目录结构是用于保存一个预训练模型（及其 tokenizer、配置等）的完整打包形式，目的是为了方便模型的加载、共享、部署和推理。下面为你总结模型目录的&lt;strong&gt;标准构成&lt;/strong&gt;、每个文件的作用、是否必须、以及它们之间的关系。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;1-目录结构总览&#34;&gt;✅ 1. 目录结构总览&lt;/h2&gt;

&lt;p&gt;一个典型的 Hugging Face 模型目录如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-plaintext&#34;&gt;/your-model-dir/
├── config.json
├── generation_config.json
├── pytorch_model.bin / model.safetensors
├── model.safetensors.index.json (如为分片模型)
├── tokenizer_config.json
├── tokenizer.json
├── vocab.json
├── merges.txt (如为 BPE)
├── special_tokens_map.json
├── README.md

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;2-各文件功能详解&#34;&gt;📁 2. 各文件功能详解&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;文件名&lt;/th&gt;
&lt;th&gt;必须&lt;/th&gt;
&lt;th&gt;类型&lt;/th&gt;
&lt;th&gt;作用&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;config.json&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;✅ 是&lt;/td&gt;
&lt;td&gt;模型配置&lt;/td&gt;
&lt;td&gt;定义模型架构参数（层数、隐藏维度、dropout 等）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;pytorch_model.bin&lt;/strong&gt; / &lt;strong&gt;model.safetensors&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;✅ 是&lt;/td&gt;
&lt;td&gt;模型权重&lt;/td&gt;
&lt;td&gt;保存模型的预训练参数（PyTorch 格式 / safetensors 更安全）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;model.safetensors.index.json&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;🚫 否&lt;/td&gt;
&lt;td&gt;权重索引&lt;/td&gt;
&lt;td&gt;用于分片模型（多块 .safetensors 的索引信息）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;tokenizer_config.json&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;✅ 是&lt;/td&gt;
&lt;td&gt;tokenizer 配置&lt;/td&gt;
&lt;td&gt;控制 tokenizer 行为，比如是否小写、是否添加 BOS/EOS 等&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;tokenizer.json&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;🚫 否&lt;/td&gt;
&lt;td&gt;Fast tokenizer&lt;/td&gt;
&lt;td&gt;Hugging Face 的 Rust 实现 tokenizer，速度快，包含 vocab+merges&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;vocab.json&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;✅ 是&lt;/td&gt;
&lt;td&gt;词表（token → id）&lt;/td&gt;
&lt;td&gt;分词器词典，主要用于 slow tokenizer&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;merges.txt&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;⚠️ 视分词器类型&lt;/td&gt;
&lt;td&gt;BPE 合并规则&lt;/td&gt;
&lt;td&gt;仅用于 BPE（GPT、Roberta 等）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;special_tokens_map.json&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;🚫 否&lt;/td&gt;
&lt;td&gt;特殊 token 映射&lt;/td&gt;
&lt;td&gt;映射 、、 等到对应 ID&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;generation_config.json&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;🚫 否&lt;/td&gt;
&lt;td&gt;推理配置&lt;/td&gt;
&lt;td&gt;定义采样策略（如 temperature, top_k, repetition_penalty 等）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;README.md&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;🚫 否&lt;/td&gt;
&lt;td&gt;文档&lt;/td&gt;
&lt;td&gt;模型说明文档（在 Hugging Face hub 上展示）&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;3-必须-vs-可选文件汇总&#34;&gt;🧠 3. 必须 vs 可选文件汇总&lt;/h2&gt;

&lt;h3 id=&#34;必须文件-用于模型加载&#34;&gt;✅ 必须文件（用于模型加载）&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;config.json&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;pytorch_model.bin&lt;/code&gt; 或 &lt;code&gt;model.safetensors&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;tokenizer_config.json&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;vocab.json&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;根据模型类型必须&#34;&gt;⚠️ 根据模型类型必须&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;merges.txt&lt;/code&gt;：如果使用 BPE tokenizer（如 GPT、Roberta）
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;可选但推荐&#34;&gt;🚫 可选但推荐&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;tokenizer.json&lt;/code&gt;：推荐用于 fast tokenizer，加快速度&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;generation_config.json&lt;/code&gt;：如模型用于生成任务（文本生成、聊天）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;special_tokens_map.json&lt;/code&gt;：对话模型、翻译、QA 常用&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;README.md&lt;/code&gt;：用于模型说明和文档展示&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;model.safetensors.index.json&lt;/code&gt;：当模型是分片结构（sharded）时才出现&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;4-加载时工作机制&#34;&gt;📦 4. 加载时工作机制&lt;/h2&gt;

&lt;p&gt;Hugging Face 的 &lt;code&gt;AutoModel&lt;/code&gt; 和 &lt;code&gt;AutoTokenizer&lt;/code&gt; 会根据目录内容自动判断使用哪种文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(&amp;quot;./your-model-dir&amp;quot;)
tokenizer = AutoTokenizer.from_pretrained(&amp;quot;./your-model-dir&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它会依次尝试加载：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;权重：&lt;code&gt;model.safetensors&lt;/code&gt; &amp;gt; &lt;code&gt;pytorch_model.bin&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;tokenizer：&lt;code&gt;tokenizer.json&lt;/code&gt; &amp;gt; &lt;code&gt;vocab.json + merges.txt&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;特殊 token：&lt;code&gt;special_tokens_map.json&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;推理策略：&lt;code&gt;generation_config.json&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;5-示例应用流程&#34;&gt;🔍 5. 示例应用流程&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;文本 → tokenizer.json/vocab.json → token ID → 模型(config+权重) → 输出 token ID → 反向 tokenizer → 文本输出

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;总结一句话-1&#34;&gt;✅ 总结一句话&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Hugging Face 模型目录包含了“模型结构（config）+权重+分词器+推理策略”的完整组件，是部署、共享、加载模型的标准打包格式。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
</description>
        </item>
        
        <item>
            <title>pytorch速查手册</title>
            <link>http://mospany.github.io/2025/08/09/pytorch-data-sheet/</link>
            <pubDate>Sat, 09 Aug 2025 11:28:11 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2025/08/09/pytorch-data-sheet/</guid>
            <description>

&lt;hr /&gt;

&lt;h2 id=&#34;1-安装与基础&#34;&gt;🧠 1. 安装与基础&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 安装 CPU 版本
pip install torch torchvision torchaudio

# 安装 GPU 版本（CUDA 12.1 示例）
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch
print(torch.__version__)    # 版本
print(torch.cuda.is_available())  # 检查 GPU
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;2-张量-tensor-基础&#34;&gt;📦 2. 张量（Tensor）基础&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 创建张量
x = torch.tensor([1, 2, 3])          # 从列表
x = torch.zeros(3, 3)                # 全零
x = torch.ones(2, 2)                  # 全一
x = torch.eye(3)                      # 单位矩阵
x = torch.rand(2, 3)                  # 均匀分布
x = torch.randn(2, 3)                 # 正态分布
x = torch.arange(0, 10, 2)            # 等差数列
x = torch.linspace(0, 1, steps=5)     # 等间隔数列

# 张量属性
print(x.shape, x.dtype, x.device)

# 数据类型
x = x.float()  # 转 float32
x = x.double() # 转 float64
x = x.int()    # 转 int32

# 张量运算
y = torch.tensor([4, 5, 6])
print(x + y)       # 加法
print(x * y)       # 逐元素乘
print(torch.dot(x, y))  # 点积
print(x @ y)       # 矩阵乘
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;3-设备-cpu-gpu-操作&#34;&gt;🔄 3. 设备（CPU/GPU）操作&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 放到 GPU
device = torch.device(&amp;quot;cuda&amp;quot; if torch.cuda.is_available() else &amp;quot;cpu&amp;quot;)
x = x.to(device)

# 直接创建在 GPU
x = torch.rand(2, 2, device=&amp;quot;cuda&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;4-自动求导-autograd&#34;&gt;🧮 4. 自动求导（Autograd）&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 自动求导
x = torch.tensor(2.0, requires_grad=True)
y = x ** 2 + 3 * x + 1
y.backward()  # dy/dx
print(x.grad)  # 输出梯度：7
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;5-神经网络构建-torch-nn&#34;&gt;🏗 5. 神经网络构建（torch.nn）&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(10, 50)
        self.fc2 = nn.Linear(50, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)

model = Net()
print(model)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;6-损失函数-优化器&#34;&gt;⚡ 6. 损失函数 &amp;amp; 优化器&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch.optim as optim

criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;7-训练循环模板&#34;&gt;🔁 7. 训练循环模板&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for epoch in range(epochs):
    for data, target in dataloader:
        optimizer.zero_grad()      # 清空梯度
        output = model(data)       # 前向传播
        loss = criterion(output, target) # 计算损失
        loss.backward()            # 反向传播
        optimizer.step()           # 更新参数
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;8-推理-评估模式&#34;&gt;🔍 8. 推理（评估模式）&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.eval()
with torch.no_grad():  # 禁用梯度
    pred = model(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;9-数据加载-dataloader&#34;&gt;📊 9. 数据加载（DataLoader）&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torch.utils.data import DataLoader, TensorDataset

dataset = TensorDataset(torch.randn(100, 10), torch.randn(100, 1))
loader = DataLoader(dataset, batch_size=16, shuffle=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;10-常用加速技巧&#34;&gt;🏎 10. 常用加速技巧&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;with torch.no_grad()&lt;/code&gt;：推理时禁用梯度&lt;/li&gt;
&lt;li&gt;&lt;code&gt;model.eval()&lt;/code&gt;：关闭 Dropout / BatchNorm&lt;/li&gt;
&lt;li&gt;使用 &lt;strong&gt;GPU&lt;/strong&gt;：&lt;code&gt;model.to(&amp;quot;cuda&amp;quot;)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;使用 &lt;strong&gt;AMP（自动混合精度）&lt;/strong&gt;：&lt;code&gt;torch.cuda.amp&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
</description>
        </item>
        
        <item>
            <title>Python基础与速查手册</title>
            <link>http://mospany.github.io/2025/08/02/python-in-ai/</link>
            <pubDate>Sat, 02 Aug 2025 15:44:09 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2025/08/02/python-in-ai/</guid>
            <description>

&lt;h2 id=&#34;一-基础语法速查&#34;&gt;🧠 一、基础语法速查&lt;/h2&gt;

&lt;h3 id=&#34;python-关键字-共-36-个-区分大小写&#34;&gt;🧠 Python 关键字（共 36 个，区分大小写）&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;关键字&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;布尔值，假&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;True&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;布尔值，真&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;空值&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;and&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;逻辑与&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;or&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;逻辑或&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;not&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;逻辑非&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;if&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;条件语句&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;elif&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;条件分支&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;else&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;条件结尾&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;while&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;条件循环&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;for&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;循环语句&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;break&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;中断循环&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;continue&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;跳过当前循环&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;in&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;成员判断、循环迭代&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;is&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;身份比较（是否是同一个对象）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pass&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;占位语句&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;def&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;定义函数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;return&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;返回值&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;yield&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;生成器返回值&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;lambda&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匿名函数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;global&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;全局变量声明&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nonlocal&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;闭包变量声明&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;import&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;导入模块&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;from&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;从模块导入指定成员&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;as&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;给导入模块/异常命名&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;类定义&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;try&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;异常捕获开始&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;except&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;异常捕获&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;finally&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;异常结束处理&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;raise&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;主动抛出异常&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;assert&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;断言&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;del&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;删除变量或元素&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;上下文管理语句&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;async&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;异步定义函数（协程）&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;await&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;异步调用（等待协程结果）&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;语法&#34;&gt;语法&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;语法&lt;/th&gt;
&lt;th&gt;示例&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;变量定义&lt;/td&gt;
&lt;td&gt;&lt;code&gt;x = 10&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;不需要声明类型&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;条件语句&lt;/td&gt;
&lt;td&gt;&lt;code&gt;if x &amp;gt; 0: ... elif x==0: ... else: ...&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;结构清晰&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;循环&lt;/td&gt;
&lt;td&gt;&lt;code&gt;for i in range(10):&lt;/code&gt;&lt;br&gt;&lt;code&gt;while x &amp;lt; 5:&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;遍历/条件循环&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;函数定义&lt;/td&gt;
&lt;td&gt;&lt;code&gt;def func(a, b=1): return a + b&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;默认参数、返回值&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;列表推导式&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[x**2 for x in range(5)]&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;快速生成新列表&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;多变量赋值&lt;/td&gt;
&lt;td&gt;&lt;code&gt;a, b = 1, 2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;解包&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;类型注解&lt;/td&gt;
&lt;td&gt;&lt;code&gt;def add(x: int) -&amp;gt; int:&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;函数签名更清晰&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;二-常用数据结构函数&#34;&gt;🧰 二、常用数据结构函数&lt;/h2&gt;

&lt;h3 id=&#34;字符串-str&#34;&gt;字符串 &lt;code&gt;str&lt;/code&gt;&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;函数&lt;/th&gt;
&lt;th&gt;示例&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;s.lower()&lt;/code&gt; / &lt;code&gt;s.upper()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;大小写转换&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;s.split(&#39;,&#39;)&lt;/code&gt; / &lt;code&gt;&#39; &#39;.join(list)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;拆分 / 合并&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;s.strip()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;去除首尾空白&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;s.replace(&#39;a&#39;,&#39;b&#39;)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;替换字符串&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;f&amp;quot;{x:.2f}&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;f-string 格式化字符串&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;列表-list&#34;&gt;列表 &lt;code&gt;list&lt;/code&gt;&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;函数&lt;/th&gt;
&lt;th&gt;示例&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;len(lst)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;获取长度&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;lst.append(x)&lt;/code&gt; / &lt;code&gt;lst.extend(...)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;添加元素&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;lst.pop()&lt;/code&gt; / &lt;code&gt;lst.remove(x)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;删除元素&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;lst.sort()&lt;/code&gt; / &lt;code&gt;sorted(lst)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;排序，原地或新列表&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;lst[::-1]&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;反转列表&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;字典-dict&#34;&gt;字典 &lt;code&gt;dict&lt;/code&gt;&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;函数&lt;/th&gt;
&lt;th&gt;示例&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;d.get(&#39;key&#39;, default)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;安全取值&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;d.keys()&lt;/code&gt; / &lt;code&gt;d.values()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;键/值视图&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;d.items()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;遍历键值对&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;dict(zip(keys, values))&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;合并为字典&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;集合-set&#34;&gt;集合 &lt;code&gt;set&lt;/code&gt;&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;操作&lt;/th&gt;
&lt;th&gt;示例&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;并&lt;/td&gt;
&lt;td&gt;`a&lt;/td&gt;
&lt;td&gt;b&lt;code&gt;/&lt;/code&gt;a.union(b)`&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;交&lt;/td&gt;
&lt;td&gt;&lt;code&gt;a &amp;amp; b&lt;/code&gt; / &lt;code&gt;a.intersection(b)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;差&lt;/td&gt;
&lt;td&gt;&lt;code&gt;a - b&lt;/code&gt; / &lt;code&gt;a.difference(b)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;三-内置函数高频使用表&#34;&gt;🔁 三、内置函数高频使用表&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;函数&lt;/th&gt;
&lt;th&gt;用途&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;len()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;长度&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;sum()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;求和&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;min()&lt;/code&gt; / &lt;code&gt;max()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;最小/最大&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;range()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;生成数列&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;enumerate()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;索引+元素遍历&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;zip(a, b)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;配对遍历&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;map(func, iterable)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;映射&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;filter(func, iterable)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;过滤&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;sorted(list, key=..., reverse=True)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;排序控制&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;any()&lt;/code&gt; / &lt;code&gt;all()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;判断是否有 / 是否都为真&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;四-模块与工具函数&#34;&gt;⚙️ 四、模块与工具函数&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模块&lt;/th&gt;
&lt;th&gt;常用函数&lt;/th&gt;
&lt;th&gt;用途&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;math&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;math.sqrt()&lt;/code&gt;, &lt;code&gt;log()&lt;/code&gt;, &lt;code&gt;ceil()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;数学运算&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;random&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;random.choice()&lt;/code&gt;, &lt;code&gt;randint()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;随机操作&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;datetime&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;datetime.now()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;时间戳处理&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;os&lt;/code&gt;, &lt;code&gt;sys&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;os.path&lt;/code&gt;, &lt;code&gt;sys.argv&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;路径与参数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;glob&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;glob.glob(&#39;*.py&#39;)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;文件批量匹配&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;re&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;re.findall()&lt;/code&gt;, &lt;code&gt;sub()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;正则匹配&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;json&lt;/code&gt; / &lt;code&gt;yaml&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;json.load()&lt;/code&gt;, &lt;code&gt;yaml.safe_load()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;配置处理&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;五-异常处理-文件操作&#34;&gt;🧪 五、异常处理 &amp;amp; 文件操作&lt;/h2&gt;

&lt;h3 id=&#34;异常处理&#34;&gt;异常处理&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try:
    risky_operation()
except ValueError as e:
    print(&amp;quot;Caught error:&amp;quot;, e)
finally:
    cleanup()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;文件操作&#34;&gt;文件操作&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with open(&#39;file.txt&#39;, &#39;r&#39;) as f:
    content = f.read()
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;七-数值计算与张量操作-numpy-pytorch&#34;&gt;🔢 七、数值计算与张量操作（NumPy &amp;amp; PyTorch）&lt;/h2&gt;

&lt;h3 id=&#34;numpy常用函数&#34;&gt;NumPy常用函数&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;函数&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;np.array()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;创建数组&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;np.zeros()&lt;/code&gt; / &lt;code&gt;np.ones()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;初始化张量&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;np.reshape()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;重塑维度&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;np.dot()&lt;/code&gt; / &lt;code&gt;np.matmul()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;向量/矩阵乘法&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;np.mean()&lt;/code&gt; / &lt;code&gt;np.std()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;均值、标准差&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;np.argmax()&lt;/code&gt; / &lt;code&gt;np.argsort()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;排序、索引最大值&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;np.concatenate()&lt;/code&gt; / &lt;code&gt;np.stack()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;合并数组&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;pytorch张量操作&#34;&gt;PyTorch张量操作&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;函数&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;torch.tensor()&lt;/code&gt; / &lt;code&gt;torch.from_numpy()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;创建张量&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;x.to(&#39;cuda&#39;)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;张量转GPU&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;x.view()&lt;/code&gt; / &lt;code&gt;x.reshape()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;改变维度&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;x.mean(dim=1)&lt;/code&gt; / &lt;code&gt;x.softmax(dim=-1)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;常见计算&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;torch.cat()&lt;/code&gt; / &lt;code&gt;torch.stack()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;拼接张量&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;torch.nn.functional&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;提供无状态的函数，如激活函数&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;八-模型开发与训练-pytorch&#34;&gt;🧠 八、模型开发与训练（PyTorch）&lt;/h2&gt;

&lt;h3 id=&#34;模型定义&#34;&gt;模型定义&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(768, 2)
    
    def forward(self, x):
        return self.linear(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;常用训练组件&#34;&gt;常用训练组件&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模块&lt;/th&gt;
&lt;th&gt;常用函数&lt;/th&gt;
&lt;th&gt;用途&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nn&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nn.Linear&lt;/code&gt;, &lt;code&gt;nn.Embedding&lt;/code&gt;, &lt;code&gt;nn.Transformer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;模型层&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;optim&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;SGD&lt;/code&gt;, &lt;code&gt;Adam&lt;/code&gt;, &lt;code&gt;lr_scheduler&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;优化器&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;loss&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;CrossEntropyLoss&lt;/code&gt;, &lt;code&gt;MSELoss&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;损失函数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;torch.utils.data.DataLoader&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;数据批加载&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;autograd&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;loss.backward()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;反向传播&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;九-大模型加载与推理相关库函数&#34;&gt;🚀 九、大模型加载与推理相关库函数&lt;/h2&gt;

&lt;h3 id=&#34;huggingface-transformers&#34;&gt;HuggingFace Transformers&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;函数&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;from_pretrained()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;加载模型和tokenizer&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;AutoModelForCausalLM&lt;/code&gt; / &lt;code&gt;AutoTokenizer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;大模型架构适配&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tokenizer.encode()&lt;/code&gt; / &lt;code&gt;decode()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;文本与token转换&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;model.generate()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;文本生成&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;推理优化相关&#34;&gt;推理优化相关&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;库&lt;/th&gt;
&lt;th&gt;函数 / 说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;torch.no_grad()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;禁用梯度，加快推理&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;torch.cuda.amp.autocast()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;半精度混合精度推理&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;torch.jit.trace()&lt;/code&gt; / &lt;code&gt;script()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;模型静态图编译&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;onnxruntime&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;InferenceSession()&lt;/code&gt;，用于ONNX推理&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;vllm&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;高效推理库（异步批次 + KV Cache 管理）&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;十-辅助工具库函数&#34;&gt;🧰 十、辅助工具库函数&lt;/h2&gt;

&lt;h3 id=&#34;配置与日志&#34;&gt;配置与日志&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;库&lt;/th&gt;
&lt;th&gt;常用函数&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;argparse&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;parser.add_argument()&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;yaml&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;yaml.safe_load(open(&#39;config.yaml&#39;))&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;logging&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;logging.info()&lt;/code&gt;，配合训练日志记录&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;文件与路径&#34;&gt;文件与路径&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;库&lt;/th&gt;
&lt;th&gt;用途&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;os&lt;/code&gt;, &lt;code&gt;pathlib&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;文件路径管理&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;glob&lt;/code&gt;, &lt;code&gt;shutil&lt;/code&gt;, &lt;code&gt;zipfile&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;文件批量操作、压缩&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tqdm&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;训练进度条显示&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;调试与性能&#34;&gt;调试与性能&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;工具&lt;/th&gt;
&lt;th&gt;用法&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;print()&lt;/code&gt; / &lt;code&gt;logging&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;日志输出&lt;/td&gt;
&lt;td&gt;可配日志等级&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;assert&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;assert x &amp;gt; 0&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;条件断言&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;%timeit&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;IPython魔法命令&lt;/td&gt;
&lt;td&gt;性能测试&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pdb.set_trace()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;中断调试&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;十一-推荐学习资源&#34;&gt;📚十一、 推荐学习资源&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主题&lt;/th&gt;
&lt;th&gt;资源&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Python 3 标准库手册&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://docs.python.org/3/library/&#34;&gt;https://docs.python.org/3/library/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;PyTorch文档&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/index.html&#34;&gt;https://pytorch.org/docs/stable/index.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;HuggingFace教程&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://huggingface.co/transformers/&#34;&gt;https://huggingface.co/transformers/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Python标准库速查&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://docs.python.org/3/library/&#34;&gt;https://docs.python.org/3/library/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;
</description>
        </item>
        
        <item>
            <title>AI中的数学知识</title>
            <link>http://mospany.github.io/2025/07/13/math-in-ai/</link>
            <pubDate>Sun, 13 Jul 2025 19:44:00 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2025/07/13/math-in-ai/</guid>
            <description>

&lt;h1 id=&#34;ai中的数学知识全景-基础-公式-证明与应用&#34;&gt;AI中的数学知识全景：基础、公式、证明与应用&lt;/h1&gt;

&lt;p&gt;人工智能（AI）作为21世纪最具变革性的技术之一，其发展离不开坚实的数学基础。无论是机器学习、深度学习、自然语言处理还是计算机视觉，背后都蕴含着丰富的数学理论与方法。本文将系统梳理AI中常用的数学知识，涵盖数学概念、所属学科、核心公式、证明过程及其在AI中的具体用途，力求为读者呈现一份详尽的“AI数学地图”。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;一-线性代数&#34;&gt;一、线性代数&lt;/h2&gt;

&lt;h3 id=&#34;1-1-基本概念与学科归属&#34;&gt;1.1 基本概念与学科归属&lt;/h3&gt;

&lt;p&gt;线性代数是研究向量、矩阵及其变换的数学分支，是AI的基石。它主要包括向量空间、线性变换、特征值与特征向量、奇异值分解等内容。&lt;/p&gt;

&lt;h3 id=&#34;1-2-主要内容与公式&#34;&gt;1.2 主要内容与公式&lt;/h3&gt;

&lt;h4 id=&#34;1-2-1-向量与矩阵&#34;&gt;1.2.1 向量与矩阵&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;向量&lt;/strong&gt;：有方向和大小的量，常用列向量表示。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;矩阵&lt;/strong&gt;：二维数组，表示线性变换或数据集。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;矩阵乘法公式&lt;/strong&gt;：&lt;br /&gt;
设$A$为$m\times n$矩阵，$B$为$n\times p$矩阵，则$C=AB$为$m\times p$矩阵，&lt;br /&gt;
$$
C&lt;em&gt;{ij} = \sum&lt;/em&gt;{k=1}^n A&lt;em&gt;{ik}B&lt;/em&gt;{kj}
$$&lt;/p&gt;

&lt;h4 id=&#34;1-2-2-特征值与特征向量&#34;&gt;1.2.2 特征值与特征向量&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义&lt;/strong&gt;：对于方阵$A$，若存在非零向量$v$和数$\lambda$，使得$Av = \lambda v$，则$\lambda$为$A$的特征值，$v$为对应特征向量。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;特征值求解公式&lt;/strong&gt;：&lt;br /&gt;
$$
\det(A - \lambda I) = 0
$$&lt;/p&gt;

&lt;h4 id=&#34;1-2-3-奇异值分解-svd&#34;&gt;1.2.3 奇异值分解（SVD）&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义&lt;/strong&gt;：任意$m\times n$矩阵$A$可分解为$A = U\Sigma V^T$，其中$U$和$V$为正交矩阵，$\Sigma$为对角矩阵。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;1-2-4-证明举例-特征值存在性&#34;&gt;1.2.4 证明举例：特征值存在性&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：任意$n$阶实对称矩阵都有$n$个实特征值。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;证明思路&lt;/strong&gt;：&lt;br /&gt;
1. 实对称矩阵$A$的特征多项式为实系数多项式，必有实根（代数基本定理）。
2. 可通过正交变换将$A$对角化，所有特征值为实数。&lt;/p&gt;

&lt;h3 id=&#34;1-3-ai中的用途&#34;&gt;1.3 AI中的用途&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;数据表示&lt;/strong&gt;：样本、特征、权重均以向量/矩阵形式存储。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;神经网络&lt;/strong&gt;：前向传播、反向传播均为矩阵运算。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;降维&lt;/strong&gt;：PCA、SVD等用于特征压缩与可视化。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;图像处理&lt;/strong&gt;：卷积操作本质为矩阵乘法。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;二-概率论与数理统计&#34;&gt;二、概率论与数理统计&lt;/h2&gt;

&lt;h3 id=&#34;2-1-基本概念与学科归属&#34;&gt;2.1 基本概念与学科归属&lt;/h3&gt;

&lt;p&gt;概率论研究随机现象的规律，数理统计则关注数据分析与推断。AI中的不确定性建模、模型评估、参数估计等均依赖概率统计。&lt;/p&gt;

&lt;h3 id=&#34;2-2-主要内容与公式&#34;&gt;2.2 主要内容与公式&lt;/h3&gt;

&lt;h4 id=&#34;2-2-1-概率空间与随机变量&#34;&gt;2.2.1 概率空间与随机变量&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;概率空间&lt;/strong&gt;：$(\Omega, \mathcal{F}, P)$，$\Omega$为样本空间，$\mathcal{F}$为事件集合，$P$为概率测度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;随机变量&lt;/strong&gt;：定义在概率空间上的实值函数。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;2-2-2-条件概率与贝叶斯公式&#34;&gt;2.2.2 条件概率与贝叶斯公式&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;条件概率&lt;/strong&gt;：$P(A|B) = \frac{P(A\cap B)}{P(B)}$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;贝叶斯公式&lt;/strong&gt;：$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;2-2-3-数学期望与方差&#34;&gt;2.2.3 数学期望与方差&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;期望&lt;/strong&gt;：$E[X] = \sum_x xP(X=x)$（离散），$E[X] = \int x f(x) dx$（连续）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;方差&lt;/strong&gt;：$Var(X) = E[(X-E[X])^2]$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;2-2-4-常见分布&#34;&gt;2.2.4 常见分布&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;正态分布&lt;/strong&gt;：$f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;伯努利分布&lt;/strong&gt;、&lt;strong&gt;二项分布&lt;/strong&gt;、&lt;strong&gt;泊松分布&lt;/strong&gt;等。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;2-2-5-极大似然估计-mle&#34;&gt;2.2.5 极大似然估计（MLE）&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义&lt;/strong&gt;：给定观测数据$X$，参数$\theta$的极大似然估计为
$$
\hat{\theta} = \arg\max_\theta P(X|\theta)
$$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;2-2-6-证明举例-正态分布的极大似然估计&#34;&gt;2.2.6 证明举例：正态分布的极大似然估计&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;：已知样本$x_1, x_2, &amp;hellip;, x_n$来自$N(\mu, \sigma^2)$，求$\mu$的MLE。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;解&lt;/strong&gt;：&lt;br /&gt;
似然函数：
$$
L(\mu) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x&lt;em&gt;i-\mu)^2}{2\sigma^2}}
$$
对数似然：
$$
\ell(\mu) = -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum&lt;/em&gt;{i=1}^n (x&lt;em&gt;i-\mu)^2
$$
对$\mu$求导并令其为零：
$$
\frac{\partial \ell}{\partial \mu} = \frac{1}{\sigma^2}\sum&lt;/em&gt;{i=1}^n (x&lt;em&gt;i-\mu) = 0 \implies \mu = \frac{1}{n}\sum&lt;/em&gt;{i=1}^n x_i
$$&lt;/p&gt;

&lt;h3 id=&#34;2-3-ai中的用途&#34;&gt;2.3 AI中的用途&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;模型建模&lt;/strong&gt;：朴素贝叶斯、隐马尔可夫模型、生成模型等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;损失函数&lt;/strong&gt;：交叉熵、对数似然等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估指标&lt;/strong&gt;：准确率、召回率、AUC等均基于概率统计。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不确定性估计&lt;/strong&gt;：贝叶斯深度学习、置信区间等。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;三-微积分与最优化&#34;&gt;三、微积分与最优化&lt;/h2&gt;

&lt;h3 id=&#34;3-1-基本概念与学科归属&#34;&gt;3.1 基本概念与学科归属&lt;/h3&gt;

&lt;p&gt;微积分研究变化率与累积量，最优化则关注函数极值的求解。AI模型训练本质是最优化问题，微积分为其提供理论基础。&lt;/p&gt;

&lt;h3 id=&#34;3-2-主要内容与公式&#34;&gt;3.2 主要内容与公式&lt;/h3&gt;

&lt;h4 id=&#34;3-2-1-导数与梯度&#34;&gt;3.2.1 导数与梯度&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;导数&lt;/strong&gt;：$f&amp;rsquo;(x) = \lim_{h\to 0} \frac{f(x+h)-f(x)}{h}$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;梯度&lt;/strong&gt;：$\nabla f(x) = \left(\frac{\partial f}{\partial x_1}, &amp;hellip;, \frac{\partial f}{\partial x_n}\right)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;3-2-2-链式法则&#34;&gt;3.2.2 链式法则&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;公式&lt;/strong&gt;：若$y = f(u), u = g(x)$，则$\frac{dy}{dx} = \frac{dy}{du}\frac{du}{dx}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;3-2-3-泰勒展开&#34;&gt;3.2.3 泰勒展开&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;公式&lt;/strong&gt;：$f(x) = f(a) + f&amp;rsquo;(a)(x-a) + \frac{f&amp;rdquo;(a)}{2!}(x-a)^2 + &amp;hellip;$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;3-2-4-最优化问题&#34;&gt;3.2.4 最优化问题&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;无约束最优化&lt;/strong&gt;：$\min_x f(x)$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;有约束最优化&lt;/strong&gt;：$\min_x f(x), \text{ s.t. } g_i(x) \leq 0, h_j(x) = 0$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;3-2-5-梯度下降法&#34;&gt;3.2.5 梯度下降法&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;公式&lt;/strong&gt;：$x_{k+1} = x_k - \eta \nabla f(x_k)$，$\eta$为学习率。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;3-2-6-拉格朗日乘子法&#34;&gt;3.2.6 拉格朗日乘子法&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;公式&lt;/strong&gt;：$\mathcal{L}(x, \lambda) = f(x) + \lambda g(x)$，对$\mathcal{L}$求偏导并令其为零。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;3-2-7-证明举例-梯度下降收敛性&#34;&gt;3.2.7 证明举例：梯度下降收敛性&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：若$f(x)$为Lipschitz连续且凸，梯度下降法收敛到全局最优。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;证明思路&lt;/strong&gt;：&lt;br /&gt;
1. 利用凸函数性质，$f(y) \geq f(x) + \nabla f(x)^T(y-x)$。
2. 通过递推不等式证明目标函数单调下降，最终收敛。&lt;/p&gt;

&lt;h3 id=&#34;3-3-ai中的用途&#34;&gt;3.3 AI中的用途&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;模型训练&lt;/strong&gt;：神经网络、支持向量机等均通过梯度下降优化损失函数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;反向传播&lt;/strong&gt;：链式法则用于多层网络的梯度计算。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;正则化&lt;/strong&gt;：通过约束优化提升模型泛化能力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自动微分&lt;/strong&gt;：深度学习框架自动计算梯度。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;四-离散数学与图论&#34;&gt;四、离散数学与图论&lt;/h2&gt;

&lt;h3 id=&#34;4-1-基本概念与学科归属&#34;&gt;4.1 基本概念与学科归属&lt;/h3&gt;

&lt;p&gt;离散数学研究离散结构，包括集合、关系、图、组合等。图论是其重要分支，AI中的知识图谱、社交网络分析等均依赖图论。&lt;/p&gt;

&lt;h3 id=&#34;4-2-主要内容与公式&#34;&gt;4.2 主要内容与公式&lt;/h3&gt;

&lt;h4 id=&#34;4-2-1-集合与关系&#34;&gt;4.2.1 集合与关系&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;集合&lt;/strong&gt;：元素的无序集合。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关系&lt;/strong&gt;：集合间的映射，如等价关系、偏序关系。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;4-2-2-图的定义&#34;&gt;4.2.2 图的定义&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;无向图&lt;/strong&gt;：$G=(V,E)$，$V$为顶点集，$E$为边集。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;有向图&lt;/strong&gt;：边有方向。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;4-2-3-路径与连通性&#34;&gt;4.2.3 路径与连通性&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;路径&lt;/strong&gt;：顶点序列，任意相邻顶点有边相连。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;连通分量&lt;/strong&gt;：最大连通子图。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;4-2-4-最短路径算法&#34;&gt;4.2.4 最短路径算法&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dijkstra算法&lt;/strong&gt;：用于无负权图的最短路径。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bellman-Ford算法&lt;/strong&gt;：可处理负权边。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;4-2-5-最大流问题&#34;&gt;4.2.5 最大流问题&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ford-Fulkerson算法&lt;/strong&gt;：求解网络最大流。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;4-2-6-证明举例-dijkstra算法正确性&#34;&gt;4.2.6 证明举例：Dijkstra算法正确性&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;思路&lt;/strong&gt;：&lt;br /&gt;
1. 每次选取未访问节点中距离起点最近的节点，更新其邻居距离。
2. 归纳法证明每个节点的最短路径在被访问时已确定。&lt;/p&gt;

&lt;h3 id=&#34;4-3-ai中的用途&#34;&gt;4.3 AI中的用途&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;知识图谱&lt;/strong&gt;：实体及其关系建模为图。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;社交网络分析&lt;/strong&gt;：节点为用户，边为关系。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;图神经网络（GNN）&lt;/strong&gt;：在图结构上进行特征传播与学习。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;推荐系统&lt;/strong&gt;：基于图的协同过滤。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;五-信息论&#34;&gt;五、信息论&lt;/h2&gt;

&lt;h3 id=&#34;5-1-基本概念与学科归属&#34;&gt;5.1 基本概念与学科归属&lt;/h3&gt;

&lt;p&gt;信息论研究信息的度量、传输与压缩。AI中的损失函数、模型评估等大量用到信息论概念。&lt;/p&gt;

&lt;h3 id=&#34;5-2-主要内容与公式&#34;&gt;5.2 主要内容与公式&lt;/h3&gt;

&lt;h4 id=&#34;5-2-1-熵&#34;&gt;5.2.1 熵&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义&lt;/strong&gt;：$H(X) = -\sum_{i} P(x_i)\log P(x_i)$，度量不确定性。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;5-2-2-交叉熵&#34;&gt;5.2.2 交叉熵&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义&lt;/strong&gt;：$H(P, Q) = -\sum_{i} P(x_i)\log Q(x_i)$，度量两个分布的差异。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;5-2-3-kl散度&#34;&gt;5.2.3 KL散度&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义&lt;/strong&gt;：$D&lt;em&gt;{KL}(P||Q) = \sum&lt;/em&gt;{i} P(x_i)\log\frac{P(x_i)}{Q(x_i)}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;5-2-4-互信息&#34;&gt;5.2.4 互信息&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义&lt;/strong&gt;：$I(X;Y) = \sum_{x,y} P(x,y)\log\frac{P(x,y)}{P(x)P(y)}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;5-2-5-证明举例-kl散度非负性&#34;&gt;5.2.5 证明举例：KL散度非负性&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：$D_{KL}(P||Q) \geq 0$，等号成立当且仅当$P=Q$。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;证明&lt;/strong&gt;：&lt;br /&gt;
利用Jensen不等式，$\log$为凹函数，&lt;br /&gt;
$$
D_{KL}(P||Q) = E_P\left[\log\frac{P(x)}{Q(x)}\right] \geq \log E_P\left[\frac{P(x)}{Q(x)}\right] = 0
$$&lt;/p&gt;

&lt;h3 id=&#34;5-3-ai中的用途&#34;&gt;5.3 AI中的用途&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;损失函数&lt;/strong&gt;：分类任务常用交叉熵损失。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;生成模型&lt;/strong&gt;：GAN、VAE等用KL散度衡量分布差异。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特征选择&lt;/strong&gt;：互信息用于评估特征与标签的相关性。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;六-数值分析与计算方法&#34;&gt;六、数值分析与计算方法&lt;/h2&gt;

&lt;h3 id=&#34;6-1-基本概念与学科归属&#34;&gt;6.1 基本概念与学科归属&lt;/h3&gt;

&lt;p&gt;数值分析关注数学问题的近似解法，AI模型训练、推理均需高效数值计算。&lt;/p&gt;

&lt;h3 id=&#34;6-2-主要内容与公式&#34;&gt;6.2 主要内容与公式&lt;/h3&gt;

&lt;h4 id=&#34;6-2-1-线性方程组求解&#34;&gt;6.2.1 线性方程组求解&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;高斯消元法&lt;/strong&gt;、&lt;strong&gt;LU分解&lt;/strong&gt;、&lt;strong&gt;共轭梯度法&lt;/strong&gt;等。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;6-2-2-特征值分解&#34;&gt;6.2.2 特征值分解&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;幂法&lt;/strong&gt;、&lt;strong&gt;Jacobi法&lt;/strong&gt;等。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;6-2-3-插值与拟合&#34;&gt;6.2.3 插值与拟合&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;拉格朗日插值&lt;/strong&gt;、&lt;strong&gt;最小二乘法&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;6-2-4-数值积分&#34;&gt;6.2.4 数值积分&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;梯形法则&lt;/strong&gt;、&lt;strong&gt;辛普森法则&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;6-2-5-证明举例-最小二乘法&#34;&gt;6.2.5 证明举例：最小二乘法&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;：拟合直线$y = ax + b$，最小化$\sum_{i=1}^n (y_i - (ax_i + b))^2$。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;解&lt;/strong&gt;：&lt;br /&gt;
对$a, b$分别求偏导并令其为零，解线性方程组得最优$a, b$。&lt;/p&gt;

&lt;h3 id=&#34;6-3-ai中的用途&#34;&gt;6.3 AI中的用途&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;大规模矩阵运算&lt;/strong&gt;：神经网络训练依赖高效线性代数库。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优化算法实现&lt;/strong&gt;：如Adam、RMSProp等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型推理加速&lt;/strong&gt;：量化、稀疏化等技术。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;七-常用数学工具与ai算法中的应用实例&#34;&gt;七、常用数学工具与AI算法中的应用实例&lt;/h2&gt;

&lt;h3 id=&#34;7-1-主成分分析-pca&#34;&gt;7.1 主成分分析（PCA）&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;数学基础&lt;/strong&gt;：协方差矩阵、特征值分解。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;公式&lt;/strong&gt;：最大化投影方差，$w^* = \arg\max_{||w||=1} w^T S w$，$S$为协方差矩阵。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;证明&lt;/strong&gt;：拉格朗日乘子法可得$Sw = \lambda w$，即$w$为$S$的特征向量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;用途&lt;/strong&gt;：降维、去噪、可视化。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;7-2-支持向量机-svm&#34;&gt;7.2 支持向量机（SVM）&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;数学基础&lt;/strong&gt;：凸优化、拉格朗日对偶性、核方法。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;公式&lt;/strong&gt;：$\min_{w,b} \frac{1}{2}||w||^2$，s.t. $y_i(w^T x_i + b) \geq 1$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;证明&lt;/strong&gt;：KKT条件推导最优解。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;用途&lt;/strong&gt;：分类、回归、异常检测。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;7-3-神经网络与反向传播&#34;&gt;7.3 神经网络与反向传播&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;数学基础&lt;/strong&gt;：链式法则、矩阵微分。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;公式&lt;/strong&gt;：$\frac{\partial L}{\partial w} = \frac{\partial L}{\partial y} \frac{\partial y}{\partial w}$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;证明&lt;/strong&gt;：逐层递推，利用链式法则。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;用途&lt;/strong&gt;：深度学习模型训练。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;7-4-马尔可夫决策过程-mdp&#34;&gt;7.4 马尔可夫决策过程（MDP）&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;数学基础&lt;/strong&gt;：概率论、动态规划。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;公式&lt;/strong&gt;：贝尔曼方程$V(s) = \max&lt;em&gt;a \sum&lt;/em&gt;{s&amp;rsquo;} P(s&amp;rsquo;|s,a)[R(s,a,s&amp;rsquo;) + \gamma V(s&amp;rsquo;)]$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;证明&lt;/strong&gt;：利用最优性原理递归推导。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;用途&lt;/strong&gt;：强化学习、自动决策。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;八-ai数学知识的综合应用案例&#34;&gt;八、AI数学知识的综合应用案例&lt;/h2&gt;

&lt;h3 id=&#34;8-1-图像识别中的数学&#34;&gt;8.1 图像识别中的数学&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;卷积神经网络（CNN）&lt;/strong&gt;：卷积操作（线性代数）、激活函数（微积分）、损失函数（信息论）、参数优化（最优化）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特征提取&lt;/strong&gt;：PCA降维（线性代数）、SIFT/ORB等算法（离散数学）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;8-2-自然语言处理中的数学&#34;&gt;8.2 自然语言处理中的数学&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;词向量&lt;/strong&gt;：Word2Vec中的Skip-gram模型用到概率建模、最大似然估计。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;注意力机制&lt;/strong&gt;：加权和（线性代数）、softmax归一化（概率论）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;序列建模&lt;/strong&gt;：RNN/LSTM中的链式法则、梯度消失/爆炸分析（微积分）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;8-3-强化学习中的数学&#34;&gt;8.3 强化学习中的数学&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;策略优化&lt;/strong&gt;：梯度上升法、策略梯度定理（微积分、概率论）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;值函数估计&lt;/strong&gt;：贝尔曼方程（动态规划）、蒙特卡洛方法（数理统计）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;8-4-生成模型中的数学&#34;&gt;8.4 生成模型中的数学&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;变分自编码器（VAE）&lt;/strong&gt;：变分推断（概率论）、KL散度（信息论）、反向传播（微积分）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;生成对抗网络（GAN）&lt;/strong&gt;：极大极小优化（最优化）、JS散度（信息论）。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;九-ai数学学习建议与资源&#34;&gt;九、AI数学学习建议与资源&lt;/h2&gt;

&lt;h3 id=&#34;9-1-学习建议&#34;&gt;9.1 学习建议&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;打好基础&lt;/strong&gt;：线性代数、概率论、微积分是AI数学的“三驾马车”。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;理论结合实践&lt;/strong&gt;：通过编程实现数学算法，加深理解。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关注证明过程&lt;/strong&gt;：理解公式背后的推导逻辑，提升抽象思维。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多做题多应用&lt;/strong&gt;：刷题、参加竞赛、参与开源项目。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;9-2-推荐书籍与课程&#34;&gt;9.2 推荐书籍与课程&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;《线性代数及其应用》（David C. Lay）&lt;/li&gt;
&lt;li&gt;《概率论与数理统计》（茆诗松）&lt;/li&gt;
&lt;li&gt;《统计学习方法》（李航）&lt;/li&gt;
&lt;li&gt;《深度学习》（Ian Goodfellow等）&lt;/li&gt;
&lt;li&gt;斯坦福CS229、CS231n、MIT 6.036等公开课&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;十-结语&#34;&gt;十、结语&lt;/h2&gt;

&lt;p&gt;AI的每一次突破，背后都离不开数学的支撑。从数据的表示、模型的构建、参数的优化，到结果的解释与评估，数学无处不在。掌握AI所需的数学知识，不仅能帮助我们更好地理解和应用现有技术，更能为创新和突破打下坚实的基础。希望本文能为广大AI学习者和从业者提供一份系统、详实的数学参考，助力大家在智能时代乘风破浪。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;（全文约5400字，涵盖AI常用数学知识的概念、学科、公式、证明与用途，适合作为系统学习与查阅之用。）&lt;/em&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>从会议室名称看AI</title>
            <link>http://mospany.github.io/2025/07/13/meeting-room-to-see-ai/</link>
            <pubDate>Sun, 13 Jul 2025 18:20:27 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2025/07/13/meeting-room-to-see-ai/</guid>
            <description>

&lt;h1 id=&#34;从会议室名称看ai-核心术语解读&#34;&gt;从会议室名称看AI：核心术语解读&lt;/h1&gt;

&lt;p&gt;在人工智能（AI）领域，许多会议室和项目组常以AI相关术语命名。以下是一些常见AI术语的详细解读，涵盖其英文名称、基本原理及应用场景。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;1-强化学习-reinforcement-learning&#34;&gt;1. 强化学习（Reinforcement Learning）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;英文名称&lt;/strong&gt;：Reinforcement Learning&lt;br /&gt;
&lt;strong&gt;原理&lt;/strong&gt;：强化学习是一种机器学习方法，强调智能体（Agent）在与环境交互过程中，通过试错获得最大化累积奖励的策略。智能体根据当前状态选择动作，环境反馈奖励或惩罚，智能体据此调整策略。强化学习广泛应用于游戏、机器人控制和自动驾驶等领域，代表性算法有Q-learning、Deep Q Network（DQN）等。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;2-深度学习-deep-learning&#34;&gt;2. 深度学习（Deep Learning）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;英文名称&lt;/strong&gt;：Deep Learning&lt;br /&gt;
&lt;strong&gt;原理&lt;/strong&gt;：深度学习是一种以多层神经网络为基础的机器学习方法。通过构建多层（深层）神经网络，能够自动从大量数据中提取高层次特征，实现图像识别、语音识别、自然语言处理等复杂任务。深度学习的核心在于端到端的特征学习和强大的表达能力，常见模型有卷积神经网络（CNN）、循环神经网络（RNN）等。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;3-知识图谱-knowledge-graph&#34;&gt;3. 知识图谱（Knowledge Graph）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;英文名称&lt;/strong&gt;：Knowledge Graph&lt;br /&gt;
&lt;strong&gt;原理&lt;/strong&gt;：知识图谱是一种以图结构组织和表达知识的方法，将实体（如人、地点、事物）及其关系以节点和边的形式存储。通过语义关联，知识图谱支持复杂的推理和查询，广泛应用于搜索引擎、智能问答和推荐系统。其构建涉及信息抽取、实体消歧、关系抽取等技术。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;4-多维缩放-multidimensional-scaling-mds&#34;&gt;4. 多维缩放（Multidimensional Scaling, MDS）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;英文名称&lt;/strong&gt;：Multidimensional Scaling (MDS)&lt;br /&gt;
&lt;strong&gt;原理&lt;/strong&gt;：多维缩放是一种降维方法，用于将高维数据映射到低维空间（通常是二维或三维），以便可视化和分析。MDS通过保持数据点之间的距离关系，尽量还原原始数据的结构。它常用于心理学、市场分析和生物信息学等领域的数据可视化。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;5-层次聚类-hierarchical-clustering&#34;&gt;5. 层次聚类（Hierarchical Clustering）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;英文名称&lt;/strong&gt;：Hierarchical Clustering&lt;br /&gt;
&lt;strong&gt;原理&lt;/strong&gt;：层次聚类是一种无监督学习方法，通过构建数据的层次结构（树状图或树状图谱）来发现数据的内在分组。算法分为自底向上（凝聚型）和自顶向下（分裂型）两类。层次聚类无需预先指定簇的数量，适用于探索数据的多层次结构。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;6-多层感知-multilayer-perceptron-mlp&#34;&gt;6. 多层感知（Multilayer Perceptron, MLP）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;英文名称&lt;/strong&gt;：Multilayer Perceptron (MLP)&lt;br /&gt;
&lt;strong&gt;原理&lt;/strong&gt;：多层感知是一种前馈神经网络，由输入层、一个或多个隐藏层和输出层组成。每层由多个神经元构成，层与层之间全连接。MLP通过反向传播算法进行训练，能够逼近任意非线性函数，是深度学习的基础结构之一，广泛用于分类和回归任务。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;7-卷积网络-convolutional-neural-network-cnn&#34;&gt;7. 卷积网络（Convolutional Neural Network, CNN）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;英文名称&lt;/strong&gt;：Convolutional Neural Network (CNN)&lt;br /&gt;
&lt;strong&gt;原理&lt;/strong&gt;：卷积神经网络是一种专门处理具有网格结构数据（如图像）的深度学习模型。其核心在于卷积层，通过局部感受野和权重共享机制提取空间特征，极大减少参数数量。CNN在图像识别、目标检测、视频分析等领域表现卓越。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;8-协同过滤-collaborative-filtering&#34;&gt;8. 协同过滤（Collaborative Filtering）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;英文名称&lt;/strong&gt;：Collaborative Filtering&lt;br /&gt;
&lt;strong&gt;原理&lt;/strong&gt;：协同过滤是一种推荐系统技术，通过分析用户与物品的交互行为（如评分、点击）来预测用户可能喜欢的物品。分为基于用户和基于物品的协同过滤。其核心思想是“物以类聚，人以群分”，即相似用户喜欢相似物品。常用于电商、视频、音乐等平台的个性化推荐。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;9-低秩适配-low-rank-adaptation-lora&#34;&gt;9. 低秩适配（Low-Rank Adaptation, LoRA）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;英文名称&lt;/strong&gt;：Low-Rank Adaptation (LoRA)&lt;br /&gt;
&lt;strong&gt;原理&lt;/strong&gt;：低秩适配是一种参数高效微调（PEFT）方法，主要用于大模型的迁移学习。LoRA通过在预训练模型的权重矩阵中插入低秩分解模块，仅训练少量参数即可适应新任务，显著降低计算和存储成本。广泛应用于自然语言处理和多模态模型的微调。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;10-监督微调-supervised-fine-tuning&#34;&gt;10. 监督微调（Supervised Fine-tuning）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;英文名称&lt;/strong&gt;：Supervised Fine-tuning&lt;br /&gt;
&lt;strong&gt;原理&lt;/strong&gt;：监督微调是指在有标签数据集上对预训练模型进行再训练，使其更好地适应特定任务。通过监督信号（如分类标签），模型参数得到针对性优化。该方法在大模型落地应用中极为常见，如BERT、GPT等模型在下游任务的微调。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;11-交叉验证-cross-validation&#34;&gt;11. 交叉验证（Cross Validation）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;英文名称&lt;/strong&gt;：Cross Validation&lt;br /&gt;
&lt;strong&gt;原理&lt;/strong&gt;：交叉验证是一种模型评估方法，将数据集划分为若干子集，轮流用其中一部分作为验证集，其余作为训练集。常见的有K折交叉验证。该方法能有效评估模型的泛化能力，减少过拟合风险，是机器学习模型选择和调优的重要工具。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;12-具身智能-embodied-intelligence&#34;&gt;12. 具身智能（Embodied Intelligence）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;英文名称&lt;/strong&gt;：Embodied Intelligence&lt;br /&gt;
&lt;strong&gt;原理&lt;/strong&gt;：具身智能强调智能体与物理世界的交互，认为智能的产生离不开身体和环境。具身智能体（如机器人）通过感知、运动和环境反馈实现学习和适应。该理念推动了机器人学、自动驾驶和虚拟现实等领域的发展，强调“智能在于行动”。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;13-模型量化-model-quantization&#34;&gt;13. 模型量化（Model Quantization）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;英文名称&lt;/strong&gt;：Model Quantization&lt;br /&gt;
&lt;strong&gt;原理&lt;/strong&gt;：模型量化是一种模型压缩技术，通过将模型参数从高精度（如32位浮点）转换为低精度（如8位整数），以减少模型体积和加速推理速度。量化可显著降低硬件资源消耗，适用于边缘设备和移动端AI部署。常见量化方法有对称量化、非对称量化等。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;14-判别分析-discriminant-analysis&#34;&gt;14. 判别分析（Discriminant Analysis）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;英文名称&lt;/strong&gt;：Discriminant Analysis&lt;br /&gt;
&lt;strong&gt;原理&lt;/strong&gt;：判别分析是一类用于分类的统计方法，通过寻找最佳投影方向，使不同类别的数据在该方向上分离度最大。典型方法有线性判别分析（LDA）和二次判别分析（QDA）。判别分析常用于模式识别、医学诊断等领域。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;15-随机森林-random-forest&#34;&gt;15. 随机森林（Random Forest）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;英文名称&lt;/strong&gt;：Random Forest&lt;br /&gt;
&lt;strong&gt;原理&lt;/strong&gt;：随机森林是一种集成学习方法，通过构建多个决策树并对其结果进行投票或平均，提升模型的准确性和鲁棒性。每棵树在训练时随机选择特征和样本，减少过拟合。随机森林广泛应用于分类、回归和特征选择等任务。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;16-知识蒸馏-knowledge-distillation&#34;&gt;16. 知识蒸馏（Knowledge Distillation）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;英文名称&lt;/strong&gt;：Knowledge Distillation&lt;br /&gt;
&lt;strong&gt;原理&lt;/strong&gt;：知识蒸馏是一种模型压缩技术，通过将大型“教师模型”的知识迁移到小型“学生模型”。学生模型在训练时不仅学习真实标签，还模仿教师模型的输出分布，从而获得更好的泛化能力。知识蒸馏常用于模型加速和部署。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;17-专家混合-mixture-of-experts-moe&#34;&gt;17. 专家混合（Mixture of Experts, MoE）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;英文名称&lt;/strong&gt;：Mixture of Experts (MoE)&lt;br /&gt;
&lt;strong&gt;原理&lt;/strong&gt;：专家混合是一种模型结构，将多个“专家”子模型与一个门控网络结合。门控网络根据输入动态选择部分专家参与计算，实现模型容量与计算效率的平衡。MoE在大规模语言模型和多任务学习中表现突出，能有效提升模型性能。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;18-多头注意-multi-head-attention&#34;&gt;18. 多头注意（Multi-Head Attention）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;英文名称&lt;/strong&gt;：Multi-Head Attention&lt;br /&gt;
&lt;strong&gt;原理&lt;/strong&gt;：多头注意是Transformer模型的核心机制，通过并行计算多个注意力头，捕捉不同子空间的信息。每个头独立学习输入序列的不同关系，最后将各头输出拼接融合。多头注意极大提升了模型对复杂依赖关系的建模能力，广泛应用于自然语言处理和计算机视觉。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;19-梯度提升-gradient-boosting&#34;&gt;19. 梯度提升（Gradient Boosting）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;英文名称&lt;/strong&gt;：Gradient Boosting&lt;br /&gt;
&lt;strong&gt;原理&lt;/strong&gt;：梯度提升是一种集成学习方法，通过逐步训练一系列弱学习器（如决策树），每一步都拟合前一步的残差。最终模型是所有弱学习器的加权和。梯度提升具有强大的拟合能力，代表性算法有XGBoost、LightGBM等，广泛应用于结构化数据建模。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;20-多智能体-multi-agent&#34;&gt;20. 多智能体（Multi-Agent）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;英文名称&lt;/strong&gt;：Multi-Agent&lt;br /&gt;
&lt;strong&gt;原理&lt;/strong&gt;：多智能体系统由多个相互作用的智能体组成，每个智能体可独立感知、决策和行动。多智能体系统强调协作、竞争和通信，适用于分布式控制、博弈论、群体智能等场景。其研究推动了自动驾驶、智能交通、机器人集群等领域的发展。&lt;/p&gt;

&lt;hr /&gt;
</description>
        </item>
        
        <item>
            <title>常用AI术语</title>
            <link>http://mospany.github.io/2025/07/13/ai-term/</link>
            <pubDate>Sun, 13 Jul 2025 18:02:09 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2025/07/13/ai-term/</guid>
            <description>

&lt;h1 id=&#34;常用ai术语归类&#34;&gt;常用AI术语归类&lt;/h1&gt;

&lt;h2 id=&#34;基础概念&#34;&gt;基础概念&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;人工智能（Artificial Intelligence, AI）&lt;/strong&gt;：使机器表现出类似人类智能的技术和方法。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;机器学习（Machine Learning, ML）&lt;/strong&gt;：让计算机通过数据自动学习和改进的技术。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;深度学习（Deep Learning, DL）&lt;/strong&gt;：基于多层神经网络的机器学习方法。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;神经网络（Neural Network, NN）&lt;/strong&gt;：模仿人脑神经元结构的计算模型。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据集（Dataset）&lt;/strong&gt;：用于训练和测试模型的数据集合。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;机器学习相关术语&#34;&gt;机器学习相关术语&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;监督学习（Supervised Learning）&lt;/strong&gt;：利用带标签数据训练模型的方法。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;无监督学习（Unsupervised Learning）&lt;/strong&gt;：利用无标签数据发现数据结构的方法。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;半监督学习（Semi-supervised Learning）&lt;/strong&gt;：结合少量有标签和大量无标签数据进行学习的方法。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强化学习（Reinforcement Learning, RL）&lt;/strong&gt;：通过奖励和惩罚机制让智能体学习策略的方法。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特征（Feature）&lt;/strong&gt;：用于描述数据的属性或变量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;标签（Label）&lt;/strong&gt;：数据对应的目标输出或分类。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;深度学习相关术语&#34;&gt;深度学习相关术语&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;卷积神经网络（Convolutional Neural Network, CNN）&lt;/strong&gt;：常用于图像处理的神经网络结构。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;循环神经网络（Recurrent Neural Network, RNN）&lt;/strong&gt;：适合处理序列数据的神经网络结构。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;生成对抗网络（Generative Adversarial Network, GAN）&lt;/strong&gt;：由生成器和判别器组成的生成模型。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自注意力机制（Self-Attention Mechanism）&lt;/strong&gt;：模型在处理序列时关注不同位置的信息机制。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;变换器（Transformer）&lt;/strong&gt;：基于自注意力机制的深度学习模型，广泛用于NLP。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;自然语言处理-nlp-相关术语&#34;&gt;自然语言处理（NLP）相关术语&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;自然语言处理（Natural Language Processing, NLP）&lt;/strong&gt;：让计算机理解和处理人类语言的技术。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;词嵌入（Word Embedding）&lt;/strong&gt;：将词语转换为向量表示的方法。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分词（Tokenization）&lt;/strong&gt;：将文本切分为词或子词的过程。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;命名实体识别（Named Entity Recognition, NER）&lt;/strong&gt;：识别文本中专有名词的任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;文本生成（Text Generation）&lt;/strong&gt;：自动生成自然语言文本的技术。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;训练与评估相关术语&#34;&gt;训练与评估相关术语&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;损失函数（Loss Function）&lt;/strong&gt;：衡量模型预测与真实值差异的函数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优化器（Optimizer）&lt;/strong&gt;：用于调整模型参数以最小化损失函数的算法。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;过拟合（Overfitting）&lt;/strong&gt;：模型在训练集上表现好但在新数据上表现差的现象。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;欠拟合（Underfitting）&lt;/strong&gt;：模型无法很好地拟合训练数据的现象。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;准确率（Accuracy）&lt;/strong&gt;：预测正确的样本占总样本的比例。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;召回率（Recall）&lt;/strong&gt;：正确预测的正样本占所有正样本的比例。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;精确率（Precision）&lt;/strong&gt;：正确预测的正样本占所有预测为正样本的比例。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;F1分数（F1 Score）&lt;/strong&gt;：精确率和召回率的调和平均数。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;其他常见术语&#34;&gt;其他常见术语&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;大语言模型（Large Language Model, LLM）&lt;/strong&gt;：参数量巨大的自然语言处理模型，如GPT、BERT等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;推理（Inference）&lt;/strong&gt;：使用训练好的模型对新数据进行预测的过程。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;微调（Fine-tuning）&lt;/strong&gt;：在预训练模型基础上针对特定任务进行再训练。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;迁移学习（Transfer Learning）&lt;/strong&gt;：将已有模型知识应用到新任务的学习方法。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;开源（Open Source）&lt;/strong&gt;：源代码公开、可自由使用和修改的软件。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;本文将持续补充更多AI相关术语，欢迎留言交流！&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        </item>
        
        <item>
            <title>AI学习全景图</title>
            <link>http://mospany.github.io/2025/07/13/ai-study-toc/</link>
            <pubDate>Sun, 13 Jul 2025 16:27:22 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2025/07/13/ai-study-toc/</guid>
            <description>

&lt;h1 id=&#34;脑图&#34;&gt;脑图&lt;/h1&gt;

&lt;p&gt;&lt;center&gt;&lt;embed src=&#34;http://blog.mospan.cn/post/img/ai/ai-study-toc.pdf&#34; width=100% height=800&gt;&lt;/center&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>vGPU方案HAMi(01): 实现细粒度 GPU 切分</title>
            <link>http://mospany.github.io/2024/02/05/hami-vgpu-split/</link>
            <pubDate>Mon, 05 Feb 2024 19:31:10 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2024/02/05/hami-vgpu-split/</guid>
            <description>

&lt;p&gt;本文主要分享一个开源的 GPU 虚拟化方案：HAMi，包括如何安装、配置以及使用。&lt;/p&gt;

&lt;p&gt;相比于上一篇分享的 TimeSlicing 方案，HAMi 除了 GPU 共享之外还可以实现 GPU core、memory 的限制，保证共享同一 GPU 的各个 Pod 都能拿到足够的资源。&lt;/p&gt;

&lt;p&gt;本文主要对开源的 vGPU 方案 HAMi 的 GPU Core&amp;amp;Memory 隔离功能进行测试。&lt;/p&gt;

&lt;h1 id=&#34;为什么需要-gpu-共享-切分等方案&#34;&gt;为什么需要 GPU 共享、切分等方案？&lt;/h1&gt;

&lt;p&gt;开始之前我们先思考一个问题，&lt;em&gt;为什么需要 GPU 共享、切分等方案？&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;或者说是另外一个问题：&lt;em&gt;明明直接在裸机环境使用，都可以多个进程共享 GPU，怎么到 k8s 环境就不行了。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;推荐阅读前面几篇文章：这两篇分享了如何在各个环境中使用 GPU，在 k8s 环境则推荐使用 NVIDIA 提供的 gpu-operator 快速部署环境。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.mospan.cn/2024/01/16/gpu-on-k8s/&#34;&gt;K8S项目实践(08): 在ECS、Docker、K8s 等环境中使用 GPU&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.mospan.cn/2024/01/17/gpu-operator/&#34;&gt;K8S项目实践(09): 使用 GPU Operator搭建AI算力环境&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;这两篇则分析了 device-plugin 原理以及在 K8s 中创建一个申请 GPU 的 Pod 后的一些列动作，最终该 Pod 是如何使用到 GPU 的。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.mospan.cn/2024/01/24/device-plugin/&#34;&gt;K8S项目实践(10): device-plugin原理到实现&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.mospan.cn/2024/01/27/pod-use-gpu/&#34;&gt;K8S项目实践(11): Pod 是如何使用到 GPU 的及源码分析&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;看完之后，大家应该就大致明白了。&lt;/p&gt;

&lt;h2 id=&#34;资源感知&#34;&gt;资源感知&lt;/h2&gt;

&lt;p&gt;首先在 k8s 中资源是和节点绑定的，对于 GPU 资源，我们使用 NVIDIA 提供的 device-plugin 进行感知，并上报到 kube-apiserver,这样我们就能在 Node 对象上看到对应的资源了。&lt;/p&gt;

&lt;p&gt;就像这样：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# kubectl  describe node k8s-worker1 | grep Capacity: -A8
Capacity:
  cpu:                   4
  ephemeral-storage:     123460788Ki
  hugepages-1Gi:         0
  hugepages-2Mi:         0
  lixueduan.com/gopher:  2
  memory:                9922140Ki
  nvidia.com/gpu:        1
  pods:                  110
[root@k8s-master1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，该节点除了基础的 cpu、memory 之外，还有一个nvidia.com/gpu: 1 信息，表示该节点上有 1 个 GPU。&lt;/p&gt;

&lt;h2 id=&#34;资源申请&#34;&gt;资源申请&lt;/h2&gt;

&lt;p&gt;然后我们就可以在创建 Pod 时申请对应的资源了，比如申请一个 GPU：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  name: cuda-vectoradd
spec:
  restartPolicy: OnFailure
  containers:
  - name: cuda-vectoradd
    image: &amp;quot;nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda10.2&amp;quot;
    resources:
      limits:
        nvidia.com/gpu: 1
    command: [&amp;quot;nvidia-smi&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;apply 该 yaml 之后，kube-scheduler 在调度该 Pod 时就会将其调度到一个拥有足够 GPU 资源的 Node 上。&lt;/p&gt;

&lt;p&gt;同时该 Pod 申请的部分资源也会标记为已使用，不会再分配给其他 Pod。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;post/20224/images/2024-02-05-hami-vgpu-split/IMG_20250205-100211174.png&#34; alt=&#34;picture 0&#34; /&gt;&lt;/p&gt;

&lt;p&gt;到这里，问题的答案就已经很明显的。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）device-plugin 感知到节点上的物理 GPU 数量，上报到 kube-apiserver&lt;/li&gt;
&lt;li&gt;2）kube-scheduler 调度 Pod 时会根据 pod 中的 Request 消耗对应资源
即：&lt;em&gt;Node 上的 GPU 资源被 Pod 申请之后，在 k8s 中就被标记为已消耗了，后续创建的 Pod 会因为资源不够导致无法调度。&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实际上：可能 GPU 性能比较好，可以支持多个 Pod 共同使用，但是因为 k8s 中的调度限制导致多个 Pod 无法正常共享。&lt;/p&gt;

&lt;p&gt;因此，我们才需要 GPU 共享、切分等方案。&lt;/p&gt;

&lt;p&gt;上一篇文章&lt;a href=&#34;http://blog.mospan.cn/2024/01/31/gpu-time-slicing/&#34;&gt;K8S项目实践(12): GPU共享方案Time Slicing&lt;/a&gt;中给大家分享了一个 GPU 共享方案。&lt;/p&gt;

&lt;p&gt;可以实现多个 Pod 共享同一个 GPU，但是存在一个问题：Pod 之间并未做任何隔离，每个 Pod 能用到多少 GPU core、memory 都靠竞争，可能会导致部分 Pod 占用大部分资源导致其他 Pod 无法正常使用的情况。&lt;/p&gt;

&lt;p&gt;今天给大家分享一个开源的 vGPU 方案 &lt;a href=&#34;https://github.com/Project-HAMi/HAMi&#34;&gt;HAMi&lt;/a&gt;。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ps：NVIDIA 也有自己的 vGPU 方案，但是需要 license&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;什么是-hami&#34;&gt;什么是 HAMi？&lt;/h1&gt;

&lt;p&gt;HAMi 全称是：Heterogeneous AI Computing Virtualization Middleware，HAMi 给自己的定位或者希望是做一个异构算力虚拟化平台。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;原 第四范式 k8s-vgpu-scheduler, 这次改名 HAMi 同时也将核心的 vCUDA 库 libvgpu.so 也开源了。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;但是现在比较完善的是对 NVIDIA GPU 的 vGPU 方案，因此我们可以简单认为他就是一个 vGPU 方案。&lt;/p&gt;

&lt;p&gt;整体架构如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-02-05-hami-vgpu-split/IMG_20250205-101305158.png&#34; alt=&#34;picture 1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到组件还是比较多的，涉及到 Webhook、Scheduler、Device Plugin、HAMi-Core 等等。&lt;/p&gt;

&lt;p&gt;这篇文章只讲使用，因此架构、原理就一笔带过，后续也会有相关文章,欢迎关注~。&lt;/p&gt;

&lt;h2 id=&#34;特性&#34;&gt;特性&lt;/h2&gt;

&lt;p&gt;使用 HAMi 最大的一个功能点就是可以实现 GPU 的细粒度的隔离，可以对 core 和 memory 使用 1% 级别的隔离。&lt;/p&gt;

&lt;p&gt;具体如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  containers:
    - name: ubuntu-container
      image: ubuntu:18.04
      command: [&amp;quot;bash&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;sleep 86400&amp;quot;]
      resources:
        limits:
          nvidia.com/gpu: 1 # 请求1个vGPUs
          nvidia.com/gpumem: 3000 # 每个vGPU申请3000m显存 （可选，整数类型）
          nvidia.com/gpucores: 30 # 每个vGPU的算力为30%实际显卡的算力 （可选，整数类型）
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;nvidia.com/gpu：请求一个 GPU&lt;/li&gt;
&lt;li&gt;nvidia.com/gpumem：只申请使用 3000M GPU Memory&lt;/li&gt;
&lt;li&gt;nvidia.com/gpucores：申请使用 30% 的 GPU core，也就是该 Pod 只能使用到 30% 的算力
相比于上文分享了 TimeSlicing 方案，HAMi 则是实现了 GPU core 和 memory 的隔离。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;在开源方案里面已经算是比较优秀的了。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;特性-1&#34;&gt;特性&lt;/h2&gt;

&lt;p&gt;HAMi 实现GPU core 和 memory 隔离、限制是使用的 vCUDA 方案，具体设计如下：
&lt;img src=&#34;post/2024/images/2024-02-05-hami-vgpu-split/IMG_20250205-102943833.png&#34; alt=&#34;picture 2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;HAMi 使用的是软件层面的 vCUDA 方案，对 NVIDIA 原生的 CUDA 驱动进行重写(libvgpu.so)，然后挂载到 Pod 中进行替换，然后在自己的实现的 CUDA 驱动中对 API 进行拦截，实现资源隔离以及限制的效果。&lt;/p&gt;

&lt;p&gt;例如：原生 libvgpu.so 在进行内存分配时，只有在 GPU 内存真的用完的时候才会提示 CUDA OOM，但是对于 HAMi 实现的 libvgpu.so 来说，检测到 Pod 中使用的内存超过了 Resource 中的申请量就直接返回 OOM，从而实现资源的一个限制。&lt;/p&gt;

&lt;p&gt;然后在执行 nvidia-smi 命令查看 GPU 信息时，也只返回 Pod Resource 中申请的资源，这样在查看时也进行隔离。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ps：需要对 CUDA 和 NVML 的部分 API 拦截。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;hami-部署&#34;&gt;HAMi 部署&lt;/h1&gt;

&lt;p&gt;HAMi 提供了 Helm Chart 安装也是比较简单的。&lt;/p&gt;

&lt;h2 id=&#34;部署-gpu-operator&#34;&gt;部署 GPU Operator&lt;/h2&gt;

&lt;p&gt;需要注意的是 HAMi 会依赖 NVIDIA 的那一套，因此推荐先部署 GPU-Operator。&lt;/p&gt;

&lt;p&gt;参考这篇文章 –&amp;gt; &lt;a href=&#34;http://blog.mospan.cn/2024/01/17/gpu-operator/&#34;&gt;K8S项目实践(09): 使用 GPU Operator搭建AI算力环境&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;部署好 GPU Operator 之后在部署 HAMi。&lt;/p&gt;

&lt;h2 id=&#34;准备镜像&#34;&gt;准备镜像&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;crictl pull ghcr.io/stackhpc/kube-webhook-certgen:v1.1.1

ctr -n k8s.io image tag  ghcr.io/stackhpc/kube-webhook-certgen:v1.1.1  liangjw/kube-webhook-certgen:v1.1.1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;部署-hami&#34;&gt;部署 HAMi&lt;/h2&gt;

&lt;p&gt;首先使用 helm 添加我们的 repo&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;helm repo add hami-charts https://project-hami.github.io/HAMi/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;随后，使用下列指令获取集群服务端版本
&amp;gt; 这里使用的是 v1.28.15版本&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl version
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在安装过程中须根据集群服务端版本（上一条指令的结果）指定调度器镜像版本，例如集群服务端版本为 v1.28.5，则可以使用如下指令进行安装&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;helm install hami hami-charts/hami --set scheduler.kubeScheduler.imageTag=v1.28.5 -n kube-system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过kubectl get pods指令看到 vgpu-device-plugin 与 vgpu-scheduler 两个pod 状态为Running 即为安装成功&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(12): GPU共享方案Time Slicing</title>
            <link>http://mospany.github.io/2024/01/31/gpu-time-slicing/</link>
            <pubDate>Wed, 31 Jan 2024 19:31:10 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2024/01/31/gpu-time-slicing/</guid>
            <description>

&lt;p&gt;本文主要分享 GPU 共享方案，包括如何安装、配置以及使用，最后通过分析源码了 TimeSlicing 的具体实现。通过配置 TimeSlicing 可以实现 Pod 共享一块物理 GPU(使用GRID GPU代替)，以提升资源利用率。&lt;/p&gt;

&lt;h1 id=&#34;1-为什么需要-gpu-共享-切分等方案&#34;&gt;1. 为什么需要 GPU 共享、切分等方案？&lt;/h1&gt;

&lt;p&gt;开始之前我们先思考一个问题，&lt;em&gt;为什么需要 GPU 共享、切分等方案?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;或者说是另外一个问题：&lt;em&gt;明明直接在裸机环境使用，都可以多个进程共享 GPU，怎么到 k8s 环境就不行了。&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-1-资源感知&#34;&gt;1.1. 资源感知&lt;/h2&gt;

&lt;p&gt;首先在 k8s 中资源是和节点绑定的，对于 GPU 资源，我们使用 NVIDIA 提供的 device-plugin 进行感知，并上报到 kube-apiserver,这样我们就能在 Node 对象上看到对应的资源了。&lt;/p&gt;

&lt;p&gt;就像这样：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@liqivm:~# k describe node gpu01|grep Capacity -A 7
Capacity:
  cpu:                128
  ephemeral-storage:  879000896Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             1056457696Ki
  nvidia.com/gpu:     8
  pods:               110
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，该节点除了基础的 cpu、memory 之外，还有一个nvidia.com/gpu: 8 信息，表示该节点上有 8 个 GPU。&lt;/p&gt;

&lt;h2 id=&#34;1-2-资源申请&#34;&gt;1.2. 资源申请&lt;/h2&gt;

&lt;p&gt;然后我们就可以在创建 Pod 时申请对应的资源了，比如申请一个 GPU：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  containers:
  - name: gpu-container
    image: nvidia/cuda:11.0-base   # 一个支持 GPU 的镜像
    resources:
      limits:
        nvidia.com/gpu: 1          # 申请 1 个 GPU
    command: [&amp;quot;nvidia-smi&amp;quot;]         # 示例命令，显示 GPU 的信息
  restartPolicy: OnFailure
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;apply 该 yaml 之后，kube-scheduler 在调度该 Pod 时就会将其调度到一个拥有足够 GPU 资源的 Node 上。&lt;/p&gt;

&lt;p&gt;同时该 Pod 申请的部分资源也会标记为已使用，不会在分配给其他 Pod。&lt;/p&gt;

&lt;p&gt;到这里，问题的答案就已经很明显的。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）device-plugin 感知到节点上的物理 GPU 数量，上报到 kube-apiserver&lt;/li&gt;
&lt;li&gt;2）kube-scheduler 调度 Pod 时会根据 pod 中的 Request 消耗对应资源&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;即：&lt;em&gt;Node 上的 GPU 资源被 Pod 申请之后，在 k8s 中就被标记为已消耗了，后续创建的 Pod 会因为资源不够导致无法调度。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;实际上：可能 GPU 性能比较好，可以支持多个 Pod 共同使用，但是因为 k8s 中的调度限制导致多个 Pod 无法正常共享。&lt;/p&gt;

&lt;p&gt;因此，我们才需要 GPU 共享、切分等方案。&lt;/p&gt;

&lt;h1 id=&#34;2-什么是-time-slicing-方案&#34;&gt;2. 什么是 Time Slicing 方案&lt;/h1&gt;

&lt;p&gt;NVIDIA 提供的 &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html&#34;&gt;Time-Slicing GPUs in Kubernetes&lt;/a&gt; 是一种通过 oversubscription(超额订阅) 来实现 GPU 共享的策略，这种策略能让多个任务在同一个 GPU 上进行，而不是每个任务都独占一个 GPU。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;虽然方案名称叫做 Time Slicing，但是device-plugin 的实现上和时间切片没有任何关系，实际上是一个 GPU 超卖方案。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;2-1-为什么要叫time-slicing-时间片&#34;&gt;2.1. 为什么要叫Time Slicing(时间片)？&lt;/h2&gt;

&lt;p&gt;虽然实现上只是 *oversubscription(超额订阅)*，但是名称中的 Time Slicing(时间片)指的是 GPU 本身的时间片调度。&lt;/p&gt;

&lt;p&gt;例如：A、B、C 三个进程共享 GPU，三个进程同时把 CUDA 任务发射到 GPU 上去，GPU 并不会同时执行，而是采用时间片轮转调度的方式。&lt;/p&gt;

&lt;p&gt;首先第一个时间片，A 任务被执行，接着第二个时间片，执行 B 任务，第三个时间片， C 任务将被执行。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;时间片是依次进行轮转调度的，分别执行A、B、C中的任务。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;2-2-效果&#34;&gt;2.2. 效果&lt;/h2&gt;

&lt;p&gt;比如节点上只有一个物理 GPU，正常安装 GPU Operator 之后，device plugin 检测到该节点上有 1 个 GPU，上报给 kubelet，然后 kubelet 更新到 kube-apiserver，我们就可以在 Node 对象上看到了：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@liqivm:~# k describe node gpu01|grep Capacity -A 7
Capacity:
  cpu:                128
  ephemeral-storage:  879000896Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             1056457696Ki
  nvidia.com/gpu:     1
  pods:               110
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时，创建一个 Pod 申请 1 个 GPU 之后，第二个 Pod 就无法使用了，因为 GPU 资源不足无法调度。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;但是 Time Slicing 可以进行 oversubscription 设置，将 device-plugin 上报的 GPU 数量进行扩大。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;比如将其数量放大 10 倍，device plugin 就会上报该节点有 1*10 = 10 个 GPU，最终 kube-apiserver 则会记录该节点有 10 个 GPU：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@liqivm:~# k describe node gpu01|grep Capacity -A 7
Capacity:
  cpu:                128
  ephemeral-storage:  879000896Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             1056457696Ki
  nvidia.com/gpu:     10
  pods:               110
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样，就可以供 10 个 Pod 使用了。&lt;/p&gt;

&lt;p&gt;当然了，Time Slicing 方案也有缺点：多个 Pod 之间没有内存或者故障隔离，完全的共享，能使用多少内存和算力全靠多个 Pod 自行竞争。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ps：就和直接在宿主机上多个进程共享一个 GPU 基本一致&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;3-time-slicing-demo&#34;&gt;3. Time Slicing Demo&lt;/h1&gt;

&lt;p&gt;Time Slicing 由于是 NVIDIA 的方案，因此使用起来比较简单，只需要在部署完成 GPU Operator 之后进行配置即可。&lt;/p&gt;

&lt;p&gt;首先参考这篇文章完成 GPU Operator 的部署 –&amp;gt; GPU 环境搭建指南：&lt;a href=&#34;http://blog.mospan.cn/2024/01/17/gpu-operator/&#34;&gt;使用 GPU Operator搭建AI算力环境&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;然后即可开始配置 TimeSlicing。&lt;/p&gt;

&lt;p&gt;整体配置分为以下 3 个步骤：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）创建 TimeSlicing 配置

&lt;ul&gt;
&lt;li&gt;根据官方文档描述，修改了 TimeSlicing 配置之后，device plugin Pod 不会自动重启，因此新的配置不会生效,需要手动重启对应 Pod。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2）修改集群策略开启 Time Slicing，并指定让 device-plugin 使用第一步中创建的配置

&lt;ul&gt;
&lt;li&gt;这里则是通过 Configmap 名称来指定&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;3）（可选）给要使用 GPU TimeSlicing 的节点打上对应 label，实现不同 Node 使用不同策略

&lt;ul&gt;
&lt;li&gt;比如不同节点上的 GPU 不同，那么可以根据 GPU 的算力或者内存情况设置不同的副本数以合理利用资源&lt;/li&gt;
&lt;li&gt;如果都是统一 GPU，则使用集群级别的统一配置即可&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;3-1-配置开启-timeslicing&#34;&gt;3.1. 配置开启 TimeSlicing&lt;/h2&gt;

&lt;h3 id=&#34;3-1-1-创建-timeslicing-配置&#34;&gt;3.1.1. 创建 TimeSlicing 配置&lt;/h3&gt;

&lt;p&gt;使用一个单独的 Configmap 来存放 TimeSlicing 的配置。&lt;/p&gt;

&lt;p&gt;这里使用集群级别的统一配置，配置文件 time-slicing-config-all.yaml 完整内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: time-slicing-config-all
data:
  any: |-
    version: v1
    flags:
      migStrategy: none
    sharing:
      timeSlicing:
        renameByDefault: false
        failRequestsGreaterThanOne: false
        resources:
          - name: nvidia.com/gpu
            replicas: 4    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体配置含义参考官方文档：&lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html#about-configuring-gpu-time-slicing&#34;&gt;about-configuring-gpu-time-slicing&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;data.&lt;key&gt;： 配置的名字，可以为不同 Node 设置单独配置，后续通过名称引用对应配置。

&lt;ul&gt;
&lt;li&gt;后续开启 TimeSlicing 时则根据 key 指定使用不同配置&lt;/li&gt;
&lt;li&gt;这里我们使用集群统一配置，因此创建一个 key 即可&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;flags.migStrategy：配置开启时间片之后如何处理 MIG 设备，默认为 none&lt;/li&gt;
&lt;li&gt;renameByDefault：是否对 GPU 资源改名。

&lt;ul&gt;
&lt;li&gt;设置为 true 之后，会使用&lt;resource-name&gt;.shared 替代原本的 &lt;resource-name&gt;。例如 nvidia.com/gpu 会变成 nvidia.com/gpu.shared ，显式告知使用者这是共享 GPU。&lt;/li&gt;
&lt;li&gt;默认为 false，即不改资源类型名，不过 Node 上的 label 也会改，比如使用时间片之前是nvidia.com/gpu.product=Tesla-T4, 使用后就会变成nvidia.com/gpu.product=Tesla-T4-SHARED 这样依旧可以通过 nodeSelector 来限制 Pod 调度节点，来控制是否使用共享的 GPU&lt;/li&gt;
&lt;li&gt;推荐使用 fasle 即可&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;failRequestsGreaterThanOne：开启后，当 Pod 请求 1 个以上的 shared GPU 时直接报错 UnexpectedAdmissionError。这个字段是通过报错的方式告诉使用者，请求多个 shared GPU 并不会增加 Pod 对该共享 GPU 的占用时间。&lt;/li&gt;
&lt;li&gt;resources.name：要通过时间分片提供访问的资源类似，比如nvidia.com/gpu&lt;/li&gt;
&lt;li&gt;resources.replicas：可共享访问的资源数量，比如这里指定的 4 也就是 1 个该类型的 GPU 可以供 4 个 Pod 共享访问，也就是最终 Pod 上看到的 GPU 数量是物理 GPU 数量的 4 倍。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;将配置 Apply 到 gpu-operator 所在的 namespace&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl create -n gpu-operator -f time-slicing-config-all.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-1-2-修改集群策略&#34;&gt;3.1.2. 修改集群策略&lt;/h3&gt;

&lt;p&gt;修改clusterpolicies.nvidia.com/cluster-policy 对象，让 device plugin 使用上一步创建的配置。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl patch clusterpolicies.nvidia.com/cluster-policy \
    -n gpu-operator --type merge \
    -p &#39;{&amp;quot;spec&amp;quot;: {&amp;quot;devicePlugin&amp;quot;: {&amp;quot;config&amp;quot;: {&amp;quot;name&amp;quot;: &amp;quot;time-slicing-config-all&amp;quot;, &amp;quot;default&amp;quot;: &amp;quot;any&amp;quot;}}}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;name：time-slicing-config-all 指定了配置文件对应的 Configmap 名称&lt;/li&gt;
&lt;li&gt;default：any：表示默认配置为这个 Configmap 中的 key 为 any 的配置
修改后 gpu-feature-discovery 和 nvidia-device-plugin-daemonset pod 会重启，使用以下命令查看重启过程&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl get events -n gpu-operator --sort-by=&#39;.lastTimestamp&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-31-gpu-time-slicing/IMG_20250201-215925170.png&#34; alt=&#34;picture 0&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;3-2-验证-timeslicing-是否生效&#34;&gt;3.2. 验证 TimeSlicing 是否生效&lt;/h2&gt;

&lt;h3 id=&#34;3-2-1-查看-node-上的-gpu-信息&#34;&gt;3.2.1. 查看 Node 上的 GPU 信息&lt;/h3&gt;

&lt;p&gt;首先查看一下 Node 信息，确认 TimeSlicing 生效了&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl  describe node k8s-worker1 | grep Capacity -A20
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-31-gpu-time-slicing/IMG_20250201-221241899.png&#34; alt=&#34;picture 1&#34; /&gt;&lt;br /&gt;
gpu为1*4=4显示成功。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;...
Labels:
                  nvidia.com/gpu.count=1
                  nvidia.com/gpu.product=GRID-T4-2Q-SHARED
                  nvidia.com/gpu.replicas=4
Capacity:
  nvidia.com/gpu: 4
  ...
Allocatable:
  nvidia.com/gpu: 4
  ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;增加了几个 label，&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nvidia.com/gpu.product=GRID-T4-2Q-SHARED&lt;/li&gt;
&lt;li&gt;nvidia.com/gpu.replicas=4&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;根据nvidia.com/gpu.count=1 可知，节点上有 1 张 GPU，然后由于使用了时间片，且配置的nvidia.com/gpu.replicas=4 副本数为 4，因此最终节点上 device plugin 上报的 GPU 数量就是 1*4 = 4 个。&lt;/p&gt;

&lt;h2 id=&#34;3-3-验证-gpu-能否正常使用&#34;&gt;3.3. 验证 GPU 能否正常使用&lt;/h2&gt;

&lt;p&gt;创建一个 Deployment 来验证，GPU 能否正常使用。&lt;/p&gt;

&lt;p&gt;这里副本数指定为 2，因为集群里只有 1 张 GPU，如果 TimeSlicing 未生效，那么有一个 Pod 肯定会应为拿不到 GPU 资源而 pending。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: time-slicing-verification
  labels:
    app: time-slicing-verification
spec:
  replicas: 2
  selector:
    matchLabels:
      app: time-slicing-verification
  template:
    metadata:
      labels:
        app: time-slicing-verification
    spec:
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      hostPID: true
      containers:
        - name: cuda-sample-vector-add
          image: &amp;quot;nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda10.2&amp;quot;
          command: [&amp;quot;/bin/bash&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;--&amp;quot;]
          args:
            - while true; do /tmp/vectorAdd; done
          resources:
           limits:
             nvidia.com/gpu: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;会启动 2个 Pod，&lt;/p&gt;

&lt;p&gt;查看情况
&lt;img src=&#34;post/2024/images/2024-01-31-gpu-time-slicing/IMG_20250201-224915379.png&#34; alt=&#34;picture 2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2 个 Pod 都启动了，说明时间片时成功的。&lt;/p&gt;

&lt;p&gt;随便查看一个 Pod 的日志&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master1 timeSlicing]# kubectl logs time-slicing-verification-56487b549d-nqn6s 
[Vector addition of 50000 elements]
Copy input data from the host memory to the CUDA device
CUDA kernel launch with 196 blocks of 256 threads
Copy output data from the CUDA device to the host memory
Test PASSED
Done
[Vector addition of 50000 elements]
Copy input data from the host memory to the CUDA device
CUDA kernel launch with 196 blocks of 256 threads
Copy output data from the CUDA device to the host memory
Test PASSED
Done
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;有 Test PASSED 则说明成功了。&lt;/p&gt;

&lt;p&gt;说明 TimeSlicing 配置生效了。&lt;/p&gt;

&lt;h1 id=&#34;4-使用-node-级别的单独配置&#34;&gt;4. 使用 Node 级别的单独配置&lt;/h1&gt;

&lt;p&gt;前面只创建了一个名称为 any 的配置，并在 clusterpolicy 中指明了使用该配置为默认配置，因此集群中的全部节点都会使用该配置来做时间片。&lt;/p&gt;

&lt;p&gt;但是可能*集群中不同节点上的 GPU 型号不同*，因此需要共享分副本数可以调整，性能好的副本数就调大一点，性能差的就小一点。&lt;/p&gt;

&lt;p&gt;本章主要记录怎么为不同的节点使用不同的配置。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;实际上是为不同的 GPU 准备不同的配置。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;4-1-创建时间片配置&#34;&gt;4.1. 创建时间片配置&lt;/h2&gt;

&lt;p&gt;同样的创建 TimeSlicing 配置，不过这次 Configmap 中写了两个配置，而且是以 GPU 型号命名的&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: time-slicing-config-fine
data:
  a100-40gb: |-
    version: v1
    flags:
      migStrategy: mixed
    sharing:
      timeSlicing:
        resources:
        - name: nvidia.com/gpu
          replicas: 8
        - name: nvidia.com/mig-1g.5gb
          replicas: 2
        - name: nvidia.com/mig-2g.10gb
          replicas: 2
        - name: nvidia.com/mig-3g.20gb
          replicas: 3
        - name: nvidia.com/mig-7g.40gb
          replicas: 7    
  tesla-t4: |-
    version: v1
    flags:
      migStrategy: none
    sharing:
      timeSlicing:
        resources:
        - name: nvidia.com/gpu
          replicas: 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，分别对 A100 和 Tesla T4 这两种 GPU 做了配置。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a100-40gb：A100 支持 MIG，因此增加了 MIG 部分的配置，若没有则指定为 none 即可

&lt;ul&gt;
&lt;li&gt;然后根据 MIG 实例分别指定不同的 replicas 数&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;tesla-t4：Tesla T4 GPU 性能比较差，因此 replicas 指定为 4 即可
将配置 Apply 到 gpu-operator 所在的 namespace&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl  create -n gpu-operator -f time-slicing-config-fine.yaml 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-31-gpu-time-slicing/IMG_20250204-201203558.png&#34; alt=&#34;picture 3&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;4-2-修改集群策略&#34;&gt;4.2. 修改集群策略&lt;/h2&gt;

&lt;p&gt;同样的，修改一下 cluster-policy 指定 device plugin 使用的 Configmap，这次与之前的区别在于，*这里没有指定 default 配置*。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl patch clusterpolicies.nvidia.com/cluster-policy \
-n gpu-operator --type merge \
-p &#39;{&amp;quot;spec&amp;quot;: {&amp;quot;devicePlugin&amp;quot;: {&amp;quot;config&amp;quot;: {&amp;quot;name&amp;quot;: &amp;quot;time-slicing-config-fine&amp;quot;}}}}&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;没有指定 default 时，device-plugin 则会根据 node 上的 label (nvidia.com/device-plugin.config)来获取要使用的配置。&lt;/p&gt;

&lt;h2 id=&#34;4-3-为节点打-label&#34;&gt;4.3. 为节点打 label&lt;/h2&gt;

&lt;p&gt;在节点上打上下面的 label，这样该节点上的 device plugin 就会根据该 label 的 value 来使用对应名字的配置了。&lt;/p&gt;

&lt;p&gt;比如这里，就是有这个 label 的节点就使用名叫 tesla-t4 的配置。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl label node k8s-worker1 nvidia.com/device-plugin.config=tesla-t4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一般都是以 GPU 型号命名，然后给使用该 GPU 的节点都打上对应 label,这样便于查看。&lt;/p&gt;

&lt;h2 id=&#34;4-4-验证&#34;&gt;4.4. 验证&lt;/h2&gt;

&lt;p&gt;查看node节点gpu卡数是否已为5.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl  describe node k8s-worker1 | grep Capacity -A20
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-31-gpu-time-slicing/IMG_20250204-203630774.png&#34; alt=&#34;picture 4&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;5-关闭-timeslicing&#34;&gt;5. 关闭 TimeSlicing&lt;/h1&gt;

&lt;p&gt;想关闭 TimeSlicing 配置也很简单，直接更新 集群策略 把 device plugin 下的 config 这一段去掉即可。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;  devicePlugin:
    config:
      default: any
      name: time-slicing-config-fine
    enabled: true
    env:
    - name: PASS_DEVICE_SPECS
      value: &amp;quot;true&amp;quot;
    - name: FAIL_ON_INIT_ERROR
      value: &amp;quot;true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl patch clusterpolicies.nvidia.com/cluster-policy -n gpu-operator --type json -p &#39;[{&amp;quot;op&amp;quot;: &amp;quot;remove&amp;quot;, &amp;quot;path&amp;quot;: &amp;quot;/spec/devicePlugin/config&amp;quot;}]&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后重启一下 device-plugin pod&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl rollout restart -n gpu-operator daemonset/nvidia-device-plugin-daemonset
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;不出意外的话就关掉了，再次查看 Pod 信息，GPU 就变成了物理 GPU 数量，说明关闭成功。
&lt;img src=&#34;post/2024/images/2024-01-31-gpu-time-slicing/IMG_20250204-204613867.png&#34; alt=&#34;picture 5&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;6-源码分析&#34;&gt;6. 源码分析&lt;/h1&gt;

&lt;p&gt;简单看下源码，分析 TimeSlicing 是怎么实现的。&lt;/p&gt;

&lt;p&gt;首先是 device-plugin 可以接收的配置&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// api/config/v1/config.go#L32
// Config is a versioned struct used to hold configuration information.
type Config struct {
	Version   string    `json:&amp;quot;version&amp;quot;             yaml:&amp;quot;version&amp;quot;`
	Flags     Flags     `json:&amp;quot;flags,omitempty&amp;quot;     yaml:&amp;quot;flags,omitempty&amp;quot;`
	Resources Resources `json:&amp;quot;resources,omitempty&amp;quot; yaml:&amp;quot;resources,omitempty&amp;quot;`
	Sharing   Sharing   `json:&amp;quot;sharing,omitempty&amp;quot;   yaml:&amp;quot;sharing,omitempty&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这也就是我们在 clusterPolicy 中配置的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: time-slicing-config-all
data:
  any: |-
    version: v1
    flags:
      migStrategy: none
    sharing:
      timeSlicing:
        renameByDefault: false
        failRequestsGreaterThanOne: false
        resources:
          - name: nvidia.com/gpu
            replicas: 4    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里我们关注 resources 中的 replicas 参数，正是这个参数定义了 oversubscription(超额订阅) 的额度。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;        resources:
          - name: nvidia.com/gpu
            replicas: 4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看下代码中是什么生效的&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// internal/rm/device_map.go#L282

// updateDeviceMapWithReplicas returns an updated map of resource names to devices with replica
// information from the active replicated resources config.
func updateDeviceMapWithReplicas(replicatedResources *spec.ReplicatedResources, oDevices DeviceMap) (DeviceMap, error) {
	devices := make(DeviceMap)

	// Begin by walking replicatedResources.Resources and building a map of just the resource names.
	names := make(map[spec.ResourceName]bool)
	for _, r := range replicatedResources.Resources {
		names[r.Name] = true
	}

	// Copy over all devices from oDevices without a resource reference in TimeSlicing.Resources.
	for r, ds := range oDevices {
		if !names[r] {
			devices[r] = ds
		}
	}

	// Walk shared Resources and update devices in the device map as appropriate.
	for _, resource := range replicatedResources.Resources {
		r := resource
		// Get the IDs of the devices we want to replicate from oDevices
		ids, err := oDevices.getIDsOfDevicesToReplicate(&amp;amp;r)
		if err != nil {
			return nil, fmt.Errorf(&amp;quot;unable to get IDs of devices to replicate for &#39;%v&#39; resource: %v&amp;quot;, r.Name, err)
		}
		// Skip any resources not matched in oDevices
		if len(ids) == 0 {
			continue
		}

		// Add any devices we don&#39;t want replicated directly into the device map.
		for _, d := range oDevices[r.Name].Difference(oDevices[r.Name].Subset(ids)) {
			devices.insert(r.Name, d)
		}

		// Create replicated devices add them to the device map.
		// Rename the resource for replicated devices as requested.
		name := r.Name
		if r.Rename != &amp;quot;&amp;quot; {
			name = r.Rename
		}
		for _, id := range ids {
			for i := 0; i &amp;lt; r.Replicas; i++ {
				annotatedID := string(NewAnnotatedID(id, i))
				replicatedDevice := *(oDevices[r.Name][id])
				replicatedDevice.ID = annotatedID
				replicatedDevice.Replicas = r.Replicas
				devices.insert(name, &amp;amp;replicatedDevice)
			}
		}
	}

	return devices, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心部分如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;for _, id := range ids {
  for i := 0; i &amp;lt; r.Replicas; i++ {
    annotatedID := string(NewAnnotatedID(id, i))
    replicatedDevice := *(oDevices[r.Name][id])
    replicatedDevice.ID = annotatedID
    replicatedDevice.Replicas = r.Replicas
    devices.insert(name, &amp;amp;replicatedDevice)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，这里是双层 for 循环，对 device 数量进行了一个复制的操作，这样每张 GPU 都可以被使用 Replicas 次了。&lt;/p&gt;

&lt;p&gt;其他属性都没变，只是把 deviceID 进行了处理，便于区分&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// NewAnnotatedID creates a new AnnotatedID from an ID and a replica number.
func NewAnnotatedID(id string, replica int) AnnotatedID {
	return AnnotatedID(fmt.Sprintf(&amp;quot;%s::%d&amp;quot;, id, replica))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在真正挂载时则进行 split 拿到 id 和 replicas 信息&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Split splits a AnnotatedID into its ID and replica number parts.
func (r AnnotatedID) Split() (string, int) {
	split := strings.SplitN(string(r), &amp;quot;::&amp;quot;, 2)
	if len(split) != 2 {
		return string(r), 0
	}
	replica, _ := strconv.ParseInt(split[1], 10, 0)
	return split[0], int(replica)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此，我们就分析完了 TimeSlicing 的具体实现，其实很简单，就是根据配置的 replicas 参数对 device plugin 感知到的设备进行复制，并在 DeviceID 使用特定格式进行标记便于区分。&lt;/p&gt;

&lt;h1 id=&#34;7-小结&#34;&gt;7. 小结&lt;/h1&gt;

&lt;p&gt;本文主要分享了 NVIDIA Time Slicing 这个 GPU 共享方案,包括即实现原理，以及配置和使用方式。&lt;/p&gt;

&lt;p&gt;最后通过分析源码的方式探索了 TimeSlicing 的代码实现。&lt;/p&gt;

&lt;h2 id=&#34;7-1-为什么需要-gpu-共享-切分&#34;&gt;7.1. 为什么需要 GPU 共享、切分？&lt;/h2&gt;

&lt;p&gt;在 k8s 中使用默认 device plugin 时，GPU 资源和物理 GPU 是一一对应的，导致一个物理 GPU 被一个 Pod 申请后，其他 Pod 就无法使用了。&lt;/p&gt;

&lt;p&gt;为了提高资源利用率，因此我们需要 GPU 共享、切分等方案。&lt;/p&gt;

&lt;h2 id=&#34;7-2-什么是-timeslicing&#34;&gt;7.2. 什么是 TimeSlicing？&lt;/h2&gt;

&lt;p&gt;TimeSlicing 是一种通过 oversubscription(超额订阅) 来实现 GPU 共享的策略，这种策略能让多个任务在同一个 GPU 上进行，而不是每个任务都独占一个 GPU。&lt;/p&gt;

&lt;h2 id=&#34;7-3-如何开启-timeslicing&#34;&gt;7.3. 如何开启 TimeSlicing&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1）创建 TimeSlicing 配置&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;可以是集群统一配置，也可以是 Node 级别的配置，主要根据不同节点上的 GPU 进行配置&lt;/li&gt;
&lt;li&gt;如果集群中所有节点 GPU 型号都一致，则使用集群统一配置即可，若不一致则根据 节点上的 GPU 性能修改配置&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2）修改 cluster-policy，增加 TimeSlicing 相关配置&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;作为这两个步骤之后，TimeSlicing 就开启了，再次查看 Node 信息时会发现 GPU 数量变多了。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;7-4-timeslicing-实现原理&#34;&gt;7.4. TimeSlicing 实现原理&lt;/h2&gt;

&lt;p&gt;根据配置的 replicas 参数对 device plugin 感知到的设备进行复制，并在 DeviceID 使用特定格式进行标记便于区分。&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(11): Pod 是如何使用到 GPU 的及源码分析</title>
            <link>http://mospany.github.io/2024/01/27/pod-use-gpu/</link>
            <pubDate>Sat, 27 Jan 2024 19:31:10 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2024/01/27/pod-use-gpu/</guid>
            <description>

&lt;p&gt;本文主要分析了在 K8s 中创建一个 Pod 并申请 GPU 资源，最终该 Pod 时怎么能够使用 GPU 的，具体的实现原理，以及 device plugin、nvidia-container-toolkit 相关源码分析。&lt;/p&gt;

&lt;h1 id=&#34;1-概述&#34;&gt;1. 概述&lt;/h1&gt;

&lt;p&gt;这篇文章则是将整个流程连起来做一个简单分析，即：宿主机上的 GPU 是怎么能够被 K8s 中的 Pod 使用的。&lt;/p&gt;

&lt;p&gt;可以分为以下两部分：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;k8s 是如何感知到 GPU 的&lt;/li&gt;
&lt;li&gt;GPU 是如何分配给 Pod 的&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;2-工作流程&#34;&gt;2. 工作流程&lt;/h1&gt;

&lt;p&gt;这部分主要分享一下 NVIDIA 的 device-plugin 以及 nvidia-container-toolkit 的工作流程，以及二者是怎么配合的。&lt;/p&gt;

&lt;h2 id=&#34;2-1-k8s-是如何感知到-gpu-的&#34;&gt;2.1. k8s 是如何感知到 GPU 的&lt;/h2&gt;

&lt;p&gt;NVIDIA 实现了&lt;a href=&#34;https://github.com/NVIDIA/k8s-device-plugin&#34;&gt;NVIDIA/k8s-device-plugin&lt;/a&gt; 来使得节点上的 GPU 能够被 k8s 感知到。&lt;/p&gt;

&lt;p&gt;这个 device plugin 主要做两件事：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）检测节点上的 GPU 设备并上报给 Kubelet，再由 Kubelet 更新节点信息时提交到 kube-apiserver。

&lt;ul&gt;
&lt;li&gt;这样 k8s 就知道每个节点上有多少 GPU 了，后续 Pod 申请 GPU 时就会往有 GPU 资源的节点上调度。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2）Pod 申请 GPU 时，为对应容器添加一个NVIDIA_VISIBLE_DEVICES环境变量，后续底层 Runtime 在真正创建容器时就能根据这些信息把 GPU 挂载到容器中

&lt;ul&gt;
&lt;li&gt;例如添加环境变量：NVIDIA_VISIBLE_DEVICES=GPU-03f69c50-207a-2038-9b45-23cac89cb67d
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;NVIDIA 这个 device plugin 比较复杂，支持多种策略，device plugin 提供的 env、mounts、device 以及 annotations 等方式它都做了支持，在部署时可以通过 DEVICE_LIST_STRATEGY 环境变量进行指定，不过默认还是用的 env。&lt;/p&gt;

&lt;p&gt;另外DEVICE_ID_STRATEGY 默认也是 uuid，因此在 Pod 中看到的 NVIDIA_VISIBLE_DEVICES 就不是 Docker 环境中常见的 0,1,2 这种编号了，而是 GPU 设备对应的 UUID。&lt;/p&gt;

&lt;h2 id=&#34;2-2-gpu-是如何分配给-pod-的&#34;&gt;2.2. GPU 是如何分配给 Pod 的&lt;/h2&gt;

&lt;p&gt;NVIDIA 提供了 nvidia-container-toolkit 来处理如何将 GPU 分配给容器的问题。&lt;/p&gt;

&lt;p&gt;核心组件有以下三个：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nvidia-container-runtime&lt;/li&gt;
&lt;li&gt;nvidia-container-runtime-hook&lt;/li&gt;
&lt;li&gt;nvidia-container-cli&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;首先需要将 docker/containerd 的 runtime 设置为nvidia-container-runtime，此后整个调用链就变成这样了：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-27-pod-use-gpu/IMG_20250127-192425835.png&#34; alt=&#34;picture 0&#34; /&gt;&lt;/p&gt;

&lt;p&gt;接下来就具体分析每个组件的作用。&lt;/p&gt;

&lt;h3 id=&#34;2-2-1-nvidia-container-runtime&#34;&gt;2.2.1. nvidia-container-runtime&lt;/h3&gt;

&lt;p&gt;nvidia-container-runtime 的作用就是负责在容器启动之前，将 nvidia-container-runtime-hook 注入到 prestart hook。
&amp;gt; 小知识：docker/containerd 都是高级 Runtime，runC 则是低级 Runtime。不同层级 Runtime 通过 OCI Spec 进行交互。&lt;/p&gt;

&lt;p&gt;也就是说 docker 调用 runC 创建容器时，会把 docker 收到的信息解析，组装成 OCI Spec，然后在往下传递。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;而 nvidia-container-runtime 的作用就是修改容器 Spec，往里面添加一个 prestart hook，这个 hook 就是 nvidia-container-runtime-hook 。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这样 runC 根据 Spec 启动容器时就会执行该 hook，即执行 nvidia-container-runtime-hook。&lt;/p&gt;

&lt;p&gt;也就是说 nvidia-container-runtime 其实没有任何逻辑，真正的逻辑都在 nvidia-container-runtime-hook 中。&lt;/p&gt;

&lt;h3 id=&#34;2-2-2-nvidia-container-runtime-hook&#34;&gt;2.2.2. nvidia-container-runtime-hook&lt;/h3&gt;

&lt;p&gt;nvidia-container-runtime-hook 包含了给容器分配 GPU 的核心逻辑，主要分为两部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）从容器 Spec 的 mounts 和 env 中解析 GPU 信息

&lt;ul&gt;
&lt;li&gt;mounts 对应前面 device plugin 中设置的 Mount 和 Device，env 则对应 Env&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2）调用 nvidia-container-cli configure 命令，保证容器内可以使用被指定的 GPU 以及对应能力

&lt;ul&gt;
&lt;li&gt;也就是说nvidia-container-runtime-hook 最终还是调用 nvidia-container-cli 来实现的给容器分配 GPU 能力的。
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;2-2-3-nvidia-container-cli&#34;&gt;2.2.3. nvidia-container-cli&lt;/h3&gt;

&lt;p&gt;nvidia-container-cli 是一个命令行工具，用于配置 Linux 容器对 GPU 硬件的使用。&lt;/p&gt;

&lt;p&gt;提供了三个命令&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;list: 打印 nvidia 驱动库及路径&lt;/li&gt;
&lt;li&gt;info: 打印所有Nvidia GPU设备&lt;/li&gt;
&lt;li&gt;configure： 进入给定进程的命名空间，执行必要操作保证容器内可以使用被指定的 GPU 以及对应能力（指定 NVIDIA 驱动库）
&lt;img src=&#34;post/2024/images/2024-01-27-pod-use-gpu/IMG_20250130-215950804.png&#34; alt=&#34;picture 3&#34; /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一般主要使用 configure 命令，它将 NVIDIA GPU Driver、CUDA Driver 等 驱动库的 so 文件 和 GPU 设备信息， 通过文件挂载的方式映射到容器中。&lt;/p&gt;

&lt;h2 id=&#34;2-3-小结&#34;&gt;2.3. 小结&lt;/h2&gt;

&lt;p&gt;整个流程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）device plugin 上报节点上的 GPU 信息&lt;/li&gt;
&lt;li&gt;2）用户创建 Pod，在 resources.rquest 中申请 GPU，Scheduler 根据各节点 GPU 资源情况，将 Pod 调度到一个有足够 GPU 的节点&lt;/li&gt;
&lt;li&gt;3）DevicePlugin 根据 Pod 中申请的 GPU 资源，为容器添加 Env 和 Devices 配置

&lt;ul&gt;
&lt;li&gt;例如添加环境变量：NVIDIA_VISIBLE_DEVICES=GPU-03f69c50-207a-2038-9b45-23cac89cb67d&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;4）docker / containerd 启动容器&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;由于配置了 nvidia-container-runtime,因此会使用 nvidia-container-runtime 来创建容器&lt;/li&gt;
&lt;li&gt;nvidia-container-runtime 额外做了一件事：将 nvidia-container-runtime-hook 作为 prestart hook 添加到容器 spec 中，然后就将容器 spec 信息往后传给 runC 了。&lt;/li&gt;
&lt;li&gt;runC 创建容器前会调用 prestart hook，其中就包括了上一步添加的 nvidia-container-runtime-hook，该 hook 主要做两件事：

&lt;ul&gt;
&lt;li&gt;从容器 Spec 的 mounts 或者 env 中解析 GPU 信息&lt;/li&gt;
&lt;li&gt;调用 nvidia-container-cli configure 命令，将 NVIDIA 的 GPU Driver、CUDA Driver 等库文件挂载进容器，保证容器内可以使用被指定的 GPU以及对应能力
以上就是在 k8s 中使用 NVIDIA GPU 的流程，简单来说就是：&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;1）device plugin 中根据 pod 申请的 GPU 资源分配 GPU，并以 ENV 环境变量方式添加到容器上。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2）nvidia-container-toolkit 则根据该 Env 拿到要分配给该容器的 GPU 最终把相关文件挂载到容器里
当然并不是只有这一种实现方法，比如天数的 ix-device-plugin 实现中就没有提供自己的 container-toolkit，只在 device plugin 中通过 Device 指定要挂载哪些设备,这样容器启动时也会把这些设备挂载到容器中：&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (p *iluvatarDevicePlugin) allocateDevicesByDeviceID(hostminor uint, num int) *pluginapi.DeviceSpec {
	var device pluginapi.DeviceSpec

	hostPathPrefix := &amp;quot;/dev/&amp;quot;
	containerPathPrefix := &amp;quot;/dev/&amp;quot;

	// Expose the device node for iluvatar pod.
	device.HostPath = hostPathPrefix + deviceName + strconv.Itoa(int(hostminor))
	device.ContainerPath = containerPathPrefix + deviceName + strconv.Itoa(num)
	device.Permissions = &amp;quot;rw&amp;quot;

	return &amp;amp;device
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;不过由于没有挂载驱动进去，因此需要容器内自带驱动才行。&lt;/p&gt;

&lt;p&gt;至此，已经分析了 k8s 创建 Pod 使用 GPU 的整个流程及大致原理，接下来简单分析下相关组件源码。&lt;/p&gt;

&lt;h1 id=&#34;3-device-plugin-源码分析&#34;&gt;3. device plugin 源码分析&lt;/h1&gt;

&lt;p&gt;NVIDIA GPU 对应的 device plugin 叫做：&lt;a href=&#34;https://github.com/NVIDIA/k8s-device-plugin&#34;&gt;NVIDIA/k8s-device-plugin&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;3-1-allocate-方法&#34;&gt;3.1. Allocate 方法&lt;/h2&gt;

&lt;p&gt;主要看为容器分配资源的 Allocate 方法&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// https://github.com/NVIDIA/k8s-device-plugin/blob/main/internal/plugin/server.go#L319-L332

// Allocate which return list of devices.
func (plugin *NvidiaDevicePlugin) Allocate(ctx context.Context, reqs *pluginapi.AllocateRequest) (*pluginapi.AllocateResponse, error) {
        responses := pluginapi.AllocateResponse{}
        for _, req := range reqs.ContainerRequests {
                if err := plugin.rm.ValidateRequest(req.DevicesIDs); err != nil {
                        return nil, fmt.Errorf(&amp;quot;invalid allocation request for %q: %w&amp;quot;, plugin.rm.Resource(), err)
                }
                response, err := plugin.getAllocateResponse(req.DevicesIDs)
                if err != nil {
                        return nil, fmt.Errorf(&amp;quot;failed to get allocate response: %v&amp;quot;, err)
                }
                responses.ContainerResponses = append(responses.ContainerResponses, response)
        }

        return &amp;amp;responses, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心逻辑在 getAllocateResponse 中：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (plugin *NvidiaDevicePlugin) getAllocateResponse(requestIds []string) (*pluginapi.ContainerAllocateResponse, error) {
	deviceIDs := plugin.deviceIDsFromAnnotatedDeviceIDs(requestIds)

	// Create an empty response that will be updated as required below.
	response := &amp;amp;pluginapi.ContainerAllocateResponse{
		Envs: make(map[string]string),
	}
	if plugin.deviceListStrategies.AnyCDIEnabled() {
		responseID := uuid.New().String()
		if err := plugin.updateResponseForCDI(response, responseID, deviceIDs...); err != nil {
			return nil, fmt.Errorf(&amp;quot;failed to get allocate response for CDI: %v&amp;quot;, err)
		}
	}
	if plugin.config.Sharing.SharingStrategy() == spec.SharingStrategyMPS {
		plugin.updateResponseForMPS(response)
	}

	// The following modifications are only made if at least one non-CDI device
	// list strategy is selected.
	if plugin.deviceListStrategies.AllCDIEnabled() {
		return response, nil
	}

	if plugin.deviceListStrategies.Includes(spec.DeviceListStrategyEnvvar) {
		plugin.updateResponseForDeviceListEnvvar(response, deviceIDs...)
	}
	if plugin.deviceListStrategies.Includes(spec.DeviceListStrategyVolumeMounts) {
		plugin.updateResponseForDeviceMounts(response, deviceIDs...)
	}
	if *plugin.config.Flags.Plugin.PassDeviceSpecs {
		response.Devices = append(response.Devices, plugin.apiDeviceSpecs(*plugin.config.Flags.NvidiaDevRoot, requestIds)...)
	}
	if *plugin.config.Flags.GDSEnabled {
		response.Envs[&amp;quot;NVIDIA_GDS&amp;quot;] = &amp;quot;enabled&amp;quot;
	}
	if *plugin.config.Flags.MOFEDEnabled {
		response.Envs[&amp;quot;NVIDIA_MOFED&amp;quot;] = &amp;quot;enabled&amp;quot;
	}
	return response, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，根据不同 flag 以及策略分为不同的设置方式&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Constants to represent the various device list strategies
const (
	DeviceListStrategyEnvvar         = &amp;quot;envvar&amp;quot;
	DeviceListStrategyVolumeMounts   = &amp;quot;volume-mounts&amp;quot;
	DeviceListStrategyCDIAnnotations = &amp;quot;cdi-annotations&amp;quot;
	DeviceListStrategyCDICRI         = &amp;quot;cdi-cri&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;东西比较多，我们主要看设置 Env 的策略&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;if plugin.deviceListStrategies.Includes(spec.DeviceListStrategyEnvvar) {
    plugin.updateResponseForDeviceListEnvvar(response, deviceIDs...)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// updateResponseForDeviceListEnvvar sets the environment variable for the requested devices.
func (plugin *NvidiaDevicePlugin) updateResponseForDeviceListEnvvar(response *pluginapi.ContainerAllocateResponse, deviceIDs ...string) {
        response.Envs[plugin.deviceListEnvvar] = strings.Join(deviceIDs, &amp;quot;,&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，逻辑很简单，就是给容器添加了一个环境变量，value 为设备 id，具体 deviceID 提供了两种策略，可以是编号或者 uuid&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;const (
        DeviceIDStrategyUUID  = &amp;quot;uuid&amp;quot;
        DeviceIDStrategyIndex = &amp;quot;index&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;key 是一个变量 plugin.deviceListEnvvar，初始化如下:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;        plugin := NvidiaDevicePlugin{
                deviceListEnvvar:     &amp;quot;NVIDIA_VISIBLE_DEVICES&amp;quot;,
                socket:               pluginPath + &amp;quot;.sock&amp;quot;,
          // ...
        }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是说 NVIDIA 这个 device plugin 实现 Allocate 主要就是给容器增加了环境变量，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;NVIDIA_VISIBLE_DEVICES=GPU-03f69c50-207a-2038-9b45-23cac89cb67d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;NVIDIA_VISIBLE_DEVICES=1,2
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-2-小结&#34;&gt;3.2. 小结&lt;/h2&gt;

&lt;p&gt;NVIDIA device plugin 核心逻辑就是给容器添加NVIDIA_VISIBLE_DEVICES 环境变量，告知后续组件，需要给该组件分配 GPU。&lt;/p&gt;

&lt;p&gt;比如当我们仅使用 Docker 时就可以在启动容器时指定 GPU，&amp;ndash;gpus flag 和 NVIDIA_VISIBLE_DEVICES 环境变量效果一致。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# --gpus
docker run --gpus device=0 -it tensorflow/tensorflow:latest-gpu bash
# 或者环境变量 NVIDIA_VISIBLE_DEVICES
docker run -e NVIDIA_VISIBLE_DEVICES=0 -it tensorflow/tensorflow:latest-gpu bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至于为什么添加了NVIDIA_VISIBLE_DEVICES 环境变量就会给该容器分配 GPU，就是接下来的 nvidi-container-toolkit 组件实现的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;nvidia 在 device plugin 中也使用NVIDIA_VISIBLE_DEVICES 环境变量正好能够兼容 nvidia-container-toolkit。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;4-nvidia-container-toolkit-源码分析&#34;&gt;4. nvidia-container-toolkit 源码分析&lt;/h1&gt;

&lt;p&gt;这部分我们主要分析，为什么添加了NVIDIA_VISIBLE_DEVICES 环境变量就会给该容器分配 GPU，nvidia-container-toolkit 中做了哪些处理。&lt;/p&gt;

&lt;p&gt;nvidia-container-toolkit 包含以下 3 个部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）&lt;a href=&#34;https://github.com/NVIDIA/nvidia-container-toolkit/tree/main/cmd/nvidia-container-runtime&#34;&gt;nvidia-container-runtime&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2）&lt;a href=&#34;https://github.com/NVIDIA/nvidia-container-toolkit/tree/main/cmd/nvidia-container-runtime-hook&#34;&gt;nvidia-container-runtime-hook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3）&lt;a href=&#34;https://github.com/NVIDIA/libnvidia-container/tree/master/src/cli&#34;&gt;nvidia-container-cli&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;4-1-nvidia-container-runtime&#34;&gt;4.1. nvidia-container-runtime&lt;/h2&gt;

&lt;p&gt;nvidia-container-runtime 可以看做是一个 docker/containerd 的底层 runtime（类似 runC），在模块在创建容器的整个调用链中处在如下位置：
&lt;img src=&#34;post/2024/images/2024-01-27-pod-use-gpu/IMG_20250128-155337940.png&#34; alt=&#34;picture 1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;它只做一件事，就是在容器启动之前，将 nvidia-container-runtime-hook 注入到 prestart hook。
&amp;gt; 以修改容器 Spec 的方式添加一个 prestart hook 进去&lt;/p&gt;

&lt;p&gt;这样，后续 runC 使用容器 Spec 创建容器时就会执行该 prestart hook。&lt;/p&gt;

&lt;p&gt;简单分析下源码，首先是启动命令：&lt;a href=&#34;https://github.com/NVIDIA/nvidia-container-toolkit/blob/main/cmd/nvidia-container-runtime/main.go&#34;&gt;nvidia-container-runtime/main.go&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;就是 New 了一个 nvidia runtime 对象，并执行其 Run 方法。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// https://github.com/NVIDIA/nvidia-container-toolkit/blob/main/cmd/nvidia-container-runtime/main.go#L9-L15

import (
    &amp;quot;os&amp;quot;

    &amp;quot;github.com/NVIDIA/nvidia-container-toolkit/internal/runtime&amp;quot;
)

func main() {
    r := runtime.New()
    err := r.Run(os.Args)
    if err != nil {
       os.Exit(1)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体的 New 方法也很简单，返回的是一个名为 Interface 的 Interface，包含一个 Run 方法&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// https://github.com/NVIDIA/nvidia-containertoolkit/blob/main/internal/runtime/api.go#L17-L26

type rt struct {
    logger       *Logger
    modeOverride string
}

// Interface is the interface for the runtime library.
type Interface interface {
    Run([]string) error
}
func New(opts ...Option) Interface {
    r := rt{}
    for _, opt := range opts {
       opt(&amp;amp;r)
    }
    if r.logger == nil {
       r.logger = NewLogger()
    }
    return &amp;amp;r
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run 方法具体实现如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// https://github.com/NVIDIA/nvidia-container-toolkit/blob/main/internal/runtime/runtime.go#L34-L91

// Run is an entry point that allows for idiomatic handling of errors
// when calling from the main function.
func (r rt) Run(argv []string) (rerr error) {
    defer func() {
       if rerr != nil {
          r.logger.Errorf(&amp;quot;%v&amp;quot;, rerr)
       }
    }()

    printVersion := hasVersionFlag(argv)
    if printVersion {
       fmt.Printf(&amp;quot;%v version %v\n&amp;quot;, &amp;quot;NVIDIA Container Runtime&amp;quot;, info.GetVersionString(fmt.Sprintf(&amp;quot;spec: %v&amp;quot;, specs.Version)))
    }

    cfg, err := config.GetConfig()
    if err != nil {
       return fmt.Errorf(&amp;quot;error loading config: %v&amp;quot;, err)
    }
    r.logger.Update(
       cfg.NVIDIAContainerRuntimeConfig.DebugFilePath,
       cfg.NVIDIAContainerRuntimeConfig.LogLevel,
       argv,
    )
    defer func() {
       if rerr != nil {
          r.logger.Errorf(&amp;quot;%v&amp;quot;, rerr)
       }
       if err := r.logger.Reset(); err != nil {
          rerr = errors.Join(rerr, fmt.Errorf(&amp;quot;failed to reset logger: %v&amp;quot;, err))
       }
    }()

    // We apply some config updates here to ensure that the config is valid in
    // all cases.
    if r.modeOverride != &amp;quot;&amp;quot; {
       cfg.NVIDIAContainerRuntimeConfig.Mode = r.modeOverride
    }
    //nolint:staticcheck  // TODO(elezar): We should swith the nvidia-container-runtime from using nvidia-ctk to using nvidia-cdi-hook.
    cfg.NVIDIACTKConfig.Path = config.ResolveNVIDIACTKPath(&amp;amp;logger.NullLogger{}, cfg.NVIDIACTKConfig.Path)
    cfg.NVIDIAContainerRuntimeHookConfig.Path = config.ResolveNVIDIAContainerRuntimeHookPath(&amp;amp;logger.NullLogger{}, cfg.NVIDIAContainerRuntimeHookConfig.Path)

    // Log the config at Trace to allow for debugging if required.
    r.logger.Tracef(&amp;quot;Running with config: %+v&amp;quot;, cfg)

    driver := root.New(
       root.WithLogger(r.logger),
       root.WithDriverRoot(cfg.NVIDIAContainerCLIConfig.Root),
    )

    r.logger.Tracef(&amp;quot;Command line arguments: %v&amp;quot;, argv)
    runtime, err := newNVIDIAContainerRuntime(r.logger, cfg, argv, driver)
    if err != nil {
       return fmt.Errorf(&amp;quot;failed to create NVIDIA Container Runtime: %v&amp;quot;, err)
    }

    if printVersion {
       fmt.Print(&amp;quot;\n&amp;quot;)
    }
    return runtime.Exec(argv)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心部分：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;runtime, err := newNVIDIAContainerRuntime(r.logger, cfg, argv, driver)
if err != nil {
   return fmt.Errorf(&amp;quot;failed to create NVIDIA Container Runtime: %v&amp;quot;, err)
}

if printVersion {
   fmt.Print(&amp;quot;\n&amp;quot;)
}
return runtime.Exec(argv)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;继续查看 newNVIDIAContainerRuntime 实现&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// https://github.com/NVIDIA/nvidia-container-toolkit/blob/main/internal/runtime/runtime_factory.go#L32-L62

// newNVIDIAContainerRuntime is a factory method that constructs a runtime based on the selected configuration and specified logger
func newNVIDIAContainerRuntime(logger logger.Interface, cfg *config.Config, argv []string, driver *root.Driver) (oci.Runtime, error) {
    lowLevelRuntime, err := oci.NewLowLevelRuntime(logger, cfg.NVIDIAContainerRuntimeConfig.Runtimes)
    if err != nil {
       return nil, fmt.Errorf(&amp;quot;error constructing low-level runtime: %v&amp;quot;, err)
    }

    logger.Tracef(&amp;quot;Using low-level runtime %v&amp;quot;, lowLevelRuntime.String())
    if !oci.HasCreateSubcommand(argv) {
       logger.Tracef(&amp;quot;Skipping modifier for non-create subcommand&amp;quot;)
       return lowLevelRuntime, nil
    }

    ociSpec, err := oci.NewSpec(logger, argv)
    if err != nil {
       return nil, fmt.Errorf(&amp;quot;error constructing OCI specification: %v&amp;quot;, err)
    }

    specModifier, err := newSpecModifier(logger, cfg, ociSpec, driver)
    if err != nil {
       return nil, fmt.Errorf(&amp;quot;failed to construct OCI spec modifier: %v&amp;quot;, err)
    }

    // Create the wrapping runtime with the specified modifier.
    r := oci.NewModifyingRuntimeWrapper(
       logger,
       lowLevelRuntime,
       ociSpec,
       specModifier,
    )

    return r, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;暂时只需要关注 specModifier 这个对象,就是它在修改容器的 spec 以添加 hook&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// newSpecModifier is a factory method that creates constructs an OCI spec modifer based on the provided config.
func newSpecModifier(logger logger.Interface, cfg *config.Config, ociSpec oci.Spec, driver *root.Driver) (oci.SpecModifier, error) {
    rawSpec, err := ociSpec.Load()
    if err != nil {
       return nil, fmt.Errorf(&amp;quot;failed to load OCI spec: %v&amp;quot;, err)
    }

    image, err := image.NewCUDAImageFromSpec(rawSpec)
    if err != nil {
       return nil, err
    }

    mode := info.ResolveAutoMode(logger, cfg.NVIDIAContainerRuntimeConfig.Mode, image)
    modeModifier, err := newModeModifier(logger, mode, cfg, ociSpec, image)
    if err != nil {
       return nil, err
    }
    // For CDI mode we make no additional modifications.
    if mode == &amp;quot;cdi&amp;quot; {
       return modeModifier, nil
    }

    graphicsModifier, err := modifier.NewGraphicsModifier(logger, cfg, image, driver)
    if err != nil {
       return nil, err
    }

    featureModifier, err := modifier.NewFeatureGatedModifier(logger, cfg, image)
    if err != nil {
       return nil, err
    }

    modifiers := modifier.Merge(
       modeModifier,
       graphicsModifier,
       featureModifier,
    )
    return modifiers, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 hook 的 modifier 在 newModeModifier 里面&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func newModeModifier(logger logger.Interface, mode string, cfg *config.Config, ociSpec oci.Spec, image image.CUDA) (oci.SpecModifier, error) {
    switch mode {
    case &amp;quot;legacy&amp;quot;:
       return modifier.NewStableRuntimeModifier(logger, cfg.NVIDIAContainerRuntimeHookConfig.Path), nil
    case &amp;quot;csv&amp;quot;:
       return modifier.NewCSVModifier(logger, cfg, image)
    case &amp;quot;cdi&amp;quot;:
       return modifier.NewCDIModifier(logger, cfg, ociSpec)
    }

    return nil, fmt.Errorf(&amp;quot;invalid runtime mode: %v&amp;quot;, cfg.NVIDIAContainerRuntimeConfig.Mode)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体为 stableRuntimeModifier：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (m stableRuntimeModifier) Modify(spec *specs.Spec) error {
    // If an NVIDIA Container Runtime Hook already exists, we don&#39;t make any modifications to the spec.
    if spec.Hooks != nil {
       for _, hook := range spec.Hooks.Prestart {
          hook := hook
          if isNVIDIAContainerRuntimeHook(&amp;amp;hook) {
             m.logger.Infof(&amp;quot;Existing nvidia prestart hook (%v) found in OCI spec&amp;quot;, hook.Path)
             return nil
          }
       }
    }

    path := m.nvidiaContainerRuntimeHookPath
    m.logger.Infof(&amp;quot;Using prestart hook path: %v&amp;quot;, path)
    args := []string{filepath.Base(path)}
    if spec.Hooks == nil {
       spec.Hooks = &amp;amp;specs.Hooks{}
    }
    spec.Hooks.Prestart = append(spec.Hooks.Prestart, specs.Hook{
       Path: path,
       Args: append(args, &amp;quot;prestart&amp;quot;),
    })

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心部分：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;path := m.nvidiaContainerRuntimeHookPath
spec.Hooks.Prestart = append(spec.Hooks.Prestart, specs.Hook{
   Path: path,
   Args: append(args, &amp;quot;prestart&amp;quot;),
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，最终就是添加了一个 prestart hook，hook 的 path 就是 nvidia-container-runtime-hook 这个二进制文件的位置。&lt;/p&gt;

&lt;p&gt;至此，nvidia-container-runtime 的工作就完成了，容器真正启动时，底层 runtime（比如 runC）检测到容器的 Spec 中有这个 hook 就会去执行了，最终 nvidia-container-runtime-hook 就会被运行了。&lt;/p&gt;

&lt;h2 id=&#34;4-2-nvidia-container-runtime-hook&#34;&gt;4.2. nvidia-container-runtime-hook&lt;/h2&gt;

&lt;p&gt;该组件则是 nvidia-container-toolkit 中的核心，所有的逻辑都在这里面实现。&lt;/p&gt;

&lt;p&gt;主要做两件事：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）从容器的 env 中解析 GPU 信息&lt;/li&gt;
&lt;li&gt;2）调用 nvidia-container-cli configure 命令，挂载相关文件，保证容器内可以使用被指定的GPU以及对应能力
也是先从启动命令看起：&lt;a href=&#34;https://github.com/NVIDIA/nvidia-container-toolkit/blob/main/cmd/nvidia-container-runtime-hook/main.go&#34;&gt;nvidia-container-runtime-hook/main.go&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;switch args[0] {
case &amp;quot;prestart&amp;quot;:
    doPrestart()
    os.Exit(0)
case &amp;quot;poststart&amp;quot;:
    fallthrough
case &amp;quot;poststop&amp;quot;:
    os.Exit(0)
default:
    flag.Usage()
    os.Exit(2)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们是添加的 prestart hook，因此会走 prestart 分支 执行doPrestart()方法。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func doPrestart() {
    var err error

    defer exit()
    log.SetFlags(0)

    hook, err := getHookConfig()
    if err != nil || hook == nil {
       log.Panicln(&amp;quot;error getting hook config:&amp;quot;, err)
    }
    cli := hook.NVIDIAContainerCLIConfig

    container := getContainerConfig(*hook)
    nvidia := container.Nvidia
    if nvidia == nil {
       // Not a GPU container, nothing to do.
       return
    }

    if !hook.NVIDIAContainerRuntimeHookConfig.SkipModeDetection &amp;amp;&amp;amp; info.ResolveAutoMode(&amp;amp;logInterceptor{}, hook.NVIDIAContainerRuntimeConfig.Mode, container.Image) != &amp;quot;legacy&amp;quot; {
       log.Panicln(&amp;quot;invoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported. Please use the NVIDIA Container Runtime (e.g. specify the --runtime=nvidia flag) instead.&amp;quot;)
    }

    rootfs := getRootfsPath(container)

    args := []string{getCLIPath(cli)}
    if cli.Root != &amp;quot;&amp;quot; {
       args = append(args, fmt.Sprintf(&amp;quot;--root=%s&amp;quot;, cli.Root))
    }
    if cli.LoadKmods {
       args = append(args, &amp;quot;--load-kmods&amp;quot;)
    }
    if cli.NoPivot {
       args = append(args, &amp;quot;--no-pivot&amp;quot;)
    }
    if *debugflag {
       args = append(args, &amp;quot;--debug=/dev/stderr&amp;quot;)
    } else if cli.Debug != &amp;quot;&amp;quot; {
       args = append(args, fmt.Sprintf(&amp;quot;--debug=%s&amp;quot;, cli.Debug))
    }
    if cli.Ldcache != &amp;quot;&amp;quot; {
       args = append(args, fmt.Sprintf(&amp;quot;--ldcache=%s&amp;quot;, cli.Ldcache))
    }
    if cli.User != &amp;quot;&amp;quot; {
       args = append(args, fmt.Sprintf(&amp;quot;--user=%s&amp;quot;, cli.User))
    }
    args = append(args, &amp;quot;configure&amp;quot;)

    if ldconfigPath := cli.NormalizeLDConfigPath(); ldconfigPath != &amp;quot;&amp;quot; {
       args = append(args, fmt.Sprintf(&amp;quot;--ldconfig=%s&amp;quot;, ldconfigPath))
    }
    if cli.NoCgroups {
       args = append(args, &amp;quot;--no-cgroups&amp;quot;)
    }
    if len(nvidia.Devices) &amp;gt; 0 {
       args = append(args, fmt.Sprintf(&amp;quot;--device=%s&amp;quot;, nvidia.Devices))
    }
    if len(nvidia.MigConfigDevices) &amp;gt; 0 {
       args = append(args, fmt.Sprintf(&amp;quot;--mig-config=%s&amp;quot;, nvidia.MigConfigDevices))
    }
    if len(nvidia.MigMonitorDevices) &amp;gt; 0 {
       args = append(args, fmt.Sprintf(&amp;quot;--mig-monitor=%s&amp;quot;, nvidia.MigMonitorDevices))
    }
    if len(nvidia.ImexChannels) &amp;gt; 0 {
       args = append(args, fmt.Sprintf(&amp;quot;--imex-channel=%s&amp;quot;, nvidia.ImexChannels))
    }

    for _, cap := range strings.Split(nvidia.DriverCapabilities, &amp;quot;,&amp;quot;) {
       if len(cap) == 0 {
          break
       }
       args = append(args, capabilityToCLI(cap))
    }

    for _, req := range nvidia.Requirements {
       args = append(args, fmt.Sprintf(&amp;quot;--require=%s&amp;quot;, req))
    }

    args = append(args, fmt.Sprintf(&amp;quot;--pid=%s&amp;quot;, strconv.FormatUint(uint64(container.Pid), 10)))
    args = append(args, rootfs)

    env := append(os.Environ(), cli.Environment...)
    //nolint:gosec // TODO: Can we harden this so that there is less risk of command injection?
    err = syscall.Exec(args[0], args, env)
    log.Panicln(&amp;quot;exec failed:&amp;quot;, err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们只需要关注下面这个就行&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;args := []string{getCLIPath(cli)}
container := getContainerConfig(*hook)
err = syscall.Exec(args[0], args, env)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个是 getContainerConfig 解析容器配置 ，另一个就是 exec 真正开始执行命令。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;这里执行的命令其实就是 nvidia-container-cli&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;4-2-1-getcontainerconfig&#34;&gt;4.2.1. getContainerConfig&lt;/h3&gt;

&lt;p&gt;这部分就是解析 Env 拿到要分配给该容器的 GPU，如果没有 NVIDIA_VISIBLE_DEVICES 环境变量就不会做任何事情。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func getContainerConfig(hook HookConfig) (config containerConfig) {
    var h HookState
    d := json.NewDecoder(os.Stdin)
    if err := d.Decode(&amp;amp;h); err != nil {
       log.Panicln(&amp;quot;could not decode container state:&amp;quot;, err)
    }

    b := h.Bundle
    if len(b) == 0 {
       b = h.BundlePath
    }

    s := loadSpec(path.Join(b, &amp;quot;config.json&amp;quot;))

    image, err := image.New(
       image.WithEnv(s.Process.Env),
       image.WithDisableRequire(hook.DisableRequire),
    )
    if err != nil {
       log.Panicln(err)
    }

    privileged := isPrivileged(s)
    return containerConfig{
       Pid:    h.Pid,
       Rootfs: s.Root.Path,
       Image:  image,
       Nvidia: getNvidiaConfig(&amp;amp;hook, image, s.Mounts, privileged),
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;构建了一个 image 对象，注意这里把 ENV 也传进去了&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;之前说了需要给容器分配什么 GPU 是通过 NVIDIA_VISIBLE_DEVICES 环境变量指定的&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;image, err := image.New(
    image.WithEnv(s.Process.Env),
    image.WithDisableRequire(hook.DisableRequire),
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后解析配置&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func getNvidiaConfig(hookConfig *HookConfig, image image.CUDA, mounts []Mount, privileged bool) *nvidiaConfig {
    legacyImage := image.IsLegacy()

    var devices string
    if d := getDevices(hookConfig, image, mounts, privileged); d != nil {
       devices = *d
    } else {
       // &#39;nil&#39; devices means this is not a GPU container.
       return nil
    }

    var migConfigDevices string
    if d := getMigConfigDevices(image); d != nil {
       migConfigDevices = *d
    }
    if !privileged &amp;amp;&amp;amp; migConfigDevices != &amp;quot;&amp;quot; {
       log.Panicln(&amp;quot;cannot set MIG_CONFIG_DEVICES in non privileged container&amp;quot;)
    }

    var migMonitorDevices string
    if d := getMigMonitorDevices(image); d != nil {
       migMonitorDevices = *d
    }
    if !privileged &amp;amp;&amp;amp; migMonitorDevices != &amp;quot;&amp;quot; {
       log.Panicln(&amp;quot;cannot set MIG_MONITOR_DEVICES in non privileged container&amp;quot;)
    }

    var imexChannels string
    if c := getImexChannels(image); c != nil {
       imexChannels = *c
    }

    driverCapabilities := hookConfig.getDriverCapabilities(image, legacyImage).String()

    requirements, err := image.GetRequirements()
    if err != nil {
       log.Panicln(&amp;quot;failed to get requirements&amp;quot;, err)
    }

    return &amp;amp;nvidiaConfig{
       Devices:            devices,
       MigConfigDevices:   migConfigDevices,
       MigMonitorDevices:  migMonitorDevices,
       ImexChannels:       imexChannels,
       DriverCapabilities: driverCapabilities,
       Requirements:       requirements,
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心是 getDevice，就是根据 Mounts 信息或者 Env 解析要分配给该容器的 GPU&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func getDevices(hookConfig *HookConfig, image image.CUDA, mounts []Mount, privileged bool) *string {
    // If enabled, try and get the device list from volume mounts first
    if hookConfig.AcceptDeviceListAsVolumeMounts {
       devices := getDevicesFromMounts(mounts)
       if devices != nil {
          return devices
       }
    }

    // Fallback to reading from the environment variable if privileges are correct
    devices := getDevicesFromEnvvar(image, hookConfig.getSwarmResourceEnvvars())
    if devices == nil {
       return nil
    }
    if privileged || hookConfig.AcceptEnvvarUnprivileged {
       return devices
    }

    configName := hookConfig.getConfigOption(&amp;quot;AcceptEnvvarUnprivileged&amp;quot;)
    log.Printf(&amp;quot;Ignoring devices specified in NVIDIA_VISIBLE_DEVICES (privileged=%v, %v=%v) &amp;quot;, privileged, configName, hookConfig.AcceptEnvvarUnprivileged)

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到这里根据配置不同，提供了两种解析 devices 的方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;getDevicesFromMounts&lt;/li&gt;
&lt;li&gt;getDevicesFromEnvvar
这也就是为什么 nvidia device plugin 除了实现 Env 之外还实现了另外的方式，二者配置应该要对应才行。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里我们只关注 getDevicesFromEnvvar，从环境变量里解析 Device：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;envNVVisibleDevices     = &amp;quot;NVIDIA_VISIBLE_DEVICES&amp;quot;

func getDevicesFromEnvvar(image image.CUDA, swarmResourceEnvvars []string) *string {
	// We check if the image has at least one of the Swarm resource envvars defined and use this
	// if specified.
	var hasSwarmEnvvar bool
	for _, envvar := range swarmResourceEnvvars {
		if image.HasEnvvar(envvar) {
			hasSwarmEnvvar = true
			break
		}
	}

	var devices []string
	if hasSwarmEnvvar {
		devices = image.DevicesFromEnvvars(swarmResourceEnvvars...).List()
	} else {
		devices = image.DevicesFromEnvvars(envNVVisibleDevices).List()
	}

	if len(devices) == 0 {
		return nil
	}

	devicesString := strings.Join(devices, &amp;quot;,&amp;quot;)

	return &amp;amp;devicesString
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;devices = image.DevicesFromEnvvars(envNVVisibleDevices).List()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从 image 里面提取NVIDIA_VISIBLE_DEVICES环境变量，至于这个 Env 是哪里来的，也是容器 Spec 中定义的，之前 image 是这样初始化的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;s := loadSpec(path.Join(b, &amp;quot;config.json&amp;quot;))

	image, err := image.New(
		image.WithEnv(s.Process.Env), // 这里把容器 env 传给了 image 对象
		image.WithDisableRequire(hook.DisableRequire),
	)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际这里还有一个特殊逻辑：&lt;em&gt;如果没有设置 NVIDIA_VISIBLE_DEVICES环境变量，也没通过其他方式解析到 device 并且还是是一个 legacy image，那么默认使用全部 GPU。&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Environment variable unset with legacy image: default to &amp;quot;all&amp;quot;.
if !isSet &amp;amp;&amp;amp; len(devices) == 0 &amp;amp;&amp;amp; i.IsLegacy() {
  return NewVisibleDevices(&amp;quot;all&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那么什么算是 legacy image 呢：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// IsLegacy returns whether the associated CUDA image is a &amp;quot;legacy&amp;quot; image. An
// image is considered legacy if it has a CUDA_VERSION environment variable defined
// and no NVIDIA_REQUIRE_CUDA environment variable defined.
func (i CUDA) IsLegacy() bool {
	legacyCudaVersion := i.env[envCUDAVersion]
	cudaRequire := i.env[envNVRequireCUDA]
	return len(legacyCudaVersion) &amp;gt; 0 &amp;amp;&amp;amp; len(cudaRequire) == 0
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这也就是为什么，有时候启动 Pod 并没有申请 GPU，但是 Pod 里面依旧可以看到所有 GPU，就是走了这个 legacy image 的分支逻辑。&lt;/p&gt;

&lt;p&gt;至此，我们知道了这边 runtime 是怎么指定要把哪些 GPU 分配给容器了，接下来进入 Exec 逻辑。&lt;/p&gt;

&lt;h3 id=&#34;4-2-2-exec&#34;&gt;4.2.2. Exec&lt;/h3&gt;

&lt;p&gt;Exec 部分比较短，就是这两行代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;args := []string{getCLIPath(cli)}
err = syscall.Exec(args[0], args, env)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先是 getCLIPath，用于寻找 nvidia-container-cli 工具的位置并作为第一个参数。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func getCLIPath(config config.ContainerCLIConfig) string {
    if config.Path != &amp;quot;&amp;quot; {
       return config.Path
    }

    if err := os.Setenv(&amp;quot;PATH&amp;quot;, lookup.GetPath(config.Root)); err != nil {
       log.Panicln(&amp;quot;couldn&#39;t set PATH variable:&amp;quot;, err)
    }

    path, err := exec.LookPath(&amp;quot;nvidia-container-cli&amp;quot;)
    if err != nil {
       log.Panicln(&amp;quot;couldn&#39;t find binary nvidia-container-cli in&amp;quot;, os.Getenv(&amp;quot;PATH&amp;quot;), &amp;quot;:&amp;quot;, err)
    }
    return path
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，如果单独配置了 cli 的位置参数就使用配置的位置，否则使用 LookPath 根据名字寻找。&lt;/p&gt;

&lt;p&gt;然后是相关的参数&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;    args := []string{getCLIPath(cli)}
    if cli.Root != &amp;quot;&amp;quot; {
       args = append(args, fmt.Sprintf(&amp;quot;--root=%s&amp;quot;, cli.Root))
    }
    if cli.LoadKmods {
       args = append(args, &amp;quot;--load-kmods&amp;quot;)
    }
    if cli.NoPivot {
       args = append(args, &amp;quot;--no-pivot&amp;quot;)
    }
    if *debugflag {
       args = append(args, &amp;quot;--debug=/dev/stderr&amp;quot;)
    } else if cli.Debug != &amp;quot;&amp;quot; {
       args = append(args, fmt.Sprintf(&amp;quot;--debug=%s&amp;quot;, cli.Debug))
    }
    if cli.Ldcache != &amp;quot;&amp;quot; {
       args = append(args, fmt.Sprintf(&amp;quot;--ldcache=%s&amp;quot;, cli.Ldcache))
    }
    if cli.User != &amp;quot;&amp;quot; {
       args = append(args, fmt.Sprintf(&amp;quot;--user=%s&amp;quot;, cli.User))
    }
    args = append(args, &amp;quot;configure&amp;quot;)

    if ldconfigPath := cli.NormalizeLDConfigPath(); ldconfigPath != &amp;quot;&amp;quot; {
       args = append(args, fmt.Sprintf(&amp;quot;--ldconfig=%s&amp;quot;, ldconfigPath))
    }
    if cli.NoCgroups {
       args = append(args, &amp;quot;--no-cgroups&amp;quot;)
    }
    if len(nvidia.Devices) &amp;gt; 0 {
       args = append(args, fmt.Sprintf(&amp;quot;--device=%s&amp;quot;, nvidia.Devices))
    }
    if len(nvidia.MigConfigDevices) &amp;gt; 0 {
       args = append(args, fmt.Sprintf(&amp;quot;--mig-config=%s&amp;quot;, nvidia.MigConfigDevices))
    }
    if len(nvidia.MigMonitorDevices) &amp;gt; 0 {
       args = append(args, fmt.Sprintf(&amp;quot;--mig-monitor=%s&amp;quot;, nvidia.MigMonitorDevices))
    }
    if len(nvidia.ImexChannels) &amp;gt; 0 {
       args = append(args, fmt.Sprintf(&amp;quot;--imex-channel=%s&amp;quot;, nvidia.ImexChannels))
    }

    for _, cap := range strings.Split(nvidia.DriverCapabilities, &amp;quot;,&amp;quot;) {
       if len(cap) == 0 {
          break
       }
       args = append(args, capabilityToCLI(cap))
    }

    for _, req := range nvidia.Requirements {
       args = append(args, fmt.Sprintf(&amp;quot;--require=%s&amp;quot;, req))
    }

    args = append(args, fmt.Sprintf(&amp;quot;--pid=%s&amp;quot;, strconv.FormatUint(uint64(container.Pid), 10)))
    args = append(args, rootfs)

    env := append(os.Environ(), cli.Environment...)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;args = append(args, &amp;quot;configure&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;表示执行的是nvidia-container-cli configure 命令。&lt;/p&gt;

&lt;p&gt;最后则是调用 syscall.Exec 真正开始执行命令&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;err = syscall.Exec(args[0], args, env)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该命令具体在做什么呢，接着分析 nvidia-container-cli 实现。&lt;/p&gt;

&lt;h2 id=&#34;4-3-nvidia-container-cli&#34;&gt;4.3. nvidia-container-cli&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/NVIDIA/libnvidia-container/tree/master/src/cli&#34;&gt;nvidia-container-cli&lt;/a&gt; 是一个 C 写的小工具，主要作用就是根据上执行命令时传递的参数，把GPU 设备及其相关依赖库挂载到容器中，使得容器能够正常使用 GPU 能力。&lt;/p&gt;

&lt;p&gt;简单看下部分代码。&lt;/p&gt;

&lt;p&gt;首先是驱动信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// https://github.com/NVIDIA/libnvidia-container/blob/master/src/cli/configure.c#L279-L288

/* Query the driver and device information. */
if (perm_set_capabilities(&amp;amp;err, CAP_EFFECTIVE, ecaps[NVC_INFO], ecaps_size(NVC_INFO)) &amp;lt; 0) {
        warnx(&amp;quot;permission error: %s&amp;quot;, err.msg);
        goto fail;
}
if ((drv = libnvc.driver_info_new(nvc, NULL)) == NULL ||
    (dev = libnvc.device_info_new(nvc, NULL)) == NULL) {
        warnx(&amp;quot;detection error: %s&amp;quot;, libnvc.error(nvc));
        goto fail;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;nvc_driver_info_new()：获取 CUDA Driver 信息&lt;/li&gt;
&lt;li&gt;nvc_device_info_new()：获取 GPU Drvier 信息
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后获取容器中可见的 GPU 列表&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// https://github.com/NVIDIA/libnvidia-container/blob/master/src/cli/configure.c#L308-L314

        /* Select the visible GPU devices. */
        if (dev-&amp;gt;ngpus &amp;gt; 0) {
                if (select_devices(&amp;amp;err, ctx-&amp;gt;devices, dev, &amp;amp;devices) &amp;lt; 0) {
                        warnx(&amp;quot;device error: %s&amp;quot;, err.msg);
                        goto fail;
                }
        }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后则是将相关驱动挂载到容器里去：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// https://github.com/NVIDIA/libnvidia-container/blob/master/src/cli/configure.c#L362-L408

/* Mount the driver, visible devices, mig-configs and mig-monitors. */
if (perm_set_capabilities(&amp;amp;err, CAP_EFFECTIVE, ecaps[NVC_MOUNT], ecaps_size(NVC_MOUNT)) &amp;lt; 0) {
        warnx(&amp;quot;permission error: %s&amp;quot;, err.msg);
        goto fail;
}
if (libnvc.driver_mount(nvc, cnt, drv) &amp;lt; 0) {
        warnx(&amp;quot;mount error: %s&amp;quot;, libnvc.error(nvc));
        goto fail;
}
for (size_t i = 0; i &amp;lt; devices.ngpus; ++i) {
        if (libnvc.device_mount(nvc, cnt, devices.gpus[i]) &amp;lt; 0) {
                warnx(&amp;quot;mount error: %s&amp;quot;, libnvc.error(nvc));
                goto fail;
        }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;libnvidia-container是采用 linux c mount &amp;ndash;bind功能将 CUDA Driver Libraries/Binaries一个个挂载到容器里，而不是将整个目录挂载到容器中。&lt;/p&gt;

&lt;p&gt;可通过NVIDIA_DRIVER_CAPABILITIES环境变量指定要挂载的 driver libraries/binaries。&lt;/p&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -e NVIDIA_VISIBLE_DEVICES=0,1 -e NVIDIA_DRIVER_CAPABILITIES=compute,utility -it tensorflow/tensorflow:latest-gpu bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;指定NVIDIA_DRIVER_CAPABILITIES=compute,utility 就会把 compute 和 utility 相关的库挂载进去。&lt;/p&gt;

&lt;p&gt;这样容器里就可以使用 GPU 了。&lt;/p&gt;

&lt;p&gt;至此，相关源码就分析完成了。&lt;/p&gt;

&lt;h1 id=&#34;5-小结&#34;&gt;5. 小结&lt;/h1&gt;

&lt;p&gt;整个流程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）device plugin 上报节点上的 GPU 信息&lt;/li&gt;
&lt;li&gt;2）用户创建 Pod，在 resources.rquest 中申请 GPU，Scheduler 根据各节点 GPU 资源情况，将 Pod 调度到一个有足够 GPU 的节点&lt;/li&gt;
&lt;li&gt;3）DevicePlugin 根据 Pod 中申请的 GPU 资源，为容器添加NVIDIA_VISIBLE_DEVICES环境变量
 &amp;gt; 例如：NVIDIA_VISIBLE_DEVICES=GPU-03f69c50-207a-2038-9b45-23cac89cb67d&lt;/li&gt;

&lt;li&gt;&lt;p&gt;4）docker / containerd 启动容器&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;由于配置了 nvidia-container-runtime,因此会使用 nvidia-container-runtime 来创建容器&lt;/li&gt;
&lt;li&gt;nvidia-container-runtime 额外做了一件事：将 nvidia-container-runtime-hook 作为 prestart hook 添加到容器 spec 中，然后就将容器 spec 信息往后传给 runC 了。&lt;/li&gt;
&lt;li&gt;runC 创建容器前会调用 prestart hook，其中就包括了上一步添加的 nvidia-container-runtime-hook，该 hook 主要做两件事：

&lt;ul&gt;
&lt;li&gt;从容器 Spec 的 mounts 或者 env 中解析 GPU 信息&lt;/li&gt;
&lt;li&gt;调用 nvidia-container-cli 命令，将 NVIDIA 的 GPU Driver、CUDA Driver 等库文件挂载进容器，保证容器内可以使用被指定的 GPU以及对应能力
核心就是两个部分：&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;device plugin 根据 GPU 资源申请为容器添加 NVIDIA_VISIBLE_DEVICES环境变量&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;nvidia-container-toolkit 则是根据 NVIDIA_VISIBLE_DEVICES环境变量将 GPU、驱动等相关文件挂载到容器里。
看源码同时顺带解决了一个，之前遇到过的问题：&lt;em&gt;为什么 Pod 明明没有申请 GPU，启动后也能看到所有 GPU？&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这是因为 nvidia-container-toolkit 中存在特殊逻辑，没有设置 NVIDIA_VISIBLE_DEVICES环境变量，也没通过其他方式解析到 device 并且还是一个 legacy image，那么默认会返回all，即：NVIDIA_VISIBLE_DEVICES=all ，因此该 Pod 能看到全部 GPU。&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(10): device-plugin原理到实现</title>
            <link>http://mospany.github.io/2024/01/24/device-plugin/</link>
            <pubDate>Wed, 24 Jan 2024 19:31:10 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2024/01/24/device-plugin/</guid>
            <description>

&lt;p&gt;本文主要分析 k8s 中的 device-plugin 机制工作原理，并通过实现一个简单的 device-plugin 来加深理解。&lt;/p&gt;

&lt;h1 id=&#34;1-背景&#34;&gt;1. 背景&lt;/h1&gt;

&lt;p&gt;默认情况下，k8s 中的 Pod 只能申请 CPU 和 Memory 这两种资源，就像下面这样：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;resources:
  requests:
    memory: &amp;quot;1024Mi&amp;quot;
    cpu: &amp;quot;100m&amp;quot;
  limits:
    memory: &amp;quot;2048Mi&amp;quot;
    cpu: &amp;quot;200m&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;随着 AI 热度越来越高，更多的业务 Pod 需要申请 GPU 资源，&lt;a href=&#34;http://127.0.0.1:1313/2024/01/16/gpu-on-k8s/&#34;&gt;在ECS、Docker、K8s 等环境中使用 GPU&lt;/a&gt;中我们分析了如何在 k8s 环境中使用 GPU，就是靠 Device Plugin 机制，通过该机制使得 k8s 能感知到节点上的 GPU 资源，就像原生的 CPU 和 Memory 资源一样使用。&lt;/p&gt;

&lt;p&gt;实际上在早期，K8s 也提供了一种名为 alpha.kubernetes.io/nvidia-gpu 的资源来支持 NVIDIA GPU，不过后面也发现了很多问题，每增加一种资源都要修改 k8s 核心代码，k8s 社区压力山大。于是在 1.8 版本引入了 device plugin 机制，通过插件形式来接入其他资源，设备厂家只需要开发对应的 xxx-device-plugin 就可以将资源接入到 k8s 了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ps：类似的还有引入 CSI 让存储插件从 Kubernetes 内部（in-tree）代码库中分离出来，改为独立的、可插拔的外部组件（out-of-tree），还有 CRI、CNI 等等，这里的 Device Plugin 也能算作其中的一种。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Device Plugin 有两层含义，下文中根据语义自行区分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先它可以代表 k8s 中的 Device Plugin framework&lt;/li&gt;
&lt;li&gt;其次也可以代表厂家的具体实现，比如 NVIDIA/k8s-device-plugin，就是用于接入 NVIDIA GPU 资源的 Device Plugin 实现&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;2-原理&#34;&gt;2. 原理&lt;/h1&gt;

&lt;p&gt;Device Plugin 的工作原理其实不复杂，可以分为 插件注册 和 kubelet 调用插件两部分。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;插件注册：DevicePlugin 启动时会想节点上的 Kubelet 发起注册，这样 Kubelet就可以感知到该插件的存在了&lt;/li&gt;
&lt;li&gt;kubelet 调用插件：注册完成后，当有 Pod 申请对于资源时，kubelet 就会调用该插件 API 实现具体功能
如 k8s 官网上的图所示：
&lt;img src=&#34;post/2024/images/2024-01-24-device-plugin/IMG_20250124-111704478.png&#34; alt=&#34;picture 0&#34; /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2-1-kubelet-部分&#34;&gt;2.1. Kubelet 部分&lt;/h2&gt;

&lt;p&gt;为了提供该功能，Kubelet 新增了一个 Registration gRPC service:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;service Registration {
	rpc Register(RegisterRequest) returns (Empty) {}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;device plugin 可以调用该接口向 Kubelet 进行注册，注册接口需要提供三个参数：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;device plugin 对应的 unix socket 名字：后续 kubelet 根据名称找到对应的 unix socket，并向插件发起调用&lt;/li&gt;
&lt;li&gt;device plugin 调 API version：用于区分不同版本的插件&lt;/li&gt;
&lt;li&gt;device plugin 提供的 ResourceName：遇到不能处理的资源申请时(CPU和Memory之外的资源)，Kubelet 就会根据申请的资源名称来匹配对应的插件&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;ResourceName 需要按照vendor-domain/resourcetype 格式，例如nvidia.com/gpu。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;2-2-device-plugin-部分&#34;&gt;2.2. device plugin 部分&lt;/h2&gt;

&lt;p&gt;要进行设备管理，device plugin 插件需要实现以下接口：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GetDevicePluginOptions：这个接口用于获取设备插件的信息，可以在其返回的响应中指定一些设备插件的配置选项，可以看做是插件的元数据&lt;/li&gt;
&lt;li&gt;ListAndWatch：该接口用于列出可用的设备并持续监视这些设备的状态变化。&lt;/li&gt;
&lt;li&gt;GetPreferredAllocation：将分配偏好信息提供给 device plugin,以便 device plugin 在分配时可以做出更好的选择&lt;/li&gt;
&lt;li&gt;Allocate：该接口用于向设备插件请求分配指定数量的设备资源。&lt;/li&gt;
&lt;li&gt;PreStartContainer： 该接口在容器启动之前调用，用于配置容器使用的设备资源。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;只有 ListAndWatch 和 Allocate 两个接口是必须的，其他都是可以选的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;2-3-工作流程&#34;&gt;2.3. 工作流程&lt;/h2&gt;

&lt;p&gt;一般所有的 Device Plugin 实现最终都会以 Pod 形式运行在 k8s 集群中，又因为需要管理所有节点，因此都会以 DaemonSet 方式部署。&lt;/p&gt;

&lt;p&gt;device plugin 启动之后第一步就是向 Kubelet 注册，让 Kubelet 知道有一个新的设备接入了。&lt;/p&gt;

&lt;p&gt;为了能够调用 Kubelet 的 Register 接口，Device Plugin Pod 会将宿主机上的 kubelet.sock 文件(unix socket)挂载到容器中，通过 kubelet.sock 文件发起调用以实现注册。&lt;/p&gt;

&lt;p&gt;集群部署后，Kubelet 就会启动，&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Kubelet 启动 Registration gRPC 服务（kubelet.sock），提供 Register 接口&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;device-plugin 启动后，通过 kubelet.sock 调用 Register 接口，向 Kubelet 进行注册，注册信息包括 device plugin 的 unix socket，API Version，ResourceName&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;注册成功后，Kubelet 通过 device-plugin 的 unix socket 向 device plugin 调用 ListAndWatch， 获取当前节点上的资源&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Kubelet 向 api-server 更新节点状态来记录上一步中发现的资源&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;此时 kubelet get node -oyaml 就能查看到 Node 对象的 Capacity 中多了对应的资源&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
&lt;li&gt;用户创建 Pod 并申请该资源，调度完成后，对应节点上的 kubelet 调用 device plugin 的 Allocate 接口进行资源分配
&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;大致如下：
&lt;img src=&#34;post/2024/images/2024-01-24-device-plugin/IMG_20250124-141629596.png&#34; alt=&#34;picture 1&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;3-实现&#34;&gt;3. 实现&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;源码：&lt;a href=&#34;https://github.com/mospany/i-device-plugin&#34;&gt;https://github.com/mospany/i-device-plugin&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;device plugin 实现大致分为三部分：&lt;/p&gt;

&lt;p&gt;1）启动时向 Kubelet 发起注册
   - 注意监控 kubelet 的重启，一般是使用 fsnotify 类似的库监控 kubelet.sock 的重新创建事件。如果 kubelet.sock 重新创建了，则认为 kubelet 是重启了，那么需要重新注册
2）gRPC Server：主要是实现 ListAndWatch 和 Allocate两个方法&lt;/p&gt;

&lt;h2 id=&#34;3-1-实现-grpc-server&#34;&gt;3.1. 实现 gRPC Server&lt;/h2&gt;

&lt;p&gt;简单起见，这里只实现了ListAndWatch 和 Allocate 这两个必须的方法。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;对 gRPC 不熟悉的童鞋可以看下这个 –&amp;gt; &lt;a href=&#34;https://www.lixueduan.com/tags/grpc/&#34;&gt;gRPC 系列教程&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;3-1-1-listandwatch&#34;&gt;3.1.1. ListAndWatch&lt;/h3&gt;

&lt;p&gt;这是一个 gRPC 的 Stream 方法，建立长连接，可以持续向 Kubelet 发送设备的信息。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// ListAndWatch returns a stream of List of Devices
// Whenever a Device state change or a Device disappears, ListAndWatch
// returns the new list
func (c *GopherDevicePlugin) ListAndWatch(_ *pluginapi.Empty, srv pluginapi.DevicePlugin_ListAndWatchServer) error {
	devs := c.dm.Devices()
	klog.Infof(&amp;quot;find devices [%s]&amp;quot;, String(devs))

	err := srv.Send(&amp;amp;pluginapi.ListAndWatchResponse{Devices: devs})
	if err != nil {
		return errors.WithMessage(err, &amp;quot;send device failed&amp;quot;)
	}

	klog.Infoln(&amp;quot;waiting for device update&amp;quot;)
	for range c.dm.notify {
		devs = c.dm.Devices()
		klog.Infof(&amp;quot;device update,new device list [%s]&amp;quot;, String(devs))
		_ = srv.Send(&amp;amp;pluginapi.ListAndWatchResponse{Devices: devs})
	}
	return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;发现设备的部分代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// List all device
func (d *DeviceMonitor) List() error {
	err := filepath.Walk(d.path, func(path string, info fs.FileInfo, err error) error {
		if info.IsDir() {
			klog.Infof(&amp;quot;%s is dir,skip&amp;quot;, path)
			return nil
		}

		d.devices[info.Name()] = &amp;amp;pluginapi.Device{
			ID:     info.Name(),
			Health: pluginapi.Healthy,
		}
		return nil
	})

	return errors.WithMessagef(err, &amp;quot;walk [%s] failed&amp;quot;, d.path)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;很简单，就是遍历查看 /etc/gophers 目录下的所有文件，每个文件都会当做一个设备。&lt;/p&gt;

&lt;p&gt;然后再启动一个 Goroutine 监控设备的变化,即/etc/gophers 目录下文件有变化时通过 chan 发送通知,将最新的设备信息发送给 Kubelet。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Watch device change
func (d *DeviceMonitor) Watch() error {
	klog.Infoln(&amp;quot;watching devices&amp;quot;)

	w, err := fsnotify.NewWatcher()
	if err != nil {
		return errors.WithMessage(err, &amp;quot;new watcher failed&amp;quot;)
	}
	defer w.Close()

	errChan := make(chan error)
	go func() {
		defer func() {
			if r := recover(); r != nil {
				errChan &amp;lt;- fmt.Errorf(&amp;quot;device watcher panic:%v&amp;quot;, r)
			}
		}()
		for {
			select {
			case event, ok := &amp;lt;-w.Events:
				if !ok {
					continue
				}
				klog.Infof(&amp;quot;fsnotify device event: %s %s&amp;quot;, event.Name, event.Op.String())

				if event.Op == fsnotify.Create {
					dev := path.Base(event.Name)
					d.devices[dev] = &amp;amp;pluginapi.Device{
						ID:     dev,
						Health: pluginapi.Healthy,
					}
					d.notify &amp;lt;- struct{}{}
					klog.Infof(&amp;quot;find new device [%s]&amp;quot;, dev)
				} else if event.Op&amp;amp;fsnotify.Remove == fsnotify.Remove {
					dev := path.Base(event.Name)
					delete(d.devices, dev)
					d.notify &amp;lt;- struct{}{}
					klog.Infof(&amp;quot;device [%s] removed&amp;quot;, dev)
				}

			case err, ok := &amp;lt;-w.Errors:
				if !ok {
					continue
				}
				klog.Errorf(&amp;quot;fsnotify watch device failed:%v&amp;quot;, err)
			}
		}
	}()

	err = w.Add(d.path)
	if err != nil {
		return fmt.Errorf(&amp;quot;watch device error:%v&amp;quot;, err)
	}

	return &amp;lt;-errChan
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-1-2-allocate&#34;&gt;3.1.2. Allocate&lt;/h3&gt;

&lt;p&gt;Allocate 则是需要告知 kubelet 怎么将设备分配给容器，这里实现比较简单，就是在对应容器中增加一个环境变量，Gopher=$deviceId&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Allocate is called during container creation so that the Device
// Plugin can run device specific operations and instruct Kubelet
// of the steps to make the Device available in the container
func (c *GopherDevicePlugin) Allocate(_ context.Context, reqs *pluginapi.AllocateRequest) (*pluginapi.AllocateResponse, error) {
	ret := &amp;amp;pluginapi.AllocateResponse{}
	for _, req := range reqs.ContainerRequests {
		klog.Infof(&amp;quot;[Allocate] received request: %v&amp;quot;, strings.Join(req.DevicesIDs, &amp;quot;,&amp;quot;))
		resp := pluginapi.ContainerAllocateResponse{
			Envs: map[string]string{
				&amp;quot;Gopher&amp;quot;: strings.Join(req.DevicesIDs, &amp;quot;,&amp;quot;),
			},
		}
		ret.ContainerResponses = append(ret.ContainerResponses, &amp;amp;resp)
	}
	return ret, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;简单看一下 NVIDIA 的 device plugin 是怎么实现 Allocate 的。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Allocate which return list of devices.
func (plugin *NvidiaDevicePlugin) Allocate(ctx context.Context, reqs *pluginapi.AllocateRequest) (*pluginapi.AllocateResponse, error) {
	responses := pluginapi.AllocateResponse{}
	for _, req := range reqs.ContainerRequests {
		if err := plugin.rm.ValidateRequest(req.DevicesIDs); err != nil {
			return nil, fmt.Errorf(&amp;quot;invalid allocation request for %q: %w&amp;quot;, plugin.rm.Resource(), err)
		}
		response, err := plugin.getAllocateResponse(req.DevicesIDs)
		if err != nil {
			return nil, fmt.Errorf(&amp;quot;failed to get allocate response: %v&amp;quot;, err)
		}
		responses.ContainerResponses = append(responses.ContainerResponses, response)
	}

	return &amp;amp;responses, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心其实是这个方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// updateResponseForDeviceListEnvvar sets the environment variable for the requested devices.
func (plugin *NvidiaDevicePlugin) updateResponseForDeviceListEnvvar(response *pluginapi.ContainerAllocateResponse, deviceIDs ...string) {
	response.Envs[plugin.deviceListEnvvar] = strings.Join(deviceIDs, &amp;quot;,&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;给容器添加了一个环境变量，value 为设备 id，具体 deviceID 提供了两种测量，可能是编号或者 uuid&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;const (
	DeviceIDStrategyUUID  = &amp;quot;uuid&amp;quot;
	DeviceIDStrategyIndex = &amp;quot;index&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;key 是一个变量 plugin.deviceListEnvvar，初始化如下:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;	plugin := NvidiaDevicePlugin{
		deviceListEnvvar:     &amp;quot;NVIDIA_VISIBLE_DEVICES&amp;quot;,
		socket:               pluginPath + &amp;quot;.sock&amp;quot;,
	  // ...
	}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是说 NVIDIA 这个 device plugin 实现 Allocate 主要就是给容器增加了环境变量，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;NVIDIA_VISIBLE_DEVICES=GPU-03f69c50-207a-2038-9b45-23cac89cb67d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;NVIDIA_VISIBLE_DEVICES=1,2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在文章 &lt;a href=&#34;http://blog.mospan.cn/2024/01/17/gpu-operator/&#34;&gt;使用 GPU Operator搭建AI算力环境&lt;/a&gt;, 提到 GPU Operator 会使用 NVIDIA Container Toolit Installer 安装 NVIDIA Container Toolit。&lt;/p&gt;

&lt;p&gt;这个 NVIDIA Container Toolit 的作用就是添加对 GPU 的支持，也包括了识别 NVIDIA_VISIBLE_DEVICES 这个环境变量，然后将对应设备挂载到容器里。&lt;/p&gt;

&lt;p&gt;除此之外还会把设备挂载到容器里：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (plugin *NvidiaDevicePlugin) apiDeviceSpecs(devRoot string, ids []string) []*pluginapi.DeviceSpec {
	optional := map[string]bool{
		&amp;quot;/dev/nvidiactl&amp;quot;:        true,
		&amp;quot;/dev/nvidia-uvm&amp;quot;:       true,
		&amp;quot;/dev/nvidia-uvm-tools&amp;quot;: true,
		&amp;quot;/dev/nvidia-modeset&amp;quot;:   true,
	}

	paths := plugin.rm.GetDevicePaths(ids)

	var specs []*pluginapi.DeviceSpec
	for _, p := range paths {
		if optional[p] {
			if _, err := os.Stat(p); err != nil {
				continue
			}
		}
		spec := &amp;amp;pluginapi.DeviceSpec{
			ContainerPath: p,
			HostPath:      filepath.Join(devRoot, p),
			Permissions:   &amp;quot;rw&amp;quot;,
		}
		specs = append(specs, spec)
	}

	return specs
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;		spec := &amp;amp;pluginapi.DeviceSpec{
			ContainerPath: p,
			HostPath:      filepath.Join(devRoot, p),
			Permissions:   &amp;quot;rw&amp;quot;,
		}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里指定了设备在宿主机上的 Path 和挂载到容器之后的 Path，后续就可以根据这些信息进行设备挂载了。&lt;/p&gt;

&lt;p&gt;实际上 device plugin 提供了多种方法来完成设备分配，实现时只需要根据具体情况选择其中一种即可：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Env: 设备插件可以通过环境变量将设备的相关信息传递给容器。这种方式通常用于将设备的配置信息、路径或其他参数传递给容器中的应用程序。&lt;/li&gt;
&lt;li&gt;Mounts: 设备插件通过挂载设备到容器的文件系统中，使容器能够直接访问设备。这个方法适用于需要直接与硬件设备交互的情况，例如 GPU、网络设备、块存储等。&lt;/li&gt;
&lt;li&gt;Devices: 设备插件可以通过 Devices 字段将设备直接映射到容器。此方法允许设备被显式地添加到容器中，类似于 Linux 中的 &amp;ndash;device 选项。&lt;/li&gt;
&lt;li&gt;Annotations: 设备插件还可以使用注解来提供额外的信息，供调度器使用。注解通常不会直接影响容器的行为，而是为 Kubernetes 调度器提供信息，帮助调度器做出更合适的决策。&lt;/li&gt;
&lt;li&gt;CDIDevices: CDI（Container Device Interface）是一种 Kubernetes API 扩展，允许设备插件通过 CDI 标准进行设备分配。通过 CDI，Kubernetes 可以更容易地管理和调度设备，如 GPU、FPGA、网络卡等。
比如 nvidia device plugin 在实现时就同时使用了 Env 和 Devices 方式。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;又比如 &lt;a href=&#34;https://github.com/Deep-Spark/ix-device-plugin/blob/master/pkg/dpm/plugin.go#L144-L156&#34;&gt;ix-device-plugin&lt;/a&gt; 就是用的 Devices 方式,直接指定分配给容器的设备在宿主机的位置，以及要挂载到容器中的位置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (p *iluvatarDevicePlugin) allocateDevicesByDeviceID(hostminor uint, num int) *pluginapi.DeviceSpec {
	var device pluginapi.DeviceSpec

	hostPathPrefix := &amp;quot;/dev/&amp;quot;
	containerPathPrefix := &amp;quot;/dev/&amp;quot;

	// Expose the device node for iluvatar pod.
	device.HostPath = hostPathPrefix + deviceName + strconv.Itoa(int(hostminor))
	device.ContainerPath = containerPathPrefix + deviceName + strconv.Itoa(num)
	device.Permissions = &amp;quot;rw&amp;quot;

	return &amp;amp;device
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-1-3-其他方法&#34;&gt;3.1.3. 其他方法&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// GetDevicePluginOptions returns options to be communicated with Device
// Manager
func (c *GopherDevicePlugin) GetDevicePluginOptions(_ context.Context, _ *pluginapi.Empty) (*pluginapi.DevicePluginOptions, error) {
	return &amp;amp;pluginapi.DevicePluginOptions{PreStartRequired: true}, nil
}

// GetPreferredAllocation returns a preferred set of devices to allocate
// from a list of available ones. The resulting preferred allocation is not
// guaranteed to be the allocation ultimately performed by the
// devicemanager. It is only designed to help the devicemanager make a more
// informed allocation decision when possible.
func (c *GopherDevicePlugin) GetPreferredAllocation(_ context.Context, _ *pluginapi.PreferredAllocationRequest) (*pluginapi.PreferredAllocationResponse, error) {
	return &amp;amp;pluginapi.PreferredAllocationResponse{}, nil
}

// PreStartContainer is called, if indicated by Device Plugin during registeration phase,
// before each container start. Device plugin can run device specific operations
// such as reseting the device before making devices available to the container
func (c *GopherDevicePlugin) PreStartContainer(_ context.Context, _ *pluginapi.PreStartContainerRequest) (*pluginapi.PreStartContainerResponse, error) {
	return &amp;amp;pluginapi.PreStartContainerResponse{}, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-2-向-kubelet-进行注册&#34;&gt;3.2. 向 Kubelet 进行注册&lt;/h2&gt;

&lt;p&gt;注册也是很简单，调用 deviceplugin 提供的 RegisterRequest 方法即可。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Register registers the device plugin for the given resourceName with Kubelet.
func (c *GopherDevicePlugin) Register() error {
	conn, err := connect(pluginapi.KubeletSocket, common.ConnectTimeout)
	if err != nil {
		return errors.WithMessagef(err, &amp;quot;connect to %s failed&amp;quot;, pluginapi.KubeletSocket)
	}
	defer conn.Close()

	client := pluginapi.NewRegistrationClient(conn)
	reqt := &amp;amp;pluginapi.RegisterRequest{
		Version:      pluginapi.Version,
		Endpoint:     path.Base(common.DeviceSocket),
		ResourceName: common.ResourceName,
	}

	_, err = client.Register(context.Background(), reqt)
	if err != nil {
		return errors.WithMessage(err, &amp;quot;register to kubelet failed&amp;quot;)
	}
	return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-3-监控-kubelet-sock-状态&#34;&gt;3.3. 监控 kubelet.sock 状态&lt;/h2&gt;

&lt;p&gt;使用 fsnotify 库监控 kubelet.sock 文件状态，通过 kubelet.sock 文件的变化来判断 kubelet 是否重启，当 kubelet 重启后 device plugin 也需要重启，然后注册到新的 kubelet.sock。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// WatchKubelet restart device plugin when kubelet restarted
func WatchKubelet(stop chan&amp;lt;- struct{}) error {
	watcher, err := fsnotify.NewWatcher()
	if err != nil {
		return errors.WithMessage(err, &amp;quot;Unable to create fsnotify watcher&amp;quot;)
	}
	defer watcher.Close()

	go func() {
		// Start listening for events.
		for {
			select {
			case event, ok := &amp;lt;-watcher.Events:
				if !ok {
					continue
				}
				klog.Infof(&amp;quot;fsnotify events: %s %v&amp;quot;, event.Name, event.Op.String())
				if event.Name == pluginapi.KubeletSocket &amp;amp;&amp;amp; event.Op == fsnotify.Create {
					klog.Warning(&amp;quot;inotify: kubelet.sock created, restarting.&amp;quot;)
					stop &amp;lt;- struct{}{}
				}
			case err, ok := &amp;lt;-watcher.Errors:
				if !ok {
					continue
				}
				klog.Errorf(&amp;quot;fsnotify failed restarting,detail:%v&amp;quot;, err)
			}
		}
	}()

	// watch kubelet.sock
	err = watcher.Add(pluginapi.KubeletSocket)
	if err != nil {
		return errors.WithMessagef(err, &amp;quot;Unable to add path %s to watcher&amp;quot;, pluginapi.KubeletSocket)
	}
	return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;为什么需要重新注册&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;因为Kubelet 中使用一个 map 来存储注册的插件，因此每次 Kubelet 重启都会丢失，所以我们在实现 device plugin 时就要监控 Kubelet 重启状态并重新注册。&lt;/p&gt;

&lt;p&gt;Kubelet Register 方法 实现如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// /pkg/kubelet/cm/devicemanager/plugin/v1beta1/server.go#L143-L165
func (s *server) Register(ctx context.Context, r *api.RegisterRequest) (*api.Empty, error) {
	klog.InfoS(&amp;quot;Got registration request from device plugin with resource&amp;quot;, &amp;quot;resourceName&amp;quot;, r.ResourceName)
	metrics.DevicePluginRegistrationCount.WithLabelValues(r.ResourceName).Inc()

	if !s.isVersionCompatibleWithPlugin(r.Version) {
		err := fmt.Errorf(errUnsupportedVersion, r.Version, api.SupportedVersions)
		klog.InfoS(&amp;quot;Bad registration request from device plugin with resource&amp;quot;, &amp;quot;resourceName&amp;quot;, r.ResourceName, &amp;quot;err&amp;quot;, err)
		return &amp;amp;api.Empty{}, err
	}

	if !v1helper.IsExtendedResourceName(core.ResourceName(r.ResourceName)) {
		err := fmt.Errorf(errInvalidResourceName, r.ResourceName)
		klog.InfoS(&amp;quot;Bad registration request from device plugin&amp;quot;, &amp;quot;err&amp;quot;, err)
		return &amp;amp;api.Empty{}, err
	}

	if err := s.connectClient(r.ResourceName, filepath.Join(s.socketDir, r.Endpoint)); err != nil {
		klog.InfoS(&amp;quot;Error connecting to device plugin client&amp;quot;, &amp;quot;err&amp;quot;, err)
		return &amp;amp;api.Empty{}, err
	}

	return &amp;amp;api.Empty{}, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心在 connectClient 方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (s *server) connectClient(name string, socketPath string) error {
	c := NewPluginClient(name, socketPath, s.chandler)

	s.registerClient(name, c)
	if err := c.Connect(); err != nil {
		s.deregisterClient(name)
		klog.ErrorS(err, &amp;quot;Failed to connect to new client&amp;quot;, &amp;quot;resource&amp;quot;, name)
		return err
	}

	go func() {
		s.runClient(name, c)
	}()

	return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;怎么保存这个 client 的呢?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (s *server) registerClient(name string, c Client) {
	s.mutex.Lock()
	defer s.mutex.Unlock()

	s.clients[name] = c
	klog.V(2).InfoS(&amp;quot;Registered client&amp;quot;, &amp;quot;name&amp;quot;, name)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type server struct {
	socketName string
	socketDir  string
	mutex      sync.Mutex
	wg         sync.WaitGroup
	grpc       *grpc.Server
	rhandler   RegistrationHandler
	chandler   ClientHandler
	clients    map[string]Client // 使用 map 存储，并为持久化
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-4-main-go&#34;&gt;3.4. main.go&lt;/h2&gt;

&lt;p&gt;main 方法分为三个部分：&lt;/p&gt;

&lt;p&gt;1) 启动 gRPC 服务
2) 向 Kubelet 进行注册
3) 监控 kubelet.sock 状态&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func main() {
	klog.Infof(&amp;quot;device plugin starting&amp;quot;)
	dp := device_plugin.NewGopherDevicePlugin()
	go dp.Run()

	// register when device plugin start
	if err := dp.Register(); err != nil {
		klog.Fatalf(&amp;quot;register to kubelet failed: %v&amp;quot;, err)
	}

	// watch kubelet.sock,when kubelet restart,exit device plugin,then will restart by DaemonSet
	stop := make(chan struct{})
	err := utils.WatchKubelet(stop)
	if err != nil {
		klog.Fatalf(&amp;quot;start to kubelet failed: %v&amp;quot;, err)
	}

	&amp;lt;-stop
	klog.Infof(&amp;quot;kubelet restart,exiting&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;4-测试&#34;&gt;4. 测试&lt;/h1&gt;

&lt;h2 id=&#34;4-1-编译&#34;&gt;4.1. 编译&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:mospany/i-device-plugin.git
cd i-device-plugin
make build-image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-24-device-plugin/IMG_20250125-200208751.png&#34; alt=&#34;picture 2&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;4-2-打包镜像&#34;&gt;4.2. 打包镜像&lt;/h2&gt;

&lt;p&gt;由于国内不能直接访问hub.docker.io了，需本地导入。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker save mospany/i-device-plugin:latest -o i-device-plugin_latest.tar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将在本地目录下生成i-device-plugin_latest.tar。&lt;/p&gt;

&lt;h2 id=&#34;4-3-导入镜像&#34;&gt;4.3. 导入镜像&lt;/h2&gt;

&lt;p&gt;将编译机上生成的镜像上传到目标worker上，再导入。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ctr -n k8s.io image import i-device-plugin_latest.tar 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-worker1 img]# crictl images | grep i-device
docker.io/mospany/i-device-plugin                                                     latest                             5954f214c8b06       23.5MB
[root@k8s-worker1 img]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;4-4-部署&#34;&gt;4.4. 部署&lt;/h2&gt;

&lt;p&gt;首先是部署 i-device-plugin，一般使用 DaemonSet 方式部署，完整 yaml 如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: i-device-plugin
  namespace: kube-system
  labels:
    app: i-device-plugin
spec:
  selector:
    matchLabels:
      app: i-device-plugin
  template:
    metadata:
      labels:
        app: i-device-plugin
    spec:
      containers:
        - name: i-device-plugin
          image: mospany/i-device-plugin:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              cpu: &amp;quot;1&amp;quot;
              memory: &amp;quot;512Mi&amp;quot;
            requests:
              cpu: &amp;quot;0.1&amp;quot;
              memory: &amp;quot;128Mi&amp;quot;
          volumeMounts:
            - name: device-plugin
              mountPath: /var/lib/kubelet/device-plugins
            - name: gophers
              mountPath: /etc/gophers
      volumes:
        - name: device-plugin
          hostPath:
            path: /var/lib/kubelet/device-plugins
        - name: gophers
          hostPath:
            path: /etc/gophers

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以 hostPath 方式将用到的两个目录挂载到 Pod 里：
&lt;img src=&#34;post/2024/images/2024-01-24-device-plugin/IMG_20250125-214446131.png&#34; alt=&#34;picture 3&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;/var/lib/kubelet/device-plugins：请求 kubelet.sock 发起调用，同时将 device-plugin gRPC 服务的 sock 文件写入该目录供 kubelet 调用&lt;/li&gt;
&lt;li&gt;/etc/gophers：在该 Demo 中，把 /etc/gophers 目录下的文件作为设备，因此需要将其挂载到 Pod 里。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用kubectl进行apply下yaml文件，确保 i-device-plugin 已经启动。
&lt;img src=&#34;post/2024/images/2024-01-24-device-plugin/IMG_20250125-214524956.png&#34; alt=&#34;picture 4&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;4-5-初始化&#34;&gt;4.5. 初始化&lt;/h2&gt;

&lt;p&gt;在该 Demo 中，把 /etc/gophers 目录下的文件作为设备，因此我们只需要到 /etc/gophers 目录下创建文件，模拟有新的设备接入即可。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir /etc/gophers

touch /etc/gophers/g1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看 device plugin 日志
&lt;img src=&#34;post/2024/images/2024-01-24-device-plugin/IMG_20250125-214911049.png&#34; alt=&#34;picture 5&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到，已经感知到新增的设备了。&lt;/p&gt;

&lt;p&gt;不出意外的话可以在 node 上看到新资源了&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-24-device-plugin/IMG_20250125-215648627.png&#34; alt=&#34;picture 6&#34; /&gt;&lt;/p&gt;

&lt;p&gt;果然，node capacity 中新增了lixueduan.com/gopher: &amp;ldquo;1&amp;rdquo;， 代表创建了2个设备：/etc/gophers/g1和/etc/gophers/g2。&lt;/p&gt;

&lt;h2 id=&#34;4-6-创建测试-pod&#34;&gt;4.6. 创建测试 Pod&lt;/h2&gt;

&lt;p&gt;接下来创建一个 Pod 申请该资源试试&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  name: gopher-pod
spec:
  containers:
  - name: gopher-container
    image: docker.m.daocloud.io/busybox
    command: [&amp;quot;sh&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;echo Hello, Kubernetes! &amp;amp;&amp;amp; sleep 3600&amp;quot;]
    resources:
      requests:
        lixueduan.com/gopher: &amp;quot;1&amp;quot;
      limits:
        lixueduan.com/gopher: &amp;quot;1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod 启动成功&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 yaml]# kubectl  get pod -A | grep gopher
default        gopher-pod                                                        1/1     Running     0               4m18s
[root@k8s-master1 yaml]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之前分配设备是添加 Gopher=xxx 这个环境变量，现在看下是否正常分配&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 yaml]# kubectl  exec -ti gopher-pod -- env | grep -i goph
HOSTNAME=gopher-pod
Gopher=g2
[root@k8s-master1 yaml]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ok,环境变量存在，可以看到分配给该 Pod 的设备是 g2。&lt;/p&gt;

&lt;h1 id=&#34;5-小结&#34;&gt;5. 小结&lt;/h1&gt;

&lt;p&gt;本文主要分析了 k8s 中的 Device Plugin 机制的工作原理，并实现了一个简单的 &lt;strong&gt;i-device-plugin&lt;/strong&gt;来进一步加深理解。&lt;/p&gt;

&lt;p&gt;Device Plugin 的工作原理其实不复杂，可以分为 插件注册 和 kubelet 调用插件两部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;插件注册：DevicePlugin 启动时会想节点上的 Kubelet 发起注册，这样 Kubelet 就可以感知到该插件的存在了&lt;/li&gt;
&lt;li&gt;kubelet 调用插件：注册完成后，当有 Pod 申请对于资源时，kubelet 就会调用该插件 API 实现具体功能&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-24-device-plugin/IMG_20250125-223139774.png&#34; alt=&#34;picture 7&#34; /&gt;&lt;/p&gt;

&lt;p&gt;以下是总结的几个常见问题：&lt;/p&gt;

&lt;p&gt;1）device plugin 是怎么感知节点上的设备的？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一般设备都会在 /dev/ 目录下，比如 NVIDIA GPU 就是 /dev/nvidia0、/dev/nvidia1 这样，不过具体逻辑还是得硬件厂商自己实现&lt;/li&gt;
&lt;li&gt;然后 device plugin 会以 DaemonSet 方式部署到所有节点，因此能发现每个节点上的设备
&lt;img src=&#34;post/2024/images/2024-01-24-device-plugin/IMG_20250125-223501456.png&#34; alt=&#34;picture 8&#34; /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2）device plugin Allocate 方法怎么实现分配设备给容器的？
&amp;gt; 需要注意一点：Allocate 方法并没有真正将设备分配给容器，因为这个时候甚至都还没创建容器，只是在该方法中可以通过 Env、Mounts、Devices、Annotations、CDIDevices 等不同形式来传递 要将那些设备分配给该容器 这个信息给后续组件。&lt;/p&gt;

&lt;p&gt;这些信息传给 Kubelet，然后 Kubelet 通过 CRI 调用 Runtime（Docker/Containerd 等等）真正开始创建容器。&lt;/p&gt;

&lt;p&gt;比如 NVIDIA 在 Allocate 中就传递了 NVIDIA_VISIBLE_DEVICES 这个 Env，然后自己实现了 nvidia-container-runtime，该 runtime 就可以根据该 Env 知道要把哪个 GPU 分配给容器，然后修改容器的 OCI Spec，最终 runC(或者其他实现)真正创建容器时就会按照这个描述去处理。&lt;/p&gt;

&lt;p&gt;又比如 ix-device-plugin 就是用的 Devices 方式,直接指定分配给容器的设备在宿主机的位置，以及要挂载到容器中的位置，这样就不需要实现自己的 container-runtime 了，runC 创建容器时也能把对应设备分配给容器。&lt;/p&gt;

&lt;p&gt;这样又引出一个小问题，既然天数(ix-device-plugin)这个实现只用 Devices 就能正常运行，那为什么 NVIDIA 实现了 Devices 又实现了一个 Env？&lt;/p&gt;

&lt;p&gt;其实这个 Env 的实现是为了兼容非 k8s 环境，比如 Docker 环境：&lt;/p&gt;

&lt;p&gt;nvidia 可以在启动容器时指定 GPU&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# --gpus
docker run --gpus device=0 -it tensorflow/tensorflow:latest-gpu bash
# 或者环境变量 NVIDIA_VISIBLE_DEVICES
docker run -e NVIDIA_VISIBLE_DEVICES=0 -it tensorflow/tensorflow:latest-gpu bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而天数则不行，就像这样：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo docker run --shm-size=&amp;quot;32g&amp;quot; -it -v /usr/src:/usr/src -v /lib/modules:/lib/modules -v /dev:/dev --privileged --cap-add=ALL --pid=host corex:4.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，需要自己使用 -v 将相关文件挂载进容器才能使用，nvidia-container-runtime 实则是将这部分进行了封装简化，用户只需要传一个参数即可。&lt;/p&gt;

&lt;p&gt;3）为什么 device plugin 要 Watch Kubelet 状态，当 Kubelet 重启后 device plugin 也要跟着重启。&lt;/p&gt;

&lt;p&gt;这个问题实际上可以翻译为：为什么 Kubelet 重启后，device plugin 需要重新向 Kubelet 注册？&lt;/p&gt;

&lt;p&gt;因为 device plugin 的注册信息 Kubelet 是存在内存中的，使用 Go 中的 Map 结构进行存储。重启后就会丢失，因此各个 device plugin 都需要重新注册。&lt;/p&gt;

&lt;p&gt;至于为什么 device plugin 一般也会跟着重启，是因为 device plugin 在启动时会调用因此注册接口，因此感知到 Kubelet 重启了，直接让 device plugin 退出即可，然后 DaemonSet 会重新拉起 Pod，这样启动后自动调用注册接口。&lt;/p&gt;

&lt;h1 id=&#34;6-参考&#34;&gt;6. 参考&lt;/h1&gt;

&lt;p&gt;【01】&lt;a href=&#34;https://www.lixueduan.com/posts/kubernetes/21-device-plugin/&#34;&gt;Kubernetes教程(二一)&amp;mdash;自定义资源支持：K8s Device Plugin 从原理到实现&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(09): 使用 GPU Operator搭建AI算力环境</title>
            <link>http://mospany.github.io/2024/01/17/gpu-operator/</link>
            <pubDate>Wed, 17 Jan 2024 19:31:10 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2024/01/17/gpu-operator/</guid>
            <description>

&lt;h1 id=&#34;1-引言&#34;&gt;1. 引言&lt;/h1&gt;

&lt;p&gt;为了学习AI应用、算法与算力等技术，应用需跑在GPU卡上，需要在节点上安装 GPU Driver、Container Toolkit 等组件，当集群规模较大时还是比较麻烦的。&lt;/p&gt;

&lt;p&gt;为了解决这个问题，NVIDIA 推出了 GPU Operator，GPU Operator 旨在简化在 Kubernetes 环境中使用 GPU 的过程，通过自动化的方式处理 GPU 驱动程序安装、Controller Toolkit、Device-Plugin 、监控等组件。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;基本上把需要手动安装、配置的地方全部自动化处理了，极大简化了 k8s 环境中的 GPU 使用。&lt;/p&gt;

&lt;p&gt;ps：只有 NVIDIA GPU 可以使用，其他厂家现在基本还是手动安装。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;2-规划&#34;&gt;2. 规划&lt;/h1&gt;

&lt;p&gt;本文主要分享如何使用 GPU Operator 快速搭建 Kubernetes GPU 环境。&lt;/p&gt;

&lt;p&gt;基于如下环境搭建：
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250117-194645644.png&#34; alt=&#34;picture 0&#34; /&gt;&lt;br /&gt;
查看worker节点GPU信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-worker1 ~]# lspci | grep -i nvidia
00:07.0 VGA compatible controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)
[root@k8s-worker1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明该worker节点有1张NVIDIA Tesla T4的GPU卡。&lt;/p&gt;

&lt;h1 id=&#34;3-组件介绍&#34;&gt;3. 组件介绍&lt;/h1&gt;

&lt;p&gt;这部分主要分析下 GPU Operator 涉及到的各个组件及其作用。&lt;/p&gt;

&lt;p&gt;NVIDIA GPU Operator总共包含如下的几个组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;NFD(Node Feature Discovery)：用于给节点打上某些标签，这些标签包括 cpu id、内核版本、操作系统版本、是不是 GPU 节点等，其中需要关注的标签是nvidia.com/gpu.present=true，如果节点存在该标签，那么说明该节点是 GPU 节点。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;GFD(GPU Feature Discovery)：用于收集节点的 GPU 设备属性（GPU 驱动版本、GPU型号等），并将这些属性以节点标签的方式透出。在k8s 集群中以 DaemonSet 方式部署，只有节点拥有标签nvidia.com/gpu.present=true时，DaemonSet 控制的 Pod 才会在该节点上运行。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;新版本 GFD 迁移到了 &lt;a href=&#34;https://github.com/NVIDIA/k8s-device-plugin/tree/main/docs/gpu-feature-discovery&#34;&gt;NVIDIA/k8s-device-plugin&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;NVIDIA Driver Installer：基于容器的方式在节点上安装 NVIDIA GPU 驱动，在 k8s 集群中以 DaemonSet 方式部署，只有节点拥有标签nvidia.com/gpu.present=true时，DaemonSet 控制的 Pod 才会在该节点上运行。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;NVIDIA Container Toolkit Installer：能够实现在容器中使用 GPU 设备，在 k8s 集群中以 DaemonSet 方式部署，同样的，只有节点拥有标签nvidia.com/gpu.present=true时，DaemonSet 控制的 Pod 才会在该节点上运行。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;NVIDIA Device Plugin：NVIDIA Device Plugin 用于实现将 GPU 设备以 Kubernetes 扩展资源的方式供用户使用，在 k8s 集群中以 DaemonSet 方式部署，只有节点拥有标签nvidia.com/gpu.present=true时，DaemonSet 控制的 Pod 才会在该节点上运行。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;DCGM Exporter：周期性的收集节点 GPU 设备的状态（当前温度、总的显存、已使用显存、使用率等）并暴露 Metrics，结合 Prometheus 和 Grafana 使用。在 k8s 集群中以DaemonSet 方式部署，只有节点拥有标签nvidia.com/gpu.present=true时，DaemonSet 控制的 Pod 才会在该节点上运行。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;首先是 GFD、NFD，二者都是用于发现 Node 上的信息，并以 label 形式添加到 k8s node 对象上，特别是 GFD 会添加nvidia.com/gpu.present=true 标签表示该节点有 GPU，只有携带该标签的节点才会安装后续组件。&lt;/p&gt;

&lt;p&gt;然后则是 Driver Installer、Container Toolkit Installer 用于安装 GPU 驱动和 container toolkit。&lt;/p&gt;

&lt;p&gt;接下来则是 device-plugin 让 k8s 能感知到 GPU 资源信息便于调度和管理。&lt;/p&gt;

&lt;p&gt;最后的 exporter 则是采集 GPU 监控并以 Prometheus Metrics 格式暴露，用于做 GPU 监控。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;这些组件基本就把需要手动配置的东西都自动化了。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;NVIDIA GPU Operator 依如下的顺序部署各个组件，并且如果前一个组件部署失败，那么其后面的组件将停止部署：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;NVIDIA Driver Installer&lt;/li&gt;
&lt;li&gt;NVIDIA Container Toolkit Installer&lt;/li&gt;
&lt;li&gt;NVIDIA Device Plugin&lt;/li&gt;
&lt;li&gt;DCGM Exporter&lt;/li&gt;
&lt;li&gt;GFD&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每个组件都是以 DaemonSet 方式部署，并且只有当节点存在标签 nvidia.com/gpu.present=true 时，各 DaemonSet控制的 Pod 才会在节点上运行。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nvidia.com/gpu.deploy.driver=pre-installed
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-1-gfd-nfd&#34;&gt;3.1. GFD &amp;amp; NFD&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;GFD：GPU Feature Discovery&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;NFD：Node Feature Discovery&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;根据名称基本能猜到这两个组件的功能，发现节点信息和 GPU 信息并以 Label 形式添加到 k8s 中的 node 对象上。&lt;/p&gt;

&lt;p&gt;其中 NFD 添加的 label 以   feature.node.kubernetes.io 作为前缀，比如:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;feature.node.kubernetes.io/cpu-cpuid.ADX=true
feature.node.kubernetes.io/system-os_release.ID=ubuntu
feature.node.kubernetes.io/system-os_release.VERSION_ID.major=22
feature.node.kubernetes.io/system-os_release.VERSION_ID.minor=04
feature.node.kubernetes.io/system-os_release.VERSION_ID=22.04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于 GFD 则主要记录 GPU 信息&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nvidia.com/cuda.runtime.major=12
nvidia.com/cuda.runtime.minor=2
nvidia.com/cuda.driver.major=535
nvidia.com/cuda.driver.minor=161
nvidia.com/gpu.product=Tesla-T4
nvidia.com/gpu.memory=15360
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-2-driver-installer&#34;&gt;3.2. Driver Installer&lt;/h2&gt;

&lt;p&gt;NVIDIA 官方提供了一种基于容器安装 NVIDIA 驱动的方式，GPU Operator 安装 nvidia 驱动也是采用的这种方式。&lt;/p&gt;

&lt;p&gt;当 NVIDIA 驱动基于容器化安装后，整个架构将演变成图中描述的样子：
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-152421553.png&#34; alt=&#34;picture 5&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Driver Installer 组件对应的 DaemonSet 就是nvidia-driver-daemonset-5.15.0-105-generic-ubuntu22.04。&lt;/p&gt;

&lt;p&gt;该 DaemonSet 对应的镜像为&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@test:~# kgo get ds nvidia-driver-daemonset-5.15.0-105-generic-ubuntu22.04 -oyaml|grep image
        image: nvcr.io/nvidia/driver:535-5.15.0-105-generic-ubuntu22.04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中 DaemonSet 名称/镜像由几部分组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nvidia-driver-daemonset 这部分为前缀&lt;/li&gt;
&lt;li&gt;5.15.0-105-generic 为内核版本，使用uname -r 命令查看&lt;/li&gt;
&lt;li&gt;ubuntu22.04 操作系统版本，使用cat /etc/os-release 命令查看&lt;/li&gt;
&lt;li&gt;535：这个是 GPU Driver 的版本号，这里表示安装 535 版本驱动，在部署时可以指定。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GPU Operator 会自动根据节点上的内核版本和操作系统生成 DaemonSet 镜像，因为是以 DaemonSet 方式运行的，所有节点上都是跑的同一个 Pod，&lt;strong&gt;因此要限制集群中的所有 GPU 节点操作系统和内核版本必须一致。&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ps：如果提前手动在节点上安装 GPU 驱动，那么 GPU Operator 检测到之后就不会在该节点上启动 Installer Pod，这样该节点就可以不需要管操作系统和内核版本。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;3-3-nvidia-container-toolkit-installer&#34;&gt;3.3. NVIDIA Container Toolkit Installer&lt;/h2&gt;

&lt;p&gt;该组件用于安装 NVIDIA Container Toolkit。&lt;/p&gt;

&lt;p&gt;手动安装的时候有两个步骤：&lt;/p&gt;

&lt;p&gt;1）安装 NVIDIA Container Toolkit&lt;/p&gt;

&lt;p&gt;2）修改 Runtime 配置指定使用 nvidia-runtime&lt;/p&gt;

&lt;p&gt;在整个调用链中新增 nvidia-container-runtime，以便处理 GPU 相关操作。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-153551818.png&#34; alt=&#34;picture 6&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这个 Installer 做的操作也就是这两步：&lt;/p&gt;

&lt;p&gt;1）将容器中NVIDIA Container Toolkit组件所涉及的命令行工具和库文件移动到/usr/local/nvidia/toolkit目录下&lt;/p&gt;

&lt;p&gt;2）在 /usr/local/nvidia/toolkit/.config/nvidia-container-runtime创建nvidia-container-runtime的配置文件config.toml，并设置nvidia-container-cli.root的值为/run/nvidia/driver。&lt;/p&gt;

&lt;h1 id=&#34;4-部署&#34;&gt;4. 部署&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;参考官方文档： &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#operator-install-guide&#34;&gt;operator-install-guide&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;4-1-准备工作&#34;&gt;4.1. 准备工作&lt;/h2&gt;

&lt;p&gt;要求：&lt;/p&gt;

&lt;p&gt;1）GPU 节点必须运行相同的操作系统，&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果提前手动在节点上安装驱动的话，该节点可以使用不同的操作系统&lt;/li&gt;
&lt;li&gt;CPU 节点操作系统没要求，因为 gpu-operator 只会在 GPU 节点上运行
2）GPU 节点必须配置相同容器引擎，例如都是 containerd 或者都是 docker&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3）如果使用了 Pod Security Admission (PSA) ，需要为 gpu-operator 标记特权模式&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# kubectl create ns gpu-operator
namespace/gpu-operator created
[root@k8s-master1 ~]# kubectl label --overwrite ns gpu-operator pod-security.kubernetes.io/enforce=privileged
namespace/gpu-operator labeled
[root@k8s-master1 ~]# kubectl get ns gpu-operator --show-labels
NAME           STATUS   AGE   LABELS
gpu-operator   Active   30s   kubernetes.io/metadata.name=gpu-operator,pod-security.kubernetes.io/enforce=privileged
[root@k8s-master1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4）集群中不要安装 NFD，如果已经安装了需要再安装 gpu-operator 时禁用 NFD 部署。&lt;/p&gt;

&lt;p&gt;使用以下命令查看集群中是否部署 NFD&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl get nodes -o json | jq &#39;.items[].metadata.labels | keys | any(startswith(&amp;quot;feature.node.kubernetes.io&amp;quot;))&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果返回 true 则说明集群中安装了 NFD。&lt;/p&gt;

&lt;h2 id=&#34;4-2-helm部署&#34;&gt;4.2. helm部署&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;参考官方文档： &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#operator-install-guide&#34;&gt;operator-install-guide&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;4-2-1-安装helm命令&#34;&gt;4.2.1. 安装helm命令&lt;/h3&gt;

&lt;p&gt;如果master节点上还未安装helm命令，需安装&lt;/p&gt;

&lt;h4 id=&#34;4-2-1-1-下载-helm-3-的最新版本&#34;&gt;4.2.1.1. 下载 Helm 3 的最新版本&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl https://get.helm.sh/helm-v3.11.1-linux-amd64.tar.gz -o helm-v3.11.1-linux-amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;4-2-1-2-解压安装包&#34;&gt;4.2.1.2. 解压安装包&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 helm]# ls
helm-v3.11.1-linux-amd64.tar.gz
[root@k8s-master1 helm]# tar -zxvf helm-v3.11.1-linux-amd64.tar.gz 
linux-amd64/
linux-amd64/helm
linux-amd64/LICENSE
linux-amd64/README.md
[root@k8s-master1 helm]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;4-2-1-3-移动-helm-到系统的可执行路径&#34;&gt;4.2.1.3. 移动 helm 到系统的可执行路径&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo mv linux-amd64/helm /usr/local/bin/helm`
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;4-2-1-4-验证安装&#34;&gt;4.2.1.4. 验证安装&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 helm]# helm version
version.BuildInfo{Version:&amp;quot;v3.11.1&amp;quot;, GitCommit:&amp;quot;293b50c65d4d56187cd4e2f390f0ada46b4c4737&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, GoVersion:&amp;quot;go1.18.10&amp;quot;}
[root@k8s-master1 helm]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明安装成功。&lt;/p&gt;

&lt;h3 id=&#34;4-2-2-chart部署&#34;&gt;4.2.2. chart部署&lt;/h3&gt;

&lt;h4 id=&#34;4-2-2-1-添加repo仓库&#34;&gt;4.2.2.1. 添加repo仓库&lt;/h4&gt;

&lt;p&gt;添加 nvidia helm 仓库并更新&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;helm repo add nvidia https://helm.ngc.nvidia.com/nvidia \
    &amp;amp;&amp;amp; helm repo update
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;4-2-2-2-拉取chart包&#34;&gt;4.2.2.2. 拉取chart包&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;helm pull nvidia/gpu-operator --version v24.9.1 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将在本地生成gpu-operator-v24.9.1.tgz文件。&lt;/p&gt;

&lt;h4 id=&#34;4-2-2-3-准备镜像&#34;&gt;4.2.2.3. 准备镜像&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点上执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;由于有些镜像国内不能直接访问，需从国内镜像地址拉取后再修改tag。
1）拉取国内代理镜像：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;crictl pull swr.cn-north-4.myhuaweicloud.com/ddn-k8s/registry.k8s.io/nfd/node-feature-discovery:v0.16.6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2）修改镜像tag：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ctr -n k8s.io image tag  swr.cn-north-4.myhuaweicloud.com/ddn-k8s/registry.k8s.io/nfd/node-feature-discovery:v0.16.6 registry.k8s.io/nfd/node-feature-discovery:v0.16.6 
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;4-2-2-4-安装chart包&#34;&gt;4.2.2.4. 安装chart包&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 添加 nvidia helm 仓库并更新
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia \
    &amp;amp;&amp;amp; helm repo update
# 以默认配置安装
helm install --wait --generate-name \
    -n gpu-operator --create-namespace \
    nvidia/gpu-operator

# 如果提前手动安装了 gpu 驱动，operator 中要禁止 gpu 安装
helm install --wait --generate-name \
     -n gpu-operator --create-namespace \
     nvidia/gpu-operator \
     --set driver.enabled=false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;完成后 会启动 Pod 安装驱动，如果节点上已经安装了驱动了，那么 gpu-operaotr 就不会启动安装驱动的 Pod,通过 label 进行筛选。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;没安装驱动的节点会打上 nvidia.com/gpu.deploy.driver=true ,表示需要安装驱动&lt;/li&gt;
&lt;li&gt;已经手动安装过驱动的节点会打上nvidia.com/gpu.deploy.driver=pre-install,Daemonset 则不会在该节点上运行
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-113127922.png&#34; alt=&#34;picture 1&#34; /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;当然，并不是每个操作系统+内核版本的组合，NVIDIA 都提供了对应的镜像，可以提前在 NVIDIA/driver tags 查看当前 NVIDIA 提供的驱动版本。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;5-测试&#34;&gt;5. 测试&lt;/h1&gt;

&lt;p&gt;部署后，会在gpu-operator namespace 下启动相关 Pod，查看一下 Pod 的运行情况，除了一个 Completed 之外其他应该都是 Running 状态。
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-113933891.png&#34; alt=&#34;picture 2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后进入nvidia-device-plugin-daemonset-xxx Pod，在该 Pod 中可以执行 nvidia-smi命令,比如查看 GPU 信息：
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-115125616.png&#34; alt=&#34;picture 3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;最后再查看 node 信息
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-115436263.png&#34; alt=&#34;picture 4&#34; /&gt;&lt;br /&gt;
可以看出nvidia.com/gpu: &amp;ldquo;1&amp;rdquo;总量为1， 可分配也为1。&lt;/p&gt;

&lt;p&gt;至此，说明我们的 GPU Operator 已经安装成功，K8s 也能感知到节点上的 GPU，接下来就可以在 Pod 中使用 GPU 了。&lt;/p&gt;

&lt;p&gt;创建一个测试 Pod，申请一个 GPU：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  name: cuda-vectoradd
spec:
  restartPolicy: OnFailure
  containers:
  - name: cuda-vectoradd
    image: &amp;quot;nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda10.2&amp;quot;
    resources:
      limits:
        nvidia.com/gpu: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正常的 Pod 日志如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 yaml]# kubectl logs cuda-vectoradd 
[Vector addition of 50000 elements]
Copy input data from the host memory to the CUDA device
CUDA kernel launch with 196 blocks of 256 threads
Copy output data from the CUDA device to the host memory
Test PASSED
Done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此，我们已经可以在 k8s 中使用 GPU 了。&lt;/p&gt;

&lt;h1 id=&#34;6-原理&#34;&gt;6. 原理&lt;/h1&gt;

&lt;p&gt;这部分主要分析一下 Driver Installer 和 NVIDIA Container Toolkit Installer 这两个组件是怎么实现的，大致原理。&lt;/p&gt;

&lt;h2 id=&#34;6-1-driver-installer&#34;&gt;6.1. Driver Installer&lt;/h2&gt;

&lt;p&gt;NVIDIA 官方提供了一种基于容器安装 NVIDIA 驱动的方式，GPU Operator 安装 nvidia 驱动也是采用的这种方式。&lt;/p&gt;

&lt;p&gt;当 NVIDIA 驱动基于容器化安装后，整个架构将演变成图中描述的样子：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-184040318.png&#34; alt=&#34;picture 7&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;6-2-nvidia-container-toolkit-installer&#34;&gt;6.2. NVIDIA Container Toolkit Installer&lt;/h2&gt;

&lt;p&gt;该组件用于安装 NVIDIA Container Toolkit。&lt;/p&gt;

&lt;p&gt;手动安装的时候有两个步骤：&lt;/p&gt;

&lt;p&gt;1）安装 NVIDIA Container Toolkit&lt;/p&gt;

&lt;p&gt;2）修改 Runtime 配置指定使用 nvidia-runtime
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-190232787.png&#34; alt=&#34;picture 9&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在整个调用链中新增 nvidia-container-runtime，以便处理 GPU 相关操作。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-185351436.png&#34; alt=&#34;picture 8&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这个 Installer 做的操作也就是这两步：&lt;/p&gt;

&lt;p&gt;1）将容器中NVIDIA Container Toolkit组件所涉及的命令行工具和库文件移动到/usr/local/nvidia/toolkit目录下
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-211806300.png&#34; alt=&#34;picture 10&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2）在 /usr/local/nvidia/toolkit/.config/nvidia-container-runtime创建nvidia-container-runtime的配置文件config.toml，并设置nvidia-container-cli.root的值为/run/nvidia/driver。
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-212557933.png&#34; alt=&#34;picture 11&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;7-小结&#34;&gt;7. 小结&lt;/h1&gt;

&lt;p&gt;小结
本文主要分享如何使用 GPU Operator 自动化完成 GPU Driver、NVIDIA Container Toolkit、device-plugin、exporter 等组件的部署，快速实现在 k8s 环境中使用 GPU。&lt;/p&gt;

&lt;p&gt;最后简单分析了 Driver Installer 和 NVIDIA Container Toolkit Installer 这两个组件的工作原理。&lt;/p&gt;

&lt;p&gt;GPU Operator 极大简化了在 k8s 中使用 GPU 的繁琐过程，但是也存在一些缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Driver Installer 以 DaemonSet 方式运行的，每个节点上运行的 Pod 都一样，但是镜像由 驱动版本+内核版本+操作系统版本拼接而成，因此需要集群中所有节点操作系统一致。&lt;/li&gt;
&lt;li&gt;NVIDIA Container Toolkit Installer 同样是以 DaemonSet 方式运行的，另外安装时需要指定 Runtime，这也造成了集群的节点必须安装相同的 Container Runtime。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;8-参考资料&#34;&gt;8. 参考资料&lt;/h1&gt;

&lt;p&gt;【01】&lt;a href=&#34;https://www.lixueduan.com/posts/ai/02-gpu-operator/#2-%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D&#34;&gt;GPU 环境搭建指南：使用 GPU Operator 加速 Kubernetes GPU 环境搭建&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(08): 在ECS、Docker、K8s 等环境中使用 GPU</title>
            <link>http://mospany.github.io/2024/01/16/gpu-on-k8s/</link>
            <pubDate>Tue, 16 Jan 2024 19:31:10 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2024/01/16/gpu-on-k8s/</guid>
            <description>

&lt;h1 id=&#34;1-引言&#34;&gt;1. 引言&lt;/h1&gt;

&lt;p&gt;本文主要分享在不同环境，例如ECS、Docker 和 Kubernetes 等环境中如何使用 GPU。
&lt;strong&gt;注&lt;/strong&gt;：由于没有物理机裸机，在阿里云上申请ECS也可满足学习使用。&lt;/p&gt;

&lt;h1 id=&#34;2-规划&#34;&gt;2. 规划&lt;/h1&gt;

&lt;p&gt;基于如下环境搭建：
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250117-194645644.png&#34; alt=&#34;picture 0&#34; /&gt;&lt;br /&gt;
查看worker节点GPU信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-worker1 ~]# lspci | grep -i nvidia
00:07.0 VGA compatible controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)
[root@k8s-worker1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明该worker节点有1张NVIDIA Tesla T4的GPU卡。&lt;/p&gt;

&lt;h1 id=&#34;3-概述&#34;&gt;3. 概述&lt;/h1&gt;

&lt;p&gt;仅以比较常见的 NVIDIA GPU 举例，系统为 Linux，对于其他厂家的 GPU 设备理论上流程都是一样的。&lt;/p&gt;

&lt;p&gt;省流：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;对于ECS环境，只需要安装对应的 &lt;a href=&#34;https://help.aliyun.com/zh/egs/user-guide/installation-guideline-for-nvidia-drivers?spm=a2c4g.11186623.help-menu-155040.d_1_5_0.726f718evZrl7t&#34;&gt;GPU Driver&lt;/a&gt;（GPU计算型实例为Tesla驱动，GPU虚拟化型实例为GRID驱动） 以及 CUDA Toolkit 。&lt;/li&gt;
&lt;li&gt;对应 Docker 环境，需要额外安装 nvidia-container-toolkit 并配置 docker 使用 nvidia runtime。&lt;/li&gt;
&lt;li&gt;对应 k8s 环境，需要额外安装对应的 device-plugin 使得 kubelet 能够感知到节点上的 GPU 设备，以便 k8s 能够进行 GPU 管理。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注：一般在 k8s 中使用都会直接使用 gpu-operator 方式进行安装，本文主要为了搞清各个组件的作用，因此进行手动安装。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ps；下一篇分享下如何使用 gpu-operator 快速完成安装&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;4-ecs环境&#34;&gt;4. ECS环境&lt;/h1&gt;

&lt;p&gt;裸机中要使用上 GPU 需要安装以下组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GPU Driver&lt;/li&gt;
&lt;li&gt;CUDA Toolkit
二者的关系如 NVIDIA 官网上的这个图所示：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250120-152256220.png&#34; alt=&#34;picture 0&#34; /&gt;&lt;/p&gt;

&lt;p&gt;GPU Driver 包括了 GPU 驱动和 CUDA 驱动，CUDA Toolkit 则包含了 CUDA Runtime。&lt;/p&gt;

&lt;p&gt;GPU 作为一个 PCIE 设备，只要安装好之后，在系统中就可以通过 lspci 命令查看到，先确认机器上是否有 GPU：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-worker1 ~]# lspci | grep -i nvidia
00:07.0 VGA compatible controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)
[root@k8s-worker1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明该worker节点有1张NVIDIA Tesla T4的GPU卡。&lt;/p&gt;

&lt;h2 id=&#34;4-1-安装grid驱动&#34;&gt;4.1. 安装GRID驱动&lt;/h2&gt;

&lt;p&gt;由于主机规格是ecs.sgn6i-vws-m2.xlarge，是虚拟化型规格，不能安装Tesla驱动的，需要安装grid驱动。
安装Tesla驱动的话将会出现&lt;a href=&#34;#61-error-unable-to-load-the-kernel-module-nvidiako&#34;&gt;ERROR: Unable to load the kernel module &amp;lsquo;nvidia.ko&amp;rsquo;.&lt;/a&gt;错误安装失败。&lt;/p&gt;

&lt;p&gt;安装步骤参考：&lt;a href=&#34;https://help.aliyun.com/zh/egs/user-guide/use-cloud-assistant-to-automatically-install-and-upgrade-grid-drivers?spm=a2c4g.11186623.help-menu-155040.d_1_5_2_1.65bd2ef7tthoW5&#34;&gt;在GPU虚拟化型实例中安装GRID驱动（Linux）&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;4-1-1-准备安装脚本&#34;&gt;4.1.1. 准备安装脚本&lt;/h3&gt;

&lt;p&gt;install-grid.sh内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
if acs-plugin-manager --list --local | grep grid_driver_install &amp;gt; /dev/null 2&amp;gt;&amp;amp;1
then
            acs-plugin-manager --remove --plugin grid_driver_install
fi

acs-plugin-manager --exec --plugin grid_driver_install
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-1-2-安装grid&#34;&gt;4.1.2. 安装grid&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-worker1 ~]# sh install-grid.sh 
RemovePlugin success, plugin[grid_driver_install]
INFO[0000] Starting environment pre-check               
INFO[0000] environment pre-check done                   
INFO[0000] Check gpu device present                     
INFO[0000] Check gpu device present done                
INFO[0000] current installed gird driver is, 470.239.06 
INFO[0000] current installed gird driver is already SWL driver 
[root@k8s-worker1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-1-3-验证&#34;&gt;4.1.3. 验证&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-worker1 ~]# nvidia-smi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行成功，可看到有一张T4-2Q的GPU卡。
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-192347251.png&#34; alt=&#34;picture 6&#34; /&gt;&lt;br /&gt;
至此，我们就安装好 GPU 驱动了，系统也能正常识别到 GPU。&lt;/p&gt;

&lt;p&gt;这里显示的 CUDA 版本表示当前驱动最大支持的 CUDA 版本。&lt;/p&gt;

&lt;h2 id=&#34;4-2-安装cuda&#34;&gt;4.2. 安装CUDA&lt;/h2&gt;

&lt;h3 id=&#34;4-2-1-安装软件&#34;&gt;4.2.1. 安装软件&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://developer.download.nvidia.com/compute/cuda/11.4.0/local_installers/cuda_11.4.0_470.42.01_linux.run
sudo sh cuda_11.4.0_470.42.01_linux.run
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于已安装了GRID驱动，需取消cuda自带的驱动安装：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-200338239.png&#34; alt=&#34;picture 7&#34; /&gt;&lt;br /&gt;
01) 安装全部组件：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-200453794.png&#34; alt=&#34;picture 8&#34; /&gt;&lt;br /&gt;
02) 输出：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-201735441.png&#34; alt=&#34;picture 9&#34; /&gt;&lt;/p&gt;

&lt;p&gt;03) 执行以下命令，重启GPU实例&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;reboot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;04) 依次执行以下命令，配置CUDA环境变量。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo &#39;export PATH=/usr/local/cuda/bin:$PATH&#39; | sudo tee /etc/profile.d/cuda.sh
source /etc/profile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;05) 检查CUDA是否成功安装。&lt;/p&gt;

&lt;p&gt;a) 执行nvcc -V命令，检查CUDA安装版本是否正确。
   &lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-202721754.png&#34; alt=&#34;picture 10&#34; /&gt;&lt;/p&gt;

&lt;p&gt;b) 依次执行以下命令，测试CUDA Samples，验证CUDA是否安装成功。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    [root@k8s-worker1 ~]# cd /usr/local/cuda-11.4/extras/demo_suite/
    [root@k8s-worker1 demo_suite]# ./deviceQuery  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果返回结果显示Result=PASS，则表示CUDA安装成功。
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-203410971.png&#34; alt=&#34;picture 11&#34; /&gt;&lt;/p&gt;

&lt;p&gt;06) 测试
我们使用一个简单的 Pytorch 程序来检测 GPU 和 CUDA 是否正常。&lt;/p&gt;

&lt;p&gt;整个调用链大概是这样的：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-204049273.png&#34; alt=&#34;picture 12&#34; /&gt;&lt;br /&gt;
使用下面代码来测试能够正常使用， check_cuda_pytorch.py 内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch

def check_cuda_with_pytorch():
    &amp;quot;&amp;quot;&amp;quot;检查 PyTorch CUDA 环境是否正常工作&amp;quot;&amp;quot;&amp;quot;
    try:
        print(&amp;quot;检查 PyTorch CUDA 环境:&amp;quot;)
        if torch.cuda.is_available():
            print(f&amp;quot;CUDA 设备可用，当前 CUDA 版本是: {torch.version.cuda}&amp;quot;)
            print(f&amp;quot;PyTorch 版本是: {torch.__version__}&amp;quot;)
            print(f&amp;quot;检测到 {torch.cuda.device_count()} 个 CUDA 设备。&amp;quot;)
            for i in range(torch.cuda.device_count()):
                print(f&amp;quot;设备 {i}: {torch.cuda.get_device_name(i)}&amp;quot;)
                print(f&amp;quot;设备 {i} 的显存总量: {torch.cuda.get_device_properties(i).total_memory / (1024 ** 3):.2f} GB&amp;quot;)
                print(f&amp;quot;设备 {i} 的显存当前使用量: {torch.cuda.memory_allocated(i) / (1024 ** 3):.2f} GB&amp;quot;)
                print(f&amp;quot;设备 {i} 的显存最大使用量: {torch.cuda.memory_reserved(i) / (1024 ** 3):.2f} GB&amp;quot;)
        else:
            print(&amp;quot;CUDA 设备不可用。&amp;quot;)
    except Exception as e:
        print(f&amp;quot;检查 PyTorch CUDA 环境时出现错误: {e}&amp;quot;)

if __name__ == &amp;quot;__main__&amp;quot;:
    check_cuda_with_pytorch()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;先安装下 torch&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install torch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行一下&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python check_cuda_pytorch.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正常输出应该是这样的：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-210940440.png&#34; alt=&#34;picture 13&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;4-3-更新cuda&#34;&gt;4.3. 更新CUDA&lt;/h2&gt;

&lt;p&gt;由于有些应用需要更高级的版本的CUDA，需升级，如下升级为12.6版本。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo
sudo dnf clean all
sudo dnf -y install cuda-toolkit-12-6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-203151909.png&#34; alt=&#34;picture 18&#34; /&gt;&lt;br /&gt;
验证:
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-203406637.png&#34; alt=&#34;picture 19&#34; /&gt;&lt;br /&gt;
说明12.6版本安装成功。
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-213858800.png&#34; alt=&#34;picture 20&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;5-docker环境&#34;&gt;5. Docker环境&lt;/h1&gt;

&lt;p&gt;上一步中我们已经在裸机上安装了 GPU Driver，CUDA Toolkit 等工具，实现了在宿主机上使用 GPU。&lt;/p&gt;

&lt;p&gt;现在希望在 Docker 容器中使用 GPU，需要怎么处理呢?&lt;/p&gt;

&lt;p&gt;为了让 Docker 容器中也能使用 GPU，大致步骤如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;安装docker，已有则跳过这步骤。&lt;/li&gt;
&lt;li&gt;安装 nvidia-container-toolkit 组件&lt;/li&gt;
&lt;li&gt;docker 配置使用 nvidia-runtime&lt;/li&gt;
&lt;li&gt;启动容器时增加 &amp;ndash;gpu 参数&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;5-1-docker安装&#34;&gt;5.1. Docker安装&lt;/h2&gt;

&lt;h3 id=&#34;5-1-1-安装必要的一些系统工具&#34;&gt;5.1.1. 安装必要的一些系统工具&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo yum install -y yum-utils
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-1-2-添加软件源信息&#34;&gt;5.1.2. 添加软件源信息&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-1-3-安装docker&#34;&gt;5.1.3. 安装Docker&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-1-4-开启docker服务&#34;&gt;5.1.4. 开启Docker服务&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo service docker start
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-1-5-验证&#34;&gt;5.1.5. 验证&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-worker1 pytorch]# docker version
Client: Docker Engine - Community
 Version:           26.1.3
 API version:       1.45
 Go version:        go1.21.10
 Git commit:        b72abbb
 Built:             Thu May 16 08:34:39 2024
 OS/Arch:           linux/amd64
 Context:           default

Server: Docker Engine - Community
 Engine:
  Version:          26.1.3
  API version:      1.45 (minimum version 1.24)
  Go version:       go1.21.10
  Git commit:       8e96db1
  Built:            Thu May 16 08:33:34 2024
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.6.32
  GitCommit:        8b3b7ca2e5ce38e8f31a34f35b2b68ceb8470d89
 runc:
  Version:          1.1.12
  GitCommit:        v1.1.12-0-g51d5e94
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0
[root@k8s-worker1 pytorch]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;5-2-安装-nvidia-container-toolkit&#34;&gt;5.2. 安装 nvidia-container-toolkit&lt;/h2&gt;

&lt;p&gt;NVIDIA Container Toolkit 的主要作用是将 NVIDIA GPU 设备挂载到容器中。
&amp;gt;兼容生态系统中的任意容器运行时，docker、containerd、cri-o 等。&lt;/p&gt;

&lt;p&gt;NVIDIA 官方安装文档：&lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html&#34;&gt;nvidia-container-toolkit-install-guide&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ALIOS或centos安装命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Configure the production repository:
curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | \
sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo

# Optionally, configure the repository to use experimental packages:
sudo yum-config-manager --enable nvidia-container-toolkit-experimental

#Install the NVIDIA Container Toolkit packages:
sudo yum install -y nvidia-container-toolkit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装成功如下：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-185247937.png&#34; alt=&#34;picture 14&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;5-3-配置使用该-runtime&#34;&gt;5.3. 配置使用该 runtime&lt;/h2&gt;

&lt;p&gt;支持 Docker, Containerd, CRI-O, Podman 等 CRI。
&amp;gt;具体见官方文档 &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuration&#34;&gt;container-toolkit#install-guide&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;这里以 Docker 为例进行配置：
旧版本需要手动在 /etc/docker/daemon.json 中增加配置，指定使用 nvidia 的 runtime。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;    &amp;quot;runtimes&amp;quot;: {
        &amp;quot;nvidia&amp;quot;: {
            &amp;quot;args&amp;quot;: [],
            &amp;quot;path&amp;quot;: &amp;quot;nvidia-container-runtime&amp;quot;
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;新版 toolkit 带了一个nvidia-ctk 工具，执行以下命令即可一键配置然后重启docker：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-191008933.png&#34; alt=&#34;picture 15&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;5-4-验证&#34;&gt;5.4. 验证&lt;/h2&gt;

&lt;p&gt;安装nvidia-container-toolkit 后，整个调用链如下：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-191407685.png&#34; alt=&#34;picture 16&#34; /&gt;&lt;/p&gt;

&lt;p&gt;调用链从 containerd –&amp;gt; runC 变成 containerd –&amp;gt; nvidia-container-runtime –&amp;gt; runC 。&lt;/p&gt;

&lt;p&gt;然后 nvidia-container-runtime 在中间拦截了容器 spec，就可以把 gpu 相关配置添加进去，再传给 runC 的 spec 里面就包含 gpu 信息了。&lt;/p&gt;

&lt;p&gt;Docker 环境中的 CUDA 调用大概是这样的：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-191542185.png&#34; alt=&#34;picture 17&#34; /&gt;&lt;/p&gt;

&lt;p&gt;从图中可以看到，CUDA Toolkit 跑到容器里了，因此宿主机上不需要再安装 CUDA Toolkit。&lt;/p&gt;

&lt;p&gt;使用一个带 CUDA Toolkit 的镜像即可。&lt;/p&gt;

&lt;p&gt;最后我们启动一个 Docker 容器进行测试，其中命令中增加 &amp;ndash;gpu参数来指定要分配给容器的 GPU。&lt;/p&gt;

&lt;p&gt;gpu 参数可选值：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;gpus all：表示将所有 GPU 都分配给该容器&lt;/li&gt;
&lt;li&gt;gpus &amp;ldquo;device=&lt;id&gt;[,&lt;id&gt;&amp;hellip;]&amp;ldquo;：对于多 GPU 场景，可以通过 id 指定分配给容器的 GPU，例如 –gpu “device=0” 表示只分配 0 号 GPU 给该容器
GPU 编号则是通过nvidia-smi 命令进行查看&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里我们直接使用一个带 cuda 的镜像来测试，启动该容器并执行nvidia-smi 命令&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-worker1 ~]# docker run --rm --gpus all  nvcr.io/nvidia/cuda:11.0.3-runtime-ubuntu20.04 nvidia-smi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-214303643.png&#34; alt=&#34;picture 22&#34; /&gt;&lt;br /&gt;
正常情况下应该是可以打印出容器中的 GPU 信息的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;： 镜像需与grid驱动版本兼容，否则报错。&lt;/p&gt;

&lt;h1 id=&#34;6-k8s环境&#34;&gt;6. K8S环境&lt;/h1&gt;

&lt;p&gt;更进一步，在 k8s 环境中使用 GPU，则需要在集群中部署以下组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;gpu-device-plugin 用于管理 GPU，device-plugin 以 DaemonSet 方式运行到集群各个节点，以感知节点上的 GPU 设备，从而让 k8s 能够对节点上的 GPU 设备进行管理。&lt;/li&gt;
&lt;li&gt;gpu-exporter：用于监控 GPU&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;各组件关系如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-215913188.png&#34; alt=&#34;picture 23&#34; /&gt;&lt;/p&gt;

&lt;p&gt;左图为手动安装的场景，只需要在集群中安装 device-plugin 和 监控即可使用。&lt;/p&gt;

&lt;p&gt;右图为使用 gpu-operotar 安装场景，本篇暂时忽略&lt;/p&gt;

&lt;p&gt;大致工作流程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每个节点的 kubelet 组件维护该节点的 GPU 设备状态（哪些已用，哪些未用）并定时报告给调度器，调度器知道每一个节点有多少张 GPU 卡可用。&lt;/li&gt;
&lt;li&gt;调度器为 pod 选择节点时，从符合条件的节点中选择一个节点。&lt;/li&gt;
&lt;li&gt;当 pod 调度到节点上后，kubelet 组件为 pod 分配 GPU 设备 ID，并将这些 ID 作为参数传递给 NVIDIA Device Plugin&lt;/li&gt;
&lt;li&gt;NVIDIA Device Plugin 将分配给该 pod 的容器的 GPU 设备 ID 写入到容器的环境变量 NVIDIA_VISIBLE_DEVICES中，然后将信息返回给 kubelet。&lt;/li&gt;
&lt;li&gt;kubelet 启动容器。&lt;/li&gt;
&lt;li&gt;NVIDIA Container Toolkit 检测容器的 spec 中存在环境变量 NVIDIA_VISIBLE_DEVICES，然后根据环境变量的值将 GPU 设备挂载到容器中。
在 Docker 环境我们在启动容器时通过 &amp;ndash;gpu 参数手动指定分配给容器的 GPU，k8s 环境则由 device-plugin 自行管理。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;6-1-集群环境&#34;&gt;6.1. 集群环境&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-220923122.png&#34; alt=&#34;picture 24&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;安装-device-plugin&#34;&gt;安装 device-plugin&lt;/h2&gt;

&lt;p&gt;device-plugin 一般由对应的 GPU 厂家提供，比如 NVIDIA 的 &lt;a href=&#34;https://github.com/NVIDIA/k8s-device-plugin&#34;&gt;k8s-device-plugin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;安装其实很简单，将对应的 yaml apply 到集群即可。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.15.0/deployments/static/nvidia-device-plugin.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就像这样
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-221359266.png&#34; alt=&#34;picture 25&#34; /&gt;&lt;/p&gt;

&lt;p&gt;device-plugin 启动之后，会感知节点上的 GPU 设备并上报给 kubelet，最终由 kubelet 提交到 kube-apiserver。&lt;/p&gt;

&lt;p&gt;因此我们可以在 Node 可分配资源中看到 GPU，就像这样：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-221933864.png&#34; alt=&#34;picture 26&#34; /&gt;&lt;br /&gt;
可以看到，除了常见的 cpu、memory 之外，还有nvidia.com/gpu, 这个就是 GPU 资源，数量为 0，应该为1，错误原因待查。
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250123-095336709.png&#34; alt=&#34;picture 27&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;安装-gpu-监控&#34;&gt;安装 GPU 监控&lt;/h2&gt;

&lt;p&gt;除此之外，如果你需要监控集群 GPU 资源使用情况，你可能还需要安装 DCCM exporter 结合 Prometheus 输出 GPU 资源监控信息。&lt;/p&gt;

&lt;p&gt;安装略。&lt;/p&gt;

&lt;h1 id=&#34;小结&#34;&gt;小结&lt;/h1&gt;

&lt;p&gt;本文主要分享了在ECS、Docker 环境、k8s 环境中如何使用 GPU。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;对于ECS环境，只需要安装对应的 GPU Driver 即可。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对应 Docker 环境，需要额外安装 nvidia-container-toolkit 并配置 docker 使用 nvidia runtime。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对应 k8s 环境，需要额外安装对应的 device-plugin 使得 kubelet 能够感知到节点上的 GPU 设备，以便 k8s 能够进行 GPU 管理。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;现在一般都是在 k8s 环境中使用，为了简化安装步骤， NVIDIA 也提供了 gpu-operator来简化安装部署，后续分享一下如何使用 gpu-operator来快速安装。&lt;/p&gt;

&lt;h1 id=&#34;7-参考资料&#34;&gt;7. 参考资料&lt;/h1&gt;

&lt;p&gt;【01】&lt;a href=&#34;https://blog.csdn.net/Doudou_Mylove/article/details/114388633&#34;&gt;阿里云GPU服务器安装驱动（完整版）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;【02】&lt;a href=&#34;https://help.aliyun.com/zh/egs/user-guide/install-a-gpu-driver-on-a-gpu-accelerated-compute-optimized-linux-instance?spm=a2c4g.11186623.help-menu-155040.d_1_5_1_1.6b70fb7cljEZnN&amp;amp;scm=20140722.H_163824._.OR_help-T_cn~zh-V_1&#34;&gt;在GPU计算型实例中手动安装Tesla驱动（Linux）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;【03】&lt;a href=&#34;https://help.aliyun.com/zh/egs/user-guide/use-cloud-assistant-to-automatically-install-and-upgrade-grid-drivers?spm=a2c4g.11186623.help-menu-155040.d_1_5_2_1.660551bd3LBP63&#34;&gt;在GPU虚拟化型实例中安装GRID驱动（Linux）&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;8-faq&#34;&gt;8. FAQ&lt;/h1&gt;

&lt;h2 id=&#34;8-1-error-unable-to-load-the-kernel-module-nvidia-ko&#34;&gt;8.1. ERROR: Unable to load the kernel module &amp;lsquo;nvidia.ko&amp;rsquo;.&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250120-155358481.png&#34; alt=&#34;picture 4&#34; /&gt;&lt;br /&gt;
错误详情提示：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250120-155501141.png&#34; alt=&#34;picture 5&#34; /&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(07): kubeadm安装k8s集群(containerd版)</title>
            <link>http://mospany.github.io/2024/01/15/kubeadm-install-k8s/</link>
            <pubDate>Mon, 15 Jan 2024 15:42:11 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2024/01/15/kubeadm-install-k8s/</guid>
            <description>

&lt;h1 id=&#34;1-规划&#34;&gt;1. 规划&lt;/h1&gt;

&lt;p&gt;使用 kubeadm 安装 Kubernetes 集群并使用 containerd 作为容器运行时（container runtime）是一种常见的安装方法。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;OS&lt;/th&gt;
&lt;th&gt;配置&lt;/th&gt;
&lt;th&gt;用途&lt;/th&gt;
&lt;th&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;aliOS(172.17.197.69)&lt;/td&gt;
&lt;td&gt;2核(vCPU) 4GiB 5 Mbps&lt;/td&gt;
&lt;td&gt;k8s-master1&lt;/td&gt;
&lt;td&gt;乌兰察布 可用区 C&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;aliOS(172.17.197.68)&lt;/td&gt;
&lt;td&gt;4核(vCPU) 10 GiB 5 Mbps GPU：NVIDIA T4/8&lt;/td&gt;
&lt;td&gt;k8s-worker1&lt;/td&gt;
&lt;td&gt;乌兰察布 可用区 C&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：这是演示 k8s 集群安装的实验环境，配置较低，生产环境中我们的服务器配置至少都是 8C/16G 的基础配置。&lt;/p&gt;

&lt;h1 id=&#34;2-版本选择&#34;&gt;2. 版本选择&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Alibaba Cloud Linux 3.2104 LTS 64位
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2-1-配置主机名&#34;&gt;2.1. 配置主机名&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# hostnamectl set-hostname k8s-master1
[root@k8s-worker1 ~]# hostnamectl set-hostname k8s-worker1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-2-关闭防火墙&#34;&gt;2.2. 关闭防火墙&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; # 关闭firewalld
2  [root@k8s-master1 ~]# systemctl stop firewalld
3  
4  # 关闭selinux
5  [root@k8s-master1 ~]# sed -i &#39;s/enforcing/disabled/&#39; /etc/selinux/config
6  [root@k8s-master1 ~]# setenforce 0
7   setenforce: SELinux is disabled
8  [root@k8s-master1 ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-3-互做本地解析&#34;&gt;2.3. 互做本地解析&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master1 ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
8.130.117.184 k8s-master1
8.130.92.114  k8s-worker1
[root@k8s-master1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-4-ssh-免密通信-可选&#34;&gt;2.4. SSH 免密通信（可选）&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;master节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master1 ~]# ssh-keygen
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250115-192630256.png&#34; alt=&#34;picture 0&#34; /&gt;&lt;br /&gt;
互发公钥&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# ssh-copy-id root@k8s-worker1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250115-193004163.png&#34; alt=&#34;picture 1&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-5-加载-br-netfilter-模块&#34;&gt;2.5. 加载 br_netfilter 模块&lt;/h2&gt;

&lt;p&gt;确保 br_netfilter 模块被加载
&amp;gt; 所有节点执行&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 加载模块
root@k8s-master1 ~]# modprobe br_netfilter
## 查看加载请看
[root@k8s-master1 ~]# lsmod | grep br_netfilter
br_netfilter           32768  0
bridge                270336  1 br_netfilter

# 永久生效
[root@k8s-master1 ~]# cat &amp;lt;&amp;lt;EOF | tee /etc/modules-load.d/k8s.conf
&amp;gt; br_netfilter
&amp;gt; EOF
br_netfilter
[root@k8s-master1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-6-允许-iptables-检查桥接流量&#34;&gt;2.6. 允许 iptables 检查桥接流量&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
&amp;gt; net.bridge.bridge-nf-call-ip6tables = 1
&amp;gt; net.bridge.bridge-nf-call-iptables = 1
&amp;gt; EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
[root@k8s-master1 ~]# sysctl --system
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-7-关闭-swap&#34;&gt;2.7. 关闭 swap&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 临时关闭
[root@k8s-master1 ~]# swapoff -a

# 永久关闭
[root@k8s-master1 ~]# sed -ri &#39;s/.*swap.*/#&amp;amp;/&#39; /etc/fstab
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-8-安装ipset及ipvsadm&#34;&gt;2.8. 安装ipset及ipvsadm&lt;/h2&gt;

&lt;p&gt;安装ipset及ipvsadm&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# dnf install -y ipset ipvsadm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置ipvsadm模块加载方式，添加需要加载的模块&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat &amp;gt; /etc/sysconfig/modules/ipvs.modules &amp;lt;&amp;lt;EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
EOF

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;授权、运行、检查是否加载&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; bash /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; lsmod | grep -e ip_vs -e nf_conntrack
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-135626938.png&#34; alt=&#34;picture 2&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;3-安装containerd&#34;&gt;3. 安装containerd&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;3-1-安装-containerd&#34;&gt;3.1. 安装 containerd&lt;/h2&gt;

&lt;p&gt;通过 yum 安装 containerd：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master1 ~]# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
Adding repo from: http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
[root@k8s-master1 ~]# yum install containerd -y

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-2-配置-containerd&#34;&gt;3.2. 配置 containerd&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ /usr/bin/containerd config default &amp;gt; /etc/containerd/config.toml
$ sed -i &#39;s/SystemdCgroup = false/SystemdCgroup = true/g&#39; /etc/containerd/config.toml
$ sed -i &#39;s/sandbox_image = &amp;quot;registry.k8s.io\/pause:3.6&amp;quot;/sandbox_image = &amp;quot;registry.aliyuncs.com\/google_containers\/pause:3.9&amp;quot;/g&#39; /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;： 记得确保配置修改成功，尤其是pause版本。&lt;/p&gt;

&lt;h2 id=&#34;3-3-启动containerd&#34;&gt;3.3. 启动containerd&lt;/h2&gt;

&lt;p&gt;启动并使 containerd 随系统启动：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master1 ~]# sudo systemctl start containerd
root@k8s-master1 ~]# sudo systemctl enable containerd
root@k8s-master1 ~]# ps -ef | grep containerd
root       37760       1  0 07:35 ?        00:00:00 /usr/bin/containerd
root       37807   36202  0 07:35 pts/0    00:00:00 grep --color=auto containerd
[root@k8s-master1 ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明containerd已启动成功。
&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-160630369.png&#34; alt=&#34;picture 3&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;4-安装-kubeadm-kubelet&#34;&gt;4. 安装 kubeadm、kubelet&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;4-1-添加-k8s-镜像源&#34;&gt;4.1. 添加 k8s 镜像源&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;地址： &lt;a href=&#34;https://developer.aliyun.com/mirror/kubernetes?spm=a2c6h.13651102.0.0.1cd01b116JYQIn&#34;&gt;https://developer.aliyun.com/mirror/kubernetes?spm=a2c6h.13651102.0.0.1cd01b116JYQIn&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat &amp;lt;&amp;lt;EOF | tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.28/rpm/
enabled=1
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.28/rpm/repodata/repomd.xml.key
EOF
setenforce 0
yum install -y --nogpgcheck kubelet kubeadm kubectl
systemctl enable kubelet &amp;amp;&amp;amp; systemctl start kubelet
echo 1 &amp;gt; /proc/sys/net/ipv4/ip_forward
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;4-2-建立-k8s-yum-缓存&#34;&gt;4.2. 建立 k8s YUM 缓存&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# yum makecache
alinux3-os                                                                                       1.3 MB/s | 3.8 kB     00:00    
alinux3-updates                                                                                  1.4 MB/s | 4.1 kB     00:00    
alinux3-module                                                                                   354 kB/s | 4.2 kB     00:00    
alinux3-plus                                                                                     645 kB/s | 3.0 kB     00:00    
alinux3-powertools                                                                               598 kB/s | 3.0 kB     00:00    
Extra Packages for Enterprise Linux 8 - x86_64                                                   1.8 MB/s | 4.4 kB     00:00    
Extra Packages for Enterprise Linux Modular 8 - x86_64                                           620 kB/s | 3.0 kB     00:00    
Kubernetes                                                                                        17 kB/s | 1.7 kB     00:00    
Metadata cache created.
[root@k8s-master1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;4-3-安装-k8s-相关工具&#34;&gt;4.3. 安装 k8s 相关工具&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# yum list kubelet --showduplicates
Last metadata expiration check: 0:01:46 ago on Wed 15 Jan 2025 10:55:44 PM CST.
Installed Packages
kubelet.x86_64                                           1.28.15-150500.1.1                                           @kubernetes
Available Packages
kubelet.aarch64                                          1.28.0-150500.1.1                                            kubernetes 
kubelet.ppc64le                                          1.28.0-150500.1.1                                            kubernetes 
kubelet.s390x                                            1.28.0-150500.1.1                                            kubernetes 
kubelet.src                                              1.28.0-150500.1.1                                            kubernetes 
kubelet.x86_64                                           1.28.0-150500.1.1                                            kubernetes 
kubelet.aarch64                                          1.28.1-150500.1.1                                            kubernetes 
kubelet.ppc64le                                          1.28.1-150500.1.1                                            kubernetes 
kubelet.s390x                                            1.28.1-150500.1.1                                            kubernetes 
kubelet.src                                              1.28.1-150500.1.1                                            kubernetes 
.....
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置crictl连接 containerd&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ crictl config --set runtime-endpoint=unix:///run/containerd/containerd.sock
$ crictl images ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置kubectl命令自动补全：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo &#39;source &amp;lt;(kubectl completion bash)&#39; &amp;gt;&amp;gt; ~/.bashrc
source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;5-master节点&#34;&gt;5. master节点&lt;/h1&gt;

&lt;h2 id=&#34;5-1-k8s-初始化&#34;&gt;5.1. k8s 初始化&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：记得修改&amp;ndash;apiserver-advertise-address与&amp;ndash;kubernetes-version修改正确。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master ~]# kubeadm init \
  --apiserver-advertise-address=172.17.197.69 \
  --image-repository registry.aliyuncs.com/google_containers \
  --kubernetes-version v1.28.15 \
  --service-cidr=10.96.0.0/12 \
  --pod-network-cidr=10.244.0.0/16 \
  --ignore-preflight-errors=all \
  --cri-socket /var/run/containerd/containerd.sock
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数说明：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;   --apiserver-advertise-address  # 集群master地址
   --image-repository             # 指定k8s镜像仓库地址
   --kubernetes-version           # 指定K8s版本（与kubeadm、kubelet版本保持一致）
   --service-cidr                 # Pod统一访问入口
   --pod-network-cidr             # Pod网络（与CNI网络保持一致）
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初始化后输出如下内容，说明成功。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.....
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.17.197.69:6443 --token ipqfom.r9ltq4t3in53cd6w \
        --discovery-token-ca-cert-hash sha256:0f01242802eae4b69408c34070a3ad8d017229faba9f8b30623ed1e9dd69d66c 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;5-2-根据输出提示创建相关文件&#34;&gt;5.2. 根据输出提示创建相关文件&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;master节点上执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master ~]# mkdir -p $HOME/.kube
[root@k8s-master ~]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[root@k8s-master ~]# chown $(id -u):$(id -g) $HOME/.kube/config
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;5-3-查看-k8s-运行的容器&#34;&gt;5.3. 查看 k8s 运行的容器&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;master节点上执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# kubectl  get pod -A -o wide
NAMESPACE     NAME                                  READY   STATUS    RESTARTS   AGE   IP              NODE          NOMINATED NODE   READINESS GATES
kube-system   coredns-66f779496c-lq5rj              0/1     Pending   0          26m   &amp;lt;none&amp;gt;          &amp;lt;none&amp;gt;        &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-66f779496c-q5pxk              0/1     Pending   0          26m   &amp;lt;none&amp;gt;          &amp;lt;none&amp;gt;        &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   etcd-k8s-master1                      1/1     Running   1          26m   172.17.197.69   k8s-master1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-apiserver-k8s-master1            1/1     Running   1          26m   172.17.197.69   k8s-master1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-controller-manager-k8s-master1   1/1     Running   1          26m   172.17.197.69   k8s-master1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-88trd                      1/1     Running   0          26m   172.17.197.69   k8s-master1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-scheduler-k8s-master1            1/1     Running   1          26m   172.17.197.69   k8s-master1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
[root@k8s-master1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;5-4-查看-k8s-节点&#34;&gt;5.4. 查看 k8s 节点&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;master节点上执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master1 ~]# kubectl  get node -o wide
NAME          STATUS     ROLES           AGE   VERSION    INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                                              KERNEL-VERSION           CONTAINER-RUNTIME
k8s-master1   NotReady   control-plane   36m   v1.28.15   172.17.197.69   &amp;lt;none&amp;gt;        Alibaba Cloud Linux 3.2104 U11 (OpenAnolis Edition)   5.10.134-18.al8.x86_64   containerd://1.6.32
[root@k8s-master1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可看到当前只有 k8s-master 节点，而且状态是 NotReady（未就绪），因为我们还没有部署网络插件（kubectl apply -f [podnetwork].yaml），于是接着部署容器网络（CNI）。&lt;/p&gt;

&lt;h2 id=&#34;5-5-容器网络-cni-部署&#34;&gt;5.5. 容器网络（CNI）部署&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;插件地址：&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/addons/&#34;&gt;https://kubernetes.io/docs/concepts/cluster-administration/addons/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;该地址在 k8s-master 初始化成功时打印出来。&lt;/p&gt;

&lt;h3 id=&#34;5-5-1-选择一个主流的容器网络插件部署-calico&#34;&gt;5.5.1. 选择一个主流的容器网络插件部署（Calico）&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-164005909.png&#34; alt=&#34;picture 4&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;5-5-2-下载yml文件&#34;&gt;5.5.2. 下载yml文件&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://docs.projectcalico.org/manifests/calico.yaml --no-check-certificate 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-5-3-看看该yaml文件所需要启动的容器&#34;&gt;5.5.3. 看看该yaml文件所需要启动的容器&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 install-k8s-cluster]# cat calico.yaml |grep image
          image: docker.io/calico/cni:v3.25.0
          imagePullPolicy: IfNotPresent
          image: docker.io/calico/cni:v3.25.0
          imagePullPolicy: IfNotPresent
          image: docker.io/calico/node:v3.25.0
          imagePullPolicy: IfNotPresent
          image: docker.io/calico/node:v3.25.0
          imagePullPolicy: IfNotPresent
          image: docker.io/calico/kube-controllers:v3.25.0
          imagePullPolicy: IfNotPresent
[root@k8s-master1 install-k8s-cluster]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-5-4-修改为国内可下载镜像&#34;&gt;5.5.4. 修改为国内可下载镜像&lt;/h3&gt;

&lt;p&gt;使用docker.m.daocloud.io代替docker.io&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sed -i s/docker.io/docker.m.daocloud.io/g calico.yaml 
$ cat calico.yaml | grep image

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-205457826.png&#34; alt=&#34;picture 5&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;5-5-5-安装calico&#34;&gt;5.5.5. 安装calico&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 calicodir]# kubectl  apply -f calico.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-205731350.png&#34; alt=&#34;picture 6&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;5-5-6-查看pod和node状态正常&#34;&gt;5.5.6. 查看pod和node状态正常&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 calicodir]# kubectl  get pod -A
[root@k8s-master1 calicodir]# kubectl  get node
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-210017047.png&#34; alt=&#34;picture 7&#34; /&gt;&lt;br /&gt;
可以看出pod已全部Running， node已变成Ready状态，说明master节点已安装成功完成。&lt;/p&gt;

&lt;h1 id=&#34;6-worker-节点&#34;&gt;6. worker 节点&lt;/h1&gt;

&lt;h2 id=&#34;6-1-worker-节点加入-k8s-集群&#34;&gt;6.1. worker 节点加入 k8s 集群&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有 work 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;复制k8s-master初始化屏幕输出的语句并在work节点执行&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-worker1 ~]# kubeadm join 172.17.197.69:6443 --token ipqfom.r9ltq4t3in53cd6w         --discovery-token-ca-cert-hash sha256:0f01242802eae4b69408c34070a3ad8d017229faba9f8b30623ed1e9dd69d66c
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with &#39;kubectl -n kube-system get cm kubeadm-config -o yaml&#39;
[kubelet-start] Writing kubelet configuration to file &amp;quot;/var/lib/kubelet/config.yaml&amp;quot;
[kubelet-start] Writing kubelet environment file with flags to file &amp;quot;/var/lib/kubelet/kubeadm-flags.env&amp;quot;
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run &#39;kubectl get nodes&#39; on the control-plane to see this node join the cluster.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;6-2-查询集群信息&#34;&gt;6.2. 查询集群信息&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master节点上执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 calicodir]# kubectl  get pod -A -o wide
[root@k8s-master1 calicodir]# kubectl  get node -o wide
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-213420388.png&#34; alt=&#34;picture 8&#34; /&gt;&lt;/p&gt;

&lt;p&gt;查看crictl和ctr是否安装成功。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 calicodir]# crictl version
[root@k8s-master1 calicodir]# ctr version
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-213647229.png&#34; alt=&#34;picture 9&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;7-验证&#34;&gt;7. 验证&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;Master节点上执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;k8s 集群部署 nginx 服务，并通过浏览器进行访问验证。&lt;/p&gt;

&lt;h2 id=&#34;7-1-创建pod&#34;&gt;7.1. 创建Pod&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 calicodir]# kubectl create deployment nginx --image=docker.m.daocloud.io/library/nginx:latest
deployment.apps/nginx created
[root@k8s-master1 calicodir]# kubectl expose deployment nginx --port=80 --type=NodePort
service/nginx exposed
[root@k8s-master1 calicodir]# kubectl  get all
NAME                        READY   STATUS    RESTARTS   AGE
pod/nginx-fbf584587-pp6ks   1/1     Running   0          2m45s

NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/kubernetes   ClusterIP   10.96.0.1       &amp;lt;none&amp;gt;        443/TCP        5h45m
service/nginx        NodePort    10.110.34.104   &amp;lt;none&amp;gt;        80:30884/TCP   28s

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx   1/1     1            1           2m45s

NAME                              DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-fbf584587   1         1         1       2m45s
[root@k8s-master1 calicodir]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;7-2-访问nginx&#34;&gt;7.2. 访问nginx&lt;/h2&gt;

&lt;h3 id=&#34;7-2-1-curl访问&#34;&gt;7.2.1. curl访问&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 calicodir]# curl 10.110.34.104:80
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;For online documentation and support please refer to
&amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
Commercial support is available at
&amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
[root@k8s-master1 calicodir]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用nodePort方式访问现象同上
&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-220403508.png&#34; alt=&#34;picture 10&#34; /&gt;&lt;/p&gt;

&lt;p&gt;至此：kubeadm方式的k8s集群已经部署完成。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
