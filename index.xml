<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>墨斯潘園 on 墨斯潘園</title>
        <link>http://mospany.github.io/</link>
        <language>zh-CN</language>
        <author>Mospan</author>
        <rights>Copyright (c) 2016, mospan; all rights reserved.</rights>
        <updated>Wed, 17 Jan 2024 19:31:10 CST</updated>
        
        <item>
            <title>K8S项目实践(09): 使用 GPU Operator搭建AI算力环境</title>
            <link>http://mospany.github.io/2024/01/17/gpu-operator/</link>
            <pubDate>Wed, 17 Jan 2024 19:31:10 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2024/01/17/gpu-operator/</guid>
            <description>

&lt;h1 id=&#34;1-引言&#34;&gt;1. 引言&lt;/h1&gt;

&lt;p&gt;为了学习AI应用、算法与算力等技术，应用需跑在GPU卡上，需要在节点上安装 GPU Driver、Container Toolkit 等组件，当集群规模较大时还是比较麻烦的。&lt;/p&gt;

&lt;p&gt;为了解决这个问题，NVIDIA 推出了 GPU Operator，GPU Operator 旨在简化在 Kubernetes 环境中使用 GPU 的过程，通过自动化的方式处理 GPU 驱动程序安装、Controller Toolkit、Device-Plugin 、监控等组件。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;基本上把需要手动安装、配置的地方全部自动化处理了，极大简化了 k8s 环境中的 GPU 使用。&lt;/p&gt;

&lt;p&gt;ps：只有 NVIDIA GPU 可以使用，其他厂家现在基本还是手动安装。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;2-规划&#34;&gt;2. 规划&lt;/h1&gt;

&lt;p&gt;本文主要分享如何使用 GPU Operator 快速搭建 Kubernetes GPU 环境。&lt;/p&gt;

&lt;p&gt;基于如下环境搭建：
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250117-194645644.png&#34; alt=&#34;picture 0&#34; /&gt;&lt;br /&gt;
查看worker节点GPU信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-worker1 ~]# lspci | grep -i nvidia
00:07.0 VGA compatible controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)
[root@k8s-worker1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明该worker节点有1张NVIDIA Tesla T4的GPU卡。&lt;/p&gt;

&lt;h1 id=&#34;3-部署&#34;&gt;3. 部署&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;参考官方文档： &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#operator-install-guide&#34;&gt;operator-install-guide&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;3-1-准备工作&#34;&gt;3.1. 准备工作&lt;/h2&gt;

&lt;p&gt;要求：&lt;/p&gt;

&lt;p&gt;1）GPU 节点必须运行相同的操作系统，&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果提前手动在节点上安装驱动的话，该节点可以使用不同的操作系统&lt;/li&gt;
&lt;li&gt;CPU 节点操作系统没要求，因为 gpu-operator 只会在 GPU 节点上运行
2）GPU 节点必须配置相同容器引擎，例如都是 containerd 或者都是 docker&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3）如果使用了 Pod Security Admission (PSA) ，需要为 gpu-operator 标记特权模式&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# kubectl create ns gpu-operator
namespace/gpu-operator created
[root@k8s-master1 ~]# kubectl label --overwrite ns gpu-operator pod-security.kubernetes.io/enforce=privileged
namespace/gpu-operator labeled
[root@k8s-master1 ~]# kubectl get ns gpu-operator --show-labels
NAME           STATUS   AGE   LABELS
gpu-operator   Active   30s   kubernetes.io/metadata.name=gpu-operator,pod-security.kubernetes.io/enforce=privileged
[root@k8s-master1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4）集群中不要安装 NFD，如果已经安装了需要再安装 gpu-operator 时禁用 NFD 部署。&lt;/p&gt;

&lt;p&gt;使用以下命令查看集群中是否部署 NFD&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl get nodes -o json | jq &#39;.items[].metadata.labels | keys | any(startswith(&amp;quot;feature.node.kubernetes.io&amp;quot;))&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果返回 true 则说明集群中安装了 NFD。&lt;/p&gt;

&lt;h2 id=&#34;3-2-helm部署&#34;&gt;3.2. helm部署&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;参考官方文档： &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#operator-install-guide&#34;&gt;operator-install-guide&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;3-2-1-安装helm命令&#34;&gt;3.2.1. 安装helm命令&lt;/h3&gt;

&lt;p&gt;如果master节点上还未安装helm命令，需安装&lt;/p&gt;

&lt;h4 id=&#34;3-2-1-1-下载-helm-3-的最新版本&#34;&gt;3.2.1.1. 下载 Helm 3 的最新版本&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl https://get.helm.sh/helm-v3.11.1-linux-amd64.tar.gz -o helm-v3.11.1-linux-amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;3-2-1-2-解压安装包&#34;&gt;3.2.1.2. 解压安装包&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 helm]# ls
helm-v3.11.1-linux-amd64.tar.gz
[root@k8s-master1 helm]# tar -zxvf helm-v3.11.1-linux-amd64.tar.gz 
linux-amd64/
linux-amd64/helm
linux-amd64/LICENSE
linux-amd64/README.md
[root@k8s-master1 helm]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;3-2-1-3-移动-helm-到系统的可执行路径&#34;&gt;3.2.1.3. 移动 helm 到系统的可执行路径&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo mv linux-amd64/helm /usr/local/bin/helm`
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;3-2-1-4-验证安装&#34;&gt;3.2.1.4. 验证安装&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 helm]# helm version
version.BuildInfo{Version:&amp;quot;v3.11.1&amp;quot;, GitCommit:&amp;quot;293b50c65d4d56187cd4e2f390f0ada46b4c4737&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, GoVersion:&amp;quot;go1.18.10&amp;quot;}
[root@k8s-master1 helm]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明安装成功。&lt;/p&gt;

&lt;h3 id=&#34;chart部署&#34;&gt;chart部署&lt;/h3&gt;

&lt;h4 id=&#34;添加repo仓库&#34;&gt;添加repo仓库&lt;/h4&gt;

&lt;p&gt;添加 nvidia helm 仓库并更新&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;helm repo add nvidia https://helm.ngc.nvidia.com/nvidia \
    &amp;amp;&amp;amp; helm repo update
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;拉取chart包&#34;&gt;拉取chart包&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;helm pull nvidia/gpu-operator --version v24.9.1 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将在本地生成gpu-operator-v24.9.1.tgz文件。&lt;/p&gt;

&lt;h4 id=&#34;准备镜像&#34;&gt;准备镜像&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点上执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;由于有些镜像国内不能直接访问，需从国内镜像地址拉取后再修改tag。
1）拉取国内代理镜像：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;crictl pull swr.cn-north-4.myhuaweicloud.com/ddn-k8s/registry.k8s.io/nfd/node-feature-discovery:v0.16.6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2）修改镜像tag：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;

ctr -n k8s.io image tag  swr.cn-north-4.myhuaweicloud.com/ddn-k8s/registry.k8s.io/nfd/node-feature-discovery:v0.16.6 registry.k8s.io/nfd/node-feature-discovery:v0.16.6 
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;安装chart包&#34;&gt;安装chart包&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 以默认配置安装
helm install --wait --generate-name \
    -n gpu-operator --create-namespace \
    nvidia/gpu-operator --set nfd.enabled=false

&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(08): 在ECS、Docker、K8s 等环境中使用 GPU</title>
            <link>http://mospany.github.io/2024/01/16/gpu-on-k8s/</link>
            <pubDate>Tue, 16 Jan 2024 19:31:10 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2024/01/16/gpu-on-k8s/</guid>
            <description>

&lt;h1 id=&#34;1-引言&#34;&gt;1. 引言&lt;/h1&gt;

&lt;p&gt;本文主要分享在不同环境，例如ECS、Docker 和 Kubernetes 等环境中如何使用 GPU。
&lt;strong&gt;注&lt;/strong&gt;：由于没有物理机裸机，在阿里云上申请ECS也可满足学习使用。&lt;/p&gt;

&lt;h1 id=&#34;2-规划&#34;&gt;2. 规划&lt;/h1&gt;

&lt;p&gt;基于如下环境搭建：
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250117-194645644.png&#34; alt=&#34;picture 0&#34; /&gt;&lt;br /&gt;
查看worker节点GPU信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-worker1 ~]# lspci | grep -i nvidia
00:07.0 VGA compatible controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)
[root@k8s-worker1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明该worker节点有1张NVIDIA Tesla T4的GPU卡。&lt;/p&gt;

&lt;h1 id=&#34;3-概述&#34;&gt;3. 概述&lt;/h1&gt;

&lt;p&gt;仅以比较常见的 NVIDIA GPU 举例，系统为 Linux，对于其他厂家的 GPU 设备理论上流程都是一样的。&lt;/p&gt;

&lt;p&gt;省流：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;对于ECS环境，只需要安装对应的 &lt;a href=&#34;https://help.aliyun.com/zh/egs/user-guide/installation-guideline-for-nvidia-drivers?spm=a2c4g.11186623.help-menu-155040.d_1_5_0.726f718evZrl7t&#34;&gt;GPU Driver&lt;/a&gt;（GPU计算型实例为Tesla驱动，GPU虚拟化型实例为GRID驱动） 以及 CUDA Toolkit 。&lt;/li&gt;
&lt;li&gt;对应 Docker 环境，需要额外安装 nvidia-container-toolkit 并配置 docker 使用 nvidia runtime。&lt;/li&gt;
&lt;li&gt;对应 k8s 环境，需要额外安装对应的 device-plugin 使得 kubelet 能够感知到节点上的 GPU 设备，以便 k8s 能够进行 GPU 管理。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注：一般在 k8s 中使用都会直接使用 gpu-operator 方式进行安装，本文主要为了搞清各个组件的作用，因此进行手动安装。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ps；下一篇分享下如何使用 gpu-operator 快速完成安装&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;4-ecs环境&#34;&gt;4. ECS环境&lt;/h1&gt;

&lt;p&gt;裸机中要使用上 GPU 需要安装以下组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GPU Driver&lt;/li&gt;
&lt;li&gt;CUDA Toolkit
二者的关系如 NVIDIA 官网上的这个图所示：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250120-152256220.png&#34; alt=&#34;picture 0&#34; /&gt;&lt;/p&gt;

&lt;p&gt;GPU Driver 包括了 GPU 驱动和 CUDA 驱动，CUDA Toolkit 则包含了 CUDA Runtime。&lt;/p&gt;

&lt;p&gt;GPU 作为一个 PCIE 设备，只要安装好之后，在系统中就可以通过 lspci 命令查看到，先确认机器上是否有 GPU：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-worker1 ~]# lspci | grep -i nvidia
00:07.0 VGA compatible controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)
[root@k8s-worker1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明该worker节点有1张NVIDIA Tesla T4的GPU卡。&lt;/p&gt;

&lt;h2 id=&#34;4-1-安装grid驱动&#34;&gt;4.1. 安装GRID驱动&lt;/h2&gt;

&lt;p&gt;由于主机规格是ecs.sgn6i-vws-m2.xlarge，是虚拟化型规格，不能安装Tesla驱动的，需要安装grid驱动。
安装Tesla驱动的话将会出现&lt;a href=&#34;#61-error-unable-to-load-the-kernel-module-nvidiako&#34;&gt;ERROR: Unable to load the kernel module &amp;lsquo;nvidia.ko&amp;rsquo;.&lt;/a&gt;错误安装失败。&lt;/p&gt;

&lt;p&gt;安装步骤参考：&lt;a href=&#34;https://help.aliyun.com/zh/egs/user-guide/use-cloud-assistant-to-automatically-install-and-upgrade-grid-drivers?spm=a2c4g.11186623.help-menu-155040.d_1_5_2_1.65bd2ef7tthoW5&#34;&gt;在GPU虚拟化型实例中安装GRID驱动（Linux）&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;4-1-1-准备安装脚本&#34;&gt;4.1.1. 准备安装脚本&lt;/h3&gt;

&lt;p&gt;install-grid.sh内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
if acs-plugin-manager --list --local | grep grid_driver_install &amp;gt; /dev/null 2&amp;gt;&amp;amp;1
then
            acs-plugin-manager --remove --plugin grid_driver_install
fi

acs-plugin-manager --exec --plugin grid_driver_install
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-1-2-安装grid&#34;&gt;4.1.2. 安装grid&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-worker1 ~]# sh install-grid.sh 
RemovePlugin success, plugin[grid_driver_install]
INFO[0000] Starting environment pre-check               
INFO[0000] environment pre-check done                   
INFO[0000] Check gpu device present                     
INFO[0000] Check gpu device present done                
INFO[0000] current installed gird driver is, 470.239.06 
INFO[0000] current installed gird driver is already SWL driver 
[root@k8s-worker1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-1-3-验证&#34;&gt;4.1.3. 验证&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-worker1 ~]# nvidia-smi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行成功，可看到有一张T4-2Q的GPU卡。
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-192347251.png&#34; alt=&#34;picture 6&#34; /&gt;&lt;br /&gt;
至此，我们就安装好 GPU 驱动了，系统也能正常识别到 GPU。&lt;/p&gt;

&lt;p&gt;这里显示的 CUDA 版本表示当前驱动最大支持的 CUDA 版本。&lt;/p&gt;

&lt;h2 id=&#34;4-2-安装cuda&#34;&gt;4.2. 安装CUDA&lt;/h2&gt;

&lt;h3 id=&#34;4-2-1-安装软件&#34;&gt;4.2.1. 安装软件&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://developer.download.nvidia.com/compute/cuda/11.4.0/local_installers/cuda_11.4.0_470.42.01_linux.run
sudo sh cuda_11.4.0_470.42.01_linux.run
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于已安装了GRID驱动，需取消cuda自带的驱动安装：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-200338239.png&#34; alt=&#34;picture 7&#34; /&gt;&lt;br /&gt;
01) 安装全部组件：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-200453794.png&#34; alt=&#34;picture 8&#34; /&gt;&lt;br /&gt;
02) 输出：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-201735441.png&#34; alt=&#34;picture 9&#34; /&gt;&lt;/p&gt;

&lt;p&gt;03) 执行以下命令，重启GPU实例&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;reboot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;04) 依次执行以下命令，配置CUDA环境变量。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo &#39;export PATH=/usr/local/cuda/bin:$PATH&#39; | sudo tee /etc/profile.d/cuda.sh
source /etc/profile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;05) 检查CUDA是否成功安装。&lt;/p&gt;

&lt;p&gt;a) 执行nvcc -V命令，检查CUDA安装版本是否正确。
   &lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-202721754.png&#34; alt=&#34;picture 10&#34; /&gt;&lt;/p&gt;

&lt;p&gt;b) 依次执行以下命令，测试CUDA Samples，验证CUDA是否安装成功。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    [root@k8s-worker1 ~]# cd /usr/local/cuda-11.4/extras/demo_suite/
    [root@k8s-worker1 demo_suite]# ./deviceQuery  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果返回结果显示Result=PASS，则表示CUDA安装成功。
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-203410971.png&#34; alt=&#34;picture 11&#34; /&gt;&lt;/p&gt;

&lt;p&gt;06) 测试
我们使用一个简单的 Pytorch 程序来检测 GPU 和 CUDA 是否正常。&lt;/p&gt;

&lt;p&gt;整个调用链大概是这样的：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-204049273.png&#34; alt=&#34;picture 12&#34; /&gt;&lt;br /&gt;
使用下面代码来测试能够正常使用， check_cuda_pytorch.py 内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch

def check_cuda_with_pytorch():
    &amp;quot;&amp;quot;&amp;quot;检查 PyTorch CUDA 环境是否正常工作&amp;quot;&amp;quot;&amp;quot;
    try:
        print(&amp;quot;检查 PyTorch CUDA 环境:&amp;quot;)
        if torch.cuda.is_available():
            print(f&amp;quot;CUDA 设备可用，当前 CUDA 版本是: {torch.version.cuda}&amp;quot;)
            print(f&amp;quot;PyTorch 版本是: {torch.__version__}&amp;quot;)
            print(f&amp;quot;检测到 {torch.cuda.device_count()} 个 CUDA 设备。&amp;quot;)
            for i in range(torch.cuda.device_count()):
                print(f&amp;quot;设备 {i}: {torch.cuda.get_device_name(i)}&amp;quot;)
                print(f&amp;quot;设备 {i} 的显存总量: {torch.cuda.get_device_properties(i).total_memory / (1024 ** 3):.2f} GB&amp;quot;)
                print(f&amp;quot;设备 {i} 的显存当前使用量: {torch.cuda.memory_allocated(i) / (1024 ** 3):.2f} GB&amp;quot;)
                print(f&amp;quot;设备 {i} 的显存最大使用量: {torch.cuda.memory_reserved(i) / (1024 ** 3):.2f} GB&amp;quot;)
        else:
            print(&amp;quot;CUDA 设备不可用。&amp;quot;)
    except Exception as e:
        print(f&amp;quot;检查 PyTorch CUDA 环境时出现错误: {e}&amp;quot;)

if __name__ == &amp;quot;__main__&amp;quot;:
    check_cuda_with_pytorch()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;先安装下 torch&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install torch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行一下&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python check_cuda_pytorch.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正常输出应该是这样的：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-210940440.png&#34; alt=&#34;picture 13&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;4-3-更新cuda&#34;&gt;4.3. 更新CUDA&lt;/h2&gt;

&lt;p&gt;由于有些应用需要更高级的版本的CUDA，需升级，如下升级为12.6版本。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo
sudo dnf clean all
sudo dnf -y install cuda-toolkit-12-6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-203151909.png&#34; alt=&#34;picture 18&#34; /&gt;&lt;br /&gt;
验证:
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-203406637.png&#34; alt=&#34;picture 19&#34; /&gt;&lt;br /&gt;
说明12.6版本安装成功。
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-213858800.png&#34; alt=&#34;picture 20&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;5-docker环境&#34;&gt;5. Docker环境&lt;/h1&gt;

&lt;p&gt;上一步中我们已经在裸机上安装了 GPU Driver，CUDA Toolkit 等工具，实现了在宿主机上使用 GPU。&lt;/p&gt;

&lt;p&gt;现在希望在 Docker 容器中使用 GPU，需要怎么处理呢?&lt;/p&gt;

&lt;p&gt;为了让 Docker 容器中也能使用 GPU，大致步骤如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;安装docker，已有则跳过这步骤。&lt;/li&gt;
&lt;li&gt;安装 nvidia-container-toolkit 组件&lt;/li&gt;
&lt;li&gt;docker 配置使用 nvidia-runtime&lt;/li&gt;
&lt;li&gt;启动容器时增加 &amp;ndash;gpu 参数&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;5-1-docker安装&#34;&gt;5.1. Docker安装&lt;/h2&gt;

&lt;h3 id=&#34;5-1-1-安装必要的一些系统工具&#34;&gt;5.1.1. 安装必要的一些系统工具&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo yum install -y yum-utils
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-1-2-添加软件源信息&#34;&gt;5.1.2. 添加软件源信息&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-1-3-安装docker&#34;&gt;5.1.3. 安装Docker&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-1-4-开启docker服务&#34;&gt;5.1.4. 开启Docker服务&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo service docker start
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-1-5-验证&#34;&gt;5.1.5. 验证&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-worker1 pytorch]# docker version
Client: Docker Engine - Community
 Version:           26.1.3
 API version:       1.45
 Go version:        go1.21.10
 Git commit:        b72abbb
 Built:             Thu May 16 08:34:39 2024
 OS/Arch:           linux/amd64
 Context:           default

Server: Docker Engine - Community
 Engine:
  Version:          26.1.3
  API version:      1.45 (minimum version 1.24)
  Go version:       go1.21.10
  Git commit:       8e96db1
  Built:            Thu May 16 08:33:34 2024
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.6.32
  GitCommit:        8b3b7ca2e5ce38e8f31a34f35b2b68ceb8470d89
 runc:
  Version:          1.1.12
  GitCommit:        v1.1.12-0-g51d5e94
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0
[root@k8s-worker1 pytorch]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;5-2-安装-nvidia-container-toolkit&#34;&gt;5.2. 安装 nvidia-container-toolkit&lt;/h2&gt;

&lt;p&gt;NVIDIA Container Toolkit 的主要作用是将 NVIDIA GPU 设备挂载到容器中。
&amp;gt;兼容生态系统中的任意容器运行时，docker、containerd、cri-o 等。&lt;/p&gt;

&lt;p&gt;NVIDIA 官方安装文档：&lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html&#34;&gt;nvidia-container-toolkit-install-guide&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ALIOS或centos安装命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Configure the production repository:
curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | \
sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo

# Optionally, configure the repository to use experimental packages:
sudo yum-config-manager --enable nvidia-container-toolkit-experimental

#Install the NVIDIA Container Toolkit packages:
sudo yum install -y nvidia-container-toolkit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装成功如下：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-185247937.png&#34; alt=&#34;picture 14&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;5-3-配置使用该-runtime&#34;&gt;5.3. 配置使用该 runtime&lt;/h2&gt;

&lt;p&gt;支持 Docker, Containerd, CRI-O, Podman 等 CRI。
&amp;gt;具体见官方文档 &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuration&#34;&gt;container-toolkit#install-guide&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;这里以 Docker 为例进行配置：
旧版本需要手动在 /etc/docker/daemon.json 中增加配置，指定使用 nvidia 的 runtime。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;    &amp;quot;runtimes&amp;quot;: {
        &amp;quot;nvidia&amp;quot;: {
            &amp;quot;args&amp;quot;: [],
            &amp;quot;path&amp;quot;: &amp;quot;nvidia-container-runtime&amp;quot;
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;新版 toolkit 带了一个nvidia-ctk 工具，执行以下命令即可一键配置然后重启docker：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-191008933.png&#34; alt=&#34;picture 15&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;5-4-验证&#34;&gt;5.4. 验证&lt;/h2&gt;

&lt;p&gt;安装nvidia-container-toolkit 后，整个调用链如下：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-191407685.png&#34; alt=&#34;picture 16&#34; /&gt;&lt;/p&gt;

&lt;p&gt;调用链从 containerd –&amp;gt; runC 变成 containerd –&amp;gt; nvidia-container-runtime –&amp;gt; runC 。&lt;/p&gt;

&lt;p&gt;然后 nvidia-container-runtime 在中间拦截了容器 spec，就可以把 gpu 相关配置添加进去，再传给 runC 的 spec 里面就包含 gpu 信息了。&lt;/p&gt;

&lt;p&gt;Docker 环境中的 CUDA 调用大概是这样的：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-191542185.png&#34; alt=&#34;picture 17&#34; /&gt;&lt;/p&gt;

&lt;p&gt;从图中可以看到，CUDA Toolkit 跑到容器里了，因此宿主机上不需要再安装 CUDA Toolkit。&lt;/p&gt;

&lt;p&gt;使用一个带 CUDA Toolkit 的镜像即可。&lt;/p&gt;

&lt;p&gt;最后我们启动一个 Docker 容器进行测试，其中命令中增加 &amp;ndash;gpu参数来指定要分配给容器的 GPU。&lt;/p&gt;

&lt;p&gt;gpu 参数可选值：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;gpus all：表示将所有 GPU 都分配给该容器&lt;/li&gt;
&lt;li&gt;gpus &amp;ldquo;device=&lt;id&gt;[,&lt;id&gt;&amp;hellip;]&amp;ldquo;：对于多 GPU 场景，可以通过 id 指定分配给容器的 GPU，例如 –gpu “device=0” 表示只分配 0 号 GPU 给该容器
GPU 编号则是通过nvidia-smi 命令进行查看&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里我们直接使用一个带 cuda 的镜像来测试，启动该容器并执行nvidia-smi 命令&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-worker1 ~]# docker run --rm --gpus all  nvcr.io/nvidia/cuda:11.0.3-runtime-ubuntu20.04 nvidia-smi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-214303643.png&#34; alt=&#34;picture 22&#34; /&gt;&lt;br /&gt;
正常情况下应该是可以打印出容器中的 GPU 信息的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;： 镜像需与grid驱动版本兼容，否则报错。&lt;/p&gt;

&lt;h1 id=&#34;6-k8s环境&#34;&gt;6. K8S环境&lt;/h1&gt;

&lt;p&gt;更进一步，在 k8s 环境中使用 GPU，则需要在集群中部署以下组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;gpu-device-plugin 用于管理 GPU，device-plugin 以 DaemonSet 方式运行到集群各个节点，以感知节点上的 GPU 设备，从而让 k8s 能够对节点上的 GPU 设备进行管理。&lt;/li&gt;
&lt;li&gt;gpu-exporter：用于监控 GPU&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;各组件关系如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-215913188.png&#34; alt=&#34;picture 23&#34; /&gt;&lt;/p&gt;

&lt;p&gt;左图为手动安装的场景，只需要在集群中安装 device-plugin 和 监控即可使用。&lt;/p&gt;

&lt;p&gt;右图为使用 gpu-operotar 安装场景，本篇暂时忽略&lt;/p&gt;

&lt;p&gt;大致工作流程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每个节点的 kubelet 组件维护该节点的 GPU 设备状态（哪些已用，哪些未用）并定时报告给调度器，调度器知道每一个节点有多少张 GPU 卡可用。&lt;/li&gt;
&lt;li&gt;调度器为 pod 选择节点时，从符合条件的节点中选择一个节点。&lt;/li&gt;
&lt;li&gt;当 pod 调度到节点上后，kubelet 组件为 pod 分配 GPU 设备 ID，并将这些 ID 作为参数传递给 NVIDIA Device Plugin&lt;/li&gt;
&lt;li&gt;NVIDIA Device Plugin 将分配给该 pod 的容器的 GPU 设备 ID 写入到容器的环境变量 NVIDIA_VISIBLE_DEVICES中，然后将信息返回给 kubelet。&lt;/li&gt;
&lt;li&gt;kubelet 启动容器。&lt;/li&gt;
&lt;li&gt;NVIDIA Container Toolkit 检测容器的 spec 中存在环境变量 NVIDIA_VISIBLE_DEVICES，然后根据环境变量的值将 GPU 设备挂载到容器中。
在 Docker 环境我们在启动容器时通过 &amp;ndash;gpu 参数手动指定分配给容器的 GPU，k8s 环境则由 device-plugin 自行管理。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;6-1-集群环境&#34;&gt;6.1. 集群环境&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-220923122.png&#34; alt=&#34;picture 24&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;安装-device-plugin&#34;&gt;安装 device-plugin&lt;/h2&gt;

&lt;p&gt;device-plugin 一般由对应的 GPU 厂家提供，比如 NVIDIA 的 &lt;a href=&#34;https://github.com/NVIDIA/k8s-device-plugin&#34;&gt;k8s-device-plugin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;安装其实很简单，将对应的 yaml apply 到集群即可。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.15.0/deployments/static/nvidia-device-plugin.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就像这样
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-221359266.png&#34; alt=&#34;picture 25&#34; /&gt;&lt;/p&gt;

&lt;p&gt;device-plugin 启动之后，会感知节点上的 GPU 设备并上报给 kubelet，最终由 kubelet 提交到 kube-apiserver。&lt;/p&gt;

&lt;p&gt;因此我们可以在 Node 可分配资源中看到 GPU，就像这样：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-221933864.png&#34; alt=&#34;picture 26&#34; /&gt;&lt;br /&gt;
可以看到，除了常见的 cpu、memory 之外，还有nvidia.com/gpu, 这个就是 GPU 资源，数量为 0，应该为1，错误原因待查。
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250123-095336709.png&#34; alt=&#34;picture 27&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;安装-gpu-监控&#34;&gt;安装 GPU 监控&lt;/h2&gt;

&lt;p&gt;除此之外，如果你需要监控集群 GPU 资源使用情况，你可能还需要安装 DCCM exporter 结合 Prometheus 输出 GPU 资源监控信息。&lt;/p&gt;

&lt;p&gt;安装略。&lt;/p&gt;

&lt;h1 id=&#34;小结&#34;&gt;小结&lt;/h1&gt;

&lt;p&gt;本文主要分享了在ECS、Docker 环境、k8s 环境中如何使用 GPU。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;对于ECS环境，只需要安装对应的 GPU Driver 即可。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对应 Docker 环境，需要额外安装 nvidia-container-toolkit 并配置 docker 使用 nvidia runtime。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对应 k8s 环境，需要额外安装对应的 device-plugin 使得 kubelet 能够感知到节点上的 GPU 设备，以便 k8s 能够进行 GPU 管理。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;现在一般都是在 k8s 环境中使用，为了简化安装步骤， NVIDIA 也提供了 gpu-operator来简化安装部署，后续分享一下如何使用 gpu-operator来快速安装。&lt;/p&gt;

&lt;h1 id=&#34;7-参考资料&#34;&gt;7. 参考资料&lt;/h1&gt;

&lt;p&gt;【01】&lt;a href=&#34;https://blog.csdn.net/Doudou_Mylove/article/details/114388633&#34;&gt;阿里云GPU服务器安装驱动（完整版）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;【02】&lt;a href=&#34;https://help.aliyun.com/zh/egs/user-guide/install-a-gpu-driver-on-a-gpu-accelerated-compute-optimized-linux-instance?spm=a2c4g.11186623.help-menu-155040.d_1_5_1_1.6b70fb7cljEZnN&amp;amp;scm=20140722.H_163824._.OR_help-T_cn~zh-V_1&#34;&gt;在GPU计算型实例中手动安装Tesla驱动（Linux）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;【03】&lt;a href=&#34;https://help.aliyun.com/zh/egs/user-guide/use-cloud-assistant-to-automatically-install-and-upgrade-grid-drivers?spm=a2c4g.11186623.help-menu-155040.d_1_5_2_1.660551bd3LBP63&#34;&gt;在GPU虚拟化型实例中安装GRID驱动（Linux）&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;8-faq&#34;&gt;8. FAQ&lt;/h1&gt;

&lt;h2 id=&#34;8-1-error-unable-to-load-the-kernel-module-nvidia-ko&#34;&gt;8.1. ERROR: Unable to load the kernel module &amp;lsquo;nvidia.ko&amp;rsquo;.&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250120-155358481.png&#34; alt=&#34;picture 4&#34; /&gt;&lt;br /&gt;
错误详情提示：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250120-155501141.png&#34; alt=&#34;picture 5&#34; /&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(07): kubeadm安装k8s集群(containerd版)</title>
            <link>http://mospany.github.io/2024/01/15/kubeadm-install-k8s/</link>
            <pubDate>Mon, 15 Jan 2024 15:42:11 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2024/01/15/kubeadm-install-k8s/</guid>
            <description>

&lt;h1 id=&#34;1-规划&#34;&gt;1. 规划&lt;/h1&gt;

&lt;p&gt;使用 kubeadm 安装 Kubernetes 集群并使用 containerd 作为容器运行时（container runtime）是一种常见的安装方法。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;OS&lt;/th&gt;
&lt;th&gt;配置&lt;/th&gt;
&lt;th&gt;用途&lt;/th&gt;
&lt;th&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;aliOS(172.17.197.69)&lt;/td&gt;
&lt;td&gt;2核(vCPU) 4GiB 5 Mbps&lt;/td&gt;
&lt;td&gt;k8s-master1&lt;/td&gt;
&lt;td&gt;乌兰察布 可用区 C&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;aliOS(172.17.197.68)&lt;/td&gt;
&lt;td&gt;4核(vCPU) 10 GiB 5 Mbps GPU：NVIDIA T4/8&lt;/td&gt;
&lt;td&gt;k8s-worker1&lt;/td&gt;
&lt;td&gt;乌兰察布 可用区 C&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：这是演示 k8s 集群安装的实验环境，配置较低，生产环境中我们的服务器配置至少都是 8C/16G 的基础配置。&lt;/p&gt;

&lt;h1 id=&#34;2-版本选择&#34;&gt;2. 版本选择&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Alibaba Cloud Linux 3.2104 LTS 64位
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2-1-配置主机名&#34;&gt;2.1. 配置主机名&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# hostnamectl set-hostname k8s-master1
[root@k8s-worker1 ~]# hostnamectl set-hostname k8s-worker1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-2-关闭防火墙&#34;&gt;2.2. 关闭防火墙&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; # 关闭firewalld
2  [root@k8s-master1 ~]# systemctl stop firewalld
3  
4  # 关闭selinux
5  [root@k8s-master1 ~]# sed -i &#39;s/enforcing/disabled/&#39; /etc/selinux/config
6  [root@k8s-master1 ~]# setenforce 0
7   setenforce: SELinux is disabled
8  [root@k8s-master1 ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-3-互做本地解析&#34;&gt;2.3. 互做本地解析&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master1 ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
8.130.117.184 k8s-master1
8.130.92.114  k8s-worker1
[root@k8s-master1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-4-ssh-免密通信-可选&#34;&gt;2.4. SSH 免密通信（可选）&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;master节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master1 ~]# ssh-keygen
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250115-192630256.png&#34; alt=&#34;picture 0&#34; /&gt;&lt;br /&gt;
互发公钥&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# ssh-copy-id root@k8s-worker1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250115-193004163.png&#34; alt=&#34;picture 1&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-5-加载-br-netfilter-模块&#34;&gt;2.5. 加载 br_netfilter 模块&lt;/h2&gt;

&lt;p&gt;确保 br_netfilter 模块被加载
&amp;gt; 所有节点执行&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 加载模块
root@k8s-master1 ~]# modprobe br_netfilter
## 查看加载请看
[root@k8s-master1 ~]# lsmod | grep br_netfilter
br_netfilter           32768  0
bridge                270336  1 br_netfilter

# 永久生效
[root@k8s-master1 ~]# cat &amp;lt;&amp;lt;EOF | tee /etc/modules-load.d/k8s.conf
&amp;gt; br_netfilter
&amp;gt; EOF
br_netfilter
[root@k8s-master1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-6-允许-iptables-检查桥接流量&#34;&gt;2.6. 允许 iptables 检查桥接流量&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
&amp;gt; net.bridge.bridge-nf-call-ip6tables = 1
&amp;gt; net.bridge.bridge-nf-call-iptables = 1
&amp;gt; EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
[root@k8s-master1 ~]# sysctl --system
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-7-关闭-swap&#34;&gt;2.7. 关闭 swap&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 临时关闭
[root@k8s-master1 ~]# swapoff -a

# 永久关闭
[root@k8s-master1 ~]# sed -ri &#39;s/.*swap.*/#&amp;amp;/&#39; /etc/fstab
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-8-安装ipset及ipvsadm&#34;&gt;2.8. 安装ipset及ipvsadm&lt;/h2&gt;

&lt;p&gt;安装ipset及ipvsadm&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# dnf install -y ipset ipvsadm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置ipvsadm模块加载方式，添加需要加载的模块&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat &amp;gt; /etc/sysconfig/modules/ipvs.modules &amp;lt;&amp;lt;EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
EOF

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;授权、运行、检查是否加载&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; bash /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; lsmod | grep -e ip_vs -e nf_conntrack
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-135626938.png&#34; alt=&#34;picture 2&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;3-安装containerd&#34;&gt;3. 安装containerd&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;3-1-安装-containerd&#34;&gt;3.1. 安装 containerd&lt;/h2&gt;

&lt;p&gt;通过 yum 安装 containerd：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master1 ~]# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
Adding repo from: http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
[root@k8s-master1 ~]# yum install containerd -y

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-2-配置-containerd&#34;&gt;3.2. 配置 containerd&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ /usr/bin/containerd config default &amp;gt; /etc/containerd/config.toml
$ sed -i &#39;s/SystemdCgroup = false/SystemdCgroup = true/g&#39; /etc/containerd/config.toml
$ sed -i &#39;s/sandbox_image = &amp;quot;registry.k8s.io\/pause:3.6&amp;quot;/sandbox_image = &amp;quot;registry.aliyuncs.com\/google_containers\/pause:3.9&amp;quot;/g&#39; /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;： 记得确保配置修改成功，尤其是pause版本。&lt;/p&gt;

&lt;h2 id=&#34;3-3-启动containerd&#34;&gt;3.3. 启动containerd&lt;/h2&gt;

&lt;p&gt;启动并使 containerd 随系统启动：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master1 ~]# sudo systemctl start containerd
root@k8s-master1 ~]# sudo systemctl enable containerd
root@k8s-master1 ~]# ps -ef | grep containerd
root       37760       1  0 07:35 ?        00:00:00 /usr/bin/containerd
root       37807   36202  0 07:35 pts/0    00:00:00 grep --color=auto containerd
[root@k8s-master1 ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明containerd已启动成功。
&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-160630369.png&#34; alt=&#34;picture 3&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;4-安装-kubeadm-kubelet&#34;&gt;4. 安装 kubeadm、kubelet&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;4-1-添加-k8s-镜像源&#34;&gt;4.1. 添加 k8s 镜像源&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;地址： &lt;a href=&#34;https://developer.aliyun.com/mirror/kubernetes?spm=a2c6h.13651102.0.0.1cd01b116JYQIn&#34;&gt;https://developer.aliyun.com/mirror/kubernetes?spm=a2c6h.13651102.0.0.1cd01b116JYQIn&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat &amp;lt;&amp;lt;EOF | tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.28/rpm/
enabled=1
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.28/rpm/repodata/repomd.xml.key
EOF
setenforce 0
yum install -y --nogpgcheck kubelet kubeadm kubectl
systemctl enable kubelet &amp;amp;&amp;amp; systemctl start kubelet
echo 1 &amp;gt; /proc/sys/net/ipv4/ip_forward
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;4-2-建立-k8s-yum-缓存&#34;&gt;4.2. 建立 k8s YUM 缓存&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# yum makecache
alinux3-os                                                                                       1.3 MB/s | 3.8 kB     00:00    
alinux3-updates                                                                                  1.4 MB/s | 4.1 kB     00:00    
alinux3-module                                                                                   354 kB/s | 4.2 kB     00:00    
alinux3-plus                                                                                     645 kB/s | 3.0 kB     00:00    
alinux3-powertools                                                                               598 kB/s | 3.0 kB     00:00    
Extra Packages for Enterprise Linux 8 - x86_64                                                   1.8 MB/s | 4.4 kB     00:00    
Extra Packages for Enterprise Linux Modular 8 - x86_64                                           620 kB/s | 3.0 kB     00:00    
Kubernetes                                                                                        17 kB/s | 1.7 kB     00:00    
Metadata cache created.
[root@k8s-master1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;4-3-安装-k8s-相关工具&#34;&gt;4.3. 安装 k8s 相关工具&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# yum list kubelet --showduplicates
Last metadata expiration check: 0:01:46 ago on Wed 15 Jan 2025 10:55:44 PM CST.
Installed Packages
kubelet.x86_64                                           1.28.15-150500.1.1                                           @kubernetes
Available Packages
kubelet.aarch64                                          1.28.0-150500.1.1                                            kubernetes 
kubelet.ppc64le                                          1.28.0-150500.1.1                                            kubernetes 
kubelet.s390x                                            1.28.0-150500.1.1                                            kubernetes 
kubelet.src                                              1.28.0-150500.1.1                                            kubernetes 
kubelet.x86_64                                           1.28.0-150500.1.1                                            kubernetes 
kubelet.aarch64                                          1.28.1-150500.1.1                                            kubernetes 
kubelet.ppc64le                                          1.28.1-150500.1.1                                            kubernetes 
kubelet.s390x                                            1.28.1-150500.1.1                                            kubernetes 
kubelet.src                                              1.28.1-150500.1.1                                            kubernetes 
.....
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置crictl连接 containerd&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ crictl config --set runtime-endpoint=unix:///run/containerd/containerd.sock
$ crictl images ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置kubectl命令自动补全：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo &#39;source &amp;lt;(kubectl completion bash)&#39; &amp;gt;&amp;gt; ~/.bashrc
source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;5-master节点&#34;&gt;5. master节点&lt;/h1&gt;

&lt;h2 id=&#34;5-1-k8s-初始化&#34;&gt;5.1. k8s 初始化&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：记得修改&amp;ndash;apiserver-advertise-address与&amp;ndash;kubernetes-version修改正确。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master ~]# kubeadm init \
  --apiserver-advertise-address=172.17.197.69 \
  --image-repository registry.aliyuncs.com/google_containers \
  --kubernetes-version v1.28.15 \
  --service-cidr=10.96.0.0/12 \
  --pod-network-cidr=10.244.0.0/16 \
  --ignore-preflight-errors=all \
  --cri-socket /var/run/containerd/containerd.sock
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数说明：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;   --apiserver-advertise-address  # 集群master地址
   --image-repository             # 指定k8s镜像仓库地址
   --kubernetes-version           # 指定K8s版本（与kubeadm、kubelet版本保持一致）
   --service-cidr                 # Pod统一访问入口
   --pod-network-cidr             # Pod网络（与CNI网络保持一致）
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初始化后输出如下内容，说明成功。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.....
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.17.197.69:6443 --token ipqfom.r9ltq4t3in53cd6w \
        --discovery-token-ca-cert-hash sha256:0f01242802eae4b69408c34070a3ad8d017229faba9f8b30623ed1e9dd69d66c 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;5-2-根据输出提示创建相关文件&#34;&gt;5.2. 根据输出提示创建相关文件&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;master节点上执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master ~]# mkdir -p $HOME/.kube
[root@k8s-master ~]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[root@k8s-master ~]# chown $(id -u):$(id -g) $HOME/.kube/config
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;5-3-查看-k8s-运行的容器&#34;&gt;5.3. 查看 k8s 运行的容器&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;master节点上执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# kubectl  get pod -A -o wide
NAMESPACE     NAME                                  READY   STATUS    RESTARTS   AGE   IP              NODE          NOMINATED NODE   READINESS GATES
kube-system   coredns-66f779496c-lq5rj              0/1     Pending   0          26m   &amp;lt;none&amp;gt;          &amp;lt;none&amp;gt;        &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-66f779496c-q5pxk              0/1     Pending   0          26m   &amp;lt;none&amp;gt;          &amp;lt;none&amp;gt;        &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   etcd-k8s-master1                      1/1     Running   1          26m   172.17.197.69   k8s-master1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-apiserver-k8s-master1            1/1     Running   1          26m   172.17.197.69   k8s-master1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-controller-manager-k8s-master1   1/1     Running   1          26m   172.17.197.69   k8s-master1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-88trd                      1/1     Running   0          26m   172.17.197.69   k8s-master1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-scheduler-k8s-master1            1/1     Running   1          26m   172.17.197.69   k8s-master1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
[root@k8s-master1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;5-4-查看-k8s-节点&#34;&gt;5.4. 查看 k8s 节点&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;master节点上执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master1 ~]# kubectl  get node -o wide
NAME          STATUS     ROLES           AGE   VERSION    INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                                              KERNEL-VERSION           CONTAINER-RUNTIME
k8s-master1   NotReady   control-plane   36m   v1.28.15   172.17.197.69   &amp;lt;none&amp;gt;        Alibaba Cloud Linux 3.2104 U11 (OpenAnolis Edition)   5.10.134-18.al8.x86_64   containerd://1.6.32
[root@k8s-master1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可看到当前只有 k8s-master 节点，而且状态是 NotReady（未就绪），因为我们还没有部署网络插件（kubectl apply -f [podnetwork].yaml），于是接着部署容器网络（CNI）。&lt;/p&gt;

&lt;h2 id=&#34;5-5-容器网络-cni-部署&#34;&gt;5.5. 容器网络（CNI）部署&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;插件地址：&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/addons/&#34;&gt;https://kubernetes.io/docs/concepts/cluster-administration/addons/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;该地址在 k8s-master 初始化成功时打印出来。&lt;/p&gt;

&lt;h3 id=&#34;5-5-1-选择一个主流的容器网络插件部署-calico&#34;&gt;5.5.1. 选择一个主流的容器网络插件部署（Calico）&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-164005909.png&#34; alt=&#34;picture 4&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;5-5-2-下载yml文件&#34;&gt;5.5.2. 下载yml文件&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://docs.projectcalico.org/manifests/calico.yaml --no-check-certificate 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-5-3-看看该yaml文件所需要启动的容器&#34;&gt;5.5.3. 看看该yaml文件所需要启动的容器&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 install-k8s-cluster]# cat calico.yaml |grep image
          image: docker.io/calico/cni:v3.25.0
          imagePullPolicy: IfNotPresent
          image: docker.io/calico/cni:v3.25.0
          imagePullPolicy: IfNotPresent
          image: docker.io/calico/node:v3.25.0
          imagePullPolicy: IfNotPresent
          image: docker.io/calico/node:v3.25.0
          imagePullPolicy: IfNotPresent
          image: docker.io/calico/kube-controllers:v3.25.0
          imagePullPolicy: IfNotPresent
[root@k8s-master1 install-k8s-cluster]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-5-4-修改为国内可下载镜像&#34;&gt;5.5.4. 修改为国内可下载镜像&lt;/h3&gt;

&lt;p&gt;使用docker.m.daocloud.io代替docker.io&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sed -i s/docker.io/docker.m.daocloud.io/g calico.yaml 
$ cat calico.yaml | grep image

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-205457826.png&#34; alt=&#34;picture 5&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;5-5-5-安装calico&#34;&gt;5.5.5. 安装calico&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 calicodir]# kubectl  apply -f calico.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-205731350.png&#34; alt=&#34;picture 6&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;5-5-6-查看pod和node状态正常&#34;&gt;5.5.6. 查看pod和node状态正常&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 calicodir]# kubectl  get pod -A
[root@k8s-master1 calicodir]# kubectl  get node
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-210017047.png&#34; alt=&#34;picture 7&#34; /&gt;&lt;br /&gt;
可以看出pod已全部Running， node已变成Ready状态，说明master节点已安装成功完成。&lt;/p&gt;

&lt;h1 id=&#34;6-worker-节点&#34;&gt;6. worker 节点&lt;/h1&gt;

&lt;h2 id=&#34;6-1-worker-节点加入-k8s-集群&#34;&gt;6.1. worker 节点加入 k8s 集群&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有 work 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;复制k8s-master初始化屏幕输出的语句并在work节点执行&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-worker1 ~]# kubeadm join 172.17.197.69:6443 --token ipqfom.r9ltq4t3in53cd6w         --discovery-token-ca-cert-hash sha256:0f01242802eae4b69408c34070a3ad8d017229faba9f8b30623ed1e9dd69d66c
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with &#39;kubectl -n kube-system get cm kubeadm-config -o yaml&#39;
[kubelet-start] Writing kubelet configuration to file &amp;quot;/var/lib/kubelet/config.yaml&amp;quot;
[kubelet-start] Writing kubelet environment file with flags to file &amp;quot;/var/lib/kubelet/kubeadm-flags.env&amp;quot;
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run &#39;kubectl get nodes&#39; on the control-plane to see this node join the cluster.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;6-2-查询集群信息&#34;&gt;6.2. 查询集群信息&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master节点上执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 calicodir]# kubectl  get pod -A -o wide
[root@k8s-master1 calicodir]# kubectl  get node -o wide
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-213420388.png&#34; alt=&#34;picture 8&#34; /&gt;&lt;/p&gt;

&lt;p&gt;查看crictl和ctr是否安装成功。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 calicodir]# crictl version
[root@k8s-master1 calicodir]# ctr version
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-213647229.png&#34; alt=&#34;picture 9&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;7-验证&#34;&gt;7. 验证&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;Master节点上执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;k8s 集群部署 nginx 服务，并通过浏览器进行访问验证。&lt;/p&gt;

&lt;h2 id=&#34;7-1-创建pod&#34;&gt;7.1. 创建Pod&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 calicodir]# kubectl create deployment nginx --image=docker.m.daocloud.io/library/nginx:latest
deployment.apps/nginx created
[root@k8s-master1 calicodir]# kubectl expose deployment nginx --port=80 --type=NodePort
service/nginx exposed
[root@k8s-master1 calicodir]# kubectl  get all
NAME                        READY   STATUS    RESTARTS   AGE
pod/nginx-fbf584587-pp6ks   1/1     Running   0          2m45s

NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/kubernetes   ClusterIP   10.96.0.1       &amp;lt;none&amp;gt;        443/TCP        5h45m
service/nginx        NodePort    10.110.34.104   &amp;lt;none&amp;gt;        80:30884/TCP   28s

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx   1/1     1            1           2m45s

NAME                              DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-fbf584587   1         1         1       2m45s
[root@k8s-master1 calicodir]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;7-2-访问nginx&#34;&gt;7.2. 访问nginx&lt;/h2&gt;

&lt;h3 id=&#34;7-2-1-curl访问&#34;&gt;7.2.1. curl访问&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 calicodir]# curl 10.110.34.104:80
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;For online documentation and support please refer to
&amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
Commercial support is available at
&amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
[root@k8s-master1 calicodir]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用nodePort方式访问现象同上
&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-220403508.png&#34; alt=&#34;picture 10&#34; /&gt;&lt;/p&gt;

&lt;p&gt;至此：kubeadm方式的k8s集群已经部署完成。&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>如何安装使用newbing和chatgpt</title>
            <link>http://mospany.github.io/2023/06/23/how-about-install-newbing-and-chatgpt/</link>
            <pubDate>Fri, 23 Jun 2023 17:36:37 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2023/06/23/how-about-install-newbing-and-chatgpt/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;org5c16ff5&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;chatgpt在2022年开始爆发，随着越来越多的人在使用，于是也打算尝试安装使用。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org53977af&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;chatgpt&#34;&gt;chatgpt&lt;/h2&gt;

&lt;p&gt;ChatGPT是一种基于人工智能技术的聊天机器人，它可以与用户进行自然语言交互，回答用户的问题，提供有用的信息和建议，甚至玩一些有趣的游戏。ChatGPT是由OpenAI开发，使用了GPT（Generative Pre-trained Transformer）技术，这是一种具有强大生成能力的神经网络模型。ChatGPT通过学习海量的语言数据集来不断提高其对自然语言的理解能力，从而能够更好地理解和回答用户的问题。与其他聊天机器人相比，ChatGPT的对话流畅度和回答准确度都很高，因此备受用户欢迎。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org292f1ea&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;newbing&#34;&gt;newbing&lt;/h2&gt;

&lt;p&gt;必应chat是微软旗下的聊天机器人服务，它是基于必应AI开发的一款智能对话系统。必应chat可以理解自然语言，能够回答用户的问题、提供服务、开玩笑等，使得用户可以与机器人进行自然的对话。必应chat的功能包括天气查询、地图导航、音乐播放、闲聊聊天、计算数学问题等，可帮助用户解决生活中的各种疑问和问题。必应chat还支持多种语言，包括英语、中文、西班牙语、法语、德语等。使用必应chat，您可以轻松地与机器人进行互动，获得便捷的服务和娱乐体验。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org79da4ca&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;安装&#34;&gt;安装&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org8197e6e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;科学上网&#34;&gt;科学上网&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;org9014292&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;自由鲸&#34;&gt;自由鲸&lt;/h3&gt;

&lt;p&gt;自由鲸 FreeWhale 是一家长期走中高端路线的 ShadowsocksR(SSR) 机场，也提供部分 V2Ray 线路，已经稳定运行多年。实际对比下来，它的性价比还是相当高的，线路又多，提供的流量也十分充足，主要推荐的套餐充分考虑了当前主流用户能够接受的价位，可以说是无可挑剔。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;特点&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;封闭邀请机制&lt;/p&gt;

&lt;p&gt;自由鲸 FreeWhale 需要邀请码才能注册成功，这样的注册机制一定程度确保了服务的稳定性。&lt;/p&gt;

&lt;p&gt;如果需要邀请码，可以使用我的邀请码:&lt;/p&gt;

&lt;p&gt;也可以直接点击我的邀请链接，进入注册。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;套餐便宜&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/chatgpt/freewhale.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;使用&lt;/p&gt;

&lt;p&gt;登录地址：&lt;a href=&#34;https://www.freewhale.world/auth/register?code=XXXX&#34;&gt;https://www.freewhale.world/auth/register?code=XXXX&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;选的是512G/360 天的套餐124元，相当于每月10元还算便宜。&lt;/p&gt;

&lt;p&gt;购买成功后将出现一堆节点列表，在用户中心里将出现订阅地址用做vpn客户端的链接信息。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a id=&#34;org9d5fbb8&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;clash&#34;&gt;clash&lt;/h3&gt;

&lt;p&gt;Clash是一款开源的多平台代理软件，支持 Windows、macOS、Linux、Android 等平台。Clash可以运行在本地并代理 HTTP、HTTPS、SOCKS5等协议，同时还支持 SS、V2Ray、Trojan 等协议，可以实现对于许多网站和应用的科学上网。Clash 的特点是支持多种协议，使用方便，功能强大，且更新频繁。Clash的开源代码托管在GitHub上，用户可以参与其开发和改进。&lt;/p&gt;

&lt;p&gt;把freeWhale上面的订阅地址粘贴到clash的配置-&amp;gt;托管配置-&amp;gt;管理里， 配置名称写为xinjieCloud。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/chatgpt/clash-face.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;使用科学上网时，查看 ip 地址可能显示在国内，以下是解决办法:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一定挂全局模式 + 支持 OpenAI 的国家（目前中俄等国不支持，推荐北美、日本等国家）；&lt;/li&gt;
&lt;li&gt;一定设置为系统代理 + 手动选择节点(DIRECT), 否则上不了外网。&lt;/li&gt;
&lt;li&gt;上面步骤完成后依然不行的话，就清空浏览器缓存然后重启浏览器或者电脑；或者直接新开一个无痕模式的窗口。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org208d97c&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;pchat&#34;&gt;Pchat&lt;/h2&gt;

&lt;p&gt;Pchat 是一款可以免费、简单易用的聊天机器人，翻墙后可以使用，不用做任何配置。它包括Pchat free版、Pchat Pro高级版本。&lt;br /&gt;
访问地址为：&lt;a href=&#34;https://www.promptboom.com&#34;&gt;https://www.promptboom.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/chatgpt/pchat.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pchat Free，一个由PromptBoom和OpenAI合作开发的AI助手。&lt;/li&gt;
&lt;li&gt;Pchat Pro，是Pchat免费版的高级版本。相比之下，Pchat免费版提供了基本的聊天功能，而Pchat Pro则包括更多的高级功能，例如语音和视频通话，自定义表情符号等等。此外，Pchat Pro还具有更高的智能和更好的性能，因为它采用了最先进的技术和算法。&lt;br /&gt;
试用期为14天。在试用期结束之前，您可以决定是否购买Pchat Pro的许可证。如果您决定购买，您将获得无限制的访问权限，并可以享受Pchat Pro提供的所有功能。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org8b69e3f&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;newbing-1&#34;&gt;newBing&lt;/h2&gt;

&lt;p&gt;微软 New Bing 是微软推出的全新搜索引擎，它使用了最新的人工智能技术和自然语言处理技术来提供更好的搜索结果和更智能的搜索体验。它可以帮助用户更快、更准确地找到他们需要的信息，并且能够根据用户的搜索历史和兴趣爱好提供更加个性化的搜索结果。&lt;/p&gt;

&lt;p&gt;1、下载最新/已支持newbing的edge浏览器&lt;/p&gt;

&lt;p&gt;2、设置bing4为默认搜索引擎&lt;br /&gt;
   URL地址写为：&lt;a href=&#34;https://www4.bing.com/search?q=%s&amp;amp;PC=U316&amp;amp;FROM=CHROMN&#34;&gt;https://www4.bing.com/search?q=%s&amp;amp;PC=U316&amp;amp;FROM=CHROMN&lt;/a&gt;&lt;br /&gt;
   &lt;img src=&#34;http://blog.mospan.cn/post/img/chatgpt/bing4-engine.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;3、设置外网DNS&lt;br /&gt;
   选择openDNS&lt;br /&gt;
   &lt;img src=&#34;http://blog.mospan.cn/post/img/chatgpt/select-dns.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;4、禁止和清理cookies&lt;br /&gt;
   禁止保存一些cn.bing.com或c.bing.com的cookies，免得老跳转到国内bing上。&lt;br /&gt;
   &lt;img src=&#34;http://blog.mospan.cn/post/img/chatgpt/forbiden-cookies.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;5、登录&lt;br /&gt;
   新建标签页，把语言设置为非国内地区&lt;br /&gt;
  &lt;img src=&#34;http://blog.mospan.cn/post/img/chatgpt/newbing-index.png&#34; alt=&#34;img&#34; /&gt;&lt;br /&gt;
   登录账号，如我个人账号为: moshengping210@gmail.com&lt;/p&gt;

&lt;p&gt;点击AI，就可以用bingChat了(不太正常，如跳转到国内bing后需清理cookies后重启浏览器)&lt;br /&gt;
&lt;img src=&#34;http://blog.mospan.cn/post/img/chatgpt/newbing.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org554ec02&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;chatgpt-1&#34;&gt;chatgpt&lt;/h2&gt;

&lt;p&gt;1、申请账号&lt;br /&gt;
   某宝上购买账号&lt;/p&gt;

&lt;p&gt;2、登录官网(需提前科学上网)&lt;br /&gt;
   ChatGPT的官方网址： &lt;a href=&#34;https://chat.openai.com/auth/login&#34;&gt;https://chat.openai.com/auth/login&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3、查询&lt;br /&gt;
  &lt;img src=&#34;http://blog.mospan.cn/post/img/chatgpt/chatgpt.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org415b7ac&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;p&gt;【01】 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/618495330&#34;&gt;当前最新稳定访问NewBing方法（操作简单）&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(06): 动手实现webhook</title>
            <link>http://mospany.github.io/2023/03/05/k8s-webhook-example/</link>
            <pubDate>Sun, 05 Mar 2023 11:32:18 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2023/03/05/k8s-webhook-example/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;orgabac5c2&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;Webhook就是一种HTTP回调，用于在某种情况下执行某些动作，Webhook不是K8S独有的，很多场景下都可以进行Webhook，比如在提交完代码后调用一个Webhook自动构建docker镜像&lt;/p&gt;

&lt;p&gt;K8S中提供了自定义资源类型和自定义控制器来扩展功能，还提供了动态准入控制，其实就是通过Webhook来实现准入控制，分为两种：验证性质的准入 Webhook （Validating Admission Webhook） 和 修改性质的准入 Webhook （Mutating Admission Webhook）&lt;/p&gt;

&lt;p&gt;Admission Webhook有哪些使用场景？如下&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在资源持久化到ETCD之前进行修改（Mutating Webhook），比如增加init Container或者sidecar Container&lt;/li&gt;
&lt;li&gt;在资源持久化到ETCD之前进行校验（Validating Webhook），不满足条件的资源直接拒绝并给出相应信息&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.mospan.cn/post/img/webhook/webhook.webp&#34;&gt;http://blog.mospan.cn/post/img/webhook/webhook.webp&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Webhook可以理解成Java Web开发中的Filter，每个请求都会经过Filter处理，从图中可以看到，先执行的是Mutating Webhook，它可以对资源进行修改，然后执行的是Validating Webhook，它可以拒绝或者接受请求，但是它不能修改请求。&lt;/p&gt;

&lt;p&gt;K8S中有已经实现了的Admission Webhook列表，详情参考每个准入控制器的作用是什么？&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org5a64fe0&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;示例原理&#34;&gt;示例原理&lt;/h1&gt;

&lt;p&gt;我们以一个简单的Webhook作为例子，该Webhook会在创建Deployment资源的时候检查它是否有相应的标签，如果没有的话，则加上（Mutating Webhook），然后在检验它是否有相应的标签（Validating Webhook），有则创建该Deployment，否则拒绝并给出相应错误提示。&lt;/p&gt;

&lt;p&gt;所有代码都在：&lt;br /&gt;
git@github.com:mospany/admission-webhook-example.git&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org02043ed&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;验证&#34;&gt;验证&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org122233e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;编译打包&#34;&gt;编译打包&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;cd admission-webhook-example/v1
DOCKER_USER=mospany bash ./build
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它将生成镜像并上传到docker.io/mospany/admission-webhook-example:v1。&lt;br /&gt;
&lt;img src=&#34;http://blog.mospan.cn/post/img/webhook/build.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org0aa6c11&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;检查是否开启了动态准入控制&#34;&gt;检查是否开启了动态准入控制&lt;/h2&gt;

&lt;p&gt;查看APIServer是否开启了MutatingAdmissionWebhook和ValidatingAdmissionWebhook&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 获取apiserver pod名字
apiserver_pod_name=`kubectl get --no-headers=true po -n kube-system | grep kube-apiserver | awk &#39;{ print $1 }&#39;`
# 查看api server的启动参数plugin
kubectl get po $apiserver_pod_name -n kube-system -o yaml | grep plugin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果输出如下，说明已经开启&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- --enable-admission-plugins=NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;否则，需要修改启动参数，请不然直接修改Pod的参数，这样修改不会成功，请修改配置文件/etc/kubernetes/manifests/kube-apiserver.yaml，加上相应的插件参数后保存，APIServer的Pod会监控该文件的变化，然后重新启动。&lt;br /&gt;
&lt;img src=&#34;http://blog.mospan.cn/post/img/webhook/check-apiserver.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orga56c5e4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;创建rbac&#34;&gt;创建RBAC&lt;/h2&gt;

&lt;p&gt;由于我们的webhook会对资源进行修改，所以需要单独给一个ServiceAccount，在K8S集群中直接创建即可&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f deployment/rbac.yaml 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;效果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deployment]# kubectl apply -f rbac.yaml
serviceaccount/admission-webhook-example-sa unchanged
clusterrole.rbac.authorization.k8s.io/admission-webhook-example-cr unchanged
clusterrolebinding.rbac.authorization.k8s.io/admission-webhook-example-crb unchanged
[root@k8s-master deployment]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org57c61b3&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;证书认证&#34;&gt;证书认证&lt;/h2&gt;

&lt;p&gt;K8S集群默认是HTTPS通信的，所以APiserver调用webhook的过程也是HTTPS的，所以需要进行证书认证，证书认证相当于是给Service的域名进行认证（Service后面会创建），将Service域名放到认证请求server.csr文件中，然后创建一个K8S证书签署请求资源CertificateSigningRequest，APIServer签署该证书后生成server-cert.pem，再将最初创建的私钥server-key.pem和签署好的证书server-cert.pem放到Secret中供Deployment调用，详细过程看脚本&lt;br /&gt;
webhook-create-signed-cert.sh&lt;/p&gt;

&lt;p&gt;认证很简单，执行该脚本即可，会创建一个名为admission-webhook-example-certs的Secret&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./deployment/webhook-create-signed-cert.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这一步顺便把Service创建了，因为证书是给该Service的域名颁发的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f deployment/service.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org9750af4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;部署deployment&#34;&gt;部署Deployment&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;kubectl  apply -f deployment.yaml 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;稍等片刻如果有类似如下输出说明Pod已经运行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deployment]# kubectl  apply -f deployment.yaml
deployment.apps/admission-webhook-example-deployment unchanged
[root@k8s-master deployment]# kubectl  get pod -A -o wide | grep web
default       admission-webhook-example-deployment-7dd75cffb6-24bhg   1/1     Running   2 (13d ago)    57d    10.244.235.217   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
[root@k8s-master deployment]# kubectl  get pod -A  | grep web
default       admission-webhook-example-deployment-7dd75cffb6-24bhg   1/1     Running   2 (13d ago)    57d
[root@k8s-master deployment]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org7111e46&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;部署validatingwebhook&#34;&gt;部署ValidatingWebhook&lt;/h2&gt;

&lt;p&gt;首先包含一个namespaceSelector，表示此webhook只针对有admission-webhook-example标签的namespace生效，当然也可以去掉&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;namespaceSelector:
   matchLabels:
     admission-webhook-example: enabled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看编排文件deployment/validatingwebhook.yaml，里面有一个占位符${CA_BUNDLE}&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;clientConfig:
 service:
 name: admission-webhook-example-svc
 namespace: default
 path: &amp;quot;/validate&amp;quot;  # Path是我们自己定义的
 caBundle: ${CA_BUNDLE}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是什么呢？webhook是APIServer调用的，此时APIServer相当于是一个客服端，webhook是一个服务端，可以对比下平时上网，打开https网站时是谁在验证域名的证书？是内置在浏览器里面的根证书在做验证，所以这里的CA_BUNDLE就类似于APIServer调用webhook的根证书，它去验证webhook证书。&lt;/p&gt;

&lt;p&gt;所以先填充这个CA_BUNDLE后再执行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 填充占位符
cat deployment/validatingwebhook.yaml | ./deployment/webhook-patch-ca-bundle.sh &amp;gt; /tmp/validatingwebhook.yaml

# 部署
kubectl apply -f /tmp/validatingwebhook.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orge4b820e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;验证-1&#34;&gt;验证&lt;/h2&gt;

&lt;p&gt;1、给default namespace添加label&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl label namespace default admission-webhook-example=enabled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、部署sleep.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f deployment/sleep.yaml 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org3ee59df&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;faq&#34;&gt;FAQ&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org896c547&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;error-unable-to-recognize-stdin-no-matches-for-kind-certificatesigningrequest-in-version-certificates-k8s-io-v1beta1&#34;&gt;error: unable to recognize &amp;ldquo;STDIN&amp;rdquo;: no matches for kind &amp;ldquo;CertificateSigningRequest&amp;rdquo; in version &amp;ldquo;certificates.k8s.io/v1beta1&amp;rdquo;&lt;/h2&gt;

&lt;p&gt;当执行`bash webhook-create-signed-cert.sh`时出现如下错误：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;error: unable to recognize &amp;ldquo;STDIN&amp;rdquo;: no matches for kind &amp;ldquo;CertificateSigningRequest&amp;rdquo; in version &amp;ldquo;certificates.k8s.io/v1beta1&amp;rdquo;`&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;需修改webhook-create-signed-cert.sh相关内容为:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# create  server cert/key CSR and  send to k8s API
cat &amp;lt;&amp;lt;EOF | kubectl create -f -
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: ${csrName}
spec:
  groups:
  - system:authenticated
  request: $(cat ${tmpdir}/server.csr | base64 | tr -d &#39;\n&#39;)
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - digital signature
  - key encipherment
  - client auth
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;详见：&lt;a href=&#34;https://bytemeta.vip/repo/morvencao/kube-sidecar-injector/issues/29&#34;&gt;missing required field &amp;ldquo;signerName&amp;rdquo; #29&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orga1e1728&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;no-matches-for-kind-validatingwebhookconfiguration-in-version-admissionregistration-k8s-io-v1beta1&#34;&gt;no matches for kind &amp;ldquo;ValidatingWebhookConfiguration&amp;rdquo; in version &amp;ldquo;admissionregistration.k8s.io/v1beta1&amp;rdquo;&lt;/h2&gt;

&lt;p&gt;当执行`kubectl apply -f validatingwebhook.yaml`时出现错误：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;no matches for kind &amp;ldquo;ValidatingWebhookConfiguration&amp;rdquo; in version &amp;ldquo;admissionregistration.k8s.io/v1beta1&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;需改为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;admissionReviewVersions: [&amp;quot;v1&amp;quot;,&amp;quot;v1beta1&amp;quot;]
sideEffects: None
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;详见： &lt;a href=&#34;https://kodekloud.com/community/t/pls-help-me-in-configuring-using-opa-in-minikbe/28428/5&#34;&gt;Pls help me in configuring/using OPA in minikbe&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgc3c8eba&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;x509-certificate-specifies-an-incompatible-key-usage&#34;&gt;x509: certificate specifies an incompatible key usage&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;orga94e650&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;p&gt;【01】&lt;a href=&#34;https://zhuanlan.zhihu.com/p/404764407&#34;&gt;从0到1开发K8S_Webhook最佳实践&lt;/a&gt;]&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>CentOS7 下安装和配置 NFS</title>
            <link>http://mospany.github.io/2023/01/15/aliyun-ecs-install-nfs/</link>
            <pubDate>Sun, 15 Jan 2023 10:44:29 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2023/01/15/aliyun-ecs-install-nfs/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;org629d3e2&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;NFS（Network File System，网络文件系统）是当前主流异构平台共享文件系统之一。主要应用在UNIX环境下。最早是由Sun Microsystems开发，现在能够支持在不同类型的系统之间通过网络进行文件共享，广泛应用在FreeBSD、SCO、Solaris等异构操作系统平台，允许一个系统在网络上与他人共享目录和文件。通过使用NFS，用户和程序可以像访问本地文件一样访问远端系统上的文件，使得每个计算机的节点能够像使用本地资源一样方便地使用网上资源。换言之，NFS可用于不同类型计算机、操作系统、网络架构和传输协议运行环境中的网络文件远程访问和共享。&lt;/p&gt;

&lt;p&gt;NFS的工作原理是使用客户端/服务器架构，由一个客户端程序和服务器程序组成。服务器程序向其他计算机提供对文件系统的访问，其过程称为输出。NFS客户端程序对共享文件系统进行访问时，把它们从NFS服务器中“输送”出来。文件通常以块为单位进行传输。其大小是8KB（虽然它可能会将操作分成更小尺寸的分片）。NFS传输协议用于服务器和客户机之间文件访问和共享的通信，从而使客户机远程地访问保存在存储设备上的数据。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org043eb1b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;服务端搭建&#34;&gt;服务端搭建&lt;/h1&gt;

&lt;p&gt;由于实现CSI需要一个后端存储，Linux提供NFS功能可以免费搭建一个NSC存储功能用来验证。&lt;br /&gt;
搭建办法详见: &lt;a href=&#34;https://www.codeleading.com/article/35162638950/&#34;&gt;阿里云服务器 CentOS7 下安装和配置 NFS&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org31fb10d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;服务端安装&#34;&gt;服务端安装&lt;/h2&gt;

&lt;p&gt;使用 yum 安装 NFS 安装包。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install nfs-utils -y 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org768e380&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;服务端配置&#34;&gt;服务端配置&lt;/h2&gt;

&lt;p&gt;设置 NFS 服务开机启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# systemctl enable rpcbind
[root@k8s-master ~]# systemctl enable nfs
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org79e8e0b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;配置共享目录&#34;&gt;配置共享目录&lt;/h2&gt;

&lt;p&gt;服务启动之后，我们在服务端配置一个共享目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# mkdir /data
[root@k8s-master ~]# chmod 755 /data
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据这个目录，相应配置导出目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim /etc/exports
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加如下配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/data/     *(rw,sync,no_root_squash,no_all_squash)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;data: 共享目录位置。&lt;br /&gt;
192.168.0.0/24: 客户端 IP 范围，* 代表所有，即没有限制（我在实验中会设置为*）。&lt;br /&gt;
rw: 权限设置，可读可写。&lt;br /&gt;
sync: 同步共享目录。&lt;br /&gt;
no_root_squash: 可以使用 root 授权。&lt;br /&gt;
no_all_squash: 可以使用普通用户授权。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orge7422da&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;启动-nfs-服务&#34;&gt;启动 NFS 服务&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# systemctl start rpcbind
[root@k8s-master ~]# systemctl start nfs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org2db5bbe&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;防火墙需要打开-rpc-bind-和-nfs-的服务&#34;&gt;防火墙需要打开 rpc-bind 和 nfs 的服务&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# firewall-cmd --zone=public --permanent --add-service={rpc-bind,mountd,nfs}
FirewallD is not running
[root@k8s-master ~]# firewall-cmd --reload
FirewallD is not running
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org9febc6e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;检查&#34;&gt;检查&lt;/h2&gt;

&lt;p&gt;可以检查一下本地的共享目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# showmount -e localhost
Export list for localhost:
/data *
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样，服务端就配置好了。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgbfea31c&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;查看端口&#34;&gt;查看端口&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# rpcinfo -p localhost
program vers proto   port  service
 100000    4   tcp    111  portmapper
 100000    3   tcp    111  portmapper
 100000    2   tcp    111  portmapper
 100000    4   udp    111  portmapper
 100000    3   udp    111  portmapper
 100000    2   udp    111  portmapper
 100005    1   udp  20048  mountd
 100005    1   tcp  20048  mountd
 100005    2   udp  20048  mountd
 100024    1   udp  54689  status
 100005    2   tcp  20048  mountd
 100024    1   tcp  46564  status
 100005    3   udp  20048  mountd
 100005    3   tcp  20048  mountd
 100003    3   tcp   2049  nfs
 100003    4   tcp   2049  nfs
 100227    3   tcp   2049  nfs_acl
 100003    3   udp   2049  nfs
 100003    4   udp   2049  nfs
 100227    3   udp   2049  nfs_acl
 100021    1   udp  40110  nlockmgr
 100021    3   udp  40110  nlockmgr
 100021    4   udp  40110  nlockmgr
 100021    1   tcp  33742  nlockmgr
 100021    3   tcp  33742  nlockmgr
 100021    4   tcp  33742  nlockmgr
 [root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orga16c2a2&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;客户端连接-nfs&#34;&gt;客户端连接 NFS&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orge9dc5a1&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;在客户端创建目录&#34;&gt;在客户端创建目录&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# mkdir -p /mnt/nfs-data/
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgce10511&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;挂载&#34;&gt;挂载&lt;/h2&gt;

&lt;p&gt;为了测试方便， 服务端和客户端均在同一台服务器上。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# mount -t nfs 127.0.0.1:/data /mnt/nfs-data
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;挂载之后，可以使用 mount 命令查看一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# mount | grep nfs
nfsd on /proc/fs/nfsd type nfsd (rw,relatime)
sunrpc on /var/lib/nfs/rpc_pipefs type rpc_pipefs (rw,relatime)
127.0.0.1:/data on /mnt/nfs-data type nfs4 (rw,relatime,vers=4.1,rsize=524288,wsize=524288,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=127.0.0.1,local_lock=none,addr=127.0.0.1)
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这说明已经挂载成功了。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgc315b27&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;测试nfs&#34;&gt;测试NFS&lt;/h1&gt;

&lt;p&gt;测试一下，在客户端向共享目录创建一个文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# echo test &amp;gt; /mnt/nfs-data/bbb.txt
[root@k8s-master ~]#
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后取 NFS 服务端查看一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# echo test &amp;gt; /mnt/nfs-data/bbb.txt
[root@k8s-master ~]#
[root@k8s-master ~]# cat /data/bbb.txt
test
[root@k8s-master ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，共享目录已经写入了。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org6f32303&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;客户端自动挂载&#34;&gt;客户端自动挂载&lt;/h1&gt;

&lt;p&gt;自动挂载很常用，客户端设置一下即可。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vim /etc/fstab
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在结尾添加类似如下配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# cat /etc/fstab

 #
 # /etc/fstab
 # Created by anaconda on Thu Jul 11 02:52:01 2019
 #
 # Accessible filesystems, by reference, are maintained under &#39;/dev/disk&#39;
 # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info
 #
 UUID=1114fe9e-2309-4580-b183-d778e6d97397 /                       ext4    defaults        1 1
 127.0.0.1:/data     /mnt/nfs-data                   nfs     defaults        0 0
 [root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于修改了 /etc/fstab，需要重新加载 systemctl。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# systemctl daemon-reload
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org284ecce&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考&#34;&gt;参考&lt;/h1&gt;

&lt;p&gt;【01】&lt;a href=&#34;https://www.codeleading.com/article/35162638950/&#34;&gt;阿里云服务器 CentOS7 下安装和配置 NFS&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(05): 动手实现CSI驱动</title>
            <link>http://mospany.github.io/2022/12/22/k8s-sci-driver/</link>
            <pubDate>Thu, 22 Dec 2022 22:21:36 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/12/22/k8s-sci-driver/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;org3f75c1c&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;外部存储接入 Kubernetes 的方式主要有两种：In-Tree 和 Out-of-Tree。其中 In-Tree 是指存储驱动的源码都在 Kubernetes 代码库中，与 Kubernetes 一起发布、迭代、管理，这种方式灵活性较差，且门槛较高。Out-of-Tree 是指存储插件由第三方编写、发布、管理，作为一种扩展与 Kubernetes 配合使用。Out-of-Tree 主要有 FlexVolume 和 CSI 两种实现方式，其中，FlexVolume 因为其命令式的特点，不易维护和管理，从 Kubernetes v1.23 版本开始已被弃用。因此 CSI 已经成为 Kubernetes 存储扩展（ Out-of-Tree ）的唯一方式。&lt;/p&gt;

&lt;p&gt;在介绍CSI之前，先梳理一下kubernetes中使用存储有哪些步骤：&lt;br /&gt;
1、创建pv对象&lt;br /&gt;
2、存储服务器上创建一个volume&lt;br /&gt;
3、把创建的volume挂载到宿主机上&lt;br /&gt;
4、格式化volume&lt;br /&gt;
5、把格式化的volume mount到pod的volume目录&lt;br /&gt;
6、创建pvc对象并和pv对象绑定&lt;br /&gt;
7、pod使用pvc&lt;/p&gt;

&lt;p&gt;代码工程：&lt;a href=&#34;https://github.com/mospany/nfscsi.git&#34;&gt;https://github.com/mospany/nfscsi.git&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orga5011fc&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;术语&#34;&gt;术语&lt;/h1&gt;

&lt;table border=&#34;2&#34; cellspacing=&#34;0&#34; cellpadding=&#34;6&#34; rules=&#34;groups&#34; frame=&#34;hsides&#34;&gt;


&lt;colgroup&gt;
&lt;col  class=&#34;org-left&#34; /&gt;

&lt;col  class=&#34;org-left&#34; /&gt;

&lt;col  class=&#34;org-left&#34; /&gt;

&lt;col  class=&#34;org-left&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;缩写&lt;/th&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;英文全称&lt;/th&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;中文全称&lt;/th&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;pvc&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;PersistentVolumeClaim&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;持久卷声明&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;描述的是pod希望使用的持久化存储的属性&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;pv&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;PersistentVolume&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;持久卷&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;描述的是具体的持久化存储数据卷信息&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;storageClass&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;storageClass&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;存储类&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;创建pv的模板&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a id=&#34;org20530b7&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;csi组成&#34;&gt;CSI组成&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/csi/csi-component.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;通常情况下：CSI Driver = DaemonSet + Deployment(StatefuleSet) 。&lt;br /&gt;
其中:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;绿色部分：Identity、Node、Controller 是需要开发者自己实现的，被称为 Custom Components。&lt;/li&gt;
&lt;li&gt;粉色部分：node-driver-registrar、external-attacher、external-provisioner 组件是 Kubernetes 团队开发和维护的，被称为 External Components，它们都是以 sidecar 的形式与 Custom Components 配合使用的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orga3ae62a&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;custom-components&#34;&gt;Custom Components&lt;/h2&gt;

&lt;p&gt;Custom Components 本质是 3 个 gRPC Services：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Identity Service 顾名思义，主要用于对外暴露这个插件本身的信息，比如驱动的名称、驱动的能力等：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//github.com/container-storage-interface/spec/lib/go/csi/csi.pb.go
type IdentityServer interface {
    GetPluginInfo(context.Context, *GetPluginInfoRequest) (*GetPluginInfoResponse, error)
    GetPluginCapabilities(context.Context, *GetPluginCapabilitiesRequest) (*GetPluginCapabilitiesResponse, error)
    Probe(context.Context, *ProbeRequest) (*ProbeResponse, error)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Controller Service 主要定义一些 无需在宿主机上执行的操作，这也是与下文的 Node Service 最根本的区别。以 CreateVolume 为例，k8s 通过调用该方法创建底层存储。比如底层使用了某云供应商的云硬盘服务，开发者在 CreateVolume 方法实现中应该调用云硬盘服务的创建 / 订购云硬盘的 API，调用 API 这个操作是不需要在特定宿主机上执行的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// github.com/container-storage-interface/spec/lib/go/csi/csi.pb.go
type ControllerServer interface {
    CreateVolume(context.Context, *CreateVolumeRequest) (*CreateVolumeResponse, error)
    DeleteVolume(context.Context, *DeleteVolumeRequest) (*DeleteVolumeResponse, error)
    ControllerPublishVolume(context.Context, *ControllerPublishVolumeRequest) (*ControllerPublishVolumeResponse, error)
    ControllerUnpublishVolume(context.Context, *ControllerUnpublishVolumeRequest) (*ControllerUnpublishVolumeResponse, error)
    ValidateVolumeCapabilities(context.Context, *ValidateVolumeCapabilitiesRequest) (*ValidateVolumeCapabilitiesResponse, error)
    ListVolumes(context.Context, *ListVolumesRequest) (*ListVolumesResponse, error)
    GetCapacity(context.Context, *GetCapacityRequest) (*GetCapacityResponse, error)
    ControllerGetCapabilities(context.Context, *ControllerGetCapabilitiesRequest) (*ControllerGetCapabilitiesResponse, error)
    CreateSnapshot(context.Context, *CreateSnapshotRequest) (*CreateSnapshotResponse, error)
    DeleteSnapshot(context.Context, *DeleteSnapshotRequest) (*DeleteSnapshotResponse, error)
    ListSnapshots(context.Context, *ListSnapshotsRequest) (*ListSnapshotsResponse, error)
    ControllerExpandVolume(context.Context, *ControllerExpandVolumeRequest) (*ControllerExpandVolumeResponse, error)
    ControllerGetVolume(context.Context, *ControllerGetVolumeRequest) (*ControllerGetVolumeResponse, error)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Node Service 定义了 需要在宿主机上执行的操作，比如：mount、unmount。在前面的部署架构图中，Node Service 使用 Daemonset 的方式部署，也是为了确保 Node Service 会被运行在每个节点，以便执行诸如 mount 之类的指令。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// github.com/container-storage-interface/spec/lib/go/csi/csi.pb.go
type NodeServer interface {
    NodeStageVolume(context.Context, *NodeStageVolumeRequest) (*NodeStageVolumeResponse, error)
    NodeUnstageVolume(context.Context, *NodeUnstageVolumeRequest) (*NodeUnstageVolumeResponse, error)
    NodePublishVolume(context.Context, *NodePublishVolumeRequest) (*NodePublishVolumeResponse, error)
    NodeUnpublishVolume(context.Context, *NodeUnpublishVolumeRequest) (*NodeUnpublishVolumeResponse, error)
    NodeGetVolumeStats(context.Context, *NodeGetVolumeStatsRequest) (*NodeGetVolumeStatsResponse, error)
    NodeExpandVolume(context.Context, *NodeExpandVolumeRequest) (*NodeExpandVolumeResponse, error)
    NodeGetCapabilities(context.Context, *NodeGetCapabilitiesRequest) (*NodeGetCapabilitiesResponse, error)
    NodeGetInfo(context.Context, *NodeGetInfoRequest) (*NodeGetInfoResponse, error)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org7bc93bd&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/csi/nfs-csi.png&#34; alt=&#34;img&#34; /&gt;&lt;br /&gt;
1、首先肯定是要在一个进程里编程实现前面提到的三个gRPC服务对应的方法，例如IdentityServer下的三个方法、ControllerServer下的CreateVolume/DeleteVolume方法以及NodeServer下的NodePublishVolume/NodeUnpublishVolume方法等，并以unix sock的方式提供gRPC服务（后文会用到），并打包成镜像；&lt;/p&gt;

&lt;p&gt;2、把node-driver-registrar和CSI服务作为两个容器放到一个pod里，这样node-driver-registrar服务就可以用unix sock的方式访问CSI进程里的gRPC服务并且向kubelet注册；&lt;/p&gt;

&lt;p&gt;3、node-driver-registrar完成注册后，后续的Mount/Unmount等操作kubelet会直接通过unix sock访问CSI。这里有两层含义：第一层含义是kubelet会直接通过unix sock访问CSI，因此CSI需要用hostPath的方式把自己unix sock文件暴露；第二层含义是kubelet直接调用CSI服务，这意味着node-driver-registrar和CSI的这个pod应该是daemonSet形式部署的；&lt;/p&gt;

&lt;p&gt;4、把external-provisioner和CSI服务作为两个容器放到一个pod里，去实现Dynamic Provisioning功能。因为Dynamic Provisioning设计创建卷和删除卷，因此这个pod应该看做是有状态的，在部署上通常是带有选举的deployment部署或者副本数为1的statefulSet部署（如果需要Attach/Detach功能，也可以再加个容器把external-attacher放到这个pod中）。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org953691d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;csi注册流程&#34;&gt;CSI注册流程&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/csi/csi-reg.png&#34; alt=&#34;img&#34; /&gt;&lt;br /&gt;
相关的步骤释义如下：&lt;/p&gt;

&lt;p&gt;1、kubelet启动后基于fsnotify监听/var/lib/kubelet/plugins_registry目录；&lt;br /&gt;
2、node-driver-registrar启动通过启动参数中配置的CSI进程的sock文件，调CSI进程的GetPluginInfo方法获取CSI插件名称；&lt;br /&gt;
3、node-driver-registrar启动后在/var/lib/kubelet/plugins_registry目录下创建自己的sock文件{csiName}-reg.sock；&lt;br /&gt;
4、kubelet的watcher监听到/var/lib/kubelet/plugins_registry目录下有sock文件创建，把该sock文件信息存入内存中的desiredStateOfWorld对象中；&lt;br /&gt;
5、kubelet中有个reconciler协程周期性的检查desiredStateOfWorld对象和actualStateOfWorld对象中的数据差异，发现有新的CSI插件需要执行注册过程；&lt;br /&gt;
6、reconciler通过/var/lib/kubelet/plugins_registry/{csiName}-reg.sock，调用node-driver-registrar下的GetInfo方法获取CSI插件的名称和CSI进程的sock文件路径等信息；&lt;br /&gt;
7、reconciler通过上一步拿到的CSI进程sock文件，调用CSI进程下NodeGetInfo方法获取一些数据用于后续的Node和CSINode对象；&lt;br /&gt;
8、组装数据调apiServer接口更新本节点对应的Node对象的annotation；&lt;br /&gt;
9、组装数据调apiServer接口创建/更新对应的CSINode对象；&lt;br /&gt;
10、reconciler通过/var/lib/kubelet/plugins_registry/{csiName}-reg.sock，调用node-driver-registrar的NotifyRegistrationStatus方法，告知其注册结果。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org34e354f&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;nfs搭建&#34;&gt;NFS搭建&lt;/h1&gt;

&lt;p&gt;由于实现CSI需要一个后端存储，Linux提供NFS功能可以免费搭建一个NSC存储功能用来验证。&lt;br /&gt;
搭建办法详见: &lt;a href=&#34;http://blog.mospan.cn/2023/01/15/aliyun-ecs-install-nfs/&#34;&gt;CentOS7 下安装和配置 NFS&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgaf35c61&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;部署&#34;&gt;部署&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org82b326f&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;部署node&#34;&gt;部署node&lt;/h2&gt;

&lt;p&gt;进入代码工程中的deploy下运行命令`kubectl apply -f node.yaml`&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master nfscsi]# pwd
/root/nfscsi
[root@k8s-master nfscsi]# cd deploy/
[root@k8s-master deploy]# ls
node.yaml  provisioner.yaml
[root@k8s-master deploy]# ll
总用量 12
-rw-r--r-- 1 root root 3720 1月   8 22:18 node.yaml
-rw-r--r-- 1 root root 4413 1月   8 22:16 provisioner.yaml

[root@k8s-master deploy]# k apply -f node.yaml
serviceaccount/nfs-csi-node created
clusterrole.rbac.authorization.k8s.io/nfs-csi-node created
clusterrolebinding.rbac.authorization.k8s.io/nfs-csi-node created
daemonset.apps/nfs-csi-node created
[root@k8s-master deploy]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看POD的运行状态：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deploy]# k get pod -A -o wide | grep nfs
kube-system   nfs-csi-node-x28zj                         2/2     Running   0                10m   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
[root@k8s-master deploy]#
[root@k8s-master deploy]# k get ds -A
NAMESPACE     NAME           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                              AGE
kube-system   calico-node    3         3         3       3            3           kubernetes.io/os=linux                                     19d
kube-system   kube-proxy     3         3         3       3            3           kubernetes.io/os=linux                                     52d
kube-system   nfs-csi-node   1         1         1       1            1           kubernetes.io/hostname=k8s-master,kubernetes.io/os=linux   10m
[root@k8s-master deploy]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pod正常启动后，先查看node-driver-registrar的日志，注册正常：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master nfscsi]# kubectl logs -n kube-system nfs-csi-node-x28zj node-driver-registrar
I0115 07:45:31.451826       1 main.go:166] Version: v2.5.0
I0115 07:45:31.451864       1 main.go:167] Running node-driver-registrar in mode=registration
I0115 07:45:31.452324       1 main.go:191] Attempting to open a gRPC connection with: &amp;quot;/csi/csi.sock&amp;quot;
I0115 07:45:32.455714       1 main.go:198] Calling CSI driver to discover driver name
I0115 07:45:32.457464       1 main.go:208] CSI driver name: &amp;quot;nfscsi&amp;quot;
I0115 07:45:32.457493       1 node_register.go:53] Starting Registration Server at: /registration/nfscsi-reg.sock
I0115 07:45:32.457582       1 node_register.go:62] Registration Server started at: /registration/nfscsi-reg.sock
I0115 07:45:32.458992       1 node_register.go:92] Skipping HTTP server because endpoint is set to: &amp;quot;&amp;quot;
I0115 07:45:33.018081       1 main.go:102] Received GetInfo call: &amp;amp;InfoRequest{}
I0115 07:45:33.018292       1 main.go:109] &amp;quot;Kubelet registration probe created&amp;quot; path=&amp;quot;/var/lib/kubelet/plugins/csi-nfsplugin/registration&amp;quot;
I0115 07:45:33.623838       1 main.go:120] Received NotifyRegistrationStatus call: &amp;amp;RegistrationStatus{PluginRegistered:true,Error:,}
[root@k8s-master nfscsi]# ll /var/lib/kubelet/plugins/csi-nfsplugin/csi.sock
srwxr-xr-x 1 root root 0 1月  15 15:45 /var/lib/kubelet/plugins/csi-nfsplugin/csi.sock
[root@k8s-master nfscsi]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再看看自己编码nfs-csi容器日志，注册过程只调用了GetPluginInfo和NodeGetInfo&lt;br /&gt;
方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master nfscsi]# kubectl logs -n kube-system nfs-csi-node-x28zj nfs-csi
2023/01/15 07:45:31 driverName: nfscsi, version: N/A, nodeID: k8s-master
2023/01/15 07:45:31 grpc server start
2023/01/15 07:45:32 GetPluginInfo request
2023/01/15 07:45:33 NodeGetInfo request
[root@k8s-master nfscsi]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;node-driver-registrar和CSI进程的sock文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master nfscsi]# ll /var/lib/kubelet/plugins_registry/
总用量 0
srwx------ 1 root root 0 1月  15 15:45 nfscsi-reg.sock
[root@k8s-master nfscsi]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;node对象的annotation：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master nfscsi]# kubectl get node k8s-master  -oyaml| grep annotations -A 9
annotations:
  csi.volume.kubernetes.io/nodeid: &#39;{&amp;quot;nfscsi&amp;quot;:&amp;quot;k8s-master&amp;quot;}&#39;
  kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
  node.alpha.kubernetes.io/ttl: &amp;quot;0&amp;quot;
  projectcalico.org/IPv4Address: 172.25.140.216/20
  projectcalico.org/IPv4IPIPTunnelAddr: 10.244.235.192
  volumes.kubernetes.io/controller-managed-attach-detach: &amp;quot;true&amp;quot;
creationTimestamp: &amp;quot;2022-11-23T15:34:38Z&amp;quot;
labels:
  app: hdls-csi-controller
[root@k8s-master nfscsi]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后验证CSINode对象：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master nfscsi]# k get csinode -o wide
NAME         DRIVERS   AGE
k8s-master   1         52d
k8s-work1    0         52d
k8s-work2    0         52d
[root@k8s-master nfscsi]# kubectl get csinode k8s-master -o yaml
apiVersion: storage.k8s.io/v1
kind: CSINode
metadata:
  annotations:
    storage.alpha.kubernetes.io/migrated-plugins: kubernetes.io/aws-ebs,kubernetes.io/azure-disk,kubernetes.io/cinder,kubernetes.io/gce-pd
  creationTimestamp: &amp;quot;2022-11-23T15:34:38Z&amp;quot;
  name: k8s-master
  ownerReferences:
  - apiVersion: v1
    kind: Node
    name: k8s-master
    uid: cb4c3e59-66e5-40a0-a385-aa2072719381
  resourceVersion: &amp;quot;278990&amp;quot;
  uid: cf0f93f0-495e-49f2-88b4-6f7126ca6176
spec:
  drivers:
  - name: nfscsi
    nodeID: k8s-master
    topologyKeys: null
[root@k8s-master nfscsi]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这里我们成功完成并验证了CSI的注册。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deploy]# k logs -n kube-system nfs-csi-node-vxkc8  nfs-csi
2023/01/15 08:33:22 driverName: nfscsi, version: N/A, nodeID: k8s-master
2023/01/15 08:33:22 grpc server start
2023/01/15 08:33:23 GetPluginInfo request
2023/01/15 08:33:23 NodeGetInfo request
2023/01/15 09:42:25 NodeGetCapabilities request
2023/01/15 09:42:25 NodeGetCapabilities request
2023/01/15 09:42:25 NodeGetCapabilities request
2023/01/15 09:42:25 NodeGetCapabilities request
2023/01/15 09:42:25 NodePublishVolume request
2023/01/15 09:42:25 source: 127.0.0.1:/data/pvc-910384b5-b5eb-4196-b6d3-876f9679ed05, targetPath: /var/lib/kubelet/pods/e86fa053-7d1b-4330-a6f2-555d350b45e9/volumes/kubernetes.io~csi/pvc-910384b5-b5eb-4196-b6d3-876f9679ed05/mount, options: []
2023/01/15 09:43:20 NodeGetCapabilities request
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当POD执行CreateVolume是将调用nodeService里的NodePublishVolume方法进行mount操作，即把nfs里的pvc-子目录mount到宿主机pod目录卷下。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgccab5fd&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;部署provisioner&#34;&gt;部署provisioner&lt;/h2&gt;

&lt;p&gt;Dynamic Provisioning原理：所谓的Dynamic Provisioning，其实就是创建pvc后会自动创建卷和pv，并把pv和pvc绑定&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/csi/provisioner-follow.jpg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;部署运行:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deploy]# kubectl apply -f provisioner.yaml
storageclass.storage.k8s.io/nfscsi created
csidriver.storage.k8s.io/nfscsi created
serviceaccount/nfs-csi-provisioner created
clusterrole.rbac.authorization.k8s.io/nfs-csi-provisioner created
clusterrolebinding.rbac.authorization.k8s.io/nfs-csi-provisioner created
deployment.apps/nfs-csi-provisioner created
[root@k8s-master deploy]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;观察对应的provisioner是否起来：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deploy]# k get pod -A -o wide | grep nfs
kube-system   nfs-csi-node-vxkc8                         2/2     Running   0               10m     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   nfs-csi-provisioner-6f7db46646-77lqv       2/2     Running   0               2m20s   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
[root@k8s-master deploy]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgbd2779f&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;创建pvc&#34;&gt;创建PVC&lt;/h3&gt;

&lt;p&gt;准备一个如下的pvc yaml，apply该yaml：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master test]# pwd
/root/nfscsi/test
[root@k8s-master test]# cat pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: nfscsi
  resources:
    requests:
      storage: 1Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看是否会自动创建卷、pv，并和pvc绑定：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master test]# kubectl apply -f pvc.yaml
persistentvolumeclaim/test-pvc created
[root@k8s-master test]# k get pvc
NAME       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
test-pvc   Bound    pvc-910384b5-b5eb-4196-b6d3-876f9679ed05   1Gi        RWO            nfscsi         20s
[root@k8s-master test]# k get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM              STORAGECLASS   REASON   AGE
pvc-910384b5-b5eb-4196-b6d3-876f9679ed05   1Gi        RWO            Delete           Bound      default/test-pvc   nfscsi                  35s
[root@k8s-master test]# ll /mnt/nfs-data/
总用量 16
drwxr-xr-x 2 root root 4096 1月  15 17:06 pvc-910384b5-b5eb-4196-b6d3-876f9679ed05
[root@k8s-master test]# ll /data/
总用量 16
drwxr-xr-x 2 root root 4096 1月   8 22:22 pvc-7115a52d-ba4f-4571-adbc-25e71941ca55
[root@k8s-master test]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看provisioner pod日志：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master test]# k logs -n kube-system  nfs-csi-provisioner-6f7db46646-77lqv nfs-csi
2023/01/15 08:41:03 driverName: nfscsi, version: N/A, nodeID: k8s-master
2023/01/15 08:41:03 grpc server start
2023/01/15 08:41:04 Probe request
2023/01/15 08:41:04 GetPluginInfo request
2023/01/15 08:41:04 GetPluginCapabilities request
2023/01/15 08:41:04 ControllerGetCapabilities request
...
2023/01/15 09:06:26 CreateVolume request
2023/01/15 09:06:26 req name:  pvc-910384b5-b5eb-4196-b6d3-876f9679ed05
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出创建pvc时自动绑定pv成功。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org252c6f3&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;删除pvc&#34;&gt;删除PVC&lt;/h3&gt;

&lt;p&gt;同理。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org20891ab&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;测试&#34;&gt;测试&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orgaaf51bd&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;验证pod使用csi&#34;&gt;验证POD使用CSI&lt;/h2&gt;

&lt;p&gt;先准备POD的yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master test]# cat pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-nginx-sci-pod
spec:
  nodeName: k8s-master  # 运行在安装了csi插件的node上
  containers:
  - name: nginx
    image: nginx:latest
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - name: nfs-pvc
      mountPath: /var/log/nginx
  volumes:
  - name: nfs-pvc
    persistentVolumeClaim:
      claimName: test-pvc
   [root@k8s-master test]#  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master test]# kubectl apply -f pod.yaml
pod/test-nginx-sci-pod created
[root@k8s-master test]# kubectl get pod -A | grep test-nginx
default       test-nginx-sci-pod                         1/1     Running   0                18s
[root@k8s-master test]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看nginx日志&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master test]# ll /mnt/nfs-data/pvc-910384b5-b5eb-4196-b6d3-876f9679ed05/
总用量 4
-rw-r--r-- 1 root root    0 1月  15 17:48 access.log
-rw-r--r-- 1 root root 1641 1月  15 17:52 error.log
[root@k8s-master test]#
[root@k8s-master test]# tail /mnt/nfs-data/pvc-910384b5-b5eb-4196-b6d3-876f9679ed05/access.log
[root@k8s-master test]# tail /mnt/nfs-data/pvc-910384b5-b5eb-4196-b6d3-876f9679ed05/error.log
2023/01/15 09:49:20 [notice] 1#1: worker process 33 exited with code 0
2023/01/15 09:49:20 [notice] 1#1: exit
2023/01/15 09:51:39 [notice] 1#1: using the &amp;quot;epoll&amp;quot; event method
2023/01/15 09:51:39 [notice] 1#1: nginx/1.21.5
2023/01/15 09:51:39 [notice] 1#1: built by gcc 10.2.1 20210110 (Debian 10.2.1-6)
2023/01/15 09:51:39 [notice] 1#1: OS: Linux 3.10.0-957.21.3.el7.x86_64
2023/01/15 09:51:39 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 65536:65536
2023/01/15 09:51:39 [notice] 1#1: start worker processes
2023/01/15 09:51:39 [notice] 1#1: start worker process 32
2023/01/15 09:51:39 [notice] 1#1: start worker process 33
[root@k8s-master test]#
[root@k8s-master test]# ll /data/pvc-910384b5-b5eb-4196-b6d3-876f9679ed05/
总用量 4
-rw-r--r-- 1 root root    0 1月  15 17:48 access.log
-rw-r--r-- 1 root root 1641 1月  15 17:52 error.log
[root@k8s-master test]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出宿主机和nfs服务器上该pvc下已经有nginx日志生成达到了目的。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orga9b7cb5&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;csc功能测试命令&#34;&gt;csc功能测试命令&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go install -v  github.com/rexray/gocsi/csc@latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用go env查看GOPATH, go install 的程序一般就放在第一个路径下的bin&lt;/p&gt;

&lt;p&gt;把它拷贝到目标机上并加可执行权限。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master csi-hdls]# ./csc
NAME
    csc -- a command line container storage interface (CSI) client

SYNOPSIS
    csc [flags] CMD

AVAILABLE COMMANDS
    controller
    identity
    node

Use &amp;quot;csc -h,--help&amp;quot; for more information
[root@k8s-master csi-hdls]# ./csc controller help
NAME
    controller -- the csi controller service rpcs

SYNOPSIS
    csc controller [flags] CMD

AVAILABLE COMMANDS
    create-snapshot
    create-volume
    delete-snapshot
    delete-volume
    expand-volume
    get-capabilities
    get-capacity
    list-snapshots
    list-volumes
    publish
    unpublish
    validate-volume-capabilities

Use &amp;quot;csc controller -h,--help&amp;quot; for more information
[root@k8s-master csi-hdls]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;emptyDir的位置应该位于运行pod的给定节点上的/var/lib/kubelet/pods/{podid}/volumes/kubernetes.io~empty-dir/中&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orgd2a986a&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考&#34;&gt;参考&lt;/h1&gt;

&lt;p&gt;【01】&lt;a href=&#34;https://zhuanlan.zhihu.com/p/583032625&#34;&gt;如何实现一个 Kubernetes CSI Driver&lt;/a&gt;&lt;br /&gt;
【02】&lt;a href=&#34;https://zhuanlan.zhihu.com/p/539307741&#34;&gt;Kubernetes CSI 驱动开发指南&lt;/a&gt;&lt;br /&gt;
【03】&lt;a href=&#34;https://www.cnblogs.com/cxt618/p/15487359.html&#34;&gt;gRPC详细入门介绍&lt;/a&gt;&lt;br /&gt;
【04】&lt;a href=&#34;https://jishuin.proginn.com/p/763bfbd3890f&#34;&gt;如何编写一个 CSI 插件&lt;/a&gt;&lt;br /&gt;
【05】&lt;a href=&#34;http://www.noobyard.com/article/p-qgxfxmfi-nv.html&#34;&gt;Kubernetes K8S之固定节点nodeName和nodeSelector调度详解&lt;/a&gt;&lt;br /&gt;
【06】&lt;a href=&#34;https://www.modb.pro/db/523598&#34;&gt;kubernetes CSI（下）&lt;/a&gt;&lt;br /&gt;
【07】&lt;a href=&#34;https://www.codeleading.com/article/35162638950/&#34;&gt;阿里云服务器 CentOS7 下安装和配置 NFS&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(04): 基于kubebuilder编写operator</title>
            <link>http://mospany.github.io/2022/12/14/operator-on-kubebuilder/</link>
            <pubDate>Wed, 14 Dec 2022 18:24:22 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/12/14/operator-on-kubebuilder/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;orgcfa7382&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;介绍&#34;&gt;介绍&lt;/h1&gt;

&lt;p&gt;Kubebuilder是一个由controller-runtime支持的出色SDK，它可以帮助您轻松快速地在 Go 中编写Kubernetes operator，方法是处理多种忙碌的事情，例如以组织良好的方式引导大量样板代码，设置有用的 Makefile make，目标是构建、运行和部署operator、构建 CRD、设置相关的 Dockefile、RBAC、涉及部署operator的多个 YAML 等等。&lt;br /&gt;
Kubebuilder 是一个使用 CRDs 构建 K8s API 的 SDK，主要是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;提供脚手架工具初始化 CRDs 工程，自动生成 boilerplate 代码和配置；&lt;/li&gt;
&lt;li&gt;提供代码库封装底层的 K8s go-client；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;方便用户从零开始开发 CRDs，Controllers 和 Admission Webhooks 来扩展 K8s。&lt;/p&gt;

&lt;p&gt;为了编写自定义的operatro，需要进行如下:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;安装kubebuilder&lt;/li&gt;
&lt;li&gt;使用kubebuilder进行项目创建&lt;/li&gt;
&lt;li&gt;编写operator代码&lt;/li&gt;
&lt;li&gt;编译打包镜像上传到镜像仓库。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a id=&#34;org0fa6862&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/kubebuilder-arch.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org78ac50d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;基础概念&#34;&gt;基础概念&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;org52cd124&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;gvks-gvrs&#34;&gt;GVKs&amp;amp;GVRs&lt;/h3&gt;

&lt;p&gt;GVK = GroupVersionKind，GVR = GroupVersionResource。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;API Group &amp;amp; Versions（GV）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;API Group 是相关 API 功能的集合，每个 Group 拥有一或多个 Versions，用于接口的演进。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kinds &amp;amp; Resources（GVR）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每个 GV 都包含多个 API 类型，称为 Kinds，在不同的 Versions 之间同一个 Kind 定义可能不同， Resource 是 Kind 的对象标识（resource type），一般来说 Kinds 和 Resources 是 1:1 的，比如 pods Resource 对应 Pod Kind，但是有时候相同的 Kind 可能对应多个 Resources，比如 Scale Kind 可能对应很多 Resources：deployments/scale，replicasets/scale，对于 CRD 来说，只会是 1:1 的关系。&lt;/p&gt;

&lt;p&gt;每一个 GVK 都关联着一个 package 中给定的 root Go type，比如 apps/v1/Deployment 就关联着 K8s 源码里面 k8s.io/api/apps/v1 package 中的 Deployment struct，我们提交的各类资源定义 YAML 文件都需要写：&lt;/p&gt;

&lt;p&gt;apiVersion：这个就是 GV 。kind：这个就是 K。&lt;/p&gt;

&lt;p&gt;根据 GVK K8s 就能找到你到底要创建什么类型的资源，根据你定义的 Spec 创建好资源之后就成为了 Resource，也就是 GVR。GVK/GVR 就是 K8s 资源的坐标，是我们创建/删除/修改/读取资源的基础。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org691e4a3&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;scheme&#34;&gt;Scheme&lt;/h3&gt;

&lt;p&gt;每一组 Controllers 都需要一个 Scheme，提供了 Kinds 与对应 Go types 的映射，也就是说给定 Go type 就知道他的 GVK，给定 GVK 就知道他的 Go type，比如说我们给定一个 Scheme: &amp;ldquo;tutotial.kubebuilder.io/api/v1&amp;rdquo;.CronJob{} 这个 Go type 映射到 batch.tutotial.kubebuilder.io/v1 的 CronJob GVK，那么从 Api Server 获取到下面的 JSON:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &amp;quot;kind&amp;quot;: &amp;quot;CronJob&amp;quot;,
    &amp;quot;apiVersion&amp;quot;: &amp;quot;batch.tutorial.kubebuilder.io/v1&amp;quot;,
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就能构造出对应的 Go type了，通过这个 Go type 也能正确地获取 GVR 的一些信息，控制器可以通过该 Go type 获取到期望状态以及其他辅助信息进行调谐逻辑。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgc45a76f&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;manager&#34;&gt;Manager&lt;/h3&gt;

&lt;p&gt;Kubebuilder 的核心组件，具有 3 个职责：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;负责运行所有的 Controllers；&lt;/li&gt;
&lt;li&gt;初始化共享 caches，包含 listAndWatch 功能；&lt;/li&gt;
&lt;li&gt;初始化 clients 用于与 Api Server 通信。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org50d3209&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;cache&#34;&gt;Cache&lt;/h3&gt;

&lt;p&gt;Kubebuilder 的核心组件，负责在 Controller 进程里面根据 Scheme 同步 Api Server 中所有该 Controller 关心 GVKs 的 GVRs，其核心是 GVK -&amp;gt; Informer 的映射，Informer 会负责监听对应 GVK 的 GVRs 的创建/删除/更新操作，以触发 Controller 的 Reconcile 逻辑。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org7fb1547&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;controller&#34;&gt;Controller&lt;/h3&gt;

&lt;p&gt;Kubebuidler 为我们生成的脚手架文件，我们只需要实现 Reconcile 方法即可。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org528499e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;client&#34;&gt;Client&lt;/h3&gt;

&lt;p&gt;在实现 Controller 的时候不可避免地需要对某些资源类型进行创建/删除/更新，就是通过该 Clients 实现的，其中查询功能实际查询是本地的 Cache，写操作直接访问 Api Server。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org16de8d6&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;index&#34;&gt;Index&lt;/h3&gt;

&lt;p&gt;由于 Controller 经常要对 Cache 进行查询，Kubebuilder 提供 Index utility 给 Cache 加索引提升查询效率。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org607691c&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;finalizer&#34;&gt;Finalizer&lt;/h3&gt;

&lt;p&gt;在一般情况下，如果资源被删除之后，我们虽然能够被触发删除事件，但是这个时候从 Cache 里面无法读取任何被删除对象的信息，这样一来，导致很多垃圾清理工作因为信息不足无法进行，K8s 的 Finalizer 字段用于处理这种情况。在 K8s 中，只要对象 ObjectMeta 里面的 Finalizers 不为空，对该对象的 delete 操作就会转变为 update 操作，具体说就是 update deletionTimestamp 字段，其意义就是告诉 K8s 的 GC“在deletionTimestamp 这个时刻之后，只要 Finalizers 为空，就立马删除掉该对象”。&lt;/p&gt;

&lt;p&gt;所以一般的使用姿势就是在创建对象时把 Finalizers 设置好（任意 string），然后处理 DeletionTimestamp 不为空的 update 操作（实际是 delete），根据 Finalizers 的值执行完所有的 pre-delete hook（此时可以在 Cache 里面读取到被删除对象的任何信息）之后将 Finalizers 置为空即可。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org0c0efe9&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;ownerreference&#34;&gt;OwnerReference&lt;/h3&gt;

&lt;p&gt;K8s GC 在删除一个对象时，任何 ownerReference 是该对象的对象都会被清除，与此同时，Kubebuidler 支持所有对象的变更都会触发 Owner 对象 controller 的 Reconcile 方法。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgb1ddf8b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;实现&#34;&gt;实现&lt;/h1&gt;

&lt;p&gt;1、构造新的scheme,&lt;br /&gt;
2、解析命令行参数，获取或默认metric和健康检查端口。&lt;br /&gt;
3、实例化 manager，参数 config&lt;br /&gt;
   3.1） 向 manager 添加 scheme&lt;br /&gt;
   3.2） 向 manager 添加 controller，该 controller 包含一个 reconciler 结构体，我们需要在 reconciler 结构体实现逻辑处理&lt;br /&gt;
4、向manager添加healthz和readyz探测。&lt;br /&gt;
5、启动 manager.start()&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org40fbcb7&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;镜像仓库&#34;&gt;镜像仓库&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org246c0bb&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;注册仓库&#34;&gt;注册仓库&lt;/h2&gt;

&lt;p&gt;登录&lt;a href=&#34;https://registry.hub.docker.com/进行注册，如用户名为mospany&#34;&gt;https://registry.hub.docker.com/进行注册，如用户名为mospany&lt;/a&gt;, 密码为自定义。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org43ccc0d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;使用镜像&#34;&gt;使用镜像&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;org829ca45&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;登录仓库&#34;&gt;登录仓库&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ docker login index.docker.io
Username: mospany
Password:
Login Succeeded

或直接 docker login默认登录docker hub。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org2fc914e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;上传镜像&#34;&gt;上传镜像&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ docker tag loggen:latest mospany/loggen:latest
$ docker push mospany/loggen:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/hub-docker.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;参见：&lt;a href=&#34;https://hub.docker.com/repository/docker/mospany/loggen&#34;&gt;https://hub.docker.com/repository/docker/mospany/loggen&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org3d0e3a5&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;下载镜像&#34;&gt;下载镜像&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ docker pull mospany/loggen:latest
latest: Pulling from mospany/loggen
Digest: sha256:0cdeece36f8a003dd6b9c463cc73dad93479deabec08c1def033e72ec9818539
Status: Image is up to date for mospany/loggen:latest
docker.io/mospany/loggen:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgc8a2f44&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;项目&#34;&gt;项目&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org3a9cf00&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;创建项目&#34;&gt;创建项目&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;mkdir guestbook
cd guestbook
go mod init guestbook
kubebuilder init --domain xiaohongshu.org --owner &amp;quot;luxiu&amp;quot;
kubebuilder create api --group redis  --version v1 --kind RedisCluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关键截图如下：&lt;br /&gt;
   &lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/kubebuilder-operator.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org1ddad49&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;修改文件&#34;&gt;修改文件&lt;/h2&gt;

&lt;p&gt;1）修改Dockerfile的gcr.io镜像为其他可访问镜像(如golang:1.18)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;为了防止出现“failed to solve with frontend dockerfile.v0: failed to create LLB definition: failed to do request: Head &amp;ldquo;&lt;a href=&#34;https://gcr.io/v2/distroless/static/manifests/nonroot&#34;&gt;https://gcr.io/v2/distroless/static/manifests/nonroot&lt;/a&gt;&amp;ldquo;: Service Unavailable”错误，需修改Dockerfile的gcr.io镜像为其他可访问镜像(如golang:1.18)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;2）修改Dockerfile添加代理&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;为了防止go mod download时不至于超时连不上，需在Run go mod download行上面添加&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;ENV GOPROXY=&amp;quot;https://goproxy.cn&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;否则会出现如下错误：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;3.469 go: cloud.google.com/go@v0.81.0: Get &amp;ldquo;&lt;a href=&#34;https://proxy.golang.org/cloud.google.com/go/@v/v0.81.0.mod&#34;&gt;https://proxy.golang.org/cloud.google.com/go/@v/v0.81.0.mod&lt;/a&gt;&amp;ldquo;: malformed HTTP response &amp;ldquo;\x00\x00\x12\x04&amp;#x2026;\x00\x00\x01&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;**切记切记**： 先修改好再编译，否则一直出现上面错误(当时找了半天)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Build the manager binary
  FROM golang:1.18 as builder
  ENV GOPROXY=&amp;quot;https://goproxy.cn&amp;quot;

  WORKDIR /workspace
  # Copy the Go Modules manifests
  COPY go.mod go.mod
  COPY go.sum go.sum
  # cache deps before building and copying source so that we don&#39;t need to re-download as much
  # and so that source changes don&#39;t invalidate our downloaded layer
  RUN go mod download

  # Copy the go source
  COPY main.go main.go
  COPY api/ api/
  COPY controllers/ controllers/

  # Build
  RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -o manager main.go

  # Use distroless as minimal base image to package the manager binary
  # Refer to https://github.com/GoogleContainerTools/distroless for more details
  #FROM gcr.io/distroless/static:nonroot
  FROM centos:latest
  WORKDIR /
  COPY --from=builder /workspace/manager .
  USER 65532:65532

  ENTRYPOINT [&amp;quot;/manager&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;必须在Dockerfile里面设置代理”ENV GOPROXY=&amp;rdquo;&lt;https://goproxy.cn&#34;“才行，如下设置也不行&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/docker-preferences.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;3）修改Makefile中的crd中的配置&lt;br /&gt;
  给kubectl加上所要连接的集群， 如本机为&amp;#x2013;context docker-desktop。可通过如下命令获得：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config get-contexts
kubectl cluster-info
kubectl config view
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/kubectl-info.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;对应的Makefile修改如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.PHONY: install
install: manifests kustomize ## Install CRDs into the K8s cluster specified in ~/.kube/config.
  $(KUSTOMIZE) build config/crd | kubectl --context docker-desktop  apply -f -

.PHONY: uninstall
uninstall: manifests kustomize ## Uninstall CRDs from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.
  $(KUSTOMIZE) build config/crd | kubectl --context docker-desktop  delete --ignore-not-found=$(ignore-not-found) -f -

.PHONY: deploy
deploy: manifests kustomize ## Deploy controller to the K8s cluster specified in ~/.kube/config.
  cd config/manager &amp;amp;&amp;amp; $(KUSTOMIZE) edit set image controller=${IMG}
  $(KUSTOMIZE) build config/default | kubectl --context docker-desktop  apply -f -

.PHONY: undeploy
undeploy: ## Undeploy controller from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.
  $(KUSTOMIZE) build config/default | kubectl --context docker-desktop  delete --ignore-not-found=$(ignore-not-found) -f -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgf288f34&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;编译&#34;&gt;编译&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;org99eec51&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;make-help&#34;&gt;make help&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;make help
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/make-help.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org54938e4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;make-build&#34;&gt;make build&lt;/h3&gt;

&lt;p&gt;编译并在bin/下生成目标可执行程序。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/make-build.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orge8e0f0a&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;make-install&#34;&gt;make install&lt;/h3&gt;

&lt;p&gt;安装crd到目标集群，这一步可能受github网络影响自动下载kustomize慢需要多试几次或隔天再试。&lt;br /&gt;
   &lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/make-install.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orge3f35bb&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;make-docker-build&#34;&gt;make docker-build&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/make-docker-build.png&#34; alt=&#34;img&#34; /&gt;&lt;br /&gt;
可以在刚生成的镜像列表中生成镜像。&lt;br /&gt;
&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/make-docker-images.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org54b15f4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;make-docker-push&#34;&gt;make docker-push&lt;/h3&gt;

&lt;p&gt;1）先增加要上传镜像的tag&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker tag controller:latest docker.io/mospany/controller:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2）make docker-push&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ make docker-push IMG=docker.io/mospany/controller:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/make-docker-push.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;查看docker hub上传效果&lt;br /&gt;
&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/make-docker-hub.png&#34; alt=&#34;img&#34; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a id=&#34;org5f3af46&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;运行&#34;&gt;运行&lt;/h2&gt;

&lt;p&gt;先在mac上安装k8s集群，详见：&lt;a href=&#34;https://blog.51cto.com/zlyang/4838042&#34;&gt;Mac系统安装k8s集群&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org861d0fa&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;本地运行&#34;&gt;本地运行&lt;/h3&gt;

&lt;p&gt;要想在本地运行 controller，只需要执行下面的命令，你将看到 controller 启动和运行时输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make run
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/make-run.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org5dc2c10&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;部署到k8s集群中运行&#34;&gt;部署到k8s集群中运行&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;make deploy IMG=docker.io/mospany/controller:v1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/make-deploy.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;查看日志&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl logs -n guestbook-system guestbook-controller-manager-7c67b5bd6c-gm5qs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/make-logs.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgf9f8f6e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;创建cr&#34;&gt;创建CR&lt;/h2&gt;

&lt;p&gt;该创建自定义资源对象CR了，如原生中的rc/deployment等对象&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# mosp @ mospdeMacBook-Pro in ~/work/pingan/arch/mysrc/guestbook [21:14:17]
$ kubectl get RedisCluster
NAME                  AGE
rediscluster-sample   48m

# mosp @ mospdeMacBook-Pro in ~/work/pingan/arch/mysrc/guestbook [21:22:49]
$ kubectl get RedisCluster -o yaml
apiVersion: v1
items:
- apiVersion: redis.xiaohongshu.org/v1
  kind: RedisCluster
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {&amp;quot;apiVersion&amp;quot;:&amp;quot;redis.xiaohongshu.org/v1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;RedisCluster&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;rediscluster-sample&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;default&amp;quot;},&amp;quot;spec&amp;quot;:null}
    creationTimestamp: &amp;quot;2022-07-29T12:34:11Z&amp;quot;
    generation: 1
    name: rediscluster-sample
    namespace: default
    resourceVersion: &amp;quot;200052&amp;quot;
    uid: 18eaf75f-9597-46af-bd88-abf7153c1377
  status: {}
kind: List
metadata:
  resourceVersion: &amp;quot;&amp;quot;
  selfLink: &amp;quot;&amp;quot;

# mosp @ mospdeMacBook-Pro in ~/work/pingan/arch/mysrc/guestbook [21:23:07]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org7b598e7&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;开发业务逻辑&#34;&gt;开发业务逻辑&lt;/h2&gt;

&lt;p&gt;下面我们将修改 CRD 的数据结构并在 controller 中增加一些日志输出。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org869a73b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;修改-crd&#34;&gt;修改 CRD&lt;/h3&gt;

&lt;p&gt;我们将修改api/v1/rediscluster_types.go 文件的内容，在 CRD 中增加 FirstName、LastName 和 Status 字段。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// RedisClusterSpec defines the desired state of RedisCluster
type RedisClusterSpec struct {
    // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster

    // Important: Run &amp;quot;make&amp;quot; to regenerate code after modifying this file

    // Foo is an example field of RedisCluster. Edit rediscluster_types.go to remove/update
    FirstName string `json:&amp;quot;firstname&amp;quot;`
    LastName  string `json:&amp;quot;lastname&amp;quot;`
}

// RedisClusterStatus defines the observed state of RedisCluster
type RedisClusterStatus struct {
    // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster

    // Important: Run &amp;quot;make&amp;quot; to regenerate code after modifying this file
    Status string `json:&amp;quot;Status&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orga981c1e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;修改-reconcile-函数&#34;&gt;修改 Reconcile 函数&lt;/h3&gt;

&lt;p&gt;Reconcile 函数是 Operator 的核心逻辑，Operator 的业务逻辑都位于 controllers/rediscluster_controller.go 文件的 Reconcile 函数中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *RedisClusterReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
    _ = log.FromContext(ctx)

    // TODO(user): your logic here

    // 获取当前的 CR，并打印
    logger := log.FromContext(ctx)
    obj := &amp;amp;redisv1.RedisCluster{}
    if err := r.Get(ctx, req.NamespacedName, obj); err != nil {
        logger.Error(err, &amp;quot;Unable to fetch object&amp;quot;)
        return ctrl.Result{}, nil
    } else {
        logger.Info(&amp;quot;Greeting from Kubebuilder to&amp;quot;, obj.Spec.FirstName, obj.Spec.LastName)
    }

    // 初始化 CR 的 Status 为 Running
    obj.Status.Status = &amp;quot;Running&amp;quot;
    if err := r.Status().Update(ctx, obj); err != nil {
        logger.Error(err, &amp;quot;unable to update status&amp;quot;)
    }

    return ctrl.Result{}, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org1cfb9cb&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;运行测试&#34;&gt;运行测试&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;安装CRD（同上）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;部署controller（同上）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建CR&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;修改 config/samples/redis_v1_rediscluster.yaml 文件中的配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: redis.xiaohongshu.org/v1
kind: RedisCluster
metadata:
  name: rediscluster-sample
spec:
  # TODO(user): Add fields here
  firstname: Jimmy
  lastname: Song
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行下面命令，创建CR：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k8sdev apply -f  config/samples/redis_v1_rediscluster.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看controller里的运行日志：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/controller-logs.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org1683514&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;p&gt;【01】&lt;a href=&#34;https://blog.csdn.net/weixin_43568070/article/details/89892620&#34;&gt;使用shell命令行登陆Docker Hub出现的404Not found的问题&lt;/a&gt;&lt;br /&gt;
【02】&lt;a href=&#34;https://blog.csdn.net/HaHa_Sir/article/details/119412754&#34;&gt;Docker镜像推送Dockerhub&lt;/a&gt;&lt;br /&gt;
【03】&lt;a href=&#34;https://www.cnblogs.com/mysql-dba/p/15982341.html&#34;&gt;使用 kubebuilder 创建并部署 k8s-operator&lt;/a&gt;&lt;br /&gt;
【04】&lt;a href=&#34;http://tnblog.net/hb/article/details/7516&#34;&gt;Kustomize的基本使用&lt;/a&gt;&lt;br /&gt;
【05】&lt;a href=&#34;https://www.cnblogs.com/lizhewei/p/13214785.html&#34;&gt;【kubebuilder2.0】安装、源码分析 &lt;/a&gt;&lt;br /&gt;
【06】&lt;a href=&#34;https://www.cnblogs.com/alisystemsoftware/p/11580202.html&#34;&gt;深入解析 Kubebuilder：让编写 CRD 变得更简单&lt;/a&gt;&lt;br /&gt;
【07】&lt;a href=&#34;https://os.51cto.com/article/661378.html&#34;&gt;一篇带给你KubeBuilder 简明教程&lt;/a&gt;&lt;br /&gt;
【08】&lt;a href=&#34;https://blog.csdn.net/chenxy02/article/details/125554680&#34;&gt;深入解析Kubebuilder&lt;/a&gt;&lt;br /&gt;
【09】&lt;a href=&#34;https://blog.csdn.net/qq_45874107/article/details/119839187&#34;&gt;什么是RBAC&lt;/a&gt;&lt;br /&gt;
【10】&lt;a href=&#34;https://blog.ihypo.net/15763910382218.html&#34;&gt;Kubernetes Controller Manager 工作原理&lt;/a&gt;&lt;br /&gt;
【11】&lt;a href=&#34;https://jishuin.proginn.com/p/763bfbd3012b&#34;&gt;controller-runtime 之 manager 实现&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(03): 随机调度器</title>
            <link>http://mospany.github.io/2022/12/11/k8s-random-scheduler/</link>
            <pubDate>Sun, 11 Dec 2022 11:51:52 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/12/11/k8s-random-scheduler/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;org3ccc143&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org6748da5&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;pod创建过程&#34;&gt;POD创建过程&lt;/h2&gt;

&lt;p&gt;1、 由kubectl解析创建pod的yaml，发送创建pod请求到APIServer。&lt;br /&gt;
2、 APIServer首先做权限认证，然后检查信息并把数据存储到ETCD里，创建deployment资源初始化。&lt;br /&gt;
3、 kube-controller通过list-watch机制，检查发现新的deployment，将资源加入到内部工作队列，检查到资源没有关联pod和replicaset,然后创建rs资源，rs controller监听到rs创建事件后再创建pod资源。&lt;br /&gt;
4、 scheduler 监听到pod创建事件，执行调度算法，将pod绑定到合适节点，然后告知APIServer更新pod的spec.nodeName&lt;br /&gt;
5、 kubelet 每隔一段时间通过其所在节点的NodeName向APIServer拉取绑定到它的pod清单，并更新本地缓存。&lt;br /&gt;
6、 kubelet发现新的pod属于自己，调用容器API来创建容器，并向APIService上报pod状态。&lt;br /&gt;
7、 Kub-proxy为新创建的pod注册动态DNS到CoreOS。为Service添加iptables/ipvs规则，用于服务发现和负载均衡。&lt;br /&gt;
8、 deploy controller对比pod的当前状态和期望来修正状态。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/scheduler/pod-life-cycle.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org44af008&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;调度器介绍&#34;&gt;调度器介绍&lt;/h2&gt;

&lt;p&gt;从上述流程中，我们能大概清楚kube-scheduler的主要工作，负责整个k8s中pod选择和绑定node的工作，这个选择的过程就是应用调度策略，包括NodeAffinity、PodAffinity、节点资源筛选、调度优先级、公平调度等等，而绑定便就是将pod资源定义里的nodeName进行更新。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org39ad97e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;设计&#34;&gt;设计&lt;/h1&gt;

&lt;p&gt;kube-scheduler的设计有两个历史阶段版本：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;基于谓词（predicate）和优先级（priority）的筛选。&lt;/li&gt;
&lt;li&gt;基于调度框架的调度器，新版本已经把所有的旧的设计都改造成扩展点插件形式(1.19+)。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所谓的谓词和优先级都是对调度算法的分类，在scheduler里，谓词调度算法是来选择出一组能够绑定pod的node，而优先级算法则是在这群node中进行打分，得出一个最高分的node。&lt;/p&gt;

&lt;p&gt;而调度框架的设计相比之前则更复杂一点，但确更加灵活和便于扩展，关于调度框架的设计细节可以查看官方文档——624-scheduling-framework，当然我也有一遍文章对其做了翻译还加了一些便于理解的补充——KEP: 624-scheduling-framework。总结来说调度框架的出现是为了解决以前webhooks扩展器的局限性，一个是扩展点只有：筛选、打分、抢占、绑定，而调度框架则在这之上又细分了11个扩展点；另一个则是通过http调用扩展进程的方式其实效率不高，调度框架的设计用的是静态编译的方式将扩展的程序代码和scheduler源码一起编译成新的scheduler，然后通过scheduler配置文件启用需要的插件，在进程内就能通过函数调用的方式执行插件。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/scheduler/scheduler-startup.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上面是一个简略版的调度器处理pod流程：&lt;/p&gt;

&lt;p&gt;首先scheduler会启动一个client-go的Informer来监听Pod事件（不只Pod其实还有Node等资源变更事件），这时候注册的Informer回调事件会区分Pod是否已经被调度（spec.nodeName），已经调度过的Pod则只是更新调度器缓存，而未被调度的Pod会加入到调度队列，然后经过调度框架执行注册的插件，在绑定周期前会进行Pod的假定动作，从而更新调度器缓存中该Pod状态，最后在绑定周期执行完向ApiServer发起BindAPI，从而完成了一次调度过程。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org8ebc078&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;实现&#34;&gt;实现&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org98f2089&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;创建调度器&#34;&gt;创建调度器&lt;/h2&gt;

&lt;p&gt;1、获取集群kubeconfig配置。&lt;br /&gt;
2、调用client-go生成clientset。&lt;br /&gt;
3、填充调度器相关参数, 包含获取node列表和关注的POD。&lt;br /&gt;
4、返回调度器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; func NewScheduler(podQueue chan *v1.Pod, quit chan struct{}) Scheduler {
  config, err := rest.InClusterConfig()
  if err != nil {
      log.Fatal(err)
  }

  clientset, err := kubernetes.NewForConfig(config)
  if err != nil {
      log.Fatal(err)
  }

  return Scheduler{
      clientset:  clientset,
      podQueue:   podQueue,
      nodeLister: initInformers(clientset, podQueue, quit),
      predicates: []predicateFunc{
          randomPredicate,
      },
      priorities: []priorityFunc{
          randomPriority,
      },
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org7e3d0d6&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;运行调度器&#34;&gt;运行调度器&lt;/h2&gt;

&lt;p&gt;不间断的从关注的POD列表中选出进行调度。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org9082797&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;找合适节点&#34;&gt;找合适节点&lt;/h3&gt;

&lt;p&gt;1、找到可用节点列表&lt;br /&gt;
2、给节点随机打100以内的分数&lt;br /&gt;
3、选择分数最高的节点&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (s *Scheduler) findFit(pod *v1.Pod) (string, error) {
    nodes, err := s.nodeLister.List(labels.Everything())
    if err != nil {
        return &amp;quot;&amp;quot;, err
    }

    filteredNodes := s.runPredicates(nodes, pod)
    if len(filteredNodes) == 0 {
        return &amp;quot;&amp;quot;, errors.New(&amp;quot;failed to find node that fits pod&amp;quot;)
    }
    priorities := s.prioritize(filteredNodes, pod)
    return s.findBestNode(priorities), nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org0354198&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;绑定pod&#34;&gt;绑定POD&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;func (s *Scheduler) bindPod(ctx context.Context, p *v1.Pod, node string) error {
    opts := metav1.CreateOptions{}
    return s.clientset.CoreV1().Pods(p.Namespace).Bind(ctx, &amp;amp;v1.Binding{
        ObjectMeta: metav1.ObjectMeta{
            Name:      p.Name,
            Namespace: p.Namespace,
        },
        Target: v1.ObjectReference{
            APIVersion: &amp;quot;v1&amp;quot;,
            Kind:       &amp;quot;Node&amp;quot;,
            Name:       node,
        },
    }, opts)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgcdd0603&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;发送event事件&#34;&gt;发送event事件&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;func (s *Scheduler) emitEvent(ctx context.Context, p *v1.Pod, message string) error {
    timestamp := time.Now().UTC()
    opts := metav1.CreateOptions{}
    _, err := s.clientset.CoreV1().Events(p.Namespace).Create(ctx, &amp;amp;v1.Event{
        Count:          1,
        Message:        message,
        Reason:         &amp;quot;Scheduled&amp;quot;,
        LastTimestamp:  metav1.NewTime(timestamp),
        FirstTimestamp: metav1.NewTime(timestamp),
        Type:           &amp;quot;Normal&amp;quot;,
        Source: v1.EventSource{
            Component: schedulerName,
        },
        InvolvedObject: v1.ObjectReference{
            Kind:      &amp;quot;Pod&amp;quot;,
            Name:      p.Name,
            Namespace: p.Namespace,
            UID:       p.UID,
        },
        ObjectMeta: metav1.ObjectMeta{
            GenerateName: p.Name + &amp;quot;-&amp;quot;,
        },
    }, opts)
    if err != nil {
        return err
    }
    return nil
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看events信息可以看出random-scheduler打出的“laced pod [default/sleep-5b6fd9944c-5scxv] on k8s-master&amp;rdquo;等信息。&amp;rdquo;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;27s         Normal    Scheduled           pod/sleep-5b6fd9944c-5scxv    Placed pod [default/sleep-5b6fd9944c-5scxv] on k8s-master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org22e94bd&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;验证&#34;&gt;验证&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org7cc5355&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;编译&#34;&gt;编译&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ make docker-image
$ make docker-push
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgede963d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;部署&#34;&gt;部署&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f rbac.yaml
$ kubectl  apply -f deployment.yaml
$ kubectl apply -f sleep.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把sleep.yaml里改为schedulerName: random-scheduler就可以使用该调度器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deployment]# kubectl  get pod -A -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS       AGE     IP               NODE         NOMINATED NODE   READINESS GATES
default       httpbin-master                       1/1     Running   2 (11h ago)    3d22h   10.244.0.36      k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       httpbin-worker                       1/1     Running   2 (11h ago)    3d22h   10.244.2.15      k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       netshoot-master                      1/1     Running   2 (11h ago)    3d22h   10.244.0.35      k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       netshoot-worker                      1/1     Running   2 (11h ago)    3d22h   10.244.2.14      k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       random-scheduler-6dc78999cc-vnxzg    1/1     Running   0              9m      10.244.0.37      k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       sleep-5b6fd9944c-7rn5m               1/1     Running   0              11h     10.244.1.6       k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       sleep-5b6fd9944c-bwb9t               1/1     Running   0              11h     10.244.0.38      k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-6d8c4cb4d-ck2x5              1/1     Running   20 (11h ago)   19d     10.244.0.34      k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-6d8c4cb4d-mbctj              1/1     Running   20 (11h ago)   19d     10.244.0.33      k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   etcd-k8s-master                      1/1     Running   22 (11h ago)   19d     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-apiserver-k8s-master            1/1     Running   24 (11h ago)   19d     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-controller-manager-k8s-master   1/1     Running   22 (11h ago)   19d     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-dnsjg                     1/1     Running   21 (11h ago)   19d     172.25.140.215   k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-r84lg                     1/1     Running   22 (11h ago)   19d     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-tbkx2                     1/1     Running   20 (11h ago)   19d     172.25.140.214   k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-scheduler-k8s-master            1/1     Running   22 (11h ago)   19d     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   minicni-node-2xq2d                   1/1     Running   2 (11h ago)    3d23h   172.25.140.214   k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   minicni-node-dsq8c                   1/1     Running   2 (11h ago)    3d23h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   minicni-node-h8hm8                   1/1     Running   2 (11h ago)    3d23h   172.25.140.215   k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出, random-scheduler-6dc78999cc-vnxzg 和 sleep pod已正常变成Running状态。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgdc135cc&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;验证-1&#34;&gt;验证&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deployment]# kubectl  logs random-scheduler-6dc78999cc-vnxzg
 I&#39;m a scheduler!
 2022/12/13 12:12:02 New Node Added to Store: k8s-master
 2022/12/13 12:12:02 New Node Added to Store: k8s-work1
 2022/12/13 12:12:02 New Node Added to Store: k8s-work2
 found a pod to schedule: default / sleep-5b6fd9944c-bwb9t
 2022/12/13 12:12:02 nodes that fit:
 2022/12/13 12:12:02 k8s-master
 2022/12/13 12:12:02 k8s-work1
 2022/12/13 12:12:02 k8s-work2
 2022/12/13 12:12:02 calculated priorities: map[k8s-master:79 k8s-work1:68 k8s-work2:15]
 Placed pod [default/sleep-5b6fd9944c-bwb9t] on k8s-master

 found a pod to schedule: default / sleep-5b6fd9944c-7rn5m
 2022/12/13 12:12:02 nodes that fit:
 2022/12/13 12:12:02 k8s-master
 2022/12/13 12:12:02 k8s-work1
 2022/12/13 12:12:02 calculated priorities: map[k8s-master:26 k8s-work1:50]
 Placed pod [default/sleep-5b6fd9944c-7rn5m] on k8s-work1

 [root@k8s-master deployment]# kubectl logs sleep-5b6fd9944c-7rn5m
 [root@k8s-master deployment]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从random-scheduler日志看， sleep容器经过random-scheduler进行调度的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deployment]# k get events
 - LAST SEEN   TYPE      REASON              OBJECT                        MESSAGE
 39m         Normal    Pulled              pod/netshoot-master           Container image &amp;quot;nicolaka/netshoot:latest&amp;quot; already present on machine
 39m         Normal    Created             pod/netshoot-master           Created container centos
 39m         Normal    Started             pod/netshoot-master           Started container centos
 39m         Normal    Pulled              pod/netshoot-worker           Container image &amp;quot;nicolaka/netshoot:latest&amp;quot; already present on machine
 39m         Normal    Created             pod/netshoot-worker           Created container centos
 39m         Normal    Started             pod/netshoot-worker           Started container centos
 27s         Normal    Scheduled           pod/sleep-5b6fd9944c-5scxv    Placed pod [default/sleep-5b6fd9944c-5scxv] on k8s-master
 26s         Normal    Pulled              pod/sleep-5b6fd9944c-5scxv    Container image &amp;quot;tutum/curl&amp;quot; already present on machine
 26s         Normal    Created             pod/sleep-5b6fd9944c-5scxv    Created container sleep
 26s         Normal    Started             pod/sleep-5b6fd9944c-5scxv    Started container sleep
 50s         Normal    Killing             pod/sleep-5b6fd9944c-7rn5m    Stopping container sleep
 50s         Normal    Killing             pod/sleep-5b6fd9944c-bwb9t    Stopping container sleep
 27s         Normal    Scheduled           pod/sleep-5b6fd9944c-rccmj    Placed pod [default/sleep-5b6fd9944c-rccmj] on k8s-work2
 26s         Normal    Pulling             pod/sleep-5b6fd9944c-rccmj    Pulling image &amp;quot;tutum/curl&amp;quot;
 27s         Normal    SuccessfulCreate    replicaset/sleep-5b6fd9944c   Created pod: sleep-5b6fd9944c-rccmj
 27s         Normal    SuccessfulCreate    replicaset/sleep-5b6fd9944c   Created pod: sleep-5b6fd9944c-5scxv
 27s         Normal    ScalingReplicaSet   deployment/sleep              Scaled up replica set sleep-5b6fd9944c to 2
 [root@k8s-master deployment]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看events信息可以看出random-scheduler打出的“laced pod [default/sleep-5b6fd9944c-5scxv] on k8s-master&amp;rdquo;等信息。&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org008f2a0&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;p&gt;[01] &lt;a href=&#34;https://www.cnblogs.com/z-gh/p/15409763.html&#34;&gt;k8s调度器介绍（调度框架版本）&lt;/a&gt;&lt;br /&gt;
[02] &lt;a href=&#34;https://zhuanlan.zhihu.com/p/400351590&#34;&gt;client-go功能详解&lt;/a&gt;&lt;br /&gt;
[03] &lt;a href=&#34;https://www.jb51.net/article/253965.htm&#34;&gt;一篇文章搞懂Go语言中的Context&lt;/a&gt;]&lt;br /&gt;
[04] &lt;a href=&#34;https://www.cnblogs.com/yangyuliufeng/p/13611126.html&#34;&gt;深入理解k8s中的informer机制&lt;/a&gt;]&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(02): 动手实现minicni</title>
            <link>http://mospany.github.io/2022/11/22/k8s-practice-minicni/</link>
            <pubDate>Tue, 22 Nov 2022 22:42:55 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/11/22/k8s-practice-minicni/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;orgfaa973a&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;不管是容器网络还是 Kubernetes 网络都需要解决以下两个核心问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;容器/Pod IP 地址的管理&lt;/li&gt;
&lt;li&gt;容器/Pod 之间的相互通信&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;容器/Pod IP 地址的管理包括容器 IP 地址的分配与回收，而容器/Pod 之间的相互通信包括同一主机的容器/Pod 之间和跨主机的容器/Pod 之间通信两种场景。这两个问题也不能完全分开来看，因为不同的解决方案往往要同时考虑以上两点。对于同一主机的容器/Pod 之间的通信来说实现相对容易，实际的挑战在于，不同容器/Pod 完全可能分布在不同的集群节点上，如何实现跨主机节点的通信不是一件容易的事情。&lt;/p&gt;

&lt;p&gt;如果不采用 SDN(Software define networking) 方式来修改底层网络设备的配置，主流方案是在主机节点的 underlay 网络平面构建新的 overlay 网络负责传输容器/Pod 之间通信数据。这种网络方案在如何复用原有的 underlay 网络平面也有不同的实现方式：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;将容器的数据包封装到原主机网络（underlay 网络平面）的三层或四层数据包中，然后使用主机网络的三层或者四层协议传输到目标主机，目标主机拆包后再转发给目标容器；&lt;/li&gt;
&lt;li&gt;把容器网络加到主机路由表中，把主机网络（underlay 网络平面）设备当作容器网关，通过路由规则转发到指定的主机，实现容器的三层互通；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org3709b23&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;cni原理&#34;&gt;CNI原理&lt;/h1&gt;

&lt;p&gt;CNI 规范相对于 CNM(Container Network Model) 对开发者的约束更少、更开放，不依赖于容器运行时，因此也更简单。关于 CNI 规范的详情请查看&lt;a href=&#34;https://github.com/containernetworking/cni/blob/master/SPEC.md&#34;&gt;官方文档&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/minicni/cni-standard.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;详见: &lt;a href=&#34;https://blog.csdn.net/elihe2011/article/details/122926399&#34;&gt;K8S 网络CNI&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;实现一个 CNI 网络插件只需要一个配置文件和一个可执行文件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;配置文件描述插件的版本、名称、描述等基本信息；&lt;/li&gt;
&lt;li&gt;可执行文件会被上层的容器管理平台调用，一个 CNI 可执行文件需要实现将容器加入到网络的 ADD 操作以及将容器从网络中删除的 DEL 操作等；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubernetes 使用 CNI 网络插件的基本工作流程是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kubelet 先创建 pause 容器创建对应的网络命名空间；&lt;/li&gt;
&lt;li&gt;根据配置调用具体的 CNI 插件，可以配置成 CNI 插件链来进行链式调用；&lt;/li&gt;
&lt;li&gt;当 CNI 插件被调用时，它根据环境变量以及命令行参数来获得网络命名空间、容器的网络设备等必要信息，然后执行 ADD 或者其他操作；&lt;/li&gt;
&lt;li&gt;CNI 插件给 pause 容器配置正确的网络，pod 中其他的容器都是复用 pause 容器的网络；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orgec132b9&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;cni配置文件&#34;&gt;CNI配置文件&lt;/h1&gt;

&lt;p&gt;现在我们来到关键的部分。一般来说，CNI 插件需要在集群的每个节点上运行，在 CNI 的规范里面，实现一个 CNI 插件首先需要一个 JSON 格式的配置文件，配置文件需要放到每个节点的 &lt;em&gt;etc/cni/net.d&lt;/em&gt; 目录，一般命名为 &amp;lt;数字&amp;gt;-&lt;CNI-plugin&gt;.conf，而且配置文件至少需要以下几个必须的字段：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;cniVersion: CNI 插件的字符串版本号，要求符合 Semantic Version 2.0 规范；&lt;/li&gt;
&lt;li&gt;name: 字符串形式的网络名；&lt;/li&gt;

&lt;li&gt;&lt;p&gt;type: 字符串表示的 CNI 插件的可运行文件；&lt;br /&gt;
除此之外，我们也可以增加一些自定义的配置字段，用于传递参数给 CNI 插件，这些配置会在运行时传递给 CNI 插件。在我们的例子里面，需要配置每个宿主机网桥的设备名、网络设备的最大传输单元(MTU)以及每个节点分配的24位子网地址，因此，我们的 CNI 插件的配置看起来会像下面这样：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  {
    &amp;quot;cniVersion&amp;quot;: &amp;quot;0.1.0&amp;quot;,
    &amp;quot;name&amp;quot;: &amp;quot;minicni&amp;quot;,
    &amp;quot;type&amp;quot;: &amp;quot;minicni&amp;quot;,
    &amp;quot;bridge&amp;quot;: &amp;quot;minicni0&amp;quot;,
    &amp;quot;mtu&amp;quot;: 1500,
    &amp;quot;subnet&amp;quot;: __NODE_SUBNET__
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: 确保配置文件放到 &lt;em&gt;etc/cni/net.d&lt;/em&gt; 目录，kubelet 默认此目录寻找 CNI 插件配置；并且，插件的配置可以分为多个插件链的形式来运行，但是为了简单起见，在我们的例子中，只配置一个独立的 CNI 插件，因为配置文件的后缀名为 .conf。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a id=&#34;orgeb0c3b0&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;cni核心实现&#34;&gt;CNI核心实现&lt;/h1&gt;

&lt;p&gt;接下来就开始看怎么实现 CNI 插件来管理 pod IP 地址以及配置容器网络设备。在此之前，我们需要明确的是，CNI 介入的时机是 kubelet 创建 pause 容器创建对应的网络命名空间之后，同时当 CNI 插件被调用的时候，kubelet 会将相关操作命令以及参数通过环境变量的形式传递给它。这些环境变量包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CNI_COMMAND: CNI 操作命令，包括 ADD, DEL, CHECK 以及 VERSION&lt;/li&gt;
&lt;li&gt;CNI_CONTAINERID: 容器 ID&lt;/li&gt;
&lt;li&gt;CNI_NETNS: pod 网络命名空间&lt;/li&gt;
&lt;li&gt;CNI_IFNAME: pod 网络设备名称&lt;/li&gt;
&lt;li&gt;CNI_PATH: CNI 插件可执行文件的搜索路径&lt;/li&gt;
&lt;li&gt;CNI_ARGS: 可选的其他参数，形式类似于 key1=value1,key2=value2&amp;#x2026;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在运行时，kubelet 通过 CNI 配置文件寻找 CNI 可执行文件，然后基于上述几个环境变量来执行相关的操作。CNI 插件必须支持的操作包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ADD: 将 pod 加入到 pod 网络中&lt;/li&gt;
&lt;li&gt;DEL: 将 pod 从 pod 网络中删除&lt;/li&gt;
&lt;li&gt;CHECK: 检查 pod 网络配置正常&lt;/li&gt;
&lt;li&gt;VERSION: 返回可选 CNI 插件的版本信息&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;让我们直接跳到 CNI 插件的入口函数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    setupLogger()

    cmd, cmdArgs, err := args.GetArgsFromEnv()
    if err != nil {
        log.Fatalf(&amp;quot;getting cmd arguments with error: %v&amp;quot;, err)
        os.Exit(1)
    }

    fh := handler.NewFileHandler(IPStore)

    switch cmd {
    case &amp;quot;ADD&amp;quot;:
        err = fh.HandleAdd(cmdArgs)
    case &amp;quot;DEL&amp;quot;:
        err = fh.HandleDel(cmdArgs)
    case &amp;quot;CHECK&amp;quot;:
        err = fh.HandleCheck(cmdArgs)
    case &amp;quot;VERSION&amp;quot;:
        err = fh.HandleVersion(cmdArgs)
    default:
        err = fmt.Errorf(&amp;quot;unknown CNI_COMMAND: %s&amp;quot;, cmd)
    }
    if err != nil {
        fmt.Fprintf(os.Stderr, &amp;quot;Failed to handle CNI_COMMAND %q: %v&amp;quot;, cmd, err)
        os.Exit(1)
    }
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，我们首先调用 GetArgsFromEnv() 函数将 CNI 插件的操作命令以及相关参数通过环境变量读入，同时从标准输入获取 CNI 插件的 JSON 配置，然后基于不同的 CNI 操作命令执行不同的处理函数。&lt;/p&gt;

&lt;p&gt;需要注意的是，我们将处理函数的集合实现为一个接口，这样就可以很容易的扩展不同的接口实现。在最基础的版本实现中，我们基本文件存储分配的 IP 信息。但是，这种实现方式存在很多问题，例如，文件存储不可靠，读写可能会发生冲突等，在后续的版本中，我们会实现基于 kubernetes 存储的接口实现，将子网信息以及 IP 信息存储到 apiserver 中，从而实现可靠存储。&lt;/p&gt;

&lt;p&gt;接下来，我们就看看基于文件的接口实现是怎么处理这些 CNI 操作命令的。&lt;br /&gt;
对于 ADD 命令：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;从标准输入获取 CNI 插件的配置信息，最重要的是当前宿主机网桥的设备名、网络设备的最大传输单元(MTU)以及当前节点分配的24位子网地址；&lt;/li&gt;
&lt;li&gt;然后从环境变量中找到对应的 CNI 操作参数，包括 pod 容器网络命名空间以及 pod 网络设备名等；&lt;/li&gt;
&lt;li&gt;接下来创建或者更新节点宿主机网桥，从当前节点分配的24位子网地址中抽取子网的网关地址，准备分配给节点宿主机网桥；&lt;/li&gt;
&lt;li&gt;接着将从文件读取已经分配的 IP 地址列表，遍历24位子网地址并从中取出第一个没有被分配的 IP 地址信息，准备分配给 pod 网络设备；pod 网络设备是 veth 设备对，一端在 pod 网络命名空间中，另外一端连接着宿主机上的网桥设备，同时所有的 pod 网络设备将宿主机上的网桥设备当作默认网关；&lt;/li&gt;
&lt;li&gt;最终成功后需要将新的 pod IP 写入到文件中。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;看起来很简单对吧？其实作为最简单的方式，这种方案可以实现最基础的 ADD 功能, kubelet调用参数如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  2022/12/09 20:17:14 cmd: ADD, ContainerID: 504032948df53a9f7bc7c35d01fb89f32b8db7a1d87514f546b78b91d2d8150a, Netns: /proc/3444/ns/net, ifName: eth0, Path: /opt/cni/bin, Args: IgnoreUnknown=1;K8S_POD_NAMESPACE=kube-system;K8S_POD_NAME=coredns-6d8c4cb4d-mbctj;K8S_POD_INFRA_CONTAINER_ID=504032948df53a9f7bc7c35d01fb89f32b8db7a1d87514f546b78b91d2d8150a, StdinData: {&amp;quot;bridge&amp;quot;:&amp;quot;minicni0&amp;quot;,&amp;quot;cniVersion&amp;quot;:&amp;quot;0.1.0&amp;quot;,&amp;quot;mtu&amp;quot;:1500,&amp;quot;name&amp;quot;:&amp;quot;minicni&amp;quot;,&amp;quot;subnet&amp;quot;:&amp;quot;10.244.0.0/24&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;minicni&amp;quot;}.

2022/12/09 20:17:14 cmd: ADD, ContainerID: 84546b1f14b339f69528d96b60e3b50a983e024daf00a6ef990819d1717aaff7, Netns: /proc/3491/ns/net, ifName: eth0, Path: /opt/cni/bin, Args: IgnoreUnknown=1;K8S_POD_NAMESPACE=kube-system;K8S_POD_NAME=coredns-6d8c4cb4d-ck2x5;K8S_POD_INFRA_CONTAINER_ID=84546b1f14b339f69528d96b60e3b50a983e024daf00a6ef990819d1717aaff7, StdinData: {&amp;quot;bridge&amp;quot;:&amp;quot;minicni0&amp;quot;,&amp;quot;cniVersion&amp;quot;:&amp;quot;0.1.0&amp;quot;,&amp;quot;mtu&amp;quot;:1500,&amp;quot;name&amp;quot;:&amp;quot;minicni&amp;quot;,&amp;quot;subnet&amp;quot;:&amp;quot;10.244.0.0/24&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;minicni&amp;quot;}.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现代码如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  func (fh *FileHandler) HandleAdd(cmdArgs *args.CmdArgs) error {
 2    cniConfig := args.CNIConfiguration{}
 3    if err := json.Unmarshal(cmdArgs.StdinData, &amp;amp;cniConfig); err != nil {
 4        return err
 5    }
 6    allIPs, err := nettool.GetAllIPs(cniConfig.Subnet)
 7    if err != nil {
 8        return err
 9    }
10    gwIP := allIPs[0]
11  
12    // open or create the file that stores all the reserved IPs
13    f, err := os.OpenFile(fh.IPStore, os.O_RDWR|os.O_CREATE, 0600)
14    if err != nil {
15        return fmt.Errorf(&amp;quot;failed to open file that stores reserved IPs %v&amp;quot;, err)
16    }
17    defer f.Close()
18  
19    // get all the reserved IPs from file
20    content, err := ioutil.ReadAll(f)
21    if err != nil {
22        return err
23    }
24    reservedIPs := strings.Split(strings.TrimSpace(string(content)), &amp;quot;\n&amp;quot;)
25  
26    podIP := &amp;quot;&amp;quot;
27    for _, ip := range allIPs[1:] {
28        reserved := false
29        for _, rip := range reservedIPs {
30            if ip == rip {
31                reserved = true
32                break
33            }
34        }
35        if !reserved {
36            podIP = ip
37            reservedIPs = append(reservedIPs, podIP)
38            break
39        }
40    }
41    if podIP == &amp;quot;&amp;quot; {
42        return fmt.Errorf(&amp;quot;no IP available&amp;quot;)
43    }
44  
45    // Create or update bridge
46    brName := cniConfig.Bridge
47    if brName != &amp;quot;&amp;quot; {
48        // fall back to default bridge name: minicni0
49        brName = &amp;quot;minicni0&amp;quot;
50    }
51    mtu := cniConfig.MTU
52    if mtu == 0 {
53        // fall back to default MTU: 1500
54        mtu = 1500
55    }
56    br, err := nettool.CreateOrUpdateBridge(brName, gwIP, mtu)
57    if err != nil {
58        return err
59    }
60  
61    netns, err := ns.GetNS(cmdArgs.Netns)
62    if err != nil {
63        return err
64    }
65  
66    if err := nettool.SetupVeth(netns, br, cmdArgs.IfName, podIP, gwIP, mtu); err != nil {
67        return err
68    }
69  
70    // write reserved IPs back into file
71    if err := ioutil.WriteFile(fh.IPStore, []byte(strings.Join(reservedIPs, &amp;quot;\n&amp;quot;)), 0600); err != nil {
72        return fmt.Errorf(&amp;quot;failed to write reserved IPs into file: %v&amp;quot;, err)
73    }
74  
75    addCmdResult := &amp;amp;AddCmdResult{
76        CniVersion: cniConfig.CniVersion,
77        IPs: &amp;amp;nettool.AllocatedIP{
78            Version: &amp;quot;IPv4&amp;quot;,
79            Address: podIP,
80            Gateway: gwIP,
81        },
82    }
83    addCmdResultBytes, err := json.Marshal(addCmdResult)
84    if err != nil {
85        return err
86    }
87  
88    // kubelet expects json format from stdout if success
89    fmt.Print(string(addCmdResultBytes))
90  
91    return nil
92  }
93  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个关键的问题是如何选择合适的 Go 语言库函数来操作 Linux 网络设备，如创建网桥设备、网络命名空间以及连接 veth 设备对。在我们的例子中，选择了比较成熟的 netlink，实际上，所有基于 iproute2 工具包的命令在 netlink 库中都有对应的 API，例如 ip link add 可以通过调用 AddLink() 函数来实现。&lt;/p&gt;

&lt;p&gt;还有一个问题需要格外小心，那就是处理网络命名空间切换、Go 协程与线程调度问题。在 Linux 中，不同的操作系统线程可能会设置不同的网络命名空间，而 Go 语言的协程会基于操作系统线程的负载以及其他信息动态地在不同的操作系统线程之间切换，这样可能会导致 Go 协程在意想不到的情况下切换到不同的网络命名空间中。&lt;/p&gt;

&lt;p&gt;比较稳妥的做法是，利用 Go 语言提供的 runtime.LockOSThread() 函数保证特定的 Go 协程绑定到当前的操作系统线程中。&lt;/p&gt;

&lt;p&gt;对于 ADD 操作的返回，确保操作成功之后向标准输出中写入 ADD 操作的返回信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    addCmdResult := &amp;amp;AddCmdResult{
    CniVersion: cniConfig.CniVersion,
    IPs: &amp;amp;nettool.AllocatedIP{
        Version: &amp;quot;IPv4&amp;quot;,
        Address: podIP,
        Gateway: gwIP,
    },
}
addCmdResultBytes, err := json.Marshal(addCmdResult)
if err != nil {
    return err
}

// kubelet expects json format from stdout if success
fmt.Print(string(addCmdResultBytes))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;保留IP文件内容如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master ~]# cat /tmp/reserved_ips
 2  10.244.0.2/24
 3  10.244.0.3/24
 4  10.244.0.4/24
 5  10.244.0.5/24
 6  10.244.0.6/24
 7  10.244.0.7/24
 8  10.244.0.8/24
 9  10.244.0.9/24
10  10.244.0.10/24
11  10.244.0.11/24
12  10.244.0.12/24
13  10.244.0.13/24
14  10.244.0.14/24
15  10.244.0.15/24
16  10.244.0.16/24
17  10.244.0.17/24
18  10.244.0.18/24
19  10.244.0.19/24
20  10.244.0.20/24
21  10.244.0.21/24
22  10.244.0.22/24
23  10.244.0.23/24
24  10.244.0.24/24
25  10.244.0.25/24
26  10.244.0.26/24
27  10.244.0.27/24  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其他三个 CNI 操作命令的处理就更简单了。DEL 操作只需要回收分配的 IP 地址，从文件中删除对应的条目，我们不需要处理 pod 网络设备的删除，原因是 kubelet 在删除 pod 网络命名空间之后这些 pod 网络设备也会自动被删除；CHECK 命令检查之前创建的网络设备与配置，暂时是可选的；VERSION 命令以 JSON 形式输出 CNI 版本信息到标准输出。&lt;br /&gt;
CNI_DEL命令参数如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2022/12/09 20:16:55 cmd: DEL, ContainerID: 88722baed9e508ab6cc4525a6b0b05da12c65efa1bbf7c1e27f0fee0c4b4f7e7, Netns: , ifName: eth0, Path: /opt/cni/bin, Args: IgnoreUnknown=1;K8S_POD_NAMESPACE=kube-system;K8S_POD_NAME=coredns-6d8c4cb4d-ck2x5;K8S_POD_INFRA_CONTAINER_ID=88722baed9e508ab6cc4525a6b0b05da12c65efa1bbf7c1e27f0fee0c4b4f7e7, StdinData: {&amp;quot;bridge&amp;quot;:&amp;quot;minicni0&amp;quot;,&amp;quot;cniVersion&amp;quot;:&amp;quot;0.1.0&amp;quot;,&amp;quot;mtu&amp;quot;:1500,&amp;quot;name&amp;quot;:&amp;quot;minicni&amp;quot;,&amp;quot;subnet&amp;quot;:&amp;quot;10.244.0.0/24&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;minicni&amp;quot;}. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;删除操作实现代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1   func (fh *FileHandler) HandleDel(cmdArgs *args.CmdArgs) error {
 2      netns, err := ns.GetNS(cmdArgs.Netns)
 3      if err != nil {
 4          return err
 5      }
 6      ip, err := nettool.GetVethIPInNS(netns, cmdArgs.IfName)
 7      if err != nil {
 8          return err
 9      }
10  
11      // open or create the file that stores all the reserved IPs
12      f, err := os.OpenFile(fh.IPStore, os.O_RDWR|os.O_CREATE, 0600)
13      if err != nil {
14          return fmt.Errorf(&amp;quot;failed to open file that stores reserved IPs %v&amp;quot;, err)
15      }
16      defer f.Close()
17  
18      // get all the reserved IPs from file
19      content, err := ioutil.ReadAll(f)
20      if err != nil {
21          return err
22      }
23      reservedIPs := strings.Split(strings.TrimSpace(string(content)), &amp;quot;\n&amp;quot;)
24  
25      for i, rip := range reservedIPs {
26          if rip == ip {
27              reservedIPs = append(reservedIPs[:i], reservedIPs[i+1:]...)
28              break
29          }
30      }
31  
32      // write reserved IPs back into file
33      if err := ioutil.WriteFile(fh.IPStore, []byte(strings.Join(reservedIPs, &amp;quot;\n&amp;quot;)), 0600); err != nil {
34          return fmt.Errorf(&amp;quot;failed to write reserved IPs into file: %v&amp;quot;, err)
35      }
36  
37      return nil
38  }
39  
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;CNI_VERSION参数如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2022/12/09 20:18:24 cmd: VERSION, ContainerID: , Netns: dummy, ifName: dummy, Path: dummy, Args: , StdinData: {&amp;quot;cniVersion&amp;quot;:&amp;quot;0.4.0&amp;quot;}. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现代码如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1  func (fh *FileHandler) HandleVersion(cmdArgs *args.CmdArgs) error {
2     versionInfo, err := json.Marshal(fh.VersionInfo)
3     if err != nil {
4         return err
5     }
6     fmt.Print(string(versionInfo))
7     return nil
8  }
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orgb4b32d9&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;cni-安装工具&#34;&gt;CNI 安装工具&lt;/h1&gt;

&lt;p&gt;CNI 插件需要运行在集群中的每个节点上，而且 CNI 插件配置信息与可运行文件必须在每个节点特殊的目录中，因此，安装 CNI 插件非常适合使用 DaemonSet 并挂载 CNI 插件目录，为了避免安装 CNI 的工具不能被正常调度，我们需要使用 hostNetwork 来使用宿主机的网络。同时，将 CNI 插件配置以 ConfigMap 的形式挂载，这样方便终端用户配置 CNI 插件。更详细的信息请查看安装工具部署文件。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# k describe node k8s-master  | grep CIDR
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外需要注意的是，我们在安装 CNI 插件的脚本中获取每个节点划分得到的24子网信息、检查是否合法然后写入到 CNI 配置信息中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  # The environment variables used to connect to the kube-apiserver
 2   SERVICE_ACCOUNT_PATH=/var/run/secrets/kubernetes.io/serviceaccount
 3   SERVICEACCOUNT_TOKEN=$(cat $SERVICE_ACCOUNT_PATH/token)
 4   KUBE_CACERT=${KUBE_CACERT:-$SERVICE_ACCOUNT_PATH/ca.crt}
 5   KUBERNETES_SERVICE_PROTOCOL=${KUBERNETES_SERVICE_PROTOCOL-https}
 6  
 7   # Check if we&#39;re running as a k8s pod.
 8   if [ -f &amp;quot;$SERVICE_ACCOUNT_PATH/token&amp;quot; ];
 9   then
10       # some variables should be automatically set inside a pod
11       if [ -z &amp;quot;${KUBERNETES_SERVICE_HOST}&amp;quot; ]; then
12           exit_with_message &amp;quot;KUBERNETES_SERVICE_HOST not set&amp;quot;
13       fi
14       if [ -z &amp;quot;${KUBERNETES_SERVICE_PORT}&amp;quot; ]; then
15           exit_with_message &amp;quot;KUBERNETES_SERVICE_PORT not set&amp;quot;
16       fi
17   fi
18  
19   # exit if the NODE_NAME environment variable is not set.
20   if [[ -z &amp;quot;${NODE_NAME}&amp;quot; ]];
21   then
22       exit_with_message &amp;quot;NODE_NAME not set.&amp;quot;
23   fi
24  
25  
26   NODE_RESOURCE_PATH=&amp;quot;${KUBERNETES_SERVICE_PROTOCOL}://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}/api/v1/nodes/${NODE_NAME}&amp;quot;
27   NODE_SUBNET=$(curl --cacert &amp;quot;${KUBE_CACERT}&amp;quot; --header &amp;quot;Authorization: Bearer ${SERVICEACCOUNT_TOKEN}&amp;quot; -X GET &amp;quot;${NODE_RESOURCE_PATH}&amp;quot; | jq &amp;quot;.spec.podCIDR&amp;quot;)
28  
29   # Check if the node subnet is valid IPv4 CIDR address
30   IPV4_CIDR_REGEX=&amp;quot;(((25[0-5]|2[0-4][0-9]|1?[0-9][0-9]?)\.){3}(25[0-5]|2[0-4][0-9]|1?[0-9][0-9]?))(\/([8-9]|[1-2][0-9]|3[0-2]))([^0-9.]|$)&amp;quot;
31   if [[ ${NODE_SUBNET} =~ ${IPV4_CIDR_REGEX} ]]
32   then
33       echo &amp;quot;${NODE_SUBNET} is a valid IPv4 CIDR address.&amp;quot;
34   else
35       exit_with_message &amp;quot;${NODE_SUBNET} is not a valid IPv4 CIDR address!&amp;quot;
36   fi
37  
38   # exit if the NODE_NAME environment variable is not set.
39   if [[ -z &amp;quot;${CNI_NETWORK_CONFIG}&amp;quot; ]];
40   then
41       exit_with_message &amp;quot;CNI_NETWORK_CONFIG not set.&amp;quot;
42   fi
43  
44   TMP_CONF=&#39;/minicni.conf.tmp&#39;
45   cat &amp;gt;&amp;quot;${TMP_CONF}&amp;quot; &amp;lt;&amp;lt;EOF
46   ${CNI_NETWORK_CONFIG}
47   EOF
48  
49   # Replace the __NODE_SUBNET__
50   grep &amp;quot;__NODE_SUBNET__&amp;quot; &amp;quot;${TMP_CONF}&amp;quot; &amp;amp;&amp;amp; sed -i s~__NODE_SUBNET__~&amp;quot;${NODE_SUBNET}&amp;quot;~g &amp;quot;${TMP_CONF}&amp;quot;
51  
52   # Log the config file
53   echo &amp;quot;CNI config: $(cat &amp;quot;${TMP_CONF}&amp;quot;)&amp;quot;
54  
55   # Move the temporary CNI config into the CNI configuration directory.
56   mv &amp;quot;${TMP_CONF}&amp;quot; &amp;quot;${CNI_NET_DIR}/${CNI_CONF_NAME}&amp;quot; || \
57     exit_with_error &amp;quot;Failed to move ${TMP_CONF} to ${CNI_CONF_NAME}.&amp;quot;
58  
59   echo &amp;quot;Created CNI config ${CNI_CONF_NAME}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org9970dc4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;编译部署&#34;&gt;编译部署&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orgd1c3c68&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-编译&#34;&gt;1、编译&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;make build
make image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/minicni/make-image-minicni.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在docker hub里已看到了minicni镜像：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/minicni/docker-hub-minicni.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org4e72f27&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;部署&#34;&gt;部署&lt;/h2&gt;

&lt;p&gt;1、登录已安装好的k8s集群，把之前已存在的cni如(calico)卸载掉再安装minici:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# kubectl  apply -f minicni.yaml
clusterrole.rbac.authorization.k8s.io/minicni created
serviceaccount/minicni created
clusterrolebinding.rbac.authorization.k8s.io/minicni created
configmap/minicni-config created
Warning: spec.template.spec.nodeSelector[beta.kubernetes.io/os]: deprecated since v1.14; use &amp;quot;kubernetes.io/os&amp;quot; instead
Warning: spec.template.metadata.annotations[scheduler.alpha.kubernetes.io/critical-pod]: non-functional in v1.16+; use the &amp;quot;priorityClassName&amp;quot; field instead
daemonset.apps/minicni-node created
[root@k8s-master minicni]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、查看minicni部署状态：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# kubectl get pod -A -o wide
 NAMESPACE     NAME                                       READY   STATUS        RESTARTS        AGE     IP               NODE         NOMINATED NODE   READINESS GATES
 default       nginx-85b98978db-qkd6h                     0/1     Completed     5               4d21h   &amp;lt;none&amp;gt;           k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   calico-kube-controllers-7b8458594b-p2fqj   0/1     Terminating   2               4d12h   &amp;lt;none&amp;gt;           k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   coredns-6d8c4cb4d-ck2x5                    0/1     Completed     7               4d22h   &amp;lt;none&amp;gt;           k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   coredns-6d8c4cb4d-mbctj                    0/1     Completed     7               4d22h   &amp;lt;none&amp;gt;           k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   etcd-k8s-master                            1/1     Running       8 (8m14s ago)   4d22h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   kube-apiserver-k8s-master                  1/1     Running       8 (8m4s ago)    4d22h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   kube-controller-manager-k8s-master         1/1     Running       8 (8m14s ago)   4d22h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   kube-proxy-dnsjg                           1/1     Running       8 (8m14s ago)   4d21h   172.25.140.215   k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   kube-proxy-r84lg                           1/1     Running       8 (8m14s ago)   4d22h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   kube-proxy-tbkx2                           1/1     Running       7 (8m14s ago)   4d21h   172.25.140.214   k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   kube-scheduler-k8s-master                  1/1     Running       8 (8m14s ago)   4d22h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   minicni-node-8lmxc                         1/1     Running       0               5m17s   172.25.140.214   k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   minicni-node-sgjmg                         1/1     Running       0               5m17s   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   minicni-node-xslgx                         1/1     Running       0               5m17s   172.25.140.215   k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出minicni处于Running状态。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org370284e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;测试&#34;&gt;测试&lt;/h1&gt;

&lt;p&gt;1、环境准备, 分别给node打上相应的标签&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# k label nodes k8s-master role=master
node/k8s-master labeled
[root@k8s-master minicni]# k label nodes k8s-work1 role=worker
node/k8s-work1 labeled
[root@k8s-master minicni]# k label nodes k8s-work2 role=worker
node/k8s-work2 labeled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、分别在 master 与 worker 节点部署 netshoot 与 httpbin：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# k apply -f test-pods.yaml
pod/httpbin-master created
pod/netshoot-master created
pod/httpbin-worker created
pod/netshoot-worker created
[root@k8s-master minicni]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、确保所有 pod 都启动并开始运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# k get pod
NAME                     READY   STATUS              RESTARTS   AGE
httpbin-master           0/1     ContainerCreating   0          8s
httpbin-worker           0/1     ContainerCreating   0          8s
netshoot-master          0/1     ContainerCreating   0          8s
netshoot-worker          0/1     ContainerCreating   0          8s
[root@k8s-master minicni]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;发现状态为ContainerCreating, 查看pod描述报错为:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Warning  FailedCreatePodSandBox  36s                kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container &amp;quot;7cb17eace85729539db7d7af1a4955a353a14eb7ffa96f84c117ee6633b3e2b5&amp;quot; network for pod &amp;quot;httpbin-master&amp;quot;: networkPlugin cni failed to set up pod &amp;quot;httpbin-master_default&amp;quot; network: error getting ClusterInformation: connection is unauthorized: Unauthorized, failed to clean up sandbox container &amp;quot;7cb17eace85729539db7d7af1a4955a353a14eb7ffa96f84c117ee6633b3e2b5&amp;quot; network for pod &amp;quot;httpbin-master&amp;quot;: networkPlugin cni failed to teardown pod &amp;quot;httpbin-master_default&amp;quot; network: error getting ClusterInformation: connection is unauthorized: Unauthorized]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果安装了calico网络插件，需要删除calico:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl delete -f  &amp;lt;yaml&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还要去所有节点etc/cni/net.d/目录下 删掉与calico相关的所有配置文件, 然后重启机器。 不然pod起不来，会报错 network: error getting ClusterInformation: connection is unauthorized: Unauthorized .&lt;/p&gt;

&lt;p&gt;4、最后再查看POD都变成Running状态了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# k get pod -A -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS       AGE     IP               NODE         NOMINATED NODE   READINESS GATES
default       httpbin-master                       1/1     Running   0              20m     10.244.0.4       k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       httpbin-worker                       1/1     Running   0              20m     10.244.2.2       k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       netshoot-master                      1/1     Running   0              20m     10.244.0.5       k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       netshoot-worker                      1/1     Running   0              20m     10.244.2.3       k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       nginx-85b98978db-2hzc9               1/1     Running   0              50m     10.244.1.2       k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-6d8c4cb4d-ck2x5              1/1     Running   9 (24h ago)    6d23h   10.244.0.2       k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-6d8c4cb4d-mbctj              1/1     Running   9 (24h ago)    6d23h   10.244.0.3       k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   etcd-k8s-master                      1/1     Running   10 (24h ago)   6d23h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-apiserver-k8s-master            1/1     Running   12 (24h ago)   6d23h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-controller-manager-k8s-master   1/1     Running   10 (24h ago)   6d23h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-dnsjg                     1/1     Running   10 (24h ago)   6d22h   172.25.140.215   k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-r84lg                     1/1     Running   10 (24h ago)   6d23h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-tbkx2                     1/1     Running   9 (24h ago)    6d22h   172.25.140.214   k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-scheduler-k8s-master            1/1     Running   10 (24h ago)   6d23h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   minicni-node-5w9fn                   1/1     Running   0              55m     172.25.140.215   k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   minicni-node-jb5cj                   1/1     Running   0              55m     172.25.140.214   k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   minicni-node-kp25h                   1/1     Running   0              55m     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
[root@k8s-master minicni]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、之后测试以下四种网络通信是否正常：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;pod 到宿主机的通信&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# kubectl exec -ti netshoot-master -- /bin/bash
bash-5.1# ping 172.25.140.216
PING 172.25.140.216 (172.25.140.216) 56(84) bytes of data.
64 bytes from 172.25.140.216: icmp_seq=1 ttl=64 time=0.074 ms
64 bytes from 172.25.140.216: icmp_seq=2 ttl=64 time=0.035 ms
64 bytes from 172.25.140.216: icmp_seq=3 ttl=64 time=0.033 ms
64 bytes from 172.25.140.216: icmp_seq=4 ttl=64 time=0.043 ms
64 bytes from 172.25.140.216: icmp_seq=5 ttl=64 time=0.048 ms
^C
--- 172.25.140.216 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 3999ms
rtt min/avg/max/mdev = 0.033/0.046/0.074/0.014 ms
bash-5.1#
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;同一个节点 pod-to-pod 通信&lt;br /&gt;
默认情况下，同一台主机上的 pod-to-pod 网络包默认会被 Linux 内核丢弃，原因是 Linux 默认会把非 default 网络命名空间的网络包看作是外部数据包，关于这个问题的具体细节，请查看 stackoverflow 上的讨论。目前，我们需要在每个集群结点上使用以下命令手动添加以下 iptables 规则来让 pod-to-pod 网络数据包顺利转发：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# iptables -t filter -A FORWARD -s 10.244.0.0/24  -j ACCEPT
[root@k8s-master ~]# iptables -t filter -A FORWARD -d 10.244.0.0/24  -j ACCEPT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再进行测试:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# kubectl  exec -ti netshoot-master -- /bin/bash
bash-5.1# ping 10.244.0.25
PING 10.244.0.25 (10.244.0.25) 56(84) bytes of data.
64 bytes from 10.244.0.25: icmp_seq=1 ttl=64 time=0.079 ms
64 bytes from 10.244.0.25: icmp_seq=2 ttl=64 time=0.061 ms 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试通信正常。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;pod 到其他主机的通信&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# kubectl  exec -ti netshoot-master -- /bin/bash
bash-5.1# ping 172.25.140.214
PING 172.25.140.214 (172.25.140.214) 56(84) bytes of data.

^C
--- 172.25.140.214 ping statistics ---
4 packets transmitted, 0 received, 100% packet loss, time 2999ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ping不通，原因是阿里云底层网络对非法IP限制，不是随便配个IP就可以通，如需通可以考虑overlay如calico或本地虚拟机搭建方式。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;跨一个节点 pod-to-pod 通信&lt;br /&gt;
对于跨节点的 pod-to-pod 网络包，需要像 Calico 那样添加宿主机的路由表，保证发往各个节点上的 pod 流量经过节点的转发。目前这些路由表需要手动添加：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ip route add 10.244.2.0/24  via 172.25.140.214 dev eth0 #run on master 
ip route add 10.244.0.0/24  via 172.25.140.216 dev eth0 #run on worker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再测试：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# kubectl  exec -ti netshoot-master -- /bin/bash
bash-5.1# ping 10.244.2.11
PING 10.244.2.11 (10.244.2.11) 56(84) bytes of data.

^C
--- 10.244.2.11 ping statistics ---
108 packets transmitted, 0 received, 100% packet loss, time 106996ms    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现象与原理同上。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orgb51c532&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;faq&#34;&gt;FAQ&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orgdc842c7&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;cannot-find-package-编译不过&#34;&gt;cannot find package 编译不过&lt;/h2&gt;

&lt;p&gt;当出现如下错误时：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ make build
Building the minicni on amd64...
cmd/main.go:8:2: cannot find package &amp;quot;github.com/morvencao/minicni/pkg/args&amp;quot; in any of:
 /usr/local/go/src/github.com/morvencao/minicni/pkg/args (from $GOROOT)
 /Users/mosp/goget/src/github.com/morvencao/minicni/pkg/args (from $GOPATH)
cmd/main.go:9:2: cannot find package &amp;quot;github.com/morvencao/minicni/pkg/handler&amp;quot; in any of:
 /usr/local/go/src/github.com/morvencao/minicni/pkg/handler (from $GOROOT)
 /Users/mosp/goget/src/github.com/morvencao/minicni/pkg/handler (from $GOPATH)
make: *** [build] Error 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要把GO111MODULE=on或auto，才能使用Go module功能，可在.bashrc或.zshrc里加上：export GO111MODULE=auto&lt;br /&gt;
详见&lt;a href=&#34;http://www.ay1.cc/article/18635.html&#34;&gt;go自动下载所有的依赖包go module使用详解_Golang&lt;/a&gt;：&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orga2808f1&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;build-constraints-exclude-all-go-files-in&#34;&gt;build constraints exclude all Go files in&lt;/h2&gt;

&lt;p&gt;当出现如下错误时:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ make build
Building the minicni on amd64...
go build github.com/containernetworking/plugins/pkg/ns: build constraints exclude all Go files in /Users/mosp/goget/pkg/mod/github.com/containernetworking/plugins@v1.1.1/pkg/ns
make: *** [build] Error 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要设置如下两个变量：&lt;br /&gt;
export GOOS=“linux”。即：不能为darwin&lt;br /&gt;
export CGO_ENABLED=“1”。&lt;br /&gt;
详见：&lt;a href=&#34;https://blog.csdn.net/weixin_42845682/article/details/124568715&#34;&gt;build constraints exclude all Go files in xxx/xxx/xxx&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orga4ae153&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;p&gt;【01】 &lt;a href=&#34;https://blog.csdn.net/u012772803/article/details/113703029&#34;&gt;find -print0和xargs -0原理及用法&lt;/a&gt;]&lt;br /&gt;
【02】 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/411181637&#34;&gt;Go语言import分组管理利器: goimports-reviser&lt;/a&gt;&lt;br /&gt;
【03】 &lt;a href=&#34;https://morven.life/posts/create-your-own-cni-with-golang/&#34;&gt;使用 Go 从零开始实现 CNI&lt;/a&gt;&lt;br /&gt;
【04】 &lt;a href=&#34;https://blog.csdn.net/a5534789/article/details/112848404&#34;&gt;centOS内网安装kubernetes集群&lt;/a&gt;&lt;br /&gt;
【05】 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/415032187&#34;&gt;Linux 路由表(RIB表、FIB表)、ARP表、MAC表整理&lt;/a&gt;]&lt;br /&gt;
【06】 &lt;a href=&#34;https://www.jianshu.com/p/75704eb30eff&#34;&gt;查看CNI中的veth pair&lt;/a&gt;&lt;br /&gt;
【07】 &lt;a href=&#34;https://mp.weixin.qq.com/s/7t_MoZ0quJF50VoIwqvKyQ&#34;&gt;一文吃透 K8S 网络模型&lt;/a&gt;&lt;br /&gt;
【08】 &lt;a href=&#34;https://blog.csdn.net/elihe2011/article/details/122926399&#34;&gt;K8S 网络CNI&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(01): kubeadm安装K8S集群</title>
            <link>http://mospany.github.io/2022/11/23/kubeadm-install-k8s/</link>
            <pubDate>Tue, 22 Nov 2022 20:58:52 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/11/23/kubeadm-install-k8s/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;org4f3c086&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;规划&#34;&gt;规划&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orgfdfecef&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;服务配置&#34;&gt;服务配置&lt;/h2&gt;

&lt;table border=&#34;2&#34; cellspacing=&#34;0&#34; cellpadding=&#34;6&#34; rules=&#34;groups&#34; frame=&#34;hsides&#34;&gt;


&lt;colgroup&gt;
&lt;col  class=&#34;org-left&#34; /&gt;

&lt;col  class=&#34;org-left&#34; /&gt;

&lt;col  class=&#34;org-left&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;OS&lt;/th&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;配置&lt;/th&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;用途&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;CentOS 7.6 (172.25.140.216)&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;2C4G&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;k8s-master&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;CentOS 7.6 (172.25.140.215)&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;2C4G&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;k8s-work1&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;CentOS 7.6 (172.25.140.214)&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;2C4G&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;k8s-work2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;**注**：这是演示 k8s 集群安装的实验环境，配置较低，生产环境中我们的服务器配置至少都是 8C/16G 的基础配置。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgbfb1037&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;版本选择&#34;&gt;版本选择&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;CentOS：7.6&lt;/li&gt;
&lt;li&gt;k8s组件版本：1.23.6&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orge0cd109&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;一-服务器基础配置&#34;&gt;一、服务器基础配置&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orgfbbc6fc&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-配置主机名&#34;&gt;1、 配置主机名&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;[root@iZ8vbfafp7g52u8976flc0Z ~]# hostnamectl set-hostname k8s-master
[root@iZ8vbfafp7g52u8976flc1Z ~]# hostnamectl set-hostname k8s-work1
[root@iZ8vbfafp7g52u8976flc2Z ~]# hostnamectl set-hostname k8s-work2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgec07355&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-关闭防火墙&#34;&gt;2、关闭防火墙&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1  # 关闭firewalld
2  [root@k8s-master ~]# systemctl stop firewalld
3  
4  # 关闭selinux
5  [root@k8s-master ~]# sed -i &#39;s/enforcing/disabled/&#39; /etc/selinux/config
6  [root@k8s-master ~]# setenforce 0
7   setenforce: SELinux is disabled
8  [root@k8s-master ~]#
9  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org1b9a204&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;3-互做本地解析&#34;&gt;3、互做本地解析&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# cat /etc/hosts
172.25.140.216 k8s-master
172.25.140.215 k8s-work1
172.25.140.214 k8s-work2

[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgf3b9e1a&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;4-ssh-免密通信-可选&#34;&gt;4、SSH 免密通信（可选）&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# ssh-keygen
  Generating public/private rsa key pair.
  Enter file in which to save the key (/root/.ssh/id_rsa):
  Enter passphrase (empty for no passphrase):
  Enter same passphrase again:
  Your identification has been saved in /root/.ssh/id_rsa.
  Your public key has been saved in /root/.ssh/id_rsa.pub.
  The key fingerprint is:
  SHA256:s+JcU9ctsItBgDC+UwxgmFhnsQGpy9SELkEOx4Lmk/0 root@k8s-master
  The key&#39;s randomart image is:
  +---[RSA 2048]----+
  |=*B+Oo ..        |
  |X=o= *.  .       |
  |+== o o   . .    |
  |o* o o   .   + . |
  |+.. +   S o o o .|
  |..   E   + + . . |
  |      . + . .    |
  |     o o .       |
  |      o          |
  +----[SHA256]-----+
  [root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行（互发公钥）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# ssh-copy-id root@k8s-work1
  /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &amp;quot;/root/.ssh/id_rsa.pub&amp;quot;
  The authenticity of host &#39;k8s-work1 (172.25.140.215)&#39; can&#39;t be established.
  ECDSA key fingerprint is SHA256:BMN7TIKDbFKdG3v1TVHmy3i6BYm7TGS8Hsnu1F9+UkI.
  ECDSA key fingerprint is MD5:71:60:e2:6c:38:e2:20:d8:9c:94:77:54:cb:10:33:32.
  Are you sure you want to continue connecting (yes/no)? yes
  /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
  /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
  root@k8s-work1&#39;s password:

  Number of key(s) added: 1

  Now try logging into the machine, with:   &amp;quot;ssh &#39;root@k8s-work1&#39;&amp;quot;
  and check to make sure that only the key(s) you wanted were added.

  [root@k8s-master ~]# ssh-copy-id root@k8s-work2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org97fcb39&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;5-加载-br-netfilter-模块&#34;&gt;5、加载 br_netfilter 模块&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;确保 br_netfilter 模块被加载&lt;br /&gt;
所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt; 1  # 加载模块
 2  [root@k8s-master ~]# modprobe br_netfilter
 3  ## 查看加载请看
 4  [root@k8s-master ~]# lsmod | grep br_netfilter
 5  br_netfilter           22256  0
 6  bridge                151336  1 br_netfilter
 7  
 8  # 永久生效
 9  [root@k8s-master ~]# cat &amp;lt;&amp;lt;EOF | tee /etc/modules-load.d/k8s.conf
10  &amp;gt; br_netfilter
11  &amp;gt; EOF
12  br_netfilter
13  [root@k8s-master ~]#
14  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org6cf490e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;6-允许-iptables-检查桥接流量&#34;&gt;6、允许 iptables 检查桥接流量&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt; 1   [root@k8s-master ~]# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
 2   &amp;gt; br_netfilter
 3   &amp;gt; EOF
 4   br_netfilter
 5   [root@k8s-master ~]# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
 6   &amp;gt; net.bridge.bridge-nf-call-ip6tables = 1
 7   &amp;gt; net.bridge.bridge-nf-call-iptables = 1
 8   &amp;gt; EOF
 9   net.bridge.bridge-nf-call-ip6tables = 1
10   net.bridge.bridge-nf-call-iptables = 1
11  [root@k8s-master ~]# sudo sysctl --system
12  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org716427b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;7-关闭-swap&#34;&gt;7、关闭 swap&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1  # 临时关闭
2  [root@k8s-master ~]# swapoff -a
3  
4  # 永久关闭
5  [root@k8s-master ~]# sed -ri &#39;s/.*swap.*/#&amp;amp;/&#39; /etc/fstab
6  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orga15d3fb&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;8-时间同步&#34;&gt;8、时间同步&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1  # 同步网络时间
2  [root@k8s-master ~]# ntpdate time.nist.gov
3  23 Nov 22:36:07 ntpdate[12307]: adjust time server 132.163.96.6 offset -0.009024 sec
4  
5  [root@k8s-master ~]#
6  # 将网络时间写入硬件时间
7  [root@k8s-master ~]# hwclock --systohc
8  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org7ff8672&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;9-安装-docker&#34;&gt;9、安装 Docker&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、使用 sudo 或 root 权限登录 Centos。&lt;/p&gt;

&lt;p&gt;2、确保 yum 包更新到最新。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum update
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、执行 Docker 安装脚本。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -fsSL https://get.docker.com/ | sh 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行这个脚本会添加 docker.repo 源并安装 Docker。&lt;/p&gt;

&lt;p&gt;4、启动 Docker 进程。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo service docker start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、验证 docker 是否安装成功并在容器中执行一个测试的镜像。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo docker run hello-world

[root@k8s-master ~]# sudo docker run hello-world

  Hello from Docker!
  This message shows that your installation appears to be working correctly.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到此，docker 在 CentOS 系统的安装完成。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orge392402&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;10-安装-kubeadm-kubelet&#34;&gt;10、安装 kubeadm、kubelet&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、添加 k8s 镜像源&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;地址：&lt;a href=&#34;https://developer.aliyun.com/mirror/kubernetes?spm=a2c6h.13651102.0.0.1cd01b116JYQIn&#34;&gt;https://developer.aliyun.com/mirror/kubernetes?spm=a2c6h.13651102.0.0.1cd01b116JYQIn&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master ~]# cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo
 2  &amp;gt; [kubernetes]
 3  &amp;gt; name=Kubernetes
 4  &amp;gt; baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
 5  &amp;gt; enabled=1
 6  &amp;gt; gpgcheck=0
 7  &amp;gt; repo_gpgcheck=0
 8  &amp;gt; gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
 9  &amp;gt; EOF
10  [root@k8s-master ~]#
11  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、建立 k8s YUM 缓存&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master ~]# yum makecache
 2  已加载插件：fastestmirror
 3  Loading mirror speeds from cached hostfile
 4  base                                                                                                                                                                                 | 3.6 kB  00:00:00
 5  docker-ce-stable                                                                                                                                                                     | 3.5 kB  00:00:00
 6  epel                                                                                                                                                                                 | 4.7 kB  00:00:00
 7  extras                                                                                                                                                                               | 2.9 kB  00:00:00
 8  updates                                                                                                                                                                              | 2.9 kB  00:00:00
 9  元数据缓存已建立
10  [root@k8s-master ~]#
11  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、安装 k8s 相关工具&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1   # 查看可安装版本
 2   [root@k8s-master ~]# yum list kubelet --showduplicates
 3  
 4   ...
 5   ...
 6   kubelet.x86_64                                           1.23.0-0                                             kubernetes
 7   kubelet.x86_64                                           1.23.1-0                                             kubernetes
 8   kubelet.x86_64                                           1.23.2-0                                             kubernetes
 9   kubelet.x86_64                                           1.23.3-0                                             kubernetes
10   kubelet.x86_64                                           1.23.4-0                                             kubernetes
11   kubelet.x86_64                                           1.23.5-0                                             kubernetes
12   kubelet.x86_64                                           1.23.6-0                                             kubernetes
13  
14   # 开始安装（指定你要安装的版本）
15   [root@k8s-master ~]# yum install -y kubelet-1.23.6 kubeadm-1.23.6 kubectl-1.23.6
16  
17  # 设置开机自启动并启动kubelet（kubelet由systemd管理）
18  [root@k8s-master ~]# systemctl enable kubelet &amp;amp;&amp;amp; systemctl start kubelet
19  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org02e7601&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;二-master-节点&#34;&gt;二、Master 节点&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org88d2a45&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-k8s-初始化&#34;&gt;1、k8s 初始化&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1   [root@k8s-master ~]# kubeadm init \
2  --apiserver-advertise-address=172.25.140.216 \
3  --image-repository registry.aliyuncs.com/google_containers \
4  --kubernetes-version v1.23.6 \
5  --service-cidr=10.96.0.0/12 \
6  --pod-network-cidr=10.244.0.0/16 \
7  --ignore-preflight-errors=all
8  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数说明：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1  --apiserver-advertise-address  # 集群master地址
2  --image-repository             # 指定k8s镜像仓库地址
3  --kubernetes-version           # 指定K8s版本（与kubeadm、kubelet版本保持一致）
4  --service-cidr                 # Pod统一访问入口
5  --pod-network-cidr             # Pod网络（与CNI网络保持一致）
6  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初始化后输出内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  ...
 2  ...
 3  [addons] Applied essential addon: CoreDNS
 4  [addons] Applied essential addon: kube-proxy
 5  
 6  Your Kubernetes control-plane has initialized successfully!
 7  
 8  To start using your cluster, you need to run the following as a regular user:
 9  
10     mkdir -p $HOME/.kube
11     sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
12     sudo chown $(id -u):$(id -g) $HOME/.kube/config
13  
14    Alternatively, if you are the root user, you can run:
15  
16    export KUBECONFIG=/etc/kubernetes/admin.conf
17  
18  You should now deploy a pod network to the cluster.
19  Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
20  https://kubernetes.io/docs/concepts/cluster-administration/addons/
21  
22  Then you can join any number of worker nodes by running the following on each as root:
23  
24  kubeadm join 172.25.140.216:6443 --token 8d9mk7.08nyz6xc2d5boiy8 \
25      --discovery-token-ca-cert-hash sha256:45542b0b380a8f959e5bc93f6dd7d1c5c78b202ff1a3eea5c97804549af9a12e
26  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org0b51700&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-根据输出提示创建相关文件&#34;&gt;2、根据输出提示创建相关文件&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1  [root@k8s-master ~]# mkdir -p $HOME/.kube
2  [root@k8s-master ~]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
3  [root@k8s-master ~]# chown $(id -u):$(id -g) $HOME/.kube/config
4  [root@k8s-master ~]#
5  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org5a02bac&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;3-查看-k8s-运行的容器&#34;&gt;3、查看 k8s 运行的容器&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt; 1  root@k8s-master ~]# kubectl get pods -n kube-system -o wide
 2  NAME                                 READY   STATUS    RESTARTS   AGE     IP               NODE         NOMINATED NODE   READINESS GATES
 3  coredns-6d8c4cb4d-ck2x5              0/1     Pending   0          8m4s    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;       &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 4  coredns-6d8c4cb4d-mbctj              0/1     Pending   0          8m4s    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;       &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 5  etcd-k8s-master                      1/1     Running   0          8m18s   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 6  kube-apiserver-k8s-master            1/1     Running   0          8m18s   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 7  kube-controller-manager-k8s-master   1/1     Running   0          8m18s   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 8  kube-proxy-r84lg                     1/1     Running   0          8m4s    172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 9  kube-scheduler-k8s-master            1/1     Running   0          8m18s   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
10  [root@k8s-master ~]#
11  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org2688836&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;4-查看-k8s-节点&#34;&gt;4、查看 k8s 节点&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1  [root@k8s-master ~]# kubectl  get node -o wide
2  NAME         STATUS     ROLES                  AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME
3  k8s-master   NotReady   control-plane,master   11m   v1.23.6   172.25.140.216   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://20.10.21
4  [root@k8s-master ~]#
5  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可看到当前只有 k8s-master 节点，而且状态是 NotReady（未就绪），因为我们还没有部署网络插件（kubectl apply -f [podnetwork].yaml），于是接着部署容器网络（CNI）。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orge6d35ae&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;5-容器网络-cni-部署&#34;&gt;5、容器网络（CNI）部署&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;br /&gt;
插件地址：&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/addons/&#34;&gt;https://kubernetes.io/docs/concepts/cluster-administration/addons/&lt;/a&gt;&lt;br /&gt;
该地址在 k8s-master 初始化成功时打印出来。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、选择一个主流的容器网络插件部署（Calico）&lt;br /&gt;
   &lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/kubeadm-install-k8s/calico-install-page.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2、下载yml文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master kubeadm-install-k8s]# wget https://docs.projectcalico.org/manifests/calico.yaml --no-check-certificate
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、根据初始化的输出提示执行启动指令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master kubeadm-install-k8s]# kubectl apply -f calico.yaml
 2  poddisruptionbudget.policy/calico-kube-controllers created
 3  serviceaccount/calico-kube-controllers created
 4  serviceaccount/calico-node created
 5  configmap/calico-config created
 6  customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
 7  customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
 8  customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
 9  customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
10  customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
11  customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
12  customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
13  customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
14  customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
15  customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
16  customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
17  customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
18  customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
19  customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
20  customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
21  customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
22  customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
23  clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
24  clusterrole.rbac.authorization.k8s.io/calico-node created
25  clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
26  clusterrolebinding.rbac.authorization.k8s.io/calico-node created
27  daemonset.apps/calico-node created
28  deployment.apps/calico-kube-controllers created
29  [root@k8s-master kubeadm-install-k8s]# 
30  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、看看该yaml文件所需要启动的容器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1   [root@k8s-master kubeadm-install-k8s]# cat calico.yaml |grep image
 2         image: docker.io/calico/cni:v3.24.5
 3         imagePullPolicy: IfNotPresent
 4         image: docker.io/calico/cni:v3.24.5
 5         imagePullPolicy: IfNotPresent
 6         image: docker.io/calico/node:v3.24.5
 7         imagePullPolicy: IfNotPresent
 8         image: docker.io/calico/node:v3.24.5
 9         imagePullPolicy: IfNotPresent
10         image: docker.io/calico/kube-controllers:v3.24.5
11         imagePullPolicy: IfNotPresent
12  [root@k8s-master kubeadm-install-k8s]   #
13  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、查看容器是否都 Running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master kubeadm-install-k8s]# kubectl get pods -n kube-system -o wide
 2  NAME                                       READY   STATUS    RESTARTS   AGE     IP               NODE         NOMINATED NODE   READINESS GATES
 3  calico-kube-controllers-7b8458594b-pdx5z   1/1     Running   0          4m37s   10.244.235.194   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 4  calico-node-t8hhf                          1/1     Running   0          4m37s   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 5  coredns-6d8c4cb4d-ck2x5                    1/1     Running   0          32m     10.244.235.195   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 6  coredns-6d8c4cb4d-mbctj                    1/1     Running   0          32m     10.244.235.193   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 7  etcd-k8s-master                            1/1     Running   0          32m     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 8  kube-apiserver-k8s-master                  1/1     Running   0          32m     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 9  kube-controller-manager-k8s-master         1/1     Running   0          32m     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
10  kube-proxy-r84lg                           1/1     Running   0          32m     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
11  kube-scheduler-k8s-master                  1/1     Running   0          32m     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
12  [root@k8s-master kubeadm-install-k8s]#
13  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org4f4a98b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;三-work-节点&#34;&gt;三、work 节点&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orgfc01846&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-work-节点加入-k8s-集群&#34;&gt;1、work 节点加入 k8s 集群&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有 work 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt; 1  # 复制k8s-master初始化屏幕输出的语句并在work节点执行
 2  [root@k8s-work1 ~]# kubeadm join 172.25.140.216:6443 --token 8d9mk7.08nyz6xc2d5boiy8 --discovery-token-ca-cert-hash sha256:45542b0b380a8f959e5bc93f6dd7d1c5c78b202ff1a3eea5c97804549af9a12e
 3  [root@k8s-work2 ~]# kubeadm join 172.25.140.216:6443 --token 8d9mk7.08nyz6xc2d5boiy8 --discovery-token-ca-cert-hash sha256:45542b0b380a8f959e5bc93f6dd7d1c5c78b202ff1a3eea5c97804549af9a12e
 4  
 5  preflight] Running pre-flight checks
 6  [preflight] Reading configuration from the cluster...
 7  [preflight] FYI: You can look at this config file with &#39;kubectl -n kube-system get cm kubeadm-config -o yaml&#39;
 8  [kubelet-start] Writing kubelet configuration to file &amp;quot;/var/lib/kubelet/config.yaml&amp;quot;
 9  [kubelet-start] Writing kubelet environment file with flags to file &amp;quot;/var/lib/kubelet/kubeadm-flags.env&amp;quot;
10  [kubelet-start] Starting the kubelet
11  [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
12  
13  This node has joined the cluster:
14  * Certificate signing request was sent to apiserver and a response was received.
15  * The Kubelet was informed of the new secure connection details.
16  
17  Run &#39;kubectl get nodes&#39; on the control-plane to see this node join the cluster.
18  
19  [root@k8s-work2 ~]#
20  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org362c07d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-查询集群节点&#34;&gt;2、查询集群节点&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1  [root@k8s-master kubeadm-install-k8s]# kubectl  get node -o wide
2  NAME         STATUS   ROLES                  AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME
3  k8s-master   Ready    control-plane,master   50m     v1.23.6   172.25.140.216   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://20.10.21
4  k8s-work1    Ready    &amp;lt;none&amp;gt;                 9m50s   v1.23.6   172.25.140.215   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://20.10.21
5  k8s-work2    Ready    &amp;lt;none&amp;gt;                 2m41s   v1.23.6   172.25.140.214   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://20.10.21
6  [root@k8s-master kubeadm-install-k8s]#
7  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;都为就绪状态了&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orge747a84&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;四-验证&#34;&gt;四、验证&lt;/h2&gt;

&lt;p&gt;k8s 集群部署 nginx 服务，并通过浏览器进行访问验证。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org5ec8220&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-创建-pod&#34;&gt;1、创建 pod&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master kubeadm-install-k8s]# kubectl create deployment nginx --image=nginx
 2  deployment.apps/nginx created
 3  [root@k8s-master kubeadm-install-k8s]# kubectl expose deployment nginx --port=80 --type=NodePort
 4  service/nginx exposed
 5  [root@k8s-master kubeadm-install-k8s]# kubectl get pod,svc
 6  NAME                         READY   STATUS    RESTARTS   AGE
 7  pod/nginx-85b98978db-qkd6h   1/1     Running   0          36s
 8  
 9  NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
10  service/kubernetes   ClusterIP   10.96.0.1       &amp;lt;none&amp;gt;        443/TCP        54m
11  service/nginx        NodePort    10.108.180.91   &amp;lt;none&amp;gt;        80:31648/TCP   15s
12  [root@k8s-master kubeadm-install-k8s]#
13  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgd537fac&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-访问-nginx&#34;&gt;2、访问 Nginx&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master kubeadm-install-k8s]# curl 10.108.180.91:80
 2  &amp;lt;!DOCTYPE html&amp;gt;
 3  &amp;lt;html&amp;gt;
 4  &amp;lt;head&amp;gt;
 5  &amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
 6  &amp;lt;style&amp;gt;
 7  html { color-scheme: light dark; }
 8  body { width: 35em; margin: 0 auto;
 9  font-family: Tahoma, Verdana, Arial, sans-serif; }
10  &amp;lt;/style&amp;gt;
11  &amp;lt;/head&amp;gt;
12  &amp;lt;body&amp;gt;
13  &amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
14  &amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
15  working. Further configuration is required.&amp;lt;/p&amp;gt;
16  
17  &amp;lt;p&amp;gt;For online documentation and support please refer to
18  &amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
19  Commercial support is available at
20  &amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;
21  
22  &amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
23  &amp;lt;/body&amp;gt;
24  &amp;lt;/html&amp;gt;
25  [root@k8s-master kubeadm-install-k8s]#
26  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此：kubeadm方式的k8s集群已经部署完成。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orga6cfda4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;faq&#34;&gt;FAQ&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org4405cb7&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-k8s编译报错&#34;&gt;1、k8s编译报错&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt; 1  ...
 2  ...
 3  [kubelet-check] It seems like the kubelet isn&#39;t running or healthy.
 4  [kubelet-check] The HTTP call equal to &#39;curl -sSL http://localhost:10248/healthz&#39; failed with error: Get &amp;quot;http://localhost:10248/healthz&amp;quot;: dial tcp [::1]:10248: connect: connection refused.
 5  
 6   Unfortunately, an error has occurred:
 7       timed out waiting for the condition
 8  
 9   This error is likely caused by:
10       - The kubelet is not running
11       - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)
12  
13   If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
14       - &#39;systemctl status kubelet&#39;
15       - &#39;journalctl -xeu kubelet&#39;
16  
17   Additionally, a control plane component may have crashed or exited when started by the container runtime.
18   To troubleshoot, list all containers using your preferred container runtimes CLI.
19  
20   Here is one example how you may list all Kubernetes containers running in docker:
21       - &#39;docker ps -a | grep kube | grep -v pause&#39;
22       Once you have found the failing container, you can inspect its logs with:
23       - &#39;docker logs CONTAINERID&#39;
24  
25  error execution phase wait-control-plane: couldn&#39;t initialize a Kubernetes cluster
26  To see the stack trace of this error execute with --v=5 or higher
27  
28  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看日志&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    Apr 26 20:33:30 test3 kubelet: I0426 20:33:30.588349   21936 docker_service.go:264] &amp;quot;Docker Info&amp;quot; dockerInfo=&amp;amp;{ID:2NSH:KJPQ:XOKI:5XHN:ULL3:L4LG:SXA4:PR6J:DITW:HHCF:2RKL:U2NJ Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:7 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:false IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:45 SystemTime:2022-04-26T20:33:30.583063427+08:00 LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion: NEventsListener:0 KernelVersion:3.10.0-1160.59.1.el7.x86_64 OperatingSystem:CentOS Linux 7 (Core) OSVersion: OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc000263340 NCPU:2 MemTotal:3873665024 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:k8s-master Labels:[] ExperimentalBuild:false ServerVersion:18.06.3-ce ClusterStore: ClusterAdvertise: Runtimes:map[runc:{Path:docker-runc Args:[] Shim:&amp;lt;nil&amp;gt;}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:&amp;lt;nil&amp;gt; Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:468a545b9edcd5932818eb9de8e72413e616e86e Expected:468a545b9edcd5932818eb9de8e72413e616e86e} RuncCommit:{ID:a592beb5bc4c4092b1b1bac971afed27687340c5 Expected:a592beb5bc4c4092b1b1bac971afed27687340c5} InitCommit:{ID:fec3683 Expected:fec3683} SecurityOptions:[name=seccomp,profile=default] ProductLicense: DefaultAddressPools:[] Warnings:[]}
Apr 26 20:33:30 test3 kubelet: E0426 20:33:30.588383   21936 server.go:302] &amp;quot;Failed to run kubelet&amp;quot; err=&amp;quot;failed to run Kubelet: misconfiguration: kubelet cgroup driver: \&amp;quot;systemd\&amp;quot; is different from docker cgroup driver: \&amp;quot;cgroupfs\&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看报错的最后解释kubelet cgroup driver: \&amp;ldquo;systemd\&amp;rdquo; is different from docker cgroup driver: \&amp;ldquo;cgroupfs\&amp;rdquo;&amp;ldquo;很明显 kubelet 与 Docker 的 cgroup 驱动程序不同，kubelet 为 systemd，而 Docker 为 cgroupfs。&lt;/p&gt;

&lt;p&gt;简单查看一下docker驱动：&lt;br /&gt;
[root@k8s-master opt]# docker info |grep Cgroup&lt;br /&gt;
Cgroup Driver: cgroupfs&lt;/p&gt;

&lt;p&gt;解决方案&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 重置初始化
[root@k8s-master ~]# kubeadm reset

# 删除相关配置文件
[root@k8s-master ~]# rm -rf $HOME/.kube/config  &amp;amp;&amp;amp; rm -rf $HOME/.kube

# 修改 Docker 驱动为 systemd（即&amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;]）
[root@k8s-master opt]# cat /etc/docker/daemon.json 
{
  &amp;quot;registry-mirrors&amp;quot;: [&amp;quot;https://q1rw9tzz.mirror.aliyuncs.com&amp;quot;],
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;]
}

# 重启 Docker
[root@k8s-master opt]# systemctl daemon-reload 
[root@k8s-master opt]# systemctl restart docker.service

# 再次初始化k8s即可
[root@k8s-master ~]# kubeadm init ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org0b33cbe&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-work-节点加入-k8s-集群报错&#34;&gt;2、work 节点加入 k8s 集群报错&lt;/h2&gt;

&lt;p&gt;报错1：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1  accepts at most 1 arg(s), received 3
2  To see the stack trace of this error execute with --v=5 or higher
3  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原因：命令不对，我是直接复制粘贴 k8s-master 初始化的终端输出结果，导致报错，所以最好先复制到 txt 文本下修改好格式再粘贴执行。&lt;/p&gt;

&lt;p&gt;报错2：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1  ...
2  [kubelet-check] The HTTP call equal to &#39;curl -sSL http://localhost:10248/healthz&#39; failed with error: Get &amp;quot;http://localhost:10248/healthz&amp;quot;: dial tcp 127.0.0.1:10248: connect: connection refused.
3  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解决方法同Master&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org9d65a10&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考&#34;&gt;参考&lt;/h1&gt;

&lt;p&gt;[01]&lt;a href=&#34;https://blog.csdn.net/IT_ZRS/article/details/124466870&#34;&gt;kubeadm 部署 k8s 集群&lt;/a&gt;&lt;br /&gt;
[02]&lt;a href=&#34;https://blog.csdn.net/oscarun/article/details/125595521?spm=1001.2014.3001.5502&#34;&gt;kubeadm系列-00-overview&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S学习笔记(07): 服务发现</title>
            <link>http://mospany.github.io/2022/10/18/k8s-service/</link>
            <pubDate>Tue, 18 Oct 2022 22:06:22 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/10/18/k8s-service/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;orga912502&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;service&#34;&gt;service&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org51c5e4e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;p&gt;01.&lt;a href=&#34;https://baijiahao.baidu.com/s?id=1681303264708121640&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;一文讲透K8s的Service概念&lt;/a&gt;&lt;br /&gt;
02.&lt;a href=&#34;https://blog.csdn.net/huahua1999/article/details/124237065&#34;&gt;k8s-service底层之 Iptables与 IPVS&lt;/a&gt;&lt;br /&gt;
03.&lt;a href=&#34;https://blog.csdn.net/Deepexi_Date/article/details/111042410&#34;&gt;K8S的ServiceIP实现原理&lt;/a&gt;&lt;br /&gt;
04.&lt;a href=&#34;http://t.zoukankan.com/fengdejiyixx-p-15568056.html&#34;&gt;http://t.zoukankan.com/fengdejiyixx-p-15568056.html&lt;/a&gt;&lt;br /&gt;
05.&lt;a href=&#34;https://blog.csdn.net/qq_37369726/article/details/121785627&#34;&gt;kubernetes pod间通信,跨namespace互访&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S学习笔记(06): 资源</title>
            <link>http://mospany.github.io/2022/09/28/resource/</link>
            <pubDate>Wed, 28 Sep 2022 20:10:36 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/09/28/resource/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;orge0656ad&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;k8s中所有的内容都抽象为资源， 资源实例化之后，叫做对象。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org32d4fb4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;资源类型介绍&#34;&gt;资源类型介绍&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;工作负载型资源对象（workload）：Pod，ReplicaSet，Deployment，StatefulSet，DaemonSet，Job，Cronjob &amp;#x2026;&lt;/li&gt;
&lt;li&gt;服务发现及均衡资源对象：Service，Ingress &amp;#x2026;&lt;/li&gt;
&lt;li&gt;配置与存储资源对象：Volume(存储卷)，CSI(容器存储接口,可以扩展各种各样的第三方存储卷)，ConfigMap，Secret，DownwardAPI&lt;/li&gt;
&lt;li&gt;集群级资源：Namespace，Node，Role，ClusterRole，RoleBinding，ClusterRoleBinding&lt;/li&gt;
&lt;li&gt;元数据型资源：HPA，PodTemplate，LimitRange&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orga7470a7&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;工作负载资源&#34;&gt;工作负载资源&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org78d773a&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;deployment&#34;&gt;Deployment&lt;/h2&gt;

&lt;p&gt;为了更好地解决服务编排的问题，k8s在V1.2版本开始，引入了deployment控制器，值得一提的是，这种控制器并不直接管理pod，&lt;br /&gt;
而是通过管理replicaset来间接管理pod，即：deployment管理replicaset，replicaset管理pod。所以deployment比replicaset的功能更强大。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/resource/deploy.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;deployment的主要功能有下面几个：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;支持replicaset的所有功能&lt;/li&gt;
&lt;li&gt;支持发布的停止、继续&lt;/li&gt;
&lt;li&gt;支持版本的滚动更新和版本回退&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org3b6bbe8&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;编写资源清单&#34;&gt;编写资源清单&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt; 1  $ cat test-deploy.yaml
 2  apiVersion: apps/v1
 3  kind: Deployment
 4  metadata:
 5    name: test-deployment
 6    namespace: dev
 7  spec:
 8    replicas: 3
 9    selector:
10      matchLabels:
11       app: nginx-pod
12    template:
13      metadata:
14        labels:
15          app: nginx-pod
16      spec:
17        containers:
18        - name: nginx
19          image: nginx:1.17.1
20  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org33be6c7&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;运行清单&#34;&gt;运行清单&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ k apply -f test-deploy.yaml
deployment.apps/test-deployment created 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org1474d63&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看效果&#34;&gt;查看效果&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ k get deploy -A
  NAMESPACE              NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
  dev                    test-deployment                3/3     3            3           94s
  guestbook-system       guestbook-controller-manager   1/1     1            1           59d
  kube-system            coredns                        2/2     2            2           63d
  kubernetes-dashboard   dashboard-metrics-scraper      1/1     1            1           14d
  kubernetes-dashboard   kubernetes-dashboard           1/1     1            1           14d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出deploy已经运行成功，3副本已处于READY状态。&lt;/p&gt;

&lt;p&gt;再查看它运行中的清单:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k get deploy -n dev test-deployment -o yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: &amp;quot;1&amp;quot;
    kubectl.kubernetes.io/last-applied-configuration: |
      {&amp;quot;apiVersion&amp;quot;:&amp;quot;apps/v1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;Deployment&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;test-deployment&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;dev&amp;quot;},&amp;quot;spec&amp;quot;:{&amp;quot;replicas&amp;quot;:3,&amp;quot;selector&amp;quot;:{&amp;quot;matchLabels&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx-pod&amp;quot;}},&amp;quot;template&amp;quot;:{&amp;quot;metadata&amp;quot;:{&amp;quot;labels&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx-pod&amp;quot;}},&amp;quot;spec&amp;quot;:{&amp;quot;containers&amp;quot;:[{&amp;quot;image&amp;quot;:&amp;quot;nginx:1.17.1&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;nginx&amp;quot;}]}}}}
  creationTimestamp: &amp;quot;2022-09-28T12:35:22Z&amp;quot;
  generation: 1
  name: test-deployment
  namespace: dev
  resourceVersion: &amp;quot;5045809&amp;quot;
  uid: 40feb568-1343-4046-8708-7e1bf5e5c384
spec:
  #spec.progressDeadlineSeconds 是可选配置项，用来指定在系统报告Deployment的failed progressing一一表现为resource的状态中 type=Progressing 、 Status=False 、 Reason=ProgressDeadlineExceeded 前可以等待的Deployment进行的秒数。Deployment controller会继续重试该Deployment。未来，在实现了自动回滚后， deployment controller在观察到这种状态时就会自动回滚。
  progressDeadlineSeconds: 600
  #.spec.replicas 是可以选字段，指定期望的pod数量，默认是1。
  replicas: 3
  revisionHistoryLimit: 10
  #.spec.selector是可选字段，用来指定 label selector ，圈定Deployment管理的pod范围。如果被指定， .spec.selector 必须匹配 .spec.template.metadata.labels，否则它将被API拒绝。如果.spec.selector 没有被指定， .spec.selector.matchLabels 默认是.spec.template.metadata.labels。在Pod的template跟.spec.template不同或者数量超过了.spec.replicas规定的数量的情况下，Deployment会杀掉label跟selector不同的Pod。
  selector:
    matchLabels:
      app: nginx-pod
  #.spec.strategy 指定新的Pod替换旧的Pod的策略。 .spec.strategy.type 可以是&amp;quot;Recreate&amp;quot;或者是&amp;quot;RollingUpdate&amp;quot;。&amp;quot;RollingUpdate&amp;quot;是默认值。
  strategy:
    rollingUpdate:
      #spec.strategy.rollingUpdate.maxSurge 是可选配置项，用来指定可以超过期望的Pod数量的最大个数。该值可以是一个绝对值（例如5）或者是期望的Pod数量的百分比（例如10%）。当 MaxUnavailable 为0时该值不可以为0。通过百分比计算的绝对值向上取整。默认值是1。
      maxSurge: 25%
      #.spec.strategy.rollingUpdate.maxUnavailable 是可选配置项，用来指定在升级过程中不可用Pod的最大数量。该值可以是一个绝对值（例如5），也可以是期望Pod数量的百分比（例如10%）。通过计算百分比的绝对值向下取整。 如 果 .spec.strategy.rollingUpdate.maxSurge 为0时，这个值不可以为0。默认值是1。例如，该值设置成30%，启动rolling update后旧的ReplicatSet将会立即缩容到期望的Pod数量的70%。新的Pod ready后，随着新的ReplicaSet的扩容，旧的ReplicaSet会进一步缩容确保在升级的所有时刻可以用的Pod数量至少是期望Pod数量的70%。
      maxUnavailable: 25%
    #滚动更新，简单定义 更新期间pod最多有几个等。可以指定 maxUnavailable 和 maxSurge 来控制 rolling update 进程。
    type: RollingUpdate
  #.spec.template 是 .spec中唯一要求的字段。.spec.template 是 pod template. 它跟 Pod有一模一样的schema，除了它是嵌套的并且不需要apiVersion 和 kind字段。另外为了划分Pod的范围，Deployment中的pod template必须指定适当的label（不要跟其他controller重复了，参考selector）和适当的重启策略。.spec.template.spec.restartPolicy 可以设置为 Always , 如果不指定的话这就是默认配置。
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx-pod
    spec:
      containers:
      - image: nginx:1.17.1
        imagePullPolicy: IfNotPresent
        name: nginx
        resources: {}
        #terminationMessagePath 表示容器的异常终止消息的路径，默认在 /dev/termination-log 下。当容器退出时，可以通过容器的状态看到退出信息。
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      #“ClusterFirst“:如果DNS查询与配置好的默认集群域名前缀不匹配，则将查询请求转发到从节点继承而来，作为查询的上游服务器。
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      #spec:schedulername参数指定调度器的名字，可以为 pod 选择某个调度器进行调度
      schedulerName: default-scheduler
      #安全上下文（Security Context）定义 Pod 或 Container 的特权与访问控制设置。
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 3
  conditions:
  - lastTransitionTime: &amp;quot;2022-09-28T12:35:25Z&amp;quot;
    lastUpdateTime: &amp;quot;2022-09-28T12:35:25Z&amp;quot;
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: &amp;quot;True&amp;quot;
    type: Available
  - lastTransitionTime: &amp;quot;2022-09-28T12:35:22Z&amp;quot;
    lastUpdateTime: &amp;quot;2022-09-28T12:35:25Z&amp;quot;
    message: ReplicaSet &amp;quot;test-deployment-5d9c9b97bb&amp;quot; has successfully progressed.
    reason: NewReplicaSetAvailable
    status: &amp;quot;True&amp;quot;
    type: Progressing
  observedGeneration: 1
  readyReplicas: 3
  replicas: 3
  updatedReplicas: 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org3fa900a&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;replicaset&#34;&gt;ReplicaSet&lt;/h2&gt;

&lt;p&gt;ReplicaSet是kubernetes中的一种副本控制器，主要作用是控制由其管理的pod，使pod副本的数量始终维持在预设的个数。&lt;/p&gt;

&lt;p&gt;kubernetes官方推荐不要直接使用ReplicaSet，用Deployments取而代之，Deployments是比ReplicaSet更高级的概念，它会管理ReplicaSet并提供很多其它有用的特性，最重要的是Deployments支持声明式更新，声明式更好相比于命令式更新的好处是不会丢失历史变更。总结起来就是：不要再直接使用ReplicaSet。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgfdcdb41&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看效果-1&#34;&gt;查看效果&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ k get rs -A
NAMESPACE              NAME                                      DESIRED   CURRENT   READY   AGE
dev                    test-deployment-5d9c9b97bb                3         3         3       23h
guestbook-system       guestbook-controller-manager-84cd65964f   1         1         1       60d
kube-system            coredns-78fcd69978                        2         2         2       64d
kubernetes-dashboard   dashboard-metrics-scraper-7c857855d9      1         1         1       15d
kubernetes-dashboard   kubernetes-dashboard-6b79449649           1         1         1       15d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看清单：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k get rs -n dev test-deployment-5d9c9b97bb -o yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  annotations:
    deployment.kubernetes.io/desired-replicas: &amp;quot;3&amp;quot;
    deployment.kubernetes.io/max-replicas: &amp;quot;4&amp;quot;
    deployment.kubernetes.io/revision: &amp;quot;1&amp;quot;
  creationTimestamp: &amp;quot;2022-09-28T12:35:22Z&amp;quot;
  generation: 1
  labels:
    app: nginx-pod
    pod-template-hash: 5d9c9b97bb
  name: test-deployment-5d9c9b97bb
  namespace: dev
  ownerReferences:
  - apiVersion: apps/v1
    #控制特定的从属对象是否可以阻止垃圾收集删除其所有者对象
    blockOwnerDeletion: true
    controller: true
    kind: Deployment
    name: test-deployment
    uid: 40feb568-1343-4046-8708-7e1bf5e5c384
  resourceVersion: &amp;quot;5045808&amp;quot;
  uid: 1dc27f38-b2db-48a1-b76a-ad044e3da2d7
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-pod
      pod-template-hash: 5d9c9b97bb
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx-pod
        pod-template-hash: 5d9c9b97bb
    spec:
      containers:
      - image: nginx:1.17.1
        imagePullPolicy: IfNotPresent
        name: nginx
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 3
  fullyLabeledReplicas: 3
  observedGeneration: 1
  readyReplicas: 3
  replicas: 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org1c8a1ca&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;pod&#34;&gt;Pod&lt;/h2&gt;

&lt;p&gt;Pod是kubernetes中最小的资源管理组件，Pod也是最小化运行容器化应用的资源对象。一个Pod代表着集群中运行的一个进程。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k get pod -A
NAMESPACE              NAME                                            READY   STATUS    RESTARTS         AGE
dev                    test-deployment-5d9c9b97bb-7hmqb                1/1     Running   0                23h
dev                    test-deployment-5d9c9b97bb-bdwhq                1/1     Running   0                23h
dev                    test-deployment-5d9c9b97bb-l9fj2                1/1     Running   0                23h
guestbook-system       guestbook-controller-manager-84cd65964f-c2z8x   2/2     Running   184 (34h ago)    60d
kube-system            coredns-78fcd69978-s52td                        1/1     Running   7 (11d ago)      64d
kube-system            coredns-78fcd69978-z5fvx                        1/1     Running   7 (11d ago)      64d
kube-system            etcd-docker-desktop                             1/1     Running   7 (11d ago)      64d
kube-system            kube-apiserver-docker-desktop                   1/1     Running   7 (11d ago)      64d
kube-system            kube-controller-manager-docker-desktop          1/1     Running   7 (11d ago)      64d
kube-system            kube-proxy-b5954                                1/1     Running   7 (11d ago)      64d
kube-system            kube-scheduler-docker-desktop                   1/1     Running   146 (34h ago)    64d
kube-system            storage-provisioner                             1/1     Running   229 (34h ago)    64d
kube-system            vpnkit-controller                               1/1     Running   4395 (17m ago)   64d
kubernetes-dashboard   dashboard-metrics-scraper-7c857855d9-g4slf      1/1     Running   1 (11d ago)      15d
kubernetes-dashboard   kubernetes-dashboard-6b79449649-lrnz8           1/1     Running   10 (3d10h ago)   15d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看清单：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k get pod -n dev test-deployment-5d9c9b97bb-7hmqb -o yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: &amp;quot;2022-09-28T12:35:22Z&amp;quot;
  generateName: test-deployment-5d9c9b97bb-
  labels:
    app: nginx-pod
    pod-template-hash: 5d9c9b97bb
  name: test-deployment-5d9c9b97bb-7hmqb
  namespace: dev
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: test-deployment-5d9c9b97bb
    uid: 1dc27f38-b2db-48a1-b76a-ad044e3da2d7
  resourceVersion: &amp;quot;5045804&amp;quot;
  uid: 06aeda67-ccaa-4715-ab1a-992ae409cd12
spec:
  containers:
  - image: nginx:1.17.1
    imagePullPolicy: IfNotPresent
    name: nginx
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-sgghr
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: docker-desktop
  #优先级在普通的 pod 执行优先级的剔除是没什么问题的，但是在 job 控制器来运行的 pod 上就是一个灾难，如果 job 控制器运行的任务计划 pod 正在执行任务，此时因为集群节点的资源不够用导致 job 的 pod 被剔除集群节点，从而导致指定运行的任务被搁置，为了解决这个问题，可以在 PriorityClass 中设置属性 preemptionPolicy ，当它的值为 preemptionLowerPriorty（默认）时，则正常执行抢占策略（表示优先级低会被剔除）。如果将值设置为 Never 时则为默认不抢占，不会被剔除而是等待调度机会。
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-sgghr
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-09-28T12:35:22Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-09-28T12:35:25Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-09-28T12:35:25Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-09-28T12:35:22Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: PodScheduled
  containerStatuses:
  - containerID: docker://1453a8ace21fbf107edd376e3357e7c9880afe6c67b2fdde535092bb8c654ddf
    image: nginx:1.17.1
    imageID: docker-pullable://nginx@sha256:b4b9b3eee194703fc2fa8afa5b7510c77ae70cfba567af1376a573a967c03dbb
    lastState: {}
    name: nginx
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: &amp;quot;2022-09-28T12:35:24Z&amp;quot;
  hostIP: 192.168.65.4
  phase: Running
  podIP: 10.1.0.57
  podIPs:
  - ip: 10.1.0.57
  qosClass: BestEffort
  startTime: &amp;quot;2022-09-28T12:35:22Z&amp;quot; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org4ffc8cc&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;statefulset&#34;&gt;Statefulset&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;orgc4ecdec&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;应用场景&#34;&gt;应用场景&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;稳定的不共享持久化存储：即每个pod的存储资源是不共享的，且pod重新调度后还是能访问到相同的持久化数据，基于pvc实现。&lt;/li&gt;
&lt;li&gt;稳定的网络标志：即pod重新调度后其PodName和HostName不变，且PodName和HostName是相同的，基于Headless Service来实现的。&lt;/li&gt;
&lt;li&gt;有序部署，有序扩展：即pod是有顺序的，在部署或者扩展的时候是根据定义的顺序依次依序部署的（即从0到N-1,在下一个Pod运行之前所有之前的pod必都是Running状态或者Ready状态），是基于init containers来实现的。&lt;/li&gt;
&lt;li&gt;有序收缩：在pod删除时是从最后一个依次往前删除，即从N-1到0.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;基于上面的特性，可以发现statefulset由以下几个部分组成：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用于定义网络标志（DNS domain）的headless service&lt;/li&gt;
&lt;li&gt;用于**创建pvc的volumeClaimTemplates**&lt;/li&gt;
&lt;li&gt;具体的statefulSet应用&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org75c4d8e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;创建yaml文件示例&#34;&gt;创建yaml文件示例&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None                                 # 要点1，可以不用指定clusterIP
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: &amp;quot;nginx&amp;quot;                           # 要点2，指定serviceName名称
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
          name: web
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org9af1bda&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看效果-2&#34;&gt;查看效果&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# mosp @ mospdeMacBook-Pro in ~/study/myblog/mospan-hugo-blog on git:master x [17:35:39]
$ k get sts -A
NAMESPACE   NAME   READY   AGE
default     web    3/3     24h

# 可以看出StatefulSet不走ReplicalSet
$ k get rs -A
NAMESPACE              NAME                                      DESIRED   CURRENT   READY   AGE
guestbook-system       guestbook-controller-manager-84cd65964f   1         1         1       63d
kube-system            coredns-78fcd69978                        2         2         2       67d
kubernetes-dashboard   dashboard-metrics-scraper-7c857855d9      1         1         1       19d
kubernetes-dashboard   kubernetes-dashboard-6b79449649           1         1         1       19d

# mosp @ mospdeMacBook-Pro in ~/study/myblog/mospan-hugo-blog on git:master x [11:11:00]
$ k get svc -A
NAMESPACE              NAME                                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
default                kubernetes                                     ClusterIP   10.96.0.1       &amp;lt;none&amp;gt;        443/TCP                  67d
default                nginx                                          ClusterIP   None            &amp;lt;none&amp;gt;        80/TCP                   24h
guestbook-system       guestbook-controller-manager-metrics-service   ClusterIP   10.107.190.68   &amp;lt;none&amp;gt;        8443/TCP                 63d
kube-system            kube-dns                                       ClusterIP   10.96.0.10      &amp;lt;none&amp;gt;        53/UDP,53/TCP,9153/TCP   67d
kubernetes-dashboard   dashboard-metrics-scraper                      ClusterIP   10.96.38.104    &amp;lt;none&amp;gt;        8000/TCP                 19d
kubernetes-dashboard   kubernetes-dashboard                           ClusterIP   10.102.50.227   &amp;lt;none&amp;gt;        443/TCP                  19d

# mosp @ mospdeMacBook-Pro in ~/study/myblog/mospan-hugo-blog on git:master x [11:11:13]
$ k get pod -A
NAMESPACE              NAME                                            READY   STATUS    RESTARTS         AGE
default                web-0                                           1/1     Running   0                24h
default                web-1                                           1/1     Running   0                24h
default                web-2                                           1/1     Running   0                24h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgad33d68&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看清单&#34;&gt;查看清单&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;查看service&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k get svc nginx -n -o yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {&amp;quot;apiVersion&amp;quot;:&amp;quot;v1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;Service&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;labels&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx&amp;quot;},&amp;quot;name&amp;quot;:&amp;quot;nginx&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;default&amp;quot;},&amp;quot;spec&amp;quot;:{&amp;quot;clusterIP&amp;quot;:&amp;quot;None&amp;quot;,&amp;quot;ports&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;web&amp;quot;,&amp;quot;port&amp;quot;:80}],&amp;quot;selector&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx&amp;quot;}}}
  creationTimestamp: &amp;quot;2022-10-02T03:06:17Z&amp;quot;
  labels:
    app: nginx
  name: nginx
  namespace: default
  resourceVersion: &amp;quot;5343708&amp;quot;
  uid: ff6a9fb8-4abd-4fda-af08-6ecdf7695b5a
spec:
  clusterIP: None
  clusterIPs:
  - None
  #kube-proxy 基于 spec.internalTrafficPolicy 的设置来过滤路由的目标服务端点。当它的值设为 Local 时，只选择节点本地的服务端点。当它的值设为 Cluster 或缺省时，则选择所有的服务端点。
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  #用spec.ipFamilyPolicy字段配置的，可设置为以下选项之一：
  #SingleStack
  #PreferDualStack
  #RequireDualStack
  #要使用双堆栈支持，你需要设置.spec.ipFamilyPolicy为PreferredDualStack或RequiredDualStack。此功能在Kubernetes中默认启用，还包括通过IPv4和IPv6地址的脱离集群出口路由。
  ipFamilyPolicy: SingleStack
  ports:
  - name: web
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  #使用svc.spec.sessionAffinity设置会话亲和性，默认是None。指定为ClientIP会使来自同一个Client IP的请求转发到同一个Pod上。
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看StatefulSet&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  $ k get sts web -o yaml
 2  apiVersion: apps/v1
 3  kind: StatefulSet
 4  metadata:
 5    annotations:
 6      kubectl.kubernetes.io/last-applied-configuration: |
 7        {&amp;quot;apiVersion&amp;quot;:&amp;quot;apps/v1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;StatefulSet&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;web&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;default&amp;quot;},&amp;quot;spec&amp;quot;:{&amp;quot;replicas&amp;quot;:3,&amp;quot;selector&amp;quot;:{&amp;quot;matchLabels&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx&amp;quot;}},&amp;quot;serviceName&amp;quot;:&amp;quot;nginx&amp;quot;,&amp;quot;template&amp;quot;:{&amp;quot;metadata&amp;quot;:{&amp;quot;labels&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx&amp;quot;}},&amp;quot;spec&amp;quot;:{&amp;quot;containers&amp;quot;:[{&amp;quot;image&amp;quot;:&amp;quot;nginx:alpine&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;nginx&amp;quot;,&amp;quot;ports&amp;quot;:[{&amp;quot;containerPort&amp;quot;:80,&amp;quot;name&amp;quot;:&amp;quot;web&amp;quot;}]}]}}}}
 8    creationTimestamp: &amp;quot;2022-10-02T03:06:17Z&amp;quot;
 9    generation: 1
10    name: web
11    namespace: default
12    resourceVersion: &amp;quot;5343757&amp;quot;
13    uid: b68fa0b0-ce5e-428c-b39e-b41956ddcc12
14  spec:
15    #pod管理策略,v1.7+可以通过.spec.podManagementPolicy设置pod的管理策略，支持以下俩中方式
16    #OrderedReady:默认方式，按照pod的次序依次创建每个pod并等待ready之后才创建后面的pod。
17    #Parallel：并行创建或删除pod，和deployment类型的pod一样。（不等待前面的pod ready就开始创建所有的pod）。
18    podManagementPolicy: OrderedReady
19    replicas: 3
20    revisionHistoryLimit: 10
21    selector:
22      matchLabels:
23        app: nginx
24    serviceName: nginx
25    template:
26      metadata:
27        creationTimestamp: null
28        labels:
29          app: nginx
30      spec:
31        containers:
32        - image: nginx:alpine
33          imagePullPolicy: IfNotPresent
34          name: nginx
35          ports:
36          - containerPort: 80
37            name: web
38            protocol: TCP
39          resources: {}
40          terminationMessagePath: /dev/termination-log
41          terminationMessagePolicy: File
42        dnsPolicy: ClusterFirst
43        restartPolicy: Always
44        schedulerName: default-scheduler
45        securityContext: {}
46        terminationGracePeriodSeconds: 30
47    updateStrategy:
48      rollingUpdate:
49        partition: 0
50      type: RollingUpdate
51  status:
52    availableReplicas: 3
53    collisionCount: 0
54    currentReplicas: 3
55    currentRevision: web-6d796b6548
56    observedGeneration: 1
57    readyReplicas: 3
58    replicas: 3
59    updateRevision: web-6d796b6548
60    updatedReplicas: 3
61  
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看Pod清单&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k get pod web-0 -o yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: &amp;quot;2022-10-02T03:06:17Z&amp;quot;
  generateName: web-
  labels:
    app: nginx
    controller-revision-hash: web-6d796b6548
    statefulset.kubernetes.io/pod-name: web-0
  name: web-0
  namespace: default
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: StatefulSet
    name: web
    uid: b68fa0b0-ce5e-428c-b39e-b41956ddcc12
  resourceVersion: &amp;quot;5343727&amp;quot;
  uid: 04b8e1f1-eed1-493e-a9b3-3b0791eca087
spec:
  containers:
  - image: nginx:alpine
    imagePullPolicy: IfNotPresent
    name: nginx
    ports:
    - containerPort: 80
      name: web
      protocol: TCP
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-v2xb5
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  hostname: web-0
  nodeName: docker-desktop
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  subdomain: nginx
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-v2xb5
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-10-02T03:06:17Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-10-02T03:06:18Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-10-02T03:06:18Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-10-02T03:06:17Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: PodScheduled
  containerStatuses:
  - containerID: docker://bd6e1b11a24457b4892fd83fb306ec2bac9adca1a280b079b9502a0672a9d521
    image: nginx:alpine
    imageID: docker-pullable://nginx@sha256:082f8c10bd47b6acc8ef15ae61ae45dd8fde0e9f389a8b5cb23c37408642bf5d
    lastState: {}
    name: nginx
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: &amp;quot;2022-10-02T03:06:18Z&amp;quot;
  hostIP: 192.168.65.4
  phase: Running
  podIP: 10.1.0.72
  podIPs:
  - ip: 10.1.0.72
  qosClass: BestEffort
  startTime: &amp;quot;2022-10-02T03:06:17Z&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a id=&#34;orga9c50ad&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;daemonset&#34;&gt;DaemonSet&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;orgface580&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;作用及概念&#34;&gt;作用及概念&lt;/h3&gt;

&lt;p&gt;DaemonSet确保集群中每个（部分）node运行一份po副本，当node加入集群时创建pod，当node离开集群时回收pod。&lt;/p&gt;

&lt;p&gt;如果删除DaemonSet，其创建的所有pod也被删除， DaemonSet中的pod覆盖整个集群。&lt;/p&gt;

&lt;p&gt;当需要在集群内每个node运行同一个pod，使用 DaemonSet是有价值的，以下是典型使用场景：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;运行集群存储守护进程，如 glusterd、ceph&lt;/li&gt;
&lt;li&gt;运行集群日志收集守护进程，如 fluent、 logstash&lt;/li&gt;
&lt;li&gt;运行节点监控守护进程，如 Prometheus Node Exporter， collectd， Datadog agent，&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org9260917&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;配置清单&#34;&gt;配置清单&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ cat daemonset-demo.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: deamonset-demo
  namespace: default
spec:
  selector:
    matchLabels:
     app: nginx-ds
  template:
    metadata:
      labels:
        app: nginx-ds
    spec:
      containers:
      - name: nginx
        image: nginx:1.17.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org89256ec&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看效果-3&#34;&gt;查看效果&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource/daemonset [18:51:51]
$ k get ds -A
NAMESPACE     NAME             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
default       deamonset-demo   1         1         1       1            1           &amp;lt;none&amp;gt;                   10m
kube-system   kube-proxy       1         1         1       1            1           kubernetes.io/os=linux   68d

# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource/daemonset [18:53:22]
$ k get rs -A
NAMESPACE              NAME                                      DESIRED   CURRENT   READY   AGE
guestbook-system       guestbook-controller-manager-84cd65964f   1         1         1       63d
kube-system            coredns-78fcd69978                        2         2         2       68d
kubernetes-dashboard   dashboard-metrics-scraper-7c857855d9      1         1         1       19d
kubernetes-dashboard   kubernetes-dashboard-6b79449649           1         1         1       19d

# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource/daemonset [18:53:28]
$ k get pod -A
NAMESPACE              NAME                                            READY   STATUS    RESTARTS          AGE
default                deamonset-demo-lbmmq                            1/1     Running   0                 10m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org7a1d726&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;job&#34;&gt;Job&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;org1def512&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;作用及概念-1&#34;&gt;作用及概念&lt;/h3&gt;

&lt;p&gt;详见：&lt;a href=&#34;https://blog.csdn.net/u012124304/article/details/107729972&#34;&gt;k8s Job详解&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org59a91b0&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;配置清单-1&#34;&gt;配置清单&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ cat job-demo.yaml

apiVersion: batch/v1
kind: Job
metadata:
  name: echo-time
spec:
  completions: 10 #任务个数
  parallelism: 5 #并行数
  backoffLimit: 2 #失败最多执行几次
  template:
    spec:
      containers:
      - name: echo-time
        image: centos:latest
        imagePullPolicy: IfNotPresent
        command:
        - /bin/bash
        - &amp;quot;-c&amp;quot;
        - &amp;quot;for i in `seq 1 100`;do echo $i: `date` &amp;amp;&amp;amp; sleep 1;done&amp;quot;
      restartPolicy: OnFailure
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org2f3a615&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看效果-4&#34;&gt;查看效果&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource/job [20:17:06]
$ k get job -A
NAMESPACE   NAME        COMPLETIONS   DURATION   AGE
default     echo-time   10/10         3m28s      5m40s

# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource/job [20:20:49]
$ k get pod -A
NAMESPACE              NAME                                            READY   STATUS      RESTARTS         AGE
default                deamonset-demo-lbmmq                            1/1     Running     1 (9h ago)       5d1h
default                echo-time--1-24lfm                              0/1     Completed   0                4m7s
default                echo-time--1-5drjv                              0/1     Completed   0                4m9s
default                echo-time--1-869rd                              0/1     Completed   0                4m7s
default                echo-time--1-8fd7m                              0/1     Completed   0                4m9s
default                echo-time--1-cqfjs                              0/1     Completed   0                5m52s
default                echo-time--1-f276r                              0/1     Completed   0                5m52s
default                echo-time--1-h4v9l                              0/1     Completed   0                5m52s
default                echo-time--1-lx4ww                              0/1     Completed   0                5m52s
default                echo-time--1-qd4kc                              0/1     Completed   0                4m8s
default                echo-time--1-zfchg                              0/1     Completed   0                5m52s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个Pod日志都能从1输出到100。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org6491cbd&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;cronjob&#34;&gt;cronJob&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;org6cd6fcb&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;作用及概念-2&#34;&gt;作用及概念&lt;/h3&gt;

&lt;p&gt;CronJobs 对于创建周期性的、反复重复的任务很有用，例如执行数据备份或者发送邮件。 CronJobs 也可以用来计划在指定时间来执行的独立任务，例如计划当集群看起来很空闲时 执行某个 Job。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org63ce1b0&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;配置清单-2&#34;&gt;配置清单&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ cat cronjob-demo.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cronjob-demo
spec:
  schedule: &amp;quot;*/1 * * * *&amp;quot;
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cronjob-demo
            image: busybox
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org2e52734&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看效果-5&#34;&gt;查看效果&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource [20:27:32]
$ k get cronjobs.batch -A
NAMESPACE   NAME           SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
default     cronjob-demo   */1 * * * *   False     0        40s             7m53s

# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource [20:27:40]
$ k get pod -A
NAMESPACE              NAME                                            READY   STATUS      RESTARTS         AGE
default                cronjob-demo-27761065--1-bjdj7                  0/1     Completed   0                2m47s
default                cronjob-demo-27761066--1-z9ttq                  0/1     Completed   0                107s
default                cronjob-demo-27761067--1-9x5fn                  0/1     Completed   0                47s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过观察发现，它只保留最新的3个pod，其他的将被删除掉免得占用资源。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org546b0d0&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;hpa&#34;&gt;HPA&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;orgba6b263&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;作用及概念-3&#34;&gt;作用及概念&lt;/h3&gt;

&lt;p&gt;&lt;a id=&#34;org278c786&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;label-annotation-selector&#34;&gt;Label&amp;amp;Annotation&amp;amp;Selector&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;orgacecd12&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;label&#34;&gt;Label&lt;/h3&gt;

&lt;p&gt;详见：&lt;a href=&#34;https://www.cnblogs.com/chuanzhang053/p/16351442.html&#34;&gt;k8s中label和label selector的基本概念及使用方法&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org1cb2230&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;annotation&#34;&gt;Annotation&lt;/h3&gt;

&lt;p&gt;详见：&lt;a href=&#34;https://www.jianshu.com/p/21275c1c701c&#34;&gt;K8s Annotation（注解）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org33a8552&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;selector&#34;&gt;Selector&lt;/h3&gt;

&lt;p&gt;Label selector的使用场景:&lt;/p&gt;

&lt;p&gt;1.kube-controller进程通过资源对象RC上定义的Label Selector来筛选要监控的Pod副本的数量，从而实现Pod副本的数量始终符合预期设定的全自动控制流程&lt;/p&gt;

&lt;p&gt;2.kupe-proxy进程通过Service的Label Selector来选择对应的Pod，自动建立器每个Service到对应Pod的请求转发路由表，从而实现Service的智能负载均衡机制&lt;/p&gt;

&lt;p&gt;3.通过对某些Node定义特定的Label,并且在Pod定义文件中使用NodeSelector这种标签调度策略，Kube-scheduler进程可以实现Pod定向调度的特性&lt;/p&gt;

&lt;p&gt;详见: &lt;a href=&#34;https://blog.csdn.net/weixin_35673021/article/details/112946553&#34;&gt;k8s selector_k8s之Label与Selector&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org14788c6&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;p&gt;1.&lt;a href=&#34;https://blog.51cto.com/u_15155091/2723613&#34;&gt;k8s之terminationMessagePath&lt;/a&gt;&lt;br /&gt;
2.&lt;a href=&#34;https://blog.csdn.net/dkfajsldfsdfsd/article/details/81209150&#34;&gt;Kubernetes之DNS&lt;/a&gt;&lt;br /&gt;
3.&lt;a href=&#34;https://www.jianshu.com/p/1f64a4694ace&#34;&gt;Kubernetes——调度器Scheduler&lt;/a&gt;&lt;br /&gt;
4.&lt;a href=&#34;https://cloud.tencent.com/developer/article/1748675&#34;&gt;k8s之securityContext&lt;/a&gt;&lt;br /&gt;
5.&lt;a href=&#34;https://www.jianshu.com/p/229ef1daf986&#34;&gt;Kubernetes的所有者和依赖&lt;/a&gt;&lt;br /&gt;
6.&lt;a href=&#34;https://blog.csdn.net/qq_44079072/article/details/121144611&#34;&gt;k8s之pod详解&lt;/a&gt;&lt;br /&gt;
7.&lt;a href=&#34;https://blog.csdn.net/weixin_45710286/article/details/125474671&#34;&gt;kubernetes 之 pod 调度策略（一）&lt;/a&gt;&lt;br /&gt;
8.&lt;a href=&#34;https://www.jianshu.com/p/ebb8f0ba67f6&#34;&gt;污点（taints）和容忍度（tolerations）&lt;/a&gt;&lt;br /&gt;
9.&lt;a href=&#34;https://www.oomspot.com//post/k8swangluomoxingfuwuneibuliuliangcelue&#34;&gt;k8s网络模型-服务内部流量策略&lt;/a&gt;&lt;br /&gt;
10.&lt;a href=&#34;https://www.jianshu.com/p/cedf8c9d18f1&#34;&gt;k8s中statefulset资源类型的深入理解&lt;/a&gt;&lt;br /&gt;
11.&lt;a href=&#34;https://copyfuture.com/blogs-details/202006012253389106o9ttg1qo5by175&#34;&gt;k8s job简介和访问&lt;/a&gt;&lt;br /&gt;
12.&lt;a href=&#34;https://blog.csdn.net/m0_47288926/article/details/122819880&#34;&gt;k8s-HPA&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S学习笔记(05): yaml语法</title>
            <link>http://mospany.github.io/2022/09/27/k8s-yaml/</link>
            <pubDate>Tue, 27 Sep 2022 19:58:47 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/09/27/k8s-yaml/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;orgfba5ff2&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;k8s支持的文件格式&#34;&gt;k8s支持的文件格式&lt;/h1&gt;

&lt;p&gt;Kubernetes支持YAML和JSON格式管理资源对象&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;JSON格式：主要用于api接口之间消息的传递&lt;/li&gt;
&lt;li&gt;YAML格式：用于配置和管理，YAML是一种简洁的非标记性语言，内容格式人性化，较易读&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org3c21164&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;yaml语言格式&#34;&gt;YAML语言格式&lt;/h1&gt;

&lt;p&gt;● 大小写敏感&lt;br /&gt;
● 使用缩进表示层级关系&lt;br /&gt;
● 不支持Tab键制表符缩进，只使用空格缩进&lt;br /&gt;
● 缩进的空格数目不重要，只要相同层级的元素左侧对齐即可，通常开头缩进两个空格&lt;br /&gt;
● 符号字符后缩进一个空格，如冒号，逗号，短横杠（-）等&lt;br /&gt;
● “&amp;#x2014;”表示YAML格式，一个文件的开始，用于分隔文件&lt;br /&gt;
● “#”表示注释&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgd843414&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;编写资源配置清单&#34;&gt;编写资源配置清单&lt;/h1&gt;

&lt;p&gt;[root@master test]# vim nginx-test.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#指定api版本标签
apiVersion: apps/v1
#定义资源的类型/角色，deployment为副本控制器
#此处资源类型可以是Deployment、Job、Ingress、Service等
kind: Deployment
#定义资源的元数据信息，比如资源的名称、namespace、标签等信息
metadata:
  #定义资源的名称，在同一个namespace空间中必须是唯一的
  name: nginx-test
  lables:
    app: nginx
#定义deployment资源需要的参数属性，诸如是否在容器失败时重新启动容器的属性
spec:
  #定义副本数量
  replicas: 3
  #定义标签选择器
  selector:
    #定义匹配标签
    matchLabels:
      #需与后面的.spec.template.metadata.labels定义的标签保持一致
      app: nginx
  #定义业务模板，如果有多个副本，所有副本的属性会按照模板的相关配置进行匹配
  template:
    metadata:
      #定义Pod副本将使用的标签，需与前面的.spec.selector.matchLabels定义的标签保持一致
      labels:
        app: nginx
    spec:
      #定义容器属性
      containers:
      #定义一个容器名，一个-name:定义一个容器
      - name: nginx
        #定义容器使用的镜像以及版本
        image: nginx:1.15.4
        ports:
        #定义容器对外的端口
        - containerPort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org32111ee&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;创建service服务对外提供访问并测试&#34;&gt;创建service服务对外提供访问并测试&lt;/h1&gt;

&lt;p&gt;[root@master test]# vim nginx-svc-test.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  labels:
    app: nginx
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
  selector:
    #此处定义的selector要与deployment所定义的selector相同
    #service依靠标签选择器来检索提供服务的nodes
    app: nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org7f24d32&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;详解k8s中的port&#34;&gt;详解k8s中的port&lt;/h1&gt;

&lt;p&gt;port是k8s集群内部访问service的端口，即通过clusterIP:port可以从Pod所在的Node上访问到service&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nodePort&lt;br /&gt;
nodePort是外部访问k8s集群中service的端口，通过nodeIP:nodePort可以从外部访问到某个service&lt;/li&gt;
&lt;li&gt;targetPort&lt;br /&gt;
targetPort是Pod的端口，从port或nodePort来的流量经过kube-proxy反向代理负载均衡转发到后端Pod的targetPort上，最后进入容器。&lt;/li&gt;
&lt;li&gt;containerPort&lt;br /&gt;
containerPort是Pod内部容器的端口，targetPort映射到containerPort。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orgbae1b9d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;获取资源配置清单的总结&#34;&gt;获取资源配置清单的总结&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;没有相关资源，使用run命令&amp;#x2013;dry-run选项&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run dryrun-test --image=nginx --port=80 --replicas=3 --dry-run -o yaml &amp;gt; dryrun-test.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;已有相关资源，使用get命令&amp;#x2013;export选项&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get deploy dryrun-test --export -o yaml &amp;gt; export-test.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt; ：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;现在不推荐使用“ export”（从1.14版本开始，通常应该在1.18版本中消失（在changelog中找不到））&lt;/li&gt;
&lt;li&gt;&amp;#x2013;dry-run is deprecated and can be replaced with &amp;#x2013;dry-run=client.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org86e8206&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.escapelife.site/posts/8032061c.html&#34;&gt;Kubernetes之YAML语法&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        
        <item>
            <title>K8S学习笔记(04): kubectl命令技巧大全</title>
            <link>http://mospany.github.io/2022/09/21/k8s-kubectl-all-cmd/</link>
            <pubDate>Wed, 21 Sep 2022 21:01:59 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/09/21/k8s-kubectl-all-cmd/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;org8c37926&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;kubectl-命令技巧大全&#34;&gt;kubectl 命令技巧大全&lt;/h1&gt;

&lt;p&gt;kubectl 命令是操作 Kubernetes 集群的最直接和最高效的途径，这个60多 MB 大小的二进制文件，到底有啥能耐呢？&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org42fef24&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;kubectl-自动补全&#34;&gt;Kubectl 自动补全&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;$ source &amp;lt;(kubectl completion bash) # setup autocomplete in bash, bash-completion package should be installed first.
$ source &amp;lt;(kubectl completion zsh)  # setup autocomplete in zsh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org27c704f&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;kubectl-上下文和配置&#34;&gt;Kubectl 上下文和配置&lt;/h1&gt;

&lt;p&gt;设置 kubectl 命令交互的 kubernetes 集群并修改配置信息。参阅 使用 kubeconfig 文件进行跨集群验证 获取关于配置文件的详细信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl config view # 显示合并后的 kubeconfig 配置

# 同时使用多个 kubeconfig 文件并查看合并后的配置
$ KUBECONFIG=~/.kube/config:~/.kube/kubconfig2 kubectl config view

# 获取 e2e 用户的密码
$ kubectl config view -o jsonpath=&#39;{.users[?(@.name == &amp;quot;e2e&amp;quot;)].user.password}&#39;

$ kubectl config current-context              # 显示当前的上下文
$ kubectl config use-context my-cluster-name  # 设置默认上下文为 my-cluster-name

# 向 kubeconf 中增加支持基本认证的新集群
$ kubectl config set-credentials kubeuser/foo.kubernetes.com --username=kubeuser --password=kubepassword

# 使用指定的用户名和 namespace 设置上下文
$ kubectl config set-context gce --user=cluster-admin --namespace=foo \
  &amp;amp;&amp;amp; kubectl config use-context gce
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgaa430dd&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;创建对象&#34;&gt;创建对象&lt;/h1&gt;

&lt;p&gt;Kubernetes 的清单文件可以使用 json 或 yaml 格式定义。可以以 .yaml、.yml、或者 .json 为扩展名。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f ./my-manifest.yaml           # 创建资源
$ kubectl create -f ./my1.yaml -f ./my2.yaml     # 使用多个文件创建资源
$ kubectl create -f ./dir                        # 使用目录下的所有清单文件来创建资源
$ kubectl create -f https://git.io/vPieo         # 使用 url 来创建资源
$ kubectl run nginx --image=nginx                # 启动一个 nginx 实例
$ kubectl explain pods,svc                       # 获取 pod 和 svc 的文档

# 从 stdin 输入中创建多个 YAML 对象
$ cat &amp;lt;&amp;lt;EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: busybox-sleep
spec:
  containers:
  - name: busybox
    image: busybox
    args:
    - sleep
    - &amp;quot;1000000&amp;quot;
---
apiVersion: v1
kind: Pod
metadata:
  name: busybox-sleep-less
spec:
  containers:
  - name: busybox
    image: busybox
    args:
    - sleep
    - &amp;quot;1000&amp;quot;
EOF

# 创建包含几个 key 的 Secret
$ cat &amp;lt;&amp;lt;EOF | kubectl create -f -
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  password: $(echo &amp;quot;s33msi4&amp;quot; | base64)
  username: $(echo &amp;quot;jane&amp;quot; | base64)
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org62782f5&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;显示和查找资源&#34;&gt;显示和查找资源&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;# Get commands with basic output
$ kubectl get services                          # 列出所有 namespace 中的所有 service
$ kubectl get pods --all-namespaces             # 列出所有 namespace 中的所有 pod
$ kubectl get pods -o wide                      # 列出所有 pod 并显示详细信息
$ kubectl get deployment my-dep                 # 列出指定 deployment

# 使用详细输出来描述命令
$ kubectl describe nodes my-node
$ kubectl describe pods my-pod

$ kubectl get services --sort-by=.metadata.name # List Services Sorted by Name

# 根据重启次数排序列出 pod
$ kubectl get pods --sort-by=&#39;.status.containerStatuses[0].restartCount&#39;

# 获取所有具有 app=cassandra 的 pod 中的 version 标签
$ kubectl get pods --selector=app=cassandra rc -o \
  jsonpath=&#39;{.items[*].metadata.labels.version}&#39;

# 获取所有节点的 ExternalIP
$ kubectl get nodes -o jsonpath=&#39;{.items[*].status.addresses[?(@.type==&amp;quot;ExternalIP&amp;quot;)].address}&#39;

# 列出属于某个 PC 的 Pod 的名字
# “jq”命令用于转换复杂的 jsonpath，参考 https://stedolan.github.io/jq/
$ sel=${$(kubectl get rc my-rc --output=json | jq -j &#39;.spec.selector | to_entries | .[] | &amp;quot;\(.key)=\(.value),&amp;quot;&#39;)%?}
$ echo $(kubectl get pods --selector=$sel --output=jsonpath={.items..metadata.name})

# 查看哪些节点已就绪
$ JSONPATH=&#39;{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}&#39; \
 &amp;amp;&amp;amp; kubectl get nodes -o jsonpath=&amp;quot;$JSONPATH&amp;quot; | grep &amp;quot;Ready=True&amp;quot;

# 列出当前 Pod 中使用的 Secret
$ kubectl get pods -o json | jq &#39;.items[].spec.containers[].env[]?.valueFrom.secretKeyRef.name&#39; | grep -v null | sort | uniq
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgc7a7c88&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;更新资源&#34;&gt;更新资源&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl rolling-update frontend-v1 -f frontend-v2.json           # 滚动更新 pod frontend-v1
$ kubectl rolling-update frontend-v1 frontend-v2 --image=image:v2  # 更新资源名称并更新镜像
$ kubectl rolling-update frontend --image=image:v2                 # 更新 frontend pod 中的镜像
$ kubectl rolling-update frontend-v1 frontend-v2 --rollback        # 退出已存在的进行中的滚动更新
$ cat pod.json | kubectl replace -f -                              # 基于 stdin 输入的 JSON 替换 pod

# 强制替换，删除后重新创建资源。会导致服务中断。
$ kubectl replace --force -f ./pod.json

# 为 nginx RC 创建服务，启用本地 80 端口连接到容器上的 8000 端口
$ kubectl expose rc nginx --port=80 --target-port=8000

# 更新单容器 pod 的镜像版本（tag）到 v4
$ kubectl get pod mypod -o yaml | sed &#39;s/\(image: myimage\):.*$/\1:v4/&#39; | kubectl replace -f -

$ kubectl label pods my-pod new-label=awesome                      # 添加标签
$ kubectl annotate pods my-pod icon-url=http://goo.gl/XXBTWq       # 添加注解
$ kubectl autoscale deployment foo --min=2 --max=10                # 自动扩展 deployment “foo”
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orga04178c&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;修补资源&#34;&gt;修补资源&lt;/h1&gt;

&lt;p&gt;使用策略合并补丁并修补资源。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl patch node k8s-node-1 -p &#39;{&amp;quot;spec&amp;quot;:{&amp;quot;unschedulable&amp;quot;:true}}&#39; # 部分更新节点

# 更新容器镜像； spec.containers[*].name 是必须的，因为这是合并的关键字
$ kubectl patch pod valid-pod -p &#39;{&amp;quot;spec&amp;quot;:{&amp;quot;containers&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;kubernetes-serve-hostname&amp;quot;,&amp;quot;image&amp;quot;:&amp;quot;new image&amp;quot;}]}}&#39;

# 使用具有位置数组的 json 补丁更新容器镜像
$ kubectl patch pod valid-pod --type=&#39;json&#39; -p=&#39;[{&amp;quot;op&amp;quot;: &amp;quot;replace&amp;quot;, &amp;quot;path&amp;quot;: &amp;quot;/spec/containers/0/image&amp;quot;, &amp;quot;value&amp;quot;:&amp;quot;new image&amp;quot;}]&#39;

# 使用具有位置数组的 json 补丁禁用 deployment 的 livenessProbe
$ kubectl patch deployment valid-deployment  --type json   -p=&#39;[{&amp;quot;op&amp;quot;: &amp;quot;remove&amp;quot;, &amp;quot;path&amp;quot;: &amp;quot;/spec/template/spec/containers/0/livenessProbe&amp;quot;}]&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org4b6db63&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;编辑资源&#34;&gt;编辑资源&lt;/h1&gt;

&lt;p&gt;在编辑器中编辑任何 API 资源。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl edit svc/docker-registry                      # 编辑名为 docker-registry 的 service
$ KUBE_EDITOR=&amp;quot;nano&amp;quot; kubectl edit svc/docker-registry   # 使用其它编辑器
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org43c4654&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;scale-资源&#34;&gt;Scale 资源&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl scale --replicas=3 rs/foo                                 # Scale a replicaset named &#39;foo&#39; to 3
$ kubectl scale --replicas=3 -f foo.yaml                            # Scale a resource specified in &amp;quot;foo.yaml&amp;quot; to 3
$ kubectl scale --current-replicas=2 --replicas=3 deployment/mysql  # If the deployment named mysql&#39;s current size is 2, scale mysql to 3
$ kubectl scale --replicas=5 rc/foo rc/bar rc/baz                   # Scale multiple replication controllers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgc4b94c3&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;删除资源&#34;&gt;删除资源&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl delete -f ./pod.json                                              # 删除 pod.json 文件中定义的类型和名称的 pod
$ kubectl delete pod,service baz foo                                        # 删除名为“baz”的 pod 和名为“foo”的 service
$ kubectl delete pods,services -l name=myLabel                              # 删除具有 name=myLabel 标签的 pod 和 serivce
$ kubectl delete pods,services -l name=myLabel --include-uninitialized      # 删除具有 name=myLabel 标签的 pod 和 service，包括尚未初始化的
$ kubectl -n my-ns delete po,svc --all                                      # 删除 my-ns namespace 下的所有 pod 和 serivce，包括尚未初始化的
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org2dfd797&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;与运行中的-pod-交互&#34;&gt;与运行中的 Pod 交互&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl logs my-pod                                 # dump 输出 pod 的日志（stdout）
$ kubectl logs my-pod -c my-container                 # dump 输出 pod 中容器的日志（stdout，pod 中有多个容器的情况下使用）
$ kubectl logs -f my-pod                              # 流式输出 pod 的日志（stdout）
$ kubectl logs -f my-pod -c my-container              # 流式输出 pod 中容器的日志（stdout，pod 中有多个容器的情况下使用）
$ kubectl run -i --tty busybox --image=busybox -- sh  # 交互式 shell 的方式运行 pod
$ kubectl attach my-pod -i                            # 连接到运行中的容器
$ kubectl port-forward my-pod 5000:6000               # 转发 pod 中的 6000 端口到本地的 5000 端口
$ kubectl exec my-pod -- ls /                         # 在已存在的容器中执行命令（只有一个容器的情况下）
$ kubectl exec my-pod -c my-container -- ls /         # 在已存在的容器中执行命令（pod 中有多个容器的情况下）
$ kubectl top pod POD_NAME --containers               # 显示指定 pod 和容器的指标度量
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org293bbe3&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;与节点和集群交互&#34;&gt;与节点和集群交互&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl cordon my-node                                                # 标记 my-node 不可调度
$ kubectl drain my-node                                                 # 清空 my-node 以待维护
$ kubectl uncordon my-node                                              # 标记 my-node 可调度
$ kubectl top node my-node                                              # 显示 my-node 的指标度量
$ kubectl cluster-info                                                  # 显示 master 和服务的地址
$ kubectl cluster-info dump                                             # 将当前集群状态输出到 stdout                                    
$ kubectl cluster-info dump --output-directory=/path/to/cluster-state   # 将当前集群状态输出到 /path/to/cluster-state

# 如果该键和影响的污点（taint）已存在，则使用指定的值替换
$ kubectl taint nodes foo dedicated=special-user:NoSchedule
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgef08c6d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;资源类型&#34;&gt;资源类型&lt;/h1&gt;

&lt;p&gt;下表列出的是 kubernetes 中所有支持的类型和缩写的别名。&lt;/p&gt;

&lt;table border=&#34;2&#34; cellspacing=&#34;0&#34; cellpadding=&#34;6&#34; rules=&#34;groups&#34; frame=&#34;hsides&#34;&gt;


&lt;colgroup&gt;
&lt;col  class=&#34;org-left&#34; /&gt;

&lt;col  class=&#34;org-left&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;资源类型&lt;/th&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;缩写别名&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;clusters&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;componentstatuses&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;cs&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;configmaps&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;cm&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;daemonsets&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;ds&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;deployments&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;deploy&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;endpoints&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;ep&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;event&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;ev&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;horizontalpodautoscalers&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;hpa&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;ingresses&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;ing&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;jobs&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;limitranges&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;limits&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;namespaces&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;ns&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;networkpolicies&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;nodes&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;no&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;statefulsets&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;persistentvolumeclaims&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;pvc&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;persistentvolumes&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;pv&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;pods&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;po&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;podsecuritypolicies&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;psp&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;podtemplates&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;replicasets&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;rs&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;replicationcontrollers&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;rc&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;resourcequotas&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;quota&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;cronjob&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;secrets&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;serviceaccount&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;sa&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;services&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;svc&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;storageclasses&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;thirdpartyresources&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a id=&#34;orgb32a4c5&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;格式化输出&#34;&gt;格式化输出&lt;/h1&gt;

&lt;p&gt;要以特定的格式向终端窗口输出详细信息，可以在 kubectl 命令中添加 -o 或者 -output 标志。&lt;/p&gt;

&lt;table border=&#34;2&#34; cellspacing=&#34;0&#34; cellpadding=&#34;6&#34; rules=&#34;groups&#34; frame=&#34;hsides&#34;&gt;


&lt;colgroup&gt;
&lt;col  class=&#34;org-left&#34; /&gt;

&lt;col  class=&#34;org-left&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;输出格式&lt;/th&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;-o=json&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;输出 JSON 格式的 API 对象&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;-o=jsonpath=template&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;打印 jsonpath 表达式中定义的字段&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;-o=jsonpath-file=filename&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;打印由 文件中的 jsonpath 表达式定义的字段&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;-o=name&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;仅打印资源名称&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;-o=wide&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;以纯文本格式输出任何附加信息，对于 Pod ，包含节点名称&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;-o=yaml&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;输出 YAML 格式的 API 对象&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a id=&#34;org8e6aac4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;kubectl-详细输出和调试&#34;&gt;Kubectl 详细输出和调试&lt;/h1&gt;

&lt;p&gt;使用 -v 或 &amp;#x2013;v 标志跟着一个整数来指定日志级别。&lt;/p&gt;

&lt;table border=&#34;2&#34; cellspacing=&#34;0&#34; cellpadding=&#34;6&#34; rules=&#34;groups&#34; frame=&#34;hsides&#34;&gt;


&lt;colgroup&gt;
&lt;col  class=&#34;org-left&#34; /&gt;

&lt;col  class=&#34;org-left&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;详细等级&lt;/th&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#x2013;v=0&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;总是对操作人员可见。&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#x2013;v=1&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;合理的默认日志级别，如果您不需要详细输出。&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#x2013;v=2&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;可能与系统的重大变化相关的，有关稳定状态的信息和重要的日志信息。这是对大多数系统推荐的日志级别。&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#x2013;v=3&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;有关更改的扩展信息。&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#x2013;v=4&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;调试级别详细输出。&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#x2013;v=6&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;显示请求的资源。&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#x2013;v=7&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;显示HTTP请求的header。&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#x2013;v=8&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;显示HTTP请求的内容。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a id=&#34;org0f9fe33&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考&#34;&gt;参考&lt;/h1&gt;

&lt;p&gt;【01】&lt;a href=&#34;https://kubernetes.io/docs/reference/kubectl/&#34;&gt;Kubectl 概览&lt;/a&gt;&lt;br /&gt;
【02】&lt;a href=&#34;https://kubernetes.io/docs/reference/kubectl/jsonpath/&#34;&gt;JsonPath 手册&lt;/a&gt;&lt;br /&gt;
【03】&lt;a href=&#34;https://kubernetes.io/docs/reference/kubectl/cheatsheet/&#34;&gt;Cheatsheet&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
