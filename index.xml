<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>墨斯潘園 on 墨斯潘園</title>
        <link>http://mospany.github.io/</link>
        <language>zh-CN</language>
        <author>Mospan</author>
        <rights>Copyright (c) 2016, mospan; all rights reserved.</rights>
        <updated>Sat, 27 Jan 2024 19:31:10 CST</updated>
        
        <item>
            <title>K8S项目实践(11): Pod 是如何使用到 GPU 的及源码分析</title>
            <link>http://mospany.github.io/2024/01/27/pod-use-gpu/</link>
            <pubDate>Sat, 27 Jan 2024 19:31:10 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2024/01/27/pod-use-gpu/</guid>
            <description>

&lt;p&gt;本文主要分析了在 K8s 中创建一个 Pod 并申请 GPU 资源，最终该 Pod 时怎么能够使用 GPU 的，具体的实现原理，以及 device plugin、nvidia-container-toolkit 相关源码分析。&lt;/p&gt;

&lt;h1 id=&#34;1-概述&#34;&gt;1. 概述&lt;/h1&gt;

&lt;p&gt;这篇文章则是将整个流程连起来做一个简单分析，即：宿主机上的 GPU 是怎么能够被 K8s 中的 Pod 使用的。&lt;/p&gt;

&lt;p&gt;可以分为以下两部分：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;k8s 是如何感知到 GPU 的&lt;/li&gt;
&lt;li&gt;GPU 是如何分配给 Pod 的&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;2-工作流程&#34;&gt;2. 工作流程&lt;/h1&gt;

&lt;p&gt;这部分主要分享一下 NVIDIA 的 device-plugin 以及 nvidia-container-toolkit 的工作流程，以及二者是怎么配合的。&lt;/p&gt;

&lt;h2 id=&#34;2-1-k8s-是如何感知到-gpu-的&#34;&gt;2.1. k8s 是如何感知到 GPU 的&lt;/h2&gt;

&lt;p&gt;NVIDIA 实现了&lt;a href=&#34;https://github.com/NVIDIA/k8s-device-plugin&#34;&gt;NVIDIA/k8s-device-plugin&lt;/a&gt; 来使得节点上的 GPU 能够被 k8s 感知到。&lt;/p&gt;

&lt;p&gt;这个 device plugin 主要做两件事：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）检测节点上的 GPU 设备并上报给 Kubelet，再由 Kubelet 更新节点信息时提交到 kube-apiserver。

&lt;ul&gt;
&lt;li&gt;这样 k8s 就知道每个节点上有多少 GPU 了，后续 Pod 申请 GPU 时就会往有 GPU 资源的节点上调度。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2）Pod 申请 GPU 时，为对应容器添加一个NVIDIA_VISIBLE_DEVICES环境变量，后续底层 Runtime 在真正创建容器时就能根据这些信息把 GPU 挂载到容器中

&lt;ul&gt;
&lt;li&gt;例如添加环境变量：NVIDIA_VISIBLE_DEVICES=GPU-03f69c50-207a-2038-9b45-23cac89cb67d
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;NVIDIA 这个 device plugin 比较复杂，支持多种策略，device plugin 提供的 env、mounts、device 以及 annotations 等方式它都做了支持，在部署时可以通过 DEVICE_LIST_STRATEGY 环境变量进行指定，不过默认还是用的 env。&lt;/p&gt;

&lt;p&gt;另外DEVICE_ID_STRATEGY 默认也是 uuid，因此在 Pod 中看到的 NVIDIA_VISIBLE_DEVICES 就不是 Docker 环境中常见的 0,1,2 这种编号了，而是 GPU 设备对应的 UUID。&lt;/p&gt;

&lt;h2 id=&#34;2-2-gpu-是如何分配给-pod-的&#34;&gt;2.2. GPU 是如何分配给 Pod 的&lt;/h2&gt;

&lt;p&gt;NVIDIA 提供了 nvidia-container-toolkit 来处理如何将 GPU 分配给容器的问题。&lt;/p&gt;

&lt;p&gt;核心组件有以下三个：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nvidia-container-runtime&lt;/li&gt;
&lt;li&gt;nvidia-container-runtime-hook&lt;/li&gt;
&lt;li&gt;nvidia-container-cli&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;首先需要将 docker/containerd 的 runtime 设置为nvidia-container-runtime，此后整个调用链就变成这样了：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-27-pod-use-gpu/IMG_20250127-192425835.png&#34; alt=&#34;picture 0&#34; /&gt;&lt;/p&gt;

&lt;p&gt;接下来就具体分析每个组件的作用。&lt;/p&gt;

&lt;h3 id=&#34;2-2-1-nvidia-container-runtime&#34;&gt;2.2.1. nvidia-container-runtime&lt;/h3&gt;

&lt;p&gt;nvidia-container-runtime 的作用就是负责在容器启动之前，将 nvidia-container-runtime-hook 注入到 prestart hook。
&amp;gt; 小知识：docker/containerd 都是高级 Runtime，runC 则是低级 Runtime。不同层级 Runtime 通过 OCI Spec 进行交互。&lt;/p&gt;

&lt;p&gt;也就是说 docker 调用 runC 创建容器时，会把 docker 收到的信息解析，组装成 OCI Spec，然后在往下传递。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;而 nvidia-container-runtime 的作用就是修改容器 Spec，往里面添加一个 prestart hook，这个 hook 就是 nvidia-container-runtime-hook 。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这样 runC 根据 Spec 启动容器时就会执行该 hook，即执行 nvidia-container-runtime-hook。&lt;/p&gt;

&lt;p&gt;也就是说 nvidia-container-runtime 其实没有任何逻辑，真正的逻辑都在 nvidia-container-runtime-hook 中。&lt;/p&gt;

&lt;h3 id=&#34;2-2-2-nvidia-container-runtime-hook&#34;&gt;2.2.2. nvidia-container-runtime-hook&lt;/h3&gt;

&lt;p&gt;nvidia-container-runtime-hook 包含了给容器分配 GPU 的核心逻辑，主要分为两部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）从容器 Spec 的 mounts 和 env 中解析 GPU 信息

&lt;ul&gt;
&lt;li&gt;mounts 对应前面 device plugin 中设置的 Mount 和 Device，env 则对应 Env&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2）调用 nvidia-container-cli configure 命令，保证容器内可以使用被指定的 GPU 以及对应能力

&lt;ul&gt;
&lt;li&gt;也就是说nvidia-container-runtime-hook 最终还是调用 nvidia-container-cli 来实现的给容器分配 GPU 能力的。
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;2-2-3-nvidia-container-cli&#34;&gt;2.2.3. nvidia-container-cli&lt;/h3&gt;

&lt;p&gt;nvidia-container-cli 是一个命令行工具，用于配置 Linux 容器对 GPU 硬件的使用。&lt;/p&gt;

&lt;p&gt;提供了三个命令&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;list: 打印 nvidia 驱动库及路径&lt;/li&gt;
&lt;li&gt;info: 打印所有Nvidia GPU设备&lt;/li&gt;
&lt;li&gt;configure： 进入给定进程的命名空间，执行必要操作保证容器内可以使用被指定的 GPU 以及对应能力（指定 NVIDIA 驱动库）
&lt;img src=&#34;post/2024/images/2024-01-27-pod-use-gpu/IMG_20250130-215950804.png&#34; alt=&#34;picture 3&#34; /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一般主要使用 configure 命令，它将 NVIDIA GPU Driver、CUDA Driver 等 驱动库的 so 文件 和 GPU 设备信息， 通过文件挂载的方式映射到容器中。&lt;/p&gt;

&lt;h2 id=&#34;2-3-小结&#34;&gt;2.3. 小结&lt;/h2&gt;

&lt;p&gt;整个流程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）device plugin 上报节点上的 GPU 信息&lt;/li&gt;
&lt;li&gt;2）用户创建 Pod，在 resources.rquest 中申请 GPU，Scheduler 根据各节点 GPU 资源情况，将 Pod 调度到一个有足够 GPU 的节点&lt;/li&gt;
&lt;li&gt;3）DevicePlugin 根据 Pod 中申请的 GPU 资源，为容器添加 Env 和 Devices 配置

&lt;ul&gt;
&lt;li&gt;例如添加环境变量：NVIDIA_VISIBLE_DEVICES=GPU-03f69c50-207a-2038-9b45-23cac89cb67d&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;4）docker / containerd 启动容器&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;由于配置了 nvidia-container-runtime,因此会使用 nvidia-container-runtime 来创建容器&lt;/li&gt;
&lt;li&gt;nvidia-container-runtime 额外做了一件事：将 nvidia-container-runtime-hook 作为 prestart hook 添加到容器 spec 中，然后就将容器 spec 信息往后传给 runC 了。&lt;/li&gt;
&lt;li&gt;runC 创建容器前会调用 prestart hook，其中就包括了上一步添加的 nvidia-container-runtime-hook，该 hook 主要做两件事：

&lt;ul&gt;
&lt;li&gt;从容器 Spec 的 mounts 或者 env 中解析 GPU 信息&lt;/li&gt;
&lt;li&gt;调用 nvidia-container-cli configure 命令，将 NVIDIA 的 GPU Driver、CUDA Driver 等库文件挂载进容器，保证容器内可以使用被指定的 GPU以及对应能力
以上就是在 k8s 中使用 NVIDIA GPU 的流程，简单来说就是：&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;1）device plugin 中根据 pod 申请的 GPU 资源分配 GPU，并以 ENV 环境变量方式添加到容器上。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2）nvidia-container-toolkit 则根据该 Env 拿到要分配给该容器的 GPU 最终把相关文件挂载到容器里
当然并不是只有这一种实现方法，比如天数的 ix-device-plugin 实现中就没有提供自己的 container-toolkit，只在 device plugin 中通过 Device 指定要挂载哪些设备,这样容器启动时也会把这些设备挂载到容器中：&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (p *iluvatarDevicePlugin) allocateDevicesByDeviceID(hostminor uint, num int) *pluginapi.DeviceSpec {
	var device pluginapi.DeviceSpec

	hostPathPrefix := &amp;quot;/dev/&amp;quot;
	containerPathPrefix := &amp;quot;/dev/&amp;quot;

	// Expose the device node for iluvatar pod.
	device.HostPath = hostPathPrefix + deviceName + strconv.Itoa(int(hostminor))
	device.ContainerPath = containerPathPrefix + deviceName + strconv.Itoa(num)
	device.Permissions = &amp;quot;rw&amp;quot;

	return &amp;amp;device
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;不过由于没有挂载驱动进去，因此需要容器内自带驱动才行。&lt;/p&gt;

&lt;p&gt;至此，已经分析了 k8s 创建 Pod 使用 GPU 的整个流程及大致原理，接下来简单分析下相关组件源码。&lt;/p&gt;

&lt;h1 id=&#34;3-device-plugin-源码分析&#34;&gt;3. device plugin 源码分析&lt;/h1&gt;

&lt;p&gt;NVIDIA GPU 对应的 device plugin 叫做：&lt;a href=&#34;https://github.com/NVIDIA/k8s-device-plugin&#34;&gt;NVIDIA/k8s-device-plugin&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;3-1-allocate-方法&#34;&gt;3.1. Allocate 方法&lt;/h2&gt;

&lt;p&gt;主要看为容器分配资源的 Allocate 方法&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// https://github.com/NVIDIA/k8s-device-plugin/blob/main/internal/plugin/server.go#L319-L332

// Allocate which return list of devices.
func (plugin *NvidiaDevicePlugin) Allocate(ctx context.Context, reqs *pluginapi.AllocateRequest) (*pluginapi.AllocateResponse, error) {
        responses := pluginapi.AllocateResponse{}
        for _, req := range reqs.ContainerRequests {
                if err := plugin.rm.ValidateRequest(req.DevicesIDs); err != nil {
                        return nil, fmt.Errorf(&amp;quot;invalid allocation request for %q: %w&amp;quot;, plugin.rm.Resource(), err)
                }
                response, err := plugin.getAllocateResponse(req.DevicesIDs)
                if err != nil {
                        return nil, fmt.Errorf(&amp;quot;failed to get allocate response: %v&amp;quot;, err)
                }
                responses.ContainerResponses = append(responses.ContainerResponses, response)
        }

        return &amp;amp;responses, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心逻辑在 getAllocateResponse 中：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (plugin *NvidiaDevicePlugin) getAllocateResponse(requestIds []string) (*pluginapi.ContainerAllocateResponse, error) {
	deviceIDs := plugin.deviceIDsFromAnnotatedDeviceIDs(requestIds)

	// Create an empty response that will be updated as required below.
	response := &amp;amp;pluginapi.ContainerAllocateResponse{
		Envs: make(map[string]string),
	}
	if plugin.deviceListStrategies.AnyCDIEnabled() {
		responseID := uuid.New().String()
		if err := plugin.updateResponseForCDI(response, responseID, deviceIDs...); err != nil {
			return nil, fmt.Errorf(&amp;quot;failed to get allocate response for CDI: %v&amp;quot;, err)
		}
	}
	if plugin.config.Sharing.SharingStrategy() == spec.SharingStrategyMPS {
		plugin.updateResponseForMPS(response)
	}

	// The following modifications are only made if at least one non-CDI device
	// list strategy is selected.
	if plugin.deviceListStrategies.AllCDIEnabled() {
		return response, nil
	}

	if plugin.deviceListStrategies.Includes(spec.DeviceListStrategyEnvvar) {
		plugin.updateResponseForDeviceListEnvvar(response, deviceIDs...)
	}
	if plugin.deviceListStrategies.Includes(spec.DeviceListStrategyVolumeMounts) {
		plugin.updateResponseForDeviceMounts(response, deviceIDs...)
	}
	if *plugin.config.Flags.Plugin.PassDeviceSpecs {
		response.Devices = append(response.Devices, plugin.apiDeviceSpecs(*plugin.config.Flags.NvidiaDevRoot, requestIds)...)
	}
	if *plugin.config.Flags.GDSEnabled {
		response.Envs[&amp;quot;NVIDIA_GDS&amp;quot;] = &amp;quot;enabled&amp;quot;
	}
	if *plugin.config.Flags.MOFEDEnabled {
		response.Envs[&amp;quot;NVIDIA_MOFED&amp;quot;] = &amp;quot;enabled&amp;quot;
	}
	return response, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，根据不同 flag 以及策略分为不同的设置方式&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Constants to represent the various device list strategies
const (
	DeviceListStrategyEnvvar         = &amp;quot;envvar&amp;quot;
	DeviceListStrategyVolumeMounts   = &amp;quot;volume-mounts&amp;quot;
	DeviceListStrategyCDIAnnotations = &amp;quot;cdi-annotations&amp;quot;
	DeviceListStrategyCDICRI         = &amp;quot;cdi-cri&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;东西比较多，我们主要看设置 Env 的策略&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;if plugin.deviceListStrategies.Includes(spec.DeviceListStrategyEnvvar) {
    plugin.updateResponseForDeviceListEnvvar(response, deviceIDs...)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// updateResponseForDeviceListEnvvar sets the environment variable for the requested devices.
func (plugin *NvidiaDevicePlugin) updateResponseForDeviceListEnvvar(response *pluginapi.ContainerAllocateResponse, deviceIDs ...string) {
        response.Envs[plugin.deviceListEnvvar] = strings.Join(deviceIDs, &amp;quot;,&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，逻辑很简单，就是给容器添加了一个环境变量，value 为设备 id，具体 deviceID 提供了两种策略，可以是编号或者 uuid&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;const (
        DeviceIDStrategyUUID  = &amp;quot;uuid&amp;quot;
        DeviceIDStrategyIndex = &amp;quot;index&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;key 是一个变量 plugin.deviceListEnvvar，初始化如下:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;        plugin := NvidiaDevicePlugin{
                deviceListEnvvar:     &amp;quot;NVIDIA_VISIBLE_DEVICES&amp;quot;,
                socket:               pluginPath + &amp;quot;.sock&amp;quot;,
          // ...
        }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是说 NVIDIA 这个 device plugin 实现 Allocate 主要就是给容器增加了环境变量，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;NVIDIA_VISIBLE_DEVICES=GPU-03f69c50-207a-2038-9b45-23cac89cb67d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;NVIDIA_VISIBLE_DEVICES=1,2
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-2-小结&#34;&gt;3.2. 小结&lt;/h2&gt;

&lt;p&gt;NVIDIA device plugin 核心逻辑就是给容器添加NVIDIA_VISIBLE_DEVICES 环境变量，告知后续组件，需要给该组件分配 GPU。&lt;/p&gt;

&lt;p&gt;比如当我们仅使用 Docker 时就可以在启动容器时指定 GPU，&amp;ndash;gpus flag 和 NVIDIA_VISIBLE_DEVICES 环境变量效果一致。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# --gpus
docker run --gpus device=0 -it tensorflow/tensorflow:latest-gpu bash
# 或者环境变量 NVIDIA_VISIBLE_DEVICES
docker run -e NVIDIA_VISIBLE_DEVICES=0 -it tensorflow/tensorflow:latest-gpu bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至于为什么添加了NVIDIA_VISIBLE_DEVICES 环境变量就会给该容器分配 GPU，就是接下来的 nvidi-container-toolkit 组件实现的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;nvidia 在 device plugin 中也使用NVIDIA_VISIBLE_DEVICES 环境变量正好能够兼容 nvidia-container-toolkit。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;4-nvidia-container-toolkit-源码分析&#34;&gt;4. nvidia-container-toolkit 源码分析&lt;/h1&gt;

&lt;p&gt;这部分我们主要分析，为什么添加了NVIDIA_VISIBLE_DEVICES 环境变量就会给该容器分配 GPU，nvidia-container-toolkit 中做了哪些处理。&lt;/p&gt;

&lt;p&gt;nvidia-container-toolkit 包含以下 3 个部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）&lt;a href=&#34;https://github.com/NVIDIA/nvidia-container-toolkit/tree/main/cmd/nvidia-container-runtime&#34;&gt;nvidia-container-runtime&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2）&lt;a href=&#34;https://github.com/NVIDIA/nvidia-container-toolkit/tree/main/cmd/nvidia-container-runtime-hook&#34;&gt;nvidia-container-runtime-hook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3）&lt;a href=&#34;https://github.com/NVIDIA/libnvidia-container/tree/master/src/cli&#34;&gt;nvidia-container-cli&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;4-1-nvidia-container-runtime&#34;&gt;4.1. nvidia-container-runtime&lt;/h2&gt;

&lt;p&gt;nvidia-container-runtime 可以看做是一个 docker/containerd 的底层 runtime（类似 runC），在模块在创建容器的整个调用链中处在如下位置：
&lt;img src=&#34;post/2024/images/2024-01-27-pod-use-gpu/IMG_20250128-155337940.png&#34; alt=&#34;picture 1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;它只做一件事，就是在容器启动之前，将 nvidia-container-runtime-hook 注入到 prestart hook。
&amp;gt; 以修改容器 Spec 的方式添加一个 prestart hook 进去&lt;/p&gt;

&lt;p&gt;这样，后续 runC 使用容器 Spec 创建容器时就会执行该 prestart hook。&lt;/p&gt;

&lt;p&gt;简单分析下源码，首先是启动命令：&lt;a href=&#34;https://github.com/NVIDIA/nvidia-container-toolkit/blob/main/cmd/nvidia-container-runtime/main.go&#34;&gt;nvidia-container-runtime/main.go&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;就是 New 了一个 nvidia runtime 对象，并执行其 Run 方法。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// https://github.com/NVIDIA/nvidia-container-toolkit/blob/main/cmd/nvidia-container-runtime/main.go#L9-L15

import (
    &amp;quot;os&amp;quot;

    &amp;quot;github.com/NVIDIA/nvidia-container-toolkit/internal/runtime&amp;quot;
)

func main() {
    r := runtime.New()
    err := r.Run(os.Args)
    if err != nil {
       os.Exit(1)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体的 New 方法也很简单，返回的是一个名为 Interface 的 Interface，包含一个 Run 方法&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// https://github.com/NVIDIA/nvidia-containertoolkit/blob/main/internal/runtime/api.go#L17-L26

type rt struct {
    logger       *Logger
    modeOverride string
}

// Interface is the interface for the runtime library.
type Interface interface {
    Run([]string) error
}
func New(opts ...Option) Interface {
    r := rt{}
    for _, opt := range opts {
       opt(&amp;amp;r)
    }
    if r.logger == nil {
       r.logger = NewLogger()
    }
    return &amp;amp;r
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run 方法具体实现如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// https://github.com/NVIDIA/nvidia-container-toolkit/blob/main/internal/runtime/runtime.go#L34-L91

// Run is an entry point that allows for idiomatic handling of errors
// when calling from the main function.
func (r rt) Run(argv []string) (rerr error) {
    defer func() {
       if rerr != nil {
          r.logger.Errorf(&amp;quot;%v&amp;quot;, rerr)
       }
    }()

    printVersion := hasVersionFlag(argv)
    if printVersion {
       fmt.Printf(&amp;quot;%v version %v\n&amp;quot;, &amp;quot;NVIDIA Container Runtime&amp;quot;, info.GetVersionString(fmt.Sprintf(&amp;quot;spec: %v&amp;quot;, specs.Version)))
    }

    cfg, err := config.GetConfig()
    if err != nil {
       return fmt.Errorf(&amp;quot;error loading config: %v&amp;quot;, err)
    }
    r.logger.Update(
       cfg.NVIDIAContainerRuntimeConfig.DebugFilePath,
       cfg.NVIDIAContainerRuntimeConfig.LogLevel,
       argv,
    )
    defer func() {
       if rerr != nil {
          r.logger.Errorf(&amp;quot;%v&amp;quot;, rerr)
       }
       if err := r.logger.Reset(); err != nil {
          rerr = errors.Join(rerr, fmt.Errorf(&amp;quot;failed to reset logger: %v&amp;quot;, err))
       }
    }()

    // We apply some config updates here to ensure that the config is valid in
    // all cases.
    if r.modeOverride != &amp;quot;&amp;quot; {
       cfg.NVIDIAContainerRuntimeConfig.Mode = r.modeOverride
    }
    //nolint:staticcheck  // TODO(elezar): We should swith the nvidia-container-runtime from using nvidia-ctk to using nvidia-cdi-hook.
    cfg.NVIDIACTKConfig.Path = config.ResolveNVIDIACTKPath(&amp;amp;logger.NullLogger{}, cfg.NVIDIACTKConfig.Path)
    cfg.NVIDIAContainerRuntimeHookConfig.Path = config.ResolveNVIDIAContainerRuntimeHookPath(&amp;amp;logger.NullLogger{}, cfg.NVIDIAContainerRuntimeHookConfig.Path)

    // Log the config at Trace to allow for debugging if required.
    r.logger.Tracef(&amp;quot;Running with config: %+v&amp;quot;, cfg)

    driver := root.New(
       root.WithLogger(r.logger),
       root.WithDriverRoot(cfg.NVIDIAContainerCLIConfig.Root),
    )

    r.logger.Tracef(&amp;quot;Command line arguments: %v&amp;quot;, argv)
    runtime, err := newNVIDIAContainerRuntime(r.logger, cfg, argv, driver)
    if err != nil {
       return fmt.Errorf(&amp;quot;failed to create NVIDIA Container Runtime: %v&amp;quot;, err)
    }

    if printVersion {
       fmt.Print(&amp;quot;\n&amp;quot;)
    }
    return runtime.Exec(argv)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心部分：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;runtime, err := newNVIDIAContainerRuntime(r.logger, cfg, argv, driver)
if err != nil {
   return fmt.Errorf(&amp;quot;failed to create NVIDIA Container Runtime: %v&amp;quot;, err)
}

if printVersion {
   fmt.Print(&amp;quot;\n&amp;quot;)
}
return runtime.Exec(argv)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;继续查看 newNVIDIAContainerRuntime 实现&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// https://github.com/NVIDIA/nvidia-container-toolkit/blob/main/internal/runtime/runtime_factory.go#L32-L62

// newNVIDIAContainerRuntime is a factory method that constructs a runtime based on the selected configuration and specified logger
func newNVIDIAContainerRuntime(logger logger.Interface, cfg *config.Config, argv []string, driver *root.Driver) (oci.Runtime, error) {
    lowLevelRuntime, err := oci.NewLowLevelRuntime(logger, cfg.NVIDIAContainerRuntimeConfig.Runtimes)
    if err != nil {
       return nil, fmt.Errorf(&amp;quot;error constructing low-level runtime: %v&amp;quot;, err)
    }

    logger.Tracef(&amp;quot;Using low-level runtime %v&amp;quot;, lowLevelRuntime.String())
    if !oci.HasCreateSubcommand(argv) {
       logger.Tracef(&amp;quot;Skipping modifier for non-create subcommand&amp;quot;)
       return lowLevelRuntime, nil
    }

    ociSpec, err := oci.NewSpec(logger, argv)
    if err != nil {
       return nil, fmt.Errorf(&amp;quot;error constructing OCI specification: %v&amp;quot;, err)
    }

    specModifier, err := newSpecModifier(logger, cfg, ociSpec, driver)
    if err != nil {
       return nil, fmt.Errorf(&amp;quot;failed to construct OCI spec modifier: %v&amp;quot;, err)
    }

    // Create the wrapping runtime with the specified modifier.
    r := oci.NewModifyingRuntimeWrapper(
       logger,
       lowLevelRuntime,
       ociSpec,
       specModifier,
    )

    return r, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;暂时只需要关注 specModifier 这个对象,就是它在修改容器的 spec 以添加 hook&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// newSpecModifier is a factory method that creates constructs an OCI spec modifer based on the provided config.
func newSpecModifier(logger logger.Interface, cfg *config.Config, ociSpec oci.Spec, driver *root.Driver) (oci.SpecModifier, error) {
    rawSpec, err := ociSpec.Load()
    if err != nil {
       return nil, fmt.Errorf(&amp;quot;failed to load OCI spec: %v&amp;quot;, err)
    }

    image, err := image.NewCUDAImageFromSpec(rawSpec)
    if err != nil {
       return nil, err
    }

    mode := info.ResolveAutoMode(logger, cfg.NVIDIAContainerRuntimeConfig.Mode, image)
    modeModifier, err := newModeModifier(logger, mode, cfg, ociSpec, image)
    if err != nil {
       return nil, err
    }
    // For CDI mode we make no additional modifications.
    if mode == &amp;quot;cdi&amp;quot; {
       return modeModifier, nil
    }

    graphicsModifier, err := modifier.NewGraphicsModifier(logger, cfg, image, driver)
    if err != nil {
       return nil, err
    }

    featureModifier, err := modifier.NewFeatureGatedModifier(logger, cfg, image)
    if err != nil {
       return nil, err
    }

    modifiers := modifier.Merge(
       modeModifier,
       graphicsModifier,
       featureModifier,
    )
    return modifiers, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 hook 的 modifier 在 newModeModifier 里面&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func newModeModifier(logger logger.Interface, mode string, cfg *config.Config, ociSpec oci.Spec, image image.CUDA) (oci.SpecModifier, error) {
    switch mode {
    case &amp;quot;legacy&amp;quot;:
       return modifier.NewStableRuntimeModifier(logger, cfg.NVIDIAContainerRuntimeHookConfig.Path), nil
    case &amp;quot;csv&amp;quot;:
       return modifier.NewCSVModifier(logger, cfg, image)
    case &amp;quot;cdi&amp;quot;:
       return modifier.NewCDIModifier(logger, cfg, ociSpec)
    }

    return nil, fmt.Errorf(&amp;quot;invalid runtime mode: %v&amp;quot;, cfg.NVIDIAContainerRuntimeConfig.Mode)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体为 stableRuntimeModifier：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (m stableRuntimeModifier) Modify(spec *specs.Spec) error {
    // If an NVIDIA Container Runtime Hook already exists, we don&#39;t make any modifications to the spec.
    if spec.Hooks != nil {
       for _, hook := range spec.Hooks.Prestart {
          hook := hook
          if isNVIDIAContainerRuntimeHook(&amp;amp;hook) {
             m.logger.Infof(&amp;quot;Existing nvidia prestart hook (%v) found in OCI spec&amp;quot;, hook.Path)
             return nil
          }
       }
    }

    path := m.nvidiaContainerRuntimeHookPath
    m.logger.Infof(&amp;quot;Using prestart hook path: %v&amp;quot;, path)
    args := []string{filepath.Base(path)}
    if spec.Hooks == nil {
       spec.Hooks = &amp;amp;specs.Hooks{}
    }
    spec.Hooks.Prestart = append(spec.Hooks.Prestart, specs.Hook{
       Path: path,
       Args: append(args, &amp;quot;prestart&amp;quot;),
    })

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心部分：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;path := m.nvidiaContainerRuntimeHookPath
spec.Hooks.Prestart = append(spec.Hooks.Prestart, specs.Hook{
   Path: path,
   Args: append(args, &amp;quot;prestart&amp;quot;),
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，最终就是添加了一个 prestart hook，hook 的 path 就是 nvidia-container-runtime-hook 这个二进制文件的位置。&lt;/p&gt;

&lt;p&gt;至此，nvidia-container-runtime 的工作就完成了，容器真正启动时，底层 runtime（比如 runC）检测到容器的 Spec 中有这个 hook 就会去执行了，最终 nvidia-container-runtime-hook 就会被运行了。&lt;/p&gt;

&lt;h2 id=&#34;4-2-nvidia-container-runtime-hook&#34;&gt;4.2. nvidia-container-runtime-hook&lt;/h2&gt;

&lt;p&gt;该组件则是 nvidia-container-toolkit 中的核心，所有的逻辑都在这里面实现。&lt;/p&gt;

&lt;p&gt;主要做两件事：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）从容器的 env 中解析 GPU 信息&lt;/li&gt;
&lt;li&gt;2）调用 nvidia-container-cli configure 命令，挂载相关文件，保证容器内可以使用被指定的GPU以及对应能力
也是先从启动命令看起：&lt;a href=&#34;https://github.com/NVIDIA/nvidia-container-toolkit/blob/main/cmd/nvidia-container-runtime-hook/main.go&#34;&gt;nvidia-container-runtime-hook/main.go&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;switch args[0] {
case &amp;quot;prestart&amp;quot;:
    doPrestart()
    os.Exit(0)
case &amp;quot;poststart&amp;quot;:
    fallthrough
case &amp;quot;poststop&amp;quot;:
    os.Exit(0)
default:
    flag.Usage()
    os.Exit(2)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们是添加的 prestart hook，因此会走 prestart 分支 执行doPrestart()方法。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func doPrestart() {
    var err error

    defer exit()
    log.SetFlags(0)

    hook, err := getHookConfig()
    if err != nil || hook == nil {
       log.Panicln(&amp;quot;error getting hook config:&amp;quot;, err)
    }
    cli := hook.NVIDIAContainerCLIConfig

    container := getContainerConfig(*hook)
    nvidia := container.Nvidia
    if nvidia == nil {
       // Not a GPU container, nothing to do.
       return
    }

    if !hook.NVIDIAContainerRuntimeHookConfig.SkipModeDetection &amp;amp;&amp;amp; info.ResolveAutoMode(&amp;amp;logInterceptor{}, hook.NVIDIAContainerRuntimeConfig.Mode, container.Image) != &amp;quot;legacy&amp;quot; {
       log.Panicln(&amp;quot;invoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported. Please use the NVIDIA Container Runtime (e.g. specify the --runtime=nvidia flag) instead.&amp;quot;)
    }

    rootfs := getRootfsPath(container)

    args := []string{getCLIPath(cli)}
    if cli.Root != &amp;quot;&amp;quot; {
       args = append(args, fmt.Sprintf(&amp;quot;--root=%s&amp;quot;, cli.Root))
    }
    if cli.LoadKmods {
       args = append(args, &amp;quot;--load-kmods&amp;quot;)
    }
    if cli.NoPivot {
       args = append(args, &amp;quot;--no-pivot&amp;quot;)
    }
    if *debugflag {
       args = append(args, &amp;quot;--debug=/dev/stderr&amp;quot;)
    } else if cli.Debug != &amp;quot;&amp;quot; {
       args = append(args, fmt.Sprintf(&amp;quot;--debug=%s&amp;quot;, cli.Debug))
    }
    if cli.Ldcache != &amp;quot;&amp;quot; {
       args = append(args, fmt.Sprintf(&amp;quot;--ldcache=%s&amp;quot;, cli.Ldcache))
    }
    if cli.User != &amp;quot;&amp;quot; {
       args = append(args, fmt.Sprintf(&amp;quot;--user=%s&amp;quot;, cli.User))
    }
    args = append(args, &amp;quot;configure&amp;quot;)

    if ldconfigPath := cli.NormalizeLDConfigPath(); ldconfigPath != &amp;quot;&amp;quot; {
       args = append(args, fmt.Sprintf(&amp;quot;--ldconfig=%s&amp;quot;, ldconfigPath))
    }
    if cli.NoCgroups {
       args = append(args, &amp;quot;--no-cgroups&amp;quot;)
    }
    if len(nvidia.Devices) &amp;gt; 0 {
       args = append(args, fmt.Sprintf(&amp;quot;--device=%s&amp;quot;, nvidia.Devices))
    }
    if len(nvidia.MigConfigDevices) &amp;gt; 0 {
       args = append(args, fmt.Sprintf(&amp;quot;--mig-config=%s&amp;quot;, nvidia.MigConfigDevices))
    }
    if len(nvidia.MigMonitorDevices) &amp;gt; 0 {
       args = append(args, fmt.Sprintf(&amp;quot;--mig-monitor=%s&amp;quot;, nvidia.MigMonitorDevices))
    }
    if len(nvidia.ImexChannels) &amp;gt; 0 {
       args = append(args, fmt.Sprintf(&amp;quot;--imex-channel=%s&amp;quot;, nvidia.ImexChannels))
    }

    for _, cap := range strings.Split(nvidia.DriverCapabilities, &amp;quot;,&amp;quot;) {
       if len(cap) == 0 {
          break
       }
       args = append(args, capabilityToCLI(cap))
    }

    for _, req := range nvidia.Requirements {
       args = append(args, fmt.Sprintf(&amp;quot;--require=%s&amp;quot;, req))
    }

    args = append(args, fmt.Sprintf(&amp;quot;--pid=%s&amp;quot;, strconv.FormatUint(uint64(container.Pid), 10)))
    args = append(args, rootfs)

    env := append(os.Environ(), cli.Environment...)
    //nolint:gosec // TODO: Can we harden this so that there is less risk of command injection?
    err = syscall.Exec(args[0], args, env)
    log.Panicln(&amp;quot;exec failed:&amp;quot;, err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们只需要关注下面这个就行&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;args := []string{getCLIPath(cli)}
container := getContainerConfig(*hook)
err = syscall.Exec(args[0], args, env)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个是 getContainerConfig 解析容器配置 ，另一个就是 exec 真正开始执行命令。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;这里执行的命令其实就是 nvidia-container-cli&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;4-2-1-getcontainerconfig&#34;&gt;4.2.1. getContainerConfig&lt;/h3&gt;

&lt;p&gt;这部分就是解析 Env 拿到要分配给该容器的 GPU，如果没有 NVIDIA_VISIBLE_DEVICES 环境变量就不会做任何事情。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func getContainerConfig(hook HookConfig) (config containerConfig) {
    var h HookState
    d := json.NewDecoder(os.Stdin)
    if err := d.Decode(&amp;amp;h); err != nil {
       log.Panicln(&amp;quot;could not decode container state:&amp;quot;, err)
    }

    b := h.Bundle
    if len(b) == 0 {
       b = h.BundlePath
    }

    s := loadSpec(path.Join(b, &amp;quot;config.json&amp;quot;))

    image, err := image.New(
       image.WithEnv(s.Process.Env),
       image.WithDisableRequire(hook.DisableRequire),
    )
    if err != nil {
       log.Panicln(err)
    }

    privileged := isPrivileged(s)
    return containerConfig{
       Pid:    h.Pid,
       Rootfs: s.Root.Path,
       Image:  image,
       Nvidia: getNvidiaConfig(&amp;amp;hook, image, s.Mounts, privileged),
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;构建了一个 image 对象，注意这里把 ENV 也传进去了&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;之前说了需要给容器分配什么 GPU 是通过 NVIDIA_VISIBLE_DEVICES 环境变量指定的&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;image, err := image.New(
    image.WithEnv(s.Process.Env),
    image.WithDisableRequire(hook.DisableRequire),
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后解析配置&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func getNvidiaConfig(hookConfig *HookConfig, image image.CUDA, mounts []Mount, privileged bool) *nvidiaConfig {
    legacyImage := image.IsLegacy()

    var devices string
    if d := getDevices(hookConfig, image, mounts, privileged); d != nil {
       devices = *d
    } else {
       // &#39;nil&#39; devices means this is not a GPU container.
       return nil
    }

    var migConfigDevices string
    if d := getMigConfigDevices(image); d != nil {
       migConfigDevices = *d
    }
    if !privileged &amp;amp;&amp;amp; migConfigDevices != &amp;quot;&amp;quot; {
       log.Panicln(&amp;quot;cannot set MIG_CONFIG_DEVICES in non privileged container&amp;quot;)
    }

    var migMonitorDevices string
    if d := getMigMonitorDevices(image); d != nil {
       migMonitorDevices = *d
    }
    if !privileged &amp;amp;&amp;amp; migMonitorDevices != &amp;quot;&amp;quot; {
       log.Panicln(&amp;quot;cannot set MIG_MONITOR_DEVICES in non privileged container&amp;quot;)
    }

    var imexChannels string
    if c := getImexChannels(image); c != nil {
       imexChannels = *c
    }

    driverCapabilities := hookConfig.getDriverCapabilities(image, legacyImage).String()

    requirements, err := image.GetRequirements()
    if err != nil {
       log.Panicln(&amp;quot;failed to get requirements&amp;quot;, err)
    }

    return &amp;amp;nvidiaConfig{
       Devices:            devices,
       MigConfigDevices:   migConfigDevices,
       MigMonitorDevices:  migMonitorDevices,
       ImexChannels:       imexChannels,
       DriverCapabilities: driverCapabilities,
       Requirements:       requirements,
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心是 getDevice，就是根据 Mounts 信息或者 Env 解析要分配给该容器的 GPU&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func getDevices(hookConfig *HookConfig, image image.CUDA, mounts []Mount, privileged bool) *string {
    // If enabled, try and get the device list from volume mounts first
    if hookConfig.AcceptDeviceListAsVolumeMounts {
       devices := getDevicesFromMounts(mounts)
       if devices != nil {
          return devices
       }
    }

    // Fallback to reading from the environment variable if privileges are correct
    devices := getDevicesFromEnvvar(image, hookConfig.getSwarmResourceEnvvars())
    if devices == nil {
       return nil
    }
    if privileged || hookConfig.AcceptEnvvarUnprivileged {
       return devices
    }

    configName := hookConfig.getConfigOption(&amp;quot;AcceptEnvvarUnprivileged&amp;quot;)
    log.Printf(&amp;quot;Ignoring devices specified in NVIDIA_VISIBLE_DEVICES (privileged=%v, %v=%v) &amp;quot;, privileged, configName, hookConfig.AcceptEnvvarUnprivileged)

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到这里根据配置不同，提供了两种解析 devices 的方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;getDevicesFromMounts&lt;/li&gt;
&lt;li&gt;getDevicesFromEnvvar
这也就是为什么 nvidia device plugin 除了实现 Env 之外还实现了另外的方式，二者配置应该要对应才行。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里我们只关注 getDevicesFromEnvvar，从环境变量里解析 Device：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;envNVVisibleDevices     = &amp;quot;NVIDIA_VISIBLE_DEVICES&amp;quot;

func getDevicesFromEnvvar(image image.CUDA, swarmResourceEnvvars []string) *string {
	// We check if the image has at least one of the Swarm resource envvars defined and use this
	// if specified.
	var hasSwarmEnvvar bool
	for _, envvar := range swarmResourceEnvvars {
		if image.HasEnvvar(envvar) {
			hasSwarmEnvvar = true
			break
		}
	}

	var devices []string
	if hasSwarmEnvvar {
		devices = image.DevicesFromEnvvars(swarmResourceEnvvars...).List()
	} else {
		devices = image.DevicesFromEnvvars(envNVVisibleDevices).List()
	}

	if len(devices) == 0 {
		return nil
	}

	devicesString := strings.Join(devices, &amp;quot;,&amp;quot;)

	return &amp;amp;devicesString
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;devices = image.DevicesFromEnvvars(envNVVisibleDevices).List()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从 image 里面提取NVIDIA_VISIBLE_DEVICES环境变量，至于这个 Env 是哪里来的，也是容器 Spec 中定义的，之前 image 是这样初始化的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;s := loadSpec(path.Join(b, &amp;quot;config.json&amp;quot;))

	image, err := image.New(
		image.WithEnv(s.Process.Env), // 这里把容器 env 传给了 image 对象
		image.WithDisableRequire(hook.DisableRequire),
	)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际这里还有一个特殊逻辑：&lt;em&gt;如果没有设置 NVIDIA_VISIBLE_DEVICES环境变量，也没通过其他方式解析到 device 并且还是是一个 legacy image，那么默认使用全部 GPU。&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Environment variable unset with legacy image: default to &amp;quot;all&amp;quot;.
if !isSet &amp;amp;&amp;amp; len(devices) == 0 &amp;amp;&amp;amp; i.IsLegacy() {
  return NewVisibleDevices(&amp;quot;all&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那么什么算是 legacy image 呢：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// IsLegacy returns whether the associated CUDA image is a &amp;quot;legacy&amp;quot; image. An
// image is considered legacy if it has a CUDA_VERSION environment variable defined
// and no NVIDIA_REQUIRE_CUDA environment variable defined.
func (i CUDA) IsLegacy() bool {
	legacyCudaVersion := i.env[envCUDAVersion]
	cudaRequire := i.env[envNVRequireCUDA]
	return len(legacyCudaVersion) &amp;gt; 0 &amp;amp;&amp;amp; len(cudaRequire) == 0
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这也就是为什么，有时候启动 Pod 并没有申请 GPU，但是 Pod 里面依旧可以看到所有 GPU，就是走了这个 legacy image 的分支逻辑。&lt;/p&gt;

&lt;p&gt;至此，我们知道了这边 runtime 是怎么指定要把哪些 GPU 分配给容器了，接下来进入 Exec 逻辑。&lt;/p&gt;

&lt;h3 id=&#34;4-2-2-exec&#34;&gt;4.2.2. Exec&lt;/h3&gt;

&lt;p&gt;Exec 部分比较短，就是这两行代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;args := []string{getCLIPath(cli)}
err = syscall.Exec(args[0], args, env)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先是 getCLIPath，用于寻找 nvidia-container-cli 工具的位置并作为第一个参数。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func getCLIPath(config config.ContainerCLIConfig) string {
    if config.Path != &amp;quot;&amp;quot; {
       return config.Path
    }

    if err := os.Setenv(&amp;quot;PATH&amp;quot;, lookup.GetPath(config.Root)); err != nil {
       log.Panicln(&amp;quot;couldn&#39;t set PATH variable:&amp;quot;, err)
    }

    path, err := exec.LookPath(&amp;quot;nvidia-container-cli&amp;quot;)
    if err != nil {
       log.Panicln(&amp;quot;couldn&#39;t find binary nvidia-container-cli in&amp;quot;, os.Getenv(&amp;quot;PATH&amp;quot;), &amp;quot;:&amp;quot;, err)
    }
    return path
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，如果单独配置了 cli 的位置参数就使用配置的位置，否则使用 LookPath 根据名字寻找。&lt;/p&gt;

&lt;p&gt;然后是相关的参数&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;    args := []string{getCLIPath(cli)}
    if cli.Root != &amp;quot;&amp;quot; {
       args = append(args, fmt.Sprintf(&amp;quot;--root=%s&amp;quot;, cli.Root))
    }
    if cli.LoadKmods {
       args = append(args, &amp;quot;--load-kmods&amp;quot;)
    }
    if cli.NoPivot {
       args = append(args, &amp;quot;--no-pivot&amp;quot;)
    }
    if *debugflag {
       args = append(args, &amp;quot;--debug=/dev/stderr&amp;quot;)
    } else if cli.Debug != &amp;quot;&amp;quot; {
       args = append(args, fmt.Sprintf(&amp;quot;--debug=%s&amp;quot;, cli.Debug))
    }
    if cli.Ldcache != &amp;quot;&amp;quot; {
       args = append(args, fmt.Sprintf(&amp;quot;--ldcache=%s&amp;quot;, cli.Ldcache))
    }
    if cli.User != &amp;quot;&amp;quot; {
       args = append(args, fmt.Sprintf(&amp;quot;--user=%s&amp;quot;, cli.User))
    }
    args = append(args, &amp;quot;configure&amp;quot;)

    if ldconfigPath := cli.NormalizeLDConfigPath(); ldconfigPath != &amp;quot;&amp;quot; {
       args = append(args, fmt.Sprintf(&amp;quot;--ldconfig=%s&amp;quot;, ldconfigPath))
    }
    if cli.NoCgroups {
       args = append(args, &amp;quot;--no-cgroups&amp;quot;)
    }
    if len(nvidia.Devices) &amp;gt; 0 {
       args = append(args, fmt.Sprintf(&amp;quot;--device=%s&amp;quot;, nvidia.Devices))
    }
    if len(nvidia.MigConfigDevices) &amp;gt; 0 {
       args = append(args, fmt.Sprintf(&amp;quot;--mig-config=%s&amp;quot;, nvidia.MigConfigDevices))
    }
    if len(nvidia.MigMonitorDevices) &amp;gt; 0 {
       args = append(args, fmt.Sprintf(&amp;quot;--mig-monitor=%s&amp;quot;, nvidia.MigMonitorDevices))
    }
    if len(nvidia.ImexChannels) &amp;gt; 0 {
       args = append(args, fmt.Sprintf(&amp;quot;--imex-channel=%s&amp;quot;, nvidia.ImexChannels))
    }

    for _, cap := range strings.Split(nvidia.DriverCapabilities, &amp;quot;,&amp;quot;) {
       if len(cap) == 0 {
          break
       }
       args = append(args, capabilityToCLI(cap))
    }

    for _, req := range nvidia.Requirements {
       args = append(args, fmt.Sprintf(&amp;quot;--require=%s&amp;quot;, req))
    }

    args = append(args, fmt.Sprintf(&amp;quot;--pid=%s&amp;quot;, strconv.FormatUint(uint64(container.Pid), 10)))
    args = append(args, rootfs)

    env := append(os.Environ(), cli.Environment...)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;args = append(args, &amp;quot;configure&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;表示执行的是nvidia-container-cli configure 命令。&lt;/p&gt;

&lt;p&gt;最后则是调用 syscall.Exec 真正开始执行命令&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;err = syscall.Exec(args[0], args, env)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该命令具体在做什么呢，接着分析 nvidia-container-cli 实现。&lt;/p&gt;

&lt;h2 id=&#34;4-3-nvidia-container-cli&#34;&gt;4.3. nvidia-container-cli&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/NVIDIA/libnvidia-container/tree/master/src/cli&#34;&gt;nvidia-container-cli&lt;/a&gt; 是一个 C 写的小工具，主要作用就是根据上执行命令时传递的参数，把GPU 设备及其相关依赖库挂载到容器中，使得容器能够正常使用 GPU 能力。&lt;/p&gt;

&lt;p&gt;简单看下部分代码。&lt;/p&gt;

&lt;p&gt;首先是驱动信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// https://github.com/NVIDIA/libnvidia-container/blob/master/src/cli/configure.c#L279-L288

/* Query the driver and device information. */
if (perm_set_capabilities(&amp;amp;err, CAP_EFFECTIVE, ecaps[NVC_INFO], ecaps_size(NVC_INFO)) &amp;lt; 0) {
        warnx(&amp;quot;permission error: %s&amp;quot;, err.msg);
        goto fail;
}
if ((drv = libnvc.driver_info_new(nvc, NULL)) == NULL ||
    (dev = libnvc.device_info_new(nvc, NULL)) == NULL) {
        warnx(&amp;quot;detection error: %s&amp;quot;, libnvc.error(nvc));
        goto fail;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;nvc_driver_info_new()：获取 CUDA Driver 信息&lt;/li&gt;
&lt;li&gt;nvc_device_info_new()：获取 GPU Drvier 信息
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后获取容器中可见的 GPU 列表&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// https://github.com/NVIDIA/libnvidia-container/blob/master/src/cli/configure.c#L308-L314

        /* Select the visible GPU devices. */
        if (dev-&amp;gt;ngpus &amp;gt; 0) {
                if (select_devices(&amp;amp;err, ctx-&amp;gt;devices, dev, &amp;amp;devices) &amp;lt; 0) {
                        warnx(&amp;quot;device error: %s&amp;quot;, err.msg);
                        goto fail;
                }
        }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后则是将相关驱动挂载到容器里去：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// https://github.com/NVIDIA/libnvidia-container/blob/master/src/cli/configure.c#L362-L408

/* Mount the driver, visible devices, mig-configs and mig-monitors. */
if (perm_set_capabilities(&amp;amp;err, CAP_EFFECTIVE, ecaps[NVC_MOUNT], ecaps_size(NVC_MOUNT)) &amp;lt; 0) {
        warnx(&amp;quot;permission error: %s&amp;quot;, err.msg);
        goto fail;
}
if (libnvc.driver_mount(nvc, cnt, drv) &amp;lt; 0) {
        warnx(&amp;quot;mount error: %s&amp;quot;, libnvc.error(nvc));
        goto fail;
}
for (size_t i = 0; i &amp;lt; devices.ngpus; ++i) {
        if (libnvc.device_mount(nvc, cnt, devices.gpus[i]) &amp;lt; 0) {
                warnx(&amp;quot;mount error: %s&amp;quot;, libnvc.error(nvc));
                goto fail;
        }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;libnvidia-container是采用 linux c mount &amp;ndash;bind功能将 CUDA Driver Libraries/Binaries一个个挂载到容器里，而不是将整个目录挂载到容器中。&lt;/p&gt;

&lt;p&gt;可通过NVIDIA_DRIVER_CAPABILITIES环境变量指定要挂载的 driver libraries/binaries。&lt;/p&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -e NVIDIA_VISIBLE_DEVICES=0,1 -e NVIDIA_DRIVER_CAPABILITIES=compute,utility -it tensorflow/tensorflow:latest-gpu bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;指定NVIDIA_DRIVER_CAPABILITIES=compute,utility 就会把 compute 和 utility 相关的库挂载进去。&lt;/p&gt;

&lt;p&gt;这样容器里就可以使用 GPU 了。&lt;/p&gt;

&lt;p&gt;至此，相关源码就分析完成了。&lt;/p&gt;

&lt;h1 id=&#34;5-小结&#34;&gt;5. 小结&lt;/h1&gt;

&lt;p&gt;整个流程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）device plugin 上报节点上的 GPU 信息&lt;/li&gt;
&lt;li&gt;2）用户创建 Pod，在 resources.rquest 中申请 GPU，Scheduler 根据各节点 GPU 资源情况，将 Pod 调度到一个有足够 GPU 的节点&lt;/li&gt;
&lt;li&gt;3）DevicePlugin 根据 Pod 中申请的 GPU 资源，为容器添加NVIDIA_VISIBLE_DEVICES环境变量
 &amp;gt; 例如：NVIDIA_VISIBLE_DEVICES=GPU-03f69c50-207a-2038-9b45-23cac89cb67d&lt;/li&gt;

&lt;li&gt;&lt;p&gt;4）docker / containerd 启动容器&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;由于配置了 nvidia-container-runtime,因此会使用 nvidia-container-runtime 来创建容器&lt;/li&gt;
&lt;li&gt;nvidia-container-runtime 额外做了一件事：将 nvidia-container-runtime-hook 作为 prestart hook 添加到容器 spec 中，然后就将容器 spec 信息往后传给 runC 了。&lt;/li&gt;
&lt;li&gt;runC 创建容器前会调用 prestart hook，其中就包括了上一步添加的 nvidia-container-runtime-hook，该 hook 主要做两件事：

&lt;ul&gt;
&lt;li&gt;从容器 Spec 的 mounts 或者 env 中解析 GPU 信息&lt;/li&gt;
&lt;li&gt;调用 nvidia-container-cli 命令，将 NVIDIA 的 GPU Driver、CUDA Driver 等库文件挂载进容器，保证容器内可以使用被指定的 GPU以及对应能力
核心就是两个部分：&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;device plugin 根据 GPU 资源申请为容器添加 NVIDIA_VISIBLE_DEVICES环境变量&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;nvidia-container-toolkit 则是根据 NVIDIA_VISIBLE_DEVICES环境变量将 GPU、驱动等相关文件挂载到容器里。
看源码同时顺带解决了一个，之前遇到过的问题：&lt;em&gt;为什么 Pod 明明没有申请 GPU，启动后也能看到所有 GPU？&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这是因为 nvidia-container-toolkit 中存在特殊逻辑，没有设置 NVIDIA_VISIBLE_DEVICES环境变量，也没通过其他方式解析到 device 并且还是一个 legacy image，那么默认会返回all，即：NVIDIA_VISIBLE_DEVICES=all ，因此该 Pod 能看到全部 GPU。&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(10): device-plugin原理到实现</title>
            <link>http://mospany.github.io/2024/01/24/device-plugin/</link>
            <pubDate>Wed, 24 Jan 2024 19:31:10 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2024/01/24/device-plugin/</guid>
            <description>

&lt;p&gt;本文主要分析 k8s 中的 device-plugin 机制工作原理，并通过实现一个简单的 device-plugin 来加深理解。&lt;/p&gt;

&lt;h1 id=&#34;1-背景&#34;&gt;1. 背景&lt;/h1&gt;

&lt;p&gt;默认情况下，k8s 中的 Pod 只能申请 CPU 和 Memory 这两种资源，就像下面这样：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;resources:
  requests:
    memory: &amp;quot;1024Mi&amp;quot;
    cpu: &amp;quot;100m&amp;quot;
  limits:
    memory: &amp;quot;2048Mi&amp;quot;
    cpu: &amp;quot;200m&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;随着 AI 热度越来越高，更多的业务 Pod 需要申请 GPU 资源，&lt;a href=&#34;http://127.0.0.1:1313/2024/01/16/gpu-on-k8s/&#34;&gt;在ECS、Docker、K8s 等环境中使用 GPU&lt;/a&gt;中我们分析了如何在 k8s 环境中使用 GPU，就是靠 Device Plugin 机制，通过该机制使得 k8s 能感知到节点上的 GPU 资源，就像原生的 CPU 和 Memory 资源一样使用。&lt;/p&gt;

&lt;p&gt;实际上在早期，K8s 也提供了一种名为 alpha.kubernetes.io/nvidia-gpu 的资源来支持 NVIDIA GPU，不过后面也发现了很多问题，每增加一种资源都要修改 k8s 核心代码，k8s 社区压力山大。于是在 1.8 版本引入了 device plugin 机制，通过插件形式来接入其他资源，设备厂家只需要开发对应的 xxx-device-plugin 就可以将资源接入到 k8s 了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ps：类似的还有引入 CSI 让存储插件从 Kubernetes 内部（in-tree）代码库中分离出来，改为独立的、可插拔的外部组件（out-of-tree），还有 CRI、CNI 等等，这里的 Device Plugin 也能算作其中的一种。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Device Plugin 有两层含义，下文中根据语义自行区分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先它可以代表 k8s 中的 Device Plugin framework&lt;/li&gt;
&lt;li&gt;其次也可以代表厂家的具体实现，比如 NVIDIA/k8s-device-plugin，就是用于接入 NVIDIA GPU 资源的 Device Plugin 实现&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;2-原理&#34;&gt;2. 原理&lt;/h1&gt;

&lt;p&gt;Device Plugin 的工作原理其实不复杂，可以分为 插件注册 和 kubelet 调用插件两部分。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;插件注册：DevicePlugin 启动时会想节点上的 Kubelet 发起注册，这样 Kubelet就可以感知到该插件的存在了&lt;/li&gt;
&lt;li&gt;kubelet 调用插件：注册完成后，当有 Pod 申请对于资源时，kubelet 就会调用该插件 API 实现具体功能
如 k8s 官网上的图所示：
&lt;img src=&#34;post/2024/images/2024-01-24-device-plugin/IMG_20250124-111704478.png&#34; alt=&#34;picture 0&#34; /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2-1-kubelet-部分&#34;&gt;2.1. Kubelet 部分&lt;/h2&gt;

&lt;p&gt;为了提供该功能，Kubelet 新增了一个 Registration gRPC service:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;service Registration {
	rpc Register(RegisterRequest) returns (Empty) {}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;device plugin 可以调用该接口向 Kubelet 进行注册，注册接口需要提供三个参数：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;device plugin 对应的 unix socket 名字：后续 kubelet 根据名称找到对应的 unix socket，并向插件发起调用&lt;/li&gt;
&lt;li&gt;device plugin 调 API version：用于区分不同版本的插件&lt;/li&gt;
&lt;li&gt;device plugin 提供的 ResourceName：遇到不能处理的资源申请时(CPU和Memory之外的资源)，Kubelet 就会根据申请的资源名称来匹配对应的插件&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;ResourceName 需要按照vendor-domain/resourcetype 格式，例如nvidia.com/gpu。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;2-2-device-plugin-部分&#34;&gt;2.2. device plugin 部分&lt;/h2&gt;

&lt;p&gt;要进行设备管理，device plugin 插件需要实现以下接口：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GetDevicePluginOptions：这个接口用于获取设备插件的信息，可以在其返回的响应中指定一些设备插件的配置选项，可以看做是插件的元数据&lt;/li&gt;
&lt;li&gt;ListAndWatch：该接口用于列出可用的设备并持续监视这些设备的状态变化。&lt;/li&gt;
&lt;li&gt;GetPreferredAllocation：将分配偏好信息提供给 device plugin,以便 device plugin 在分配时可以做出更好的选择&lt;/li&gt;
&lt;li&gt;Allocate：该接口用于向设备插件请求分配指定数量的设备资源。&lt;/li&gt;
&lt;li&gt;PreStartContainer： 该接口在容器启动之前调用，用于配置容器使用的设备资源。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;只有 ListAndWatch 和 Allocate 两个接口是必须的，其他都是可以选的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;2-3-工作流程&#34;&gt;2.3. 工作流程&lt;/h2&gt;

&lt;p&gt;一般所有的 Device Plugin 实现最终都会以 Pod 形式运行在 k8s 集群中，又因为需要管理所有节点，因此都会以 DaemonSet 方式部署。&lt;/p&gt;

&lt;p&gt;device plugin 启动之后第一步就是向 Kubelet 注册，让 Kubelet 知道有一个新的设备接入了。&lt;/p&gt;

&lt;p&gt;为了能够调用 Kubelet 的 Register 接口，Device Plugin Pod 会将宿主机上的 kubelet.sock 文件(unix socket)挂载到容器中，通过 kubelet.sock 文件发起调用以实现注册。&lt;/p&gt;

&lt;p&gt;集群部署后，Kubelet 就会启动，&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Kubelet 启动 Registration gRPC 服务（kubelet.sock），提供 Register 接口&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;device-plugin 启动后，通过 kubelet.sock 调用 Register 接口，向 Kubelet 进行注册，注册信息包括 device plugin 的 unix socket，API Version，ResourceName&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;注册成功后，Kubelet 通过 device-plugin 的 unix socket 向 device plugin 调用 ListAndWatch， 获取当前节点上的资源&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Kubelet 向 api-server 更新节点状态来记录上一步中发现的资源&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;此时 kubelet get node -oyaml 就能查看到 Node 对象的 Capacity 中多了对应的资源&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
&lt;li&gt;用户创建 Pod 并申请该资源，调度完成后，对应节点上的 kubelet 调用 device plugin 的 Allocate 接口进行资源分配
&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;大致如下：
&lt;img src=&#34;post/2024/images/2024-01-24-device-plugin/IMG_20250124-141629596.png&#34; alt=&#34;picture 1&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;3-实现&#34;&gt;3. 实现&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;源码：&lt;a href=&#34;https://github.com/mospany/i-device-plugin&#34;&gt;https://github.com/mospany/i-device-plugin&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;device plugin 实现大致分为三部分：&lt;/p&gt;

&lt;p&gt;1）启动时向 Kubelet 发起注册
   - 注意监控 kubelet 的重启，一般是使用 fsnotify 类似的库监控 kubelet.sock 的重新创建事件。如果 kubelet.sock 重新创建了，则认为 kubelet 是重启了，那么需要重新注册
2）gRPC Server：主要是实现 ListAndWatch 和 Allocate两个方法&lt;/p&gt;

&lt;h2 id=&#34;3-1-实现-grpc-server&#34;&gt;3.1. 实现 gRPC Server&lt;/h2&gt;

&lt;p&gt;简单起见，这里只实现了ListAndWatch 和 Allocate 这两个必须的方法。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;对 gRPC 不熟悉的童鞋可以看下这个 –&amp;gt; &lt;a href=&#34;https://www.lixueduan.com/tags/grpc/&#34;&gt;gRPC 系列教程&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;3-1-1-listandwatch&#34;&gt;3.1.1. ListAndWatch&lt;/h3&gt;

&lt;p&gt;这是一个 gRPC 的 Stream 方法，建立长连接，可以持续向 Kubelet 发送设备的信息。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// ListAndWatch returns a stream of List of Devices
// Whenever a Device state change or a Device disappears, ListAndWatch
// returns the new list
func (c *GopherDevicePlugin) ListAndWatch(_ *pluginapi.Empty, srv pluginapi.DevicePlugin_ListAndWatchServer) error {
	devs := c.dm.Devices()
	klog.Infof(&amp;quot;find devices [%s]&amp;quot;, String(devs))

	err := srv.Send(&amp;amp;pluginapi.ListAndWatchResponse{Devices: devs})
	if err != nil {
		return errors.WithMessage(err, &amp;quot;send device failed&amp;quot;)
	}

	klog.Infoln(&amp;quot;waiting for device update&amp;quot;)
	for range c.dm.notify {
		devs = c.dm.Devices()
		klog.Infof(&amp;quot;device update,new device list [%s]&amp;quot;, String(devs))
		_ = srv.Send(&amp;amp;pluginapi.ListAndWatchResponse{Devices: devs})
	}
	return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;发现设备的部分代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// List all device
func (d *DeviceMonitor) List() error {
	err := filepath.Walk(d.path, func(path string, info fs.FileInfo, err error) error {
		if info.IsDir() {
			klog.Infof(&amp;quot;%s is dir,skip&amp;quot;, path)
			return nil
		}

		d.devices[info.Name()] = &amp;amp;pluginapi.Device{
			ID:     info.Name(),
			Health: pluginapi.Healthy,
		}
		return nil
	})

	return errors.WithMessagef(err, &amp;quot;walk [%s] failed&amp;quot;, d.path)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;很简单，就是遍历查看 /etc/gophers 目录下的所有文件，每个文件都会当做一个设备。&lt;/p&gt;

&lt;p&gt;然后再启动一个 Goroutine 监控设备的变化,即/etc/gophers 目录下文件有变化时通过 chan 发送通知,将最新的设备信息发送给 Kubelet。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Watch device change
func (d *DeviceMonitor) Watch() error {
	klog.Infoln(&amp;quot;watching devices&amp;quot;)

	w, err := fsnotify.NewWatcher()
	if err != nil {
		return errors.WithMessage(err, &amp;quot;new watcher failed&amp;quot;)
	}
	defer w.Close()

	errChan := make(chan error)
	go func() {
		defer func() {
			if r := recover(); r != nil {
				errChan &amp;lt;- fmt.Errorf(&amp;quot;device watcher panic:%v&amp;quot;, r)
			}
		}()
		for {
			select {
			case event, ok := &amp;lt;-w.Events:
				if !ok {
					continue
				}
				klog.Infof(&amp;quot;fsnotify device event: %s %s&amp;quot;, event.Name, event.Op.String())

				if event.Op == fsnotify.Create {
					dev := path.Base(event.Name)
					d.devices[dev] = &amp;amp;pluginapi.Device{
						ID:     dev,
						Health: pluginapi.Healthy,
					}
					d.notify &amp;lt;- struct{}{}
					klog.Infof(&amp;quot;find new device [%s]&amp;quot;, dev)
				} else if event.Op&amp;amp;fsnotify.Remove == fsnotify.Remove {
					dev := path.Base(event.Name)
					delete(d.devices, dev)
					d.notify &amp;lt;- struct{}{}
					klog.Infof(&amp;quot;device [%s] removed&amp;quot;, dev)
				}

			case err, ok := &amp;lt;-w.Errors:
				if !ok {
					continue
				}
				klog.Errorf(&amp;quot;fsnotify watch device failed:%v&amp;quot;, err)
			}
		}
	}()

	err = w.Add(d.path)
	if err != nil {
		return fmt.Errorf(&amp;quot;watch device error:%v&amp;quot;, err)
	}

	return &amp;lt;-errChan
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-1-2-allocate&#34;&gt;3.1.2. Allocate&lt;/h3&gt;

&lt;p&gt;Allocate 则是需要告知 kubelet 怎么将设备分配给容器，这里实现比较简单，就是在对应容器中增加一个环境变量，Gopher=$deviceId&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Allocate is called during container creation so that the Device
// Plugin can run device specific operations and instruct Kubelet
// of the steps to make the Device available in the container
func (c *GopherDevicePlugin) Allocate(_ context.Context, reqs *pluginapi.AllocateRequest) (*pluginapi.AllocateResponse, error) {
	ret := &amp;amp;pluginapi.AllocateResponse{}
	for _, req := range reqs.ContainerRequests {
		klog.Infof(&amp;quot;[Allocate] received request: %v&amp;quot;, strings.Join(req.DevicesIDs, &amp;quot;,&amp;quot;))
		resp := pluginapi.ContainerAllocateResponse{
			Envs: map[string]string{
				&amp;quot;Gopher&amp;quot;: strings.Join(req.DevicesIDs, &amp;quot;,&amp;quot;),
			},
		}
		ret.ContainerResponses = append(ret.ContainerResponses, &amp;amp;resp)
	}
	return ret, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;简单看一下 NVIDIA 的 device plugin 是怎么实现 Allocate 的。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Allocate which return list of devices.
func (plugin *NvidiaDevicePlugin) Allocate(ctx context.Context, reqs *pluginapi.AllocateRequest) (*pluginapi.AllocateResponse, error) {
	responses := pluginapi.AllocateResponse{}
	for _, req := range reqs.ContainerRequests {
		if err := plugin.rm.ValidateRequest(req.DevicesIDs); err != nil {
			return nil, fmt.Errorf(&amp;quot;invalid allocation request for %q: %w&amp;quot;, plugin.rm.Resource(), err)
		}
		response, err := plugin.getAllocateResponse(req.DevicesIDs)
		if err != nil {
			return nil, fmt.Errorf(&amp;quot;failed to get allocate response: %v&amp;quot;, err)
		}
		responses.ContainerResponses = append(responses.ContainerResponses, response)
	}

	return &amp;amp;responses, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心其实是这个方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// updateResponseForDeviceListEnvvar sets the environment variable for the requested devices.
func (plugin *NvidiaDevicePlugin) updateResponseForDeviceListEnvvar(response *pluginapi.ContainerAllocateResponse, deviceIDs ...string) {
	response.Envs[plugin.deviceListEnvvar] = strings.Join(deviceIDs, &amp;quot;,&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;给容器添加了一个环境变量，value 为设备 id，具体 deviceID 提供了两种测量，可能是编号或者 uuid&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;const (
	DeviceIDStrategyUUID  = &amp;quot;uuid&amp;quot;
	DeviceIDStrategyIndex = &amp;quot;index&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;key 是一个变量 plugin.deviceListEnvvar，初始化如下:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;	plugin := NvidiaDevicePlugin{
		deviceListEnvvar:     &amp;quot;NVIDIA_VISIBLE_DEVICES&amp;quot;,
		socket:               pluginPath + &amp;quot;.sock&amp;quot;,
	  // ...
	}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是说 NVIDIA 这个 device plugin 实现 Allocate 主要就是给容器增加了环境变量，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;NVIDIA_VISIBLE_DEVICES=GPU-03f69c50-207a-2038-9b45-23cac89cb67d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;NVIDIA_VISIBLE_DEVICES=1,2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在文章 &lt;a href=&#34;http://blog.mospan.cn/2024/01/17/gpu-operator/&#34;&gt;使用 GPU Operator搭建AI算力环境&lt;/a&gt;, 提到 GPU Operator 会使用 NVIDIA Container Toolit Installer 安装 NVIDIA Container Toolit。&lt;/p&gt;

&lt;p&gt;这个 NVIDIA Container Toolit 的作用就是添加对 GPU 的支持，也包括了识别 NVIDIA_VISIBLE_DEVICES 这个环境变量，然后将对应设备挂载到容器里。&lt;/p&gt;

&lt;p&gt;除此之外还会把设备挂载到容器里：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (plugin *NvidiaDevicePlugin) apiDeviceSpecs(devRoot string, ids []string) []*pluginapi.DeviceSpec {
	optional := map[string]bool{
		&amp;quot;/dev/nvidiactl&amp;quot;:        true,
		&amp;quot;/dev/nvidia-uvm&amp;quot;:       true,
		&amp;quot;/dev/nvidia-uvm-tools&amp;quot;: true,
		&amp;quot;/dev/nvidia-modeset&amp;quot;:   true,
	}

	paths := plugin.rm.GetDevicePaths(ids)

	var specs []*pluginapi.DeviceSpec
	for _, p := range paths {
		if optional[p] {
			if _, err := os.Stat(p); err != nil {
				continue
			}
		}
		spec := &amp;amp;pluginapi.DeviceSpec{
			ContainerPath: p,
			HostPath:      filepath.Join(devRoot, p),
			Permissions:   &amp;quot;rw&amp;quot;,
		}
		specs = append(specs, spec)
	}

	return specs
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;		spec := &amp;amp;pluginapi.DeviceSpec{
			ContainerPath: p,
			HostPath:      filepath.Join(devRoot, p),
			Permissions:   &amp;quot;rw&amp;quot;,
		}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里指定了设备在宿主机上的 Path 和挂载到容器之后的 Path，后续就可以根据这些信息进行设备挂载了。&lt;/p&gt;

&lt;p&gt;实际上 device plugin 提供了多种方法来完成设备分配，实现时只需要根据具体情况选择其中一种即可：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Env: 设备插件可以通过环境变量将设备的相关信息传递给容器。这种方式通常用于将设备的配置信息、路径或其他参数传递给容器中的应用程序。&lt;/li&gt;
&lt;li&gt;Mounts: 设备插件通过挂载设备到容器的文件系统中，使容器能够直接访问设备。这个方法适用于需要直接与硬件设备交互的情况，例如 GPU、网络设备、块存储等。&lt;/li&gt;
&lt;li&gt;Devices: 设备插件可以通过 Devices 字段将设备直接映射到容器。此方法允许设备被显式地添加到容器中，类似于 Linux 中的 &amp;ndash;device 选项。&lt;/li&gt;
&lt;li&gt;Annotations: 设备插件还可以使用注解来提供额外的信息，供调度器使用。注解通常不会直接影响容器的行为，而是为 Kubernetes 调度器提供信息，帮助调度器做出更合适的决策。&lt;/li&gt;
&lt;li&gt;CDIDevices: CDI（Container Device Interface）是一种 Kubernetes API 扩展，允许设备插件通过 CDI 标准进行设备分配。通过 CDI，Kubernetes 可以更容易地管理和调度设备，如 GPU、FPGA、网络卡等。
比如 nvidia device plugin 在实现时就同时使用了 Env 和 Devices 方式。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;又比如 &lt;a href=&#34;https://github.com/Deep-Spark/ix-device-plugin/blob/master/pkg/dpm/plugin.go#L144-L156&#34;&gt;ix-device-plugin&lt;/a&gt; 就是用的 Devices 方式,直接指定分配给容器的设备在宿主机的位置，以及要挂载到容器中的位置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (p *iluvatarDevicePlugin) allocateDevicesByDeviceID(hostminor uint, num int) *pluginapi.DeviceSpec {
	var device pluginapi.DeviceSpec

	hostPathPrefix := &amp;quot;/dev/&amp;quot;
	containerPathPrefix := &amp;quot;/dev/&amp;quot;

	// Expose the device node for iluvatar pod.
	device.HostPath = hostPathPrefix + deviceName + strconv.Itoa(int(hostminor))
	device.ContainerPath = containerPathPrefix + deviceName + strconv.Itoa(num)
	device.Permissions = &amp;quot;rw&amp;quot;

	return &amp;amp;device
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-1-3-其他方法&#34;&gt;3.1.3. 其他方法&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// GetDevicePluginOptions returns options to be communicated with Device
// Manager
func (c *GopherDevicePlugin) GetDevicePluginOptions(_ context.Context, _ *pluginapi.Empty) (*pluginapi.DevicePluginOptions, error) {
	return &amp;amp;pluginapi.DevicePluginOptions{PreStartRequired: true}, nil
}

// GetPreferredAllocation returns a preferred set of devices to allocate
// from a list of available ones. The resulting preferred allocation is not
// guaranteed to be the allocation ultimately performed by the
// devicemanager. It is only designed to help the devicemanager make a more
// informed allocation decision when possible.
func (c *GopherDevicePlugin) GetPreferredAllocation(_ context.Context, _ *pluginapi.PreferredAllocationRequest) (*pluginapi.PreferredAllocationResponse, error) {
	return &amp;amp;pluginapi.PreferredAllocationResponse{}, nil
}

// PreStartContainer is called, if indicated by Device Plugin during registeration phase,
// before each container start. Device plugin can run device specific operations
// such as reseting the device before making devices available to the container
func (c *GopherDevicePlugin) PreStartContainer(_ context.Context, _ *pluginapi.PreStartContainerRequest) (*pluginapi.PreStartContainerResponse, error) {
	return &amp;amp;pluginapi.PreStartContainerResponse{}, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-2-向-kubelet-进行注册&#34;&gt;3.2. 向 Kubelet 进行注册&lt;/h2&gt;

&lt;p&gt;注册也是很简单，调用 deviceplugin 提供的 RegisterRequest 方法即可。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Register registers the device plugin for the given resourceName with Kubelet.
func (c *GopherDevicePlugin) Register() error {
	conn, err := connect(pluginapi.KubeletSocket, common.ConnectTimeout)
	if err != nil {
		return errors.WithMessagef(err, &amp;quot;connect to %s failed&amp;quot;, pluginapi.KubeletSocket)
	}
	defer conn.Close()

	client := pluginapi.NewRegistrationClient(conn)
	reqt := &amp;amp;pluginapi.RegisterRequest{
		Version:      pluginapi.Version,
		Endpoint:     path.Base(common.DeviceSocket),
		ResourceName: common.ResourceName,
	}

	_, err = client.Register(context.Background(), reqt)
	if err != nil {
		return errors.WithMessage(err, &amp;quot;register to kubelet failed&amp;quot;)
	}
	return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-3-监控-kubelet-sock-状态&#34;&gt;3.3. 监控 kubelet.sock 状态&lt;/h2&gt;

&lt;p&gt;使用 fsnotify 库监控 kubelet.sock 文件状态，通过 kubelet.sock 文件的变化来判断 kubelet 是否重启，当 kubelet 重启后 device plugin 也需要重启，然后注册到新的 kubelet.sock。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// WatchKubelet restart device plugin when kubelet restarted
func WatchKubelet(stop chan&amp;lt;- struct{}) error {
	watcher, err := fsnotify.NewWatcher()
	if err != nil {
		return errors.WithMessage(err, &amp;quot;Unable to create fsnotify watcher&amp;quot;)
	}
	defer watcher.Close()

	go func() {
		// Start listening for events.
		for {
			select {
			case event, ok := &amp;lt;-watcher.Events:
				if !ok {
					continue
				}
				klog.Infof(&amp;quot;fsnotify events: %s %v&amp;quot;, event.Name, event.Op.String())
				if event.Name == pluginapi.KubeletSocket &amp;amp;&amp;amp; event.Op == fsnotify.Create {
					klog.Warning(&amp;quot;inotify: kubelet.sock created, restarting.&amp;quot;)
					stop &amp;lt;- struct{}{}
				}
			case err, ok := &amp;lt;-watcher.Errors:
				if !ok {
					continue
				}
				klog.Errorf(&amp;quot;fsnotify failed restarting,detail:%v&amp;quot;, err)
			}
		}
	}()

	// watch kubelet.sock
	err = watcher.Add(pluginapi.KubeletSocket)
	if err != nil {
		return errors.WithMessagef(err, &amp;quot;Unable to add path %s to watcher&amp;quot;, pluginapi.KubeletSocket)
	}
	return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;为什么需要重新注册&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;因为Kubelet 中使用一个 map 来存储注册的插件，因此每次 Kubelet 重启都会丢失，所以我们在实现 device plugin 时就要监控 Kubelet 重启状态并重新注册。&lt;/p&gt;

&lt;p&gt;Kubelet Register 方法 实现如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// /pkg/kubelet/cm/devicemanager/plugin/v1beta1/server.go#L143-L165
func (s *server) Register(ctx context.Context, r *api.RegisterRequest) (*api.Empty, error) {
	klog.InfoS(&amp;quot;Got registration request from device plugin with resource&amp;quot;, &amp;quot;resourceName&amp;quot;, r.ResourceName)
	metrics.DevicePluginRegistrationCount.WithLabelValues(r.ResourceName).Inc()

	if !s.isVersionCompatibleWithPlugin(r.Version) {
		err := fmt.Errorf(errUnsupportedVersion, r.Version, api.SupportedVersions)
		klog.InfoS(&amp;quot;Bad registration request from device plugin with resource&amp;quot;, &amp;quot;resourceName&amp;quot;, r.ResourceName, &amp;quot;err&amp;quot;, err)
		return &amp;amp;api.Empty{}, err
	}

	if !v1helper.IsExtendedResourceName(core.ResourceName(r.ResourceName)) {
		err := fmt.Errorf(errInvalidResourceName, r.ResourceName)
		klog.InfoS(&amp;quot;Bad registration request from device plugin&amp;quot;, &amp;quot;err&amp;quot;, err)
		return &amp;amp;api.Empty{}, err
	}

	if err := s.connectClient(r.ResourceName, filepath.Join(s.socketDir, r.Endpoint)); err != nil {
		klog.InfoS(&amp;quot;Error connecting to device plugin client&amp;quot;, &amp;quot;err&amp;quot;, err)
		return &amp;amp;api.Empty{}, err
	}

	return &amp;amp;api.Empty{}, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心在 connectClient 方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (s *server) connectClient(name string, socketPath string) error {
	c := NewPluginClient(name, socketPath, s.chandler)

	s.registerClient(name, c)
	if err := c.Connect(); err != nil {
		s.deregisterClient(name)
		klog.ErrorS(err, &amp;quot;Failed to connect to new client&amp;quot;, &amp;quot;resource&amp;quot;, name)
		return err
	}

	go func() {
		s.runClient(name, c)
	}()

	return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;怎么保存这个 client 的呢?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (s *server) registerClient(name string, c Client) {
	s.mutex.Lock()
	defer s.mutex.Unlock()

	s.clients[name] = c
	klog.V(2).InfoS(&amp;quot;Registered client&amp;quot;, &amp;quot;name&amp;quot;, name)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type server struct {
	socketName string
	socketDir  string
	mutex      sync.Mutex
	wg         sync.WaitGroup
	grpc       *grpc.Server
	rhandler   RegistrationHandler
	chandler   ClientHandler
	clients    map[string]Client // 使用 map 存储，并为持久化
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-4-main-go&#34;&gt;3.4. main.go&lt;/h2&gt;

&lt;p&gt;main 方法分为三个部分：&lt;/p&gt;

&lt;p&gt;1) 启动 gRPC 服务
2) 向 Kubelet 进行注册
3) 监控 kubelet.sock 状态&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func main() {
	klog.Infof(&amp;quot;device plugin starting&amp;quot;)
	dp := device_plugin.NewGopherDevicePlugin()
	go dp.Run()

	// register when device plugin start
	if err := dp.Register(); err != nil {
		klog.Fatalf(&amp;quot;register to kubelet failed: %v&amp;quot;, err)
	}

	// watch kubelet.sock,when kubelet restart,exit device plugin,then will restart by DaemonSet
	stop := make(chan struct{})
	err := utils.WatchKubelet(stop)
	if err != nil {
		klog.Fatalf(&amp;quot;start to kubelet failed: %v&amp;quot;, err)
	}

	&amp;lt;-stop
	klog.Infof(&amp;quot;kubelet restart,exiting&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;4-测试&#34;&gt;4. 测试&lt;/h1&gt;

&lt;h2 id=&#34;4-1-编译&#34;&gt;4.1. 编译&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:mospany/i-device-plugin.git
cd i-device-plugin
make build-image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-24-device-plugin/IMG_20250125-200208751.png&#34; alt=&#34;picture 2&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;4-2-打包镜像&#34;&gt;4.2. 打包镜像&lt;/h2&gt;

&lt;p&gt;由于国内不能直接访问hub.docker.io了，需本地导入。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker save mospany/i-device-plugin:latest -o i-device-plugin_latest.tar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将在本地目录下生成i-device-plugin_latest.tar。&lt;/p&gt;

&lt;h2 id=&#34;4-3-导入镜像&#34;&gt;4.3. 导入镜像&lt;/h2&gt;

&lt;p&gt;将编译机上生成的镜像上传到目标worker上，再导入。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ctr -n k8s.io image import i-device-plugin_latest.tar 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-worker1 img]# crictl images | grep i-device
docker.io/mospany/i-device-plugin                                                     latest                             5954f214c8b06       23.5MB
[root@k8s-worker1 img]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;4-4-部署&#34;&gt;4.4. 部署&lt;/h2&gt;

&lt;p&gt;首先是部署 i-device-plugin，一般使用 DaemonSet 方式部署，完整 yaml 如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: i-device-plugin
  namespace: kube-system
  labels:
    app: i-device-plugin
spec:
  selector:
    matchLabels:
      app: i-device-plugin
  template:
    metadata:
      labels:
        app: i-device-plugin
    spec:
      containers:
        - name: i-device-plugin
          image: mospany/i-device-plugin:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              cpu: &amp;quot;1&amp;quot;
              memory: &amp;quot;512Mi&amp;quot;
            requests:
              cpu: &amp;quot;0.1&amp;quot;
              memory: &amp;quot;128Mi&amp;quot;
          volumeMounts:
            - name: device-plugin
              mountPath: /var/lib/kubelet/device-plugins
            - name: gophers
              mountPath: /etc/gophers
      volumes:
        - name: device-plugin
          hostPath:
            path: /var/lib/kubelet/device-plugins
        - name: gophers
          hostPath:
            path: /etc/gophers

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以 hostPath 方式将用到的两个目录挂载到 Pod 里：
&lt;img src=&#34;post/2024/images/2024-01-24-device-plugin/IMG_20250125-214446131.png&#34; alt=&#34;picture 3&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;/var/lib/kubelet/device-plugins：请求 kubelet.sock 发起调用，同时将 device-plugin gRPC 服务的 sock 文件写入该目录供 kubelet 调用&lt;/li&gt;
&lt;li&gt;/etc/gophers：在该 Demo 中，把 /etc/gophers 目录下的文件作为设备，因此需要将其挂载到 Pod 里。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用kubectl进行apply下yaml文件，确保 i-device-plugin 已经启动。
&lt;img src=&#34;post/2024/images/2024-01-24-device-plugin/IMG_20250125-214524956.png&#34; alt=&#34;picture 4&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;4-5-初始化&#34;&gt;4.5. 初始化&lt;/h2&gt;

&lt;p&gt;在该 Demo 中，把 /etc/gophers 目录下的文件作为设备，因此我们只需要到 /etc/gophers 目录下创建文件，模拟有新的设备接入即可。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir /etc/gophers

touch /etc/gophers/g1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看 device plugin 日志
&lt;img src=&#34;post/2024/images/2024-01-24-device-plugin/IMG_20250125-214911049.png&#34; alt=&#34;picture 5&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到，已经感知到新增的设备了。&lt;/p&gt;

&lt;p&gt;不出意外的话可以在 node 上看到新资源了&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-24-device-plugin/IMG_20250125-215648627.png&#34; alt=&#34;picture 6&#34; /&gt;&lt;/p&gt;

&lt;p&gt;果然，node capacity 中新增了lixueduan.com/gopher: &amp;ldquo;1&amp;rdquo;， 代表创建了2个设备：/etc/gophers/g1和/etc/gophers/g2。&lt;/p&gt;

&lt;h2 id=&#34;4-6-创建测试-pod&#34;&gt;4.6. 创建测试 Pod&lt;/h2&gt;

&lt;p&gt;接下来创建一个 Pod 申请该资源试试&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  name: gopher-pod
spec:
  containers:
  - name: gopher-container
    image: docker.m.daocloud.io/busybox
    command: [&amp;quot;sh&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;echo Hello, Kubernetes! &amp;amp;&amp;amp; sleep 3600&amp;quot;]
    resources:
      requests:
        lixueduan.com/gopher: &amp;quot;1&amp;quot;
      limits:
        lixueduan.com/gopher: &amp;quot;1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod 启动成功&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 yaml]# kubectl  get pod -A | grep gopher
default        gopher-pod                                                        1/1     Running     0               4m18s
[root@k8s-master1 yaml]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之前分配设备是添加 Gopher=xxx 这个环境变量，现在看下是否正常分配&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 yaml]# kubectl  exec -ti gopher-pod -- env | grep -i goph
HOSTNAME=gopher-pod
Gopher=g2
[root@k8s-master1 yaml]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ok,环境变量存在，可以看到分配给该 Pod 的设备是 g2。&lt;/p&gt;

&lt;h1 id=&#34;5-小结&#34;&gt;5. 小结&lt;/h1&gt;

&lt;p&gt;本文主要分析了 k8s 中的 Device Plugin 机制的工作原理，并实现了一个简单的 &lt;strong&gt;i-device-plugin&lt;/strong&gt;来进一步加深理解。&lt;/p&gt;

&lt;p&gt;Device Plugin 的工作原理其实不复杂，可以分为 插件注册 和 kubelet 调用插件两部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;插件注册：DevicePlugin 启动时会想节点上的 Kubelet 发起注册，这样 Kubelet 就可以感知到该插件的存在了&lt;/li&gt;
&lt;li&gt;kubelet 调用插件：注册完成后，当有 Pod 申请对于资源时，kubelet 就会调用该插件 API 实现具体功能&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-24-device-plugin/IMG_20250125-223139774.png&#34; alt=&#34;picture 7&#34; /&gt;&lt;/p&gt;

&lt;p&gt;以下是总结的几个常见问题：&lt;/p&gt;

&lt;p&gt;1）device plugin 是怎么感知节点上的设备的？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一般设备都会在 /dev/ 目录下，比如 NVIDIA GPU 就是 /dev/nvidia0、/dev/nvidia1 这样，不过具体逻辑还是得硬件厂商自己实现&lt;/li&gt;
&lt;li&gt;然后 device plugin 会以 DaemonSet 方式部署到所有节点，因此能发现每个节点上的设备
&lt;img src=&#34;post/2024/images/2024-01-24-device-plugin/IMG_20250125-223501456.png&#34; alt=&#34;picture 8&#34; /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2）device plugin Allocate 方法怎么实现分配设备给容器的？
&amp;gt; 需要注意一点：Allocate 方法并没有真正将设备分配给容器，因为这个时候甚至都还没创建容器，只是在该方法中可以通过 Env、Mounts、Devices、Annotations、CDIDevices 等不同形式来传递 要将那些设备分配给该容器 这个信息给后续组件。&lt;/p&gt;

&lt;p&gt;这些信息传给 Kubelet，然后 Kubelet 通过 CRI 调用 Runtime（Docker/Containerd 等等）真正开始创建容器。&lt;/p&gt;

&lt;p&gt;比如 NVIDIA 在 Allocate 中就传递了 NVIDIA_VISIBLE_DEVICES 这个 Env，然后自己实现了 nvidia-container-runtime，该 runtime 就可以根据该 Env 知道要把哪个 GPU 分配给容器，然后修改容器的 OCI Spec，最终 runC(或者其他实现)真正创建容器时就会按照这个描述去处理。&lt;/p&gt;

&lt;p&gt;又比如 ix-device-plugin 就是用的 Devices 方式,直接指定分配给容器的设备在宿主机的位置，以及要挂载到容器中的位置，这样就不需要实现自己的 container-runtime 了，runC 创建容器时也能把对应设备分配给容器。&lt;/p&gt;

&lt;p&gt;这样又引出一个小问题，既然天数(ix-device-plugin)这个实现只用 Devices 就能正常运行，那为什么 NVIDIA 实现了 Devices 又实现了一个 Env？&lt;/p&gt;

&lt;p&gt;其实这个 Env 的实现是为了兼容非 k8s 环境，比如 Docker 环境：&lt;/p&gt;

&lt;p&gt;nvidia 可以在启动容器时指定 GPU&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# --gpus
docker run --gpus device=0 -it tensorflow/tensorflow:latest-gpu bash
# 或者环境变量 NVIDIA_VISIBLE_DEVICES
docker run -e NVIDIA_VISIBLE_DEVICES=0 -it tensorflow/tensorflow:latest-gpu bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而天数则不行，就像这样：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo docker run --shm-size=&amp;quot;32g&amp;quot; -it -v /usr/src:/usr/src -v /lib/modules:/lib/modules -v /dev:/dev --privileged --cap-add=ALL --pid=host corex:4.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，需要自己使用 -v 将相关文件挂载进容器才能使用，nvidia-container-runtime 实则是将这部分进行了封装简化，用户只需要传一个参数即可。&lt;/p&gt;

&lt;p&gt;3）为什么 device plugin 要 Watch Kubelet 状态，当 Kubelet 重启后 device plugin 也要跟着重启。&lt;/p&gt;

&lt;p&gt;这个问题实际上可以翻译为：为什么 Kubelet 重启后，device plugin 需要重新向 Kubelet 注册？&lt;/p&gt;

&lt;p&gt;因为 device plugin 的注册信息 Kubelet 是存在内存中的，使用 Go 中的 Map 结构进行存储。重启后就会丢失，因此各个 device plugin 都需要重新注册。&lt;/p&gt;

&lt;p&gt;至于为什么 device plugin 一般也会跟着重启，是因为 device plugin 在启动时会调用因此注册接口，因此感知到 Kubelet 重启了，直接让 device plugin 退出即可，然后 DaemonSet 会重新拉起 Pod，这样启动后自动调用注册接口。&lt;/p&gt;

&lt;h1 id=&#34;6-参考&#34;&gt;6. 参考&lt;/h1&gt;

&lt;p&gt;【01】&lt;a href=&#34;https://www.lixueduan.com/posts/kubernetes/21-device-plugin/&#34;&gt;Kubernetes教程(二一)&amp;mdash;自定义资源支持：K8s Device Plugin 从原理到实现&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(09): 使用 GPU Operator搭建AI算力环境</title>
            <link>http://mospany.github.io/2024/01/17/gpu-operator/</link>
            <pubDate>Wed, 17 Jan 2024 19:31:10 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2024/01/17/gpu-operator/</guid>
            <description>

&lt;h1 id=&#34;1-引言&#34;&gt;1. 引言&lt;/h1&gt;

&lt;p&gt;为了学习AI应用、算法与算力等技术，应用需跑在GPU卡上，需要在节点上安装 GPU Driver、Container Toolkit 等组件，当集群规模较大时还是比较麻烦的。&lt;/p&gt;

&lt;p&gt;为了解决这个问题，NVIDIA 推出了 GPU Operator，GPU Operator 旨在简化在 Kubernetes 环境中使用 GPU 的过程，通过自动化的方式处理 GPU 驱动程序安装、Controller Toolkit、Device-Plugin 、监控等组件。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;基本上把需要手动安装、配置的地方全部自动化处理了，极大简化了 k8s 环境中的 GPU 使用。&lt;/p&gt;

&lt;p&gt;ps：只有 NVIDIA GPU 可以使用，其他厂家现在基本还是手动安装。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;2-规划&#34;&gt;2. 规划&lt;/h1&gt;

&lt;p&gt;本文主要分享如何使用 GPU Operator 快速搭建 Kubernetes GPU 环境。&lt;/p&gt;

&lt;p&gt;基于如下环境搭建：
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250117-194645644.png&#34; alt=&#34;picture 0&#34; /&gt;&lt;br /&gt;
查看worker节点GPU信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-worker1 ~]# lspci | grep -i nvidia
00:07.0 VGA compatible controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)
[root@k8s-worker1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明该worker节点有1张NVIDIA Tesla T4的GPU卡。&lt;/p&gt;

&lt;h1 id=&#34;3-组件介绍&#34;&gt;3. 组件介绍&lt;/h1&gt;

&lt;p&gt;这部分主要分析下 GPU Operator 涉及到的各个组件及其作用。&lt;/p&gt;

&lt;p&gt;NVIDIA GPU Operator总共包含如下的几个组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;NFD(Node Feature Discovery)：用于给节点打上某些标签，这些标签包括 cpu id、内核版本、操作系统版本、是不是 GPU 节点等，其中需要关注的标签是nvidia.com/gpu.present=true，如果节点存在该标签，那么说明该节点是 GPU 节点。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;GFD(GPU Feature Discovery)：用于收集节点的 GPU 设备属性（GPU 驱动版本、GPU型号等），并将这些属性以节点标签的方式透出。在k8s 集群中以 DaemonSet 方式部署，只有节点拥有标签nvidia.com/gpu.present=true时，DaemonSet 控制的 Pod 才会在该节点上运行。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;新版本 GFD 迁移到了 &lt;a href=&#34;https://github.com/NVIDIA/k8s-device-plugin/tree/main/docs/gpu-feature-discovery&#34;&gt;NVIDIA/k8s-device-plugin&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;NVIDIA Driver Installer：基于容器的方式在节点上安装 NVIDIA GPU 驱动，在 k8s 集群中以 DaemonSet 方式部署，只有节点拥有标签nvidia.com/gpu.present=true时，DaemonSet 控制的 Pod 才会在该节点上运行。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;NVIDIA Container Toolkit Installer：能够实现在容器中使用 GPU 设备，在 k8s 集群中以 DaemonSet 方式部署，同样的，只有节点拥有标签nvidia.com/gpu.present=true时，DaemonSet 控制的 Pod 才会在该节点上运行。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;NVIDIA Device Plugin：NVIDIA Device Plugin 用于实现将 GPU 设备以 Kubernetes 扩展资源的方式供用户使用，在 k8s 集群中以 DaemonSet 方式部署，只有节点拥有标签nvidia.com/gpu.present=true时，DaemonSet 控制的 Pod 才会在该节点上运行。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;DCGM Exporter：周期性的收集节点 GPU 设备的状态（当前温度、总的显存、已使用显存、使用率等）并暴露 Metrics，结合 Prometheus 和 Grafana 使用。在 k8s 集群中以DaemonSet 方式部署，只有节点拥有标签nvidia.com/gpu.present=true时，DaemonSet 控制的 Pod 才会在该节点上运行。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;首先是 GFD、NFD，二者都是用于发现 Node 上的信息，并以 label 形式添加到 k8s node 对象上，特别是 GFD 会添加nvidia.com/gpu.present=true 标签表示该节点有 GPU，只有携带该标签的节点才会安装后续组件。&lt;/p&gt;

&lt;p&gt;然后则是 Driver Installer、Container Toolkit Installer 用于安装 GPU 驱动和 container toolkit。&lt;/p&gt;

&lt;p&gt;接下来则是 device-plugin 让 k8s 能感知到 GPU 资源信息便于调度和管理。&lt;/p&gt;

&lt;p&gt;最后的 exporter 则是采集 GPU 监控并以 Prometheus Metrics 格式暴露，用于做 GPU 监控。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;这些组件基本就把需要手动配置的东西都自动化了。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;NVIDIA GPU Operator 依如下的顺序部署各个组件，并且如果前一个组件部署失败，那么其后面的组件将停止部署：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;NVIDIA Driver Installer&lt;/li&gt;
&lt;li&gt;NVIDIA Container Toolkit Installer&lt;/li&gt;
&lt;li&gt;NVIDIA Device Plugin&lt;/li&gt;
&lt;li&gt;DCGM Exporter&lt;/li&gt;
&lt;li&gt;GFD&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每个组件都是以 DaemonSet 方式部署，并且只有当节点存在标签 nvidia.com/gpu.present=true 时，各 DaemonSet控制的 Pod 才会在节点上运行。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nvidia.com/gpu.deploy.driver=pre-installed
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-1-gfd-nfd&#34;&gt;3.1. GFD &amp;amp; NFD&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;GFD：GPU Feature Discovery&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;NFD：Node Feature Discovery&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;根据名称基本能猜到这两个组件的功能，发现节点信息和 GPU 信息并以 Label 形式添加到 k8s 中的 node 对象上。&lt;/p&gt;

&lt;p&gt;其中 NFD 添加的 label 以   feature.node.kubernetes.io 作为前缀，比如:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;feature.node.kubernetes.io/cpu-cpuid.ADX=true
feature.node.kubernetes.io/system-os_release.ID=ubuntu
feature.node.kubernetes.io/system-os_release.VERSION_ID.major=22
feature.node.kubernetes.io/system-os_release.VERSION_ID.minor=04
feature.node.kubernetes.io/system-os_release.VERSION_ID=22.04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于 GFD 则主要记录 GPU 信息&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nvidia.com/cuda.runtime.major=12
nvidia.com/cuda.runtime.minor=2
nvidia.com/cuda.driver.major=535
nvidia.com/cuda.driver.minor=161
nvidia.com/gpu.product=Tesla-T4
nvidia.com/gpu.memory=15360
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-2-driver-installer&#34;&gt;3.2. Driver Installer&lt;/h2&gt;

&lt;p&gt;NVIDIA 官方提供了一种基于容器安装 NVIDIA 驱动的方式，GPU Operator 安装 nvidia 驱动也是采用的这种方式。&lt;/p&gt;

&lt;p&gt;当 NVIDIA 驱动基于容器化安装后，整个架构将演变成图中描述的样子：
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-152421553.png&#34; alt=&#34;picture 5&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Driver Installer 组件对应的 DaemonSet 就是nvidia-driver-daemonset-5.15.0-105-generic-ubuntu22.04。&lt;/p&gt;

&lt;p&gt;该 DaemonSet 对应的镜像为&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@test:~# kgo get ds nvidia-driver-daemonset-5.15.0-105-generic-ubuntu22.04 -oyaml|grep image
        image: nvcr.io/nvidia/driver:535-5.15.0-105-generic-ubuntu22.04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中 DaemonSet 名称/镜像由几部分组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nvidia-driver-daemonset 这部分为前缀&lt;/li&gt;
&lt;li&gt;5.15.0-105-generic 为内核版本，使用uname -r 命令查看&lt;/li&gt;
&lt;li&gt;ubuntu22.04 操作系统版本，使用cat /etc/os-release 命令查看&lt;/li&gt;
&lt;li&gt;535：这个是 GPU Driver 的版本号，这里表示安装 535 版本驱动，在部署时可以指定。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GPU Operator 会自动根据节点上的内核版本和操作系统生成 DaemonSet 镜像，因为是以 DaemonSet 方式运行的，所有节点上都是跑的同一个 Pod，&lt;strong&gt;因此要限制集群中的所有 GPU 节点操作系统和内核版本必须一致。&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ps：如果提前手动在节点上安装 GPU 驱动，那么 GPU Operator 检测到之后就不会在该节点上启动 Installer Pod，这样该节点就可以不需要管操作系统和内核版本。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;3-3-nvidia-container-toolkit-installer&#34;&gt;3.3. NVIDIA Container Toolkit Installer&lt;/h2&gt;

&lt;p&gt;该组件用于安装 NVIDIA Container Toolkit。&lt;/p&gt;

&lt;p&gt;手动安装的时候有两个步骤：&lt;/p&gt;

&lt;p&gt;1）安装 NVIDIA Container Toolkit&lt;/p&gt;

&lt;p&gt;2）修改 Runtime 配置指定使用 nvidia-runtime&lt;/p&gt;

&lt;p&gt;在整个调用链中新增 nvidia-container-runtime，以便处理 GPU 相关操作。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-153551818.png&#34; alt=&#34;picture 6&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这个 Installer 做的操作也就是这两步：&lt;/p&gt;

&lt;p&gt;1）将容器中NVIDIA Container Toolkit组件所涉及的命令行工具和库文件移动到/usr/local/nvidia/toolkit目录下&lt;/p&gt;

&lt;p&gt;2）在 /usr/local/nvidia/toolkit/.config/nvidia-container-runtime创建nvidia-container-runtime的配置文件config.toml，并设置nvidia-container-cli.root的值为/run/nvidia/driver。&lt;/p&gt;

&lt;h1 id=&#34;4-部署&#34;&gt;4. 部署&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;参考官方文档： &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#operator-install-guide&#34;&gt;operator-install-guide&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;4-1-准备工作&#34;&gt;4.1. 准备工作&lt;/h2&gt;

&lt;p&gt;要求：&lt;/p&gt;

&lt;p&gt;1）GPU 节点必须运行相同的操作系统，&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果提前手动在节点上安装驱动的话，该节点可以使用不同的操作系统&lt;/li&gt;
&lt;li&gt;CPU 节点操作系统没要求，因为 gpu-operator 只会在 GPU 节点上运行
2）GPU 节点必须配置相同容器引擎，例如都是 containerd 或者都是 docker&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3）如果使用了 Pod Security Admission (PSA) ，需要为 gpu-operator 标记特权模式&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# kubectl create ns gpu-operator
namespace/gpu-operator created
[root@k8s-master1 ~]# kubectl label --overwrite ns gpu-operator pod-security.kubernetes.io/enforce=privileged
namespace/gpu-operator labeled
[root@k8s-master1 ~]# kubectl get ns gpu-operator --show-labels
NAME           STATUS   AGE   LABELS
gpu-operator   Active   30s   kubernetes.io/metadata.name=gpu-operator,pod-security.kubernetes.io/enforce=privileged
[root@k8s-master1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4）集群中不要安装 NFD，如果已经安装了需要再安装 gpu-operator 时禁用 NFD 部署。&lt;/p&gt;

&lt;p&gt;使用以下命令查看集群中是否部署 NFD&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl get nodes -o json | jq &#39;.items[].metadata.labels | keys | any(startswith(&amp;quot;feature.node.kubernetes.io&amp;quot;))&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果返回 true 则说明集群中安装了 NFD。&lt;/p&gt;

&lt;h2 id=&#34;4-2-helm部署&#34;&gt;4.2. helm部署&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;参考官方文档： &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#operator-install-guide&#34;&gt;operator-install-guide&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;4-2-1-安装helm命令&#34;&gt;4.2.1. 安装helm命令&lt;/h3&gt;

&lt;p&gt;如果master节点上还未安装helm命令，需安装&lt;/p&gt;

&lt;h4 id=&#34;4-2-1-1-下载-helm-3-的最新版本&#34;&gt;4.2.1.1. 下载 Helm 3 的最新版本&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl https://get.helm.sh/helm-v3.11.1-linux-amd64.tar.gz -o helm-v3.11.1-linux-amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;4-2-1-2-解压安装包&#34;&gt;4.2.1.2. 解压安装包&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 helm]# ls
helm-v3.11.1-linux-amd64.tar.gz
[root@k8s-master1 helm]# tar -zxvf helm-v3.11.1-linux-amd64.tar.gz 
linux-amd64/
linux-amd64/helm
linux-amd64/LICENSE
linux-amd64/README.md
[root@k8s-master1 helm]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;4-2-1-3-移动-helm-到系统的可执行路径&#34;&gt;4.2.1.3. 移动 helm 到系统的可执行路径&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo mv linux-amd64/helm /usr/local/bin/helm`
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;4-2-1-4-验证安装&#34;&gt;4.2.1.4. 验证安装&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 helm]# helm version
version.BuildInfo{Version:&amp;quot;v3.11.1&amp;quot;, GitCommit:&amp;quot;293b50c65d4d56187cd4e2f390f0ada46b4c4737&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, GoVersion:&amp;quot;go1.18.10&amp;quot;}
[root@k8s-master1 helm]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明安装成功。&lt;/p&gt;

&lt;h3 id=&#34;4-2-2-chart部署&#34;&gt;4.2.2. chart部署&lt;/h3&gt;

&lt;h4 id=&#34;4-2-2-1-添加repo仓库&#34;&gt;4.2.2.1. 添加repo仓库&lt;/h4&gt;

&lt;p&gt;添加 nvidia helm 仓库并更新&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;helm repo add nvidia https://helm.ngc.nvidia.com/nvidia \
    &amp;amp;&amp;amp; helm repo update
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;4-2-2-2-拉取chart包&#34;&gt;4.2.2.2. 拉取chart包&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;helm pull nvidia/gpu-operator --version v24.9.1 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将在本地生成gpu-operator-v24.9.1.tgz文件。&lt;/p&gt;

&lt;h4 id=&#34;4-2-2-3-准备镜像&#34;&gt;4.2.2.3. 准备镜像&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点上执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;由于有些镜像国内不能直接访问，需从国内镜像地址拉取后再修改tag。
1）拉取国内代理镜像：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;crictl pull swr.cn-north-4.myhuaweicloud.com/ddn-k8s/registry.k8s.io/nfd/node-feature-discovery:v0.16.6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2）修改镜像tag：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ctr -n k8s.io image tag  swr.cn-north-4.myhuaweicloud.com/ddn-k8s/registry.k8s.io/nfd/node-feature-discovery:v0.16.6 registry.k8s.io/nfd/node-feature-discovery:v0.16.6 
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;4-2-2-4-安装chart包&#34;&gt;4.2.2.4. 安装chart包&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 添加 nvidia helm 仓库并更新
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia \
    &amp;amp;&amp;amp; helm repo update
# 以默认配置安装
helm install --wait --generate-name \
    -n gpu-operator --create-namespace \
    nvidia/gpu-operator

# 如果提前手动安装了 gpu 驱动，operator 中要禁止 gpu 安装
helm install --wait --generate-name \
     -n gpu-operator --create-namespace \
     nvidia/gpu-operator \
     --set driver.enabled=false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;完成后 会启动 Pod 安装驱动，如果节点上已经安装了驱动了，那么 gpu-operaotr 就不会启动安装驱动的 Pod,通过 label 进行筛选。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;没安装驱动的节点会打上 nvidia.com/gpu.deploy.driver=true ,表示需要安装驱动&lt;/li&gt;
&lt;li&gt;已经手动安装过驱动的节点会打上nvidia.com/gpu.deploy.driver=pre-install,Daemonset 则不会在该节点上运行
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-113127922.png&#34; alt=&#34;picture 1&#34; /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;当然，并不是每个操作系统+内核版本的组合，NVIDIA 都提供了对应的镜像，可以提前在 NVIDIA/driver tags 查看当前 NVIDIA 提供的驱动版本。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;5-测试&#34;&gt;5. 测试&lt;/h1&gt;

&lt;p&gt;部署后，会在gpu-operator namespace 下启动相关 Pod，查看一下 Pod 的运行情况，除了一个 Completed 之外其他应该都是 Running 状态。
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-113933891.png&#34; alt=&#34;picture 2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后进入nvidia-device-plugin-daemonset-xxx Pod，在该 Pod 中可以执行 nvidia-smi命令,比如查看 GPU 信息：
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-115125616.png&#34; alt=&#34;picture 3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;最后再查看 node 信息
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-115436263.png&#34; alt=&#34;picture 4&#34; /&gt;&lt;br /&gt;
可以看出nvidia.com/gpu: &amp;ldquo;1&amp;rdquo;总量为1， 可分配也为1。&lt;/p&gt;

&lt;p&gt;至此，说明我们的 GPU Operator 已经安装成功，K8s 也能感知到节点上的 GPU，接下来就可以在 Pod 中使用 GPU 了。&lt;/p&gt;

&lt;p&gt;创建一个测试 Pod，申请一个 GPU：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  name: cuda-vectoradd
spec:
  restartPolicy: OnFailure
  containers:
  - name: cuda-vectoradd
    image: &amp;quot;nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda10.2&amp;quot;
    resources:
      limits:
        nvidia.com/gpu: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正常的 Pod 日志如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 yaml]# kubectl logs cuda-vectoradd 
[Vector addition of 50000 elements]
Copy input data from the host memory to the CUDA device
CUDA kernel launch with 196 blocks of 256 threads
Copy output data from the CUDA device to the host memory
Test PASSED
Done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此，我们已经可以在 k8s 中使用 GPU 了。&lt;/p&gt;

&lt;h1 id=&#34;6-原理&#34;&gt;6. 原理&lt;/h1&gt;

&lt;p&gt;这部分主要分析一下 Driver Installer 和 NVIDIA Container Toolkit Installer 这两个组件是怎么实现的，大致原理。&lt;/p&gt;

&lt;h2 id=&#34;6-1-driver-installer&#34;&gt;6.1. Driver Installer&lt;/h2&gt;

&lt;p&gt;NVIDIA 官方提供了一种基于容器安装 NVIDIA 驱动的方式，GPU Operator 安装 nvidia 驱动也是采用的这种方式。&lt;/p&gt;

&lt;p&gt;当 NVIDIA 驱动基于容器化安装后，整个架构将演变成图中描述的样子：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-184040318.png&#34; alt=&#34;picture 7&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;6-2-nvidia-container-toolkit-installer&#34;&gt;6.2. NVIDIA Container Toolkit Installer&lt;/h2&gt;

&lt;p&gt;该组件用于安装 NVIDIA Container Toolkit。&lt;/p&gt;

&lt;p&gt;手动安装的时候有两个步骤：&lt;/p&gt;

&lt;p&gt;1）安装 NVIDIA Container Toolkit&lt;/p&gt;

&lt;p&gt;2）修改 Runtime 配置指定使用 nvidia-runtime
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-190232787.png&#34; alt=&#34;picture 9&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在整个调用链中新增 nvidia-container-runtime，以便处理 GPU 相关操作。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-185351436.png&#34; alt=&#34;picture 8&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这个 Installer 做的操作也就是这两步：&lt;/p&gt;

&lt;p&gt;1）将容器中NVIDIA Container Toolkit组件所涉及的命令行工具和库文件移动到/usr/local/nvidia/toolkit目录下
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-211806300.png&#34; alt=&#34;picture 10&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2）在 /usr/local/nvidia/toolkit/.config/nvidia-container-runtime创建nvidia-container-runtime的配置文件config.toml，并设置nvidia-container-cli.root的值为/run/nvidia/driver。
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250123-212557933.png&#34; alt=&#34;picture 11&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;7-小结&#34;&gt;7. 小结&lt;/h1&gt;

&lt;p&gt;小结
本文主要分享如何使用 GPU Operator 自动化完成 GPU Driver、NVIDIA Container Toolkit、device-plugin、exporter 等组件的部署，快速实现在 k8s 环境中使用 GPU。&lt;/p&gt;

&lt;p&gt;最后简单分析了 Driver Installer 和 NVIDIA Container Toolkit Installer 这两个组件的工作原理。&lt;/p&gt;

&lt;p&gt;GPU Operator 极大简化了在 k8s 中使用 GPU 的繁琐过程，但是也存在一些缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Driver Installer 以 DaemonSet 方式运行的，每个节点上运行的 Pod 都一样，但是镜像由 驱动版本+内核版本+操作系统版本拼接而成，因此需要集群中所有节点操作系统一致。&lt;/li&gt;
&lt;li&gt;NVIDIA Container Toolkit Installer 同样是以 DaemonSet 方式运行的，另外安装时需要指定 Runtime，这也造成了集群的节点必须安装相同的 Container Runtime。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;8-参考资料&#34;&gt;8. 参考资料&lt;/h1&gt;

&lt;p&gt;【01】&lt;a href=&#34;https://www.lixueduan.com/posts/ai/02-gpu-operator/#2-%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D&#34;&gt;GPU 环境搭建指南：使用 GPU Operator 加速 Kubernetes GPU 环境搭建&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(08): 在ECS、Docker、K8s 等环境中使用 GPU</title>
            <link>http://mospany.github.io/2024/01/16/gpu-on-k8s/</link>
            <pubDate>Tue, 16 Jan 2024 19:31:10 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2024/01/16/gpu-on-k8s/</guid>
            <description>

&lt;h1 id=&#34;1-引言&#34;&gt;1. 引言&lt;/h1&gt;

&lt;p&gt;本文主要分享在不同环境，例如ECS、Docker 和 Kubernetes 等环境中如何使用 GPU。
&lt;strong&gt;注&lt;/strong&gt;：由于没有物理机裸机，在阿里云上申请ECS也可满足学习使用。&lt;/p&gt;

&lt;h1 id=&#34;2-规划&#34;&gt;2. 规划&lt;/h1&gt;

&lt;p&gt;基于如下环境搭建：
&lt;img src=&#34;post/2024/images/2024-01-17-gpu-operator/IMG_20250117-194645644.png&#34; alt=&#34;picture 0&#34; /&gt;&lt;br /&gt;
查看worker节点GPU信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-worker1 ~]# lspci | grep -i nvidia
00:07.0 VGA compatible controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)
[root@k8s-worker1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明该worker节点有1张NVIDIA Tesla T4的GPU卡。&lt;/p&gt;

&lt;h1 id=&#34;3-概述&#34;&gt;3. 概述&lt;/h1&gt;

&lt;p&gt;仅以比较常见的 NVIDIA GPU 举例，系统为 Linux，对于其他厂家的 GPU 设备理论上流程都是一样的。&lt;/p&gt;

&lt;p&gt;省流：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;对于ECS环境，只需要安装对应的 &lt;a href=&#34;https://help.aliyun.com/zh/egs/user-guide/installation-guideline-for-nvidia-drivers?spm=a2c4g.11186623.help-menu-155040.d_1_5_0.726f718evZrl7t&#34;&gt;GPU Driver&lt;/a&gt;（GPU计算型实例为Tesla驱动，GPU虚拟化型实例为GRID驱动） 以及 CUDA Toolkit 。&lt;/li&gt;
&lt;li&gt;对应 Docker 环境，需要额外安装 nvidia-container-toolkit 并配置 docker 使用 nvidia runtime。&lt;/li&gt;
&lt;li&gt;对应 k8s 环境，需要额外安装对应的 device-plugin 使得 kubelet 能够感知到节点上的 GPU 设备，以便 k8s 能够进行 GPU 管理。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注：一般在 k8s 中使用都会直接使用 gpu-operator 方式进行安装，本文主要为了搞清各个组件的作用，因此进行手动安装。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ps；下一篇分享下如何使用 gpu-operator 快速完成安装&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;4-ecs环境&#34;&gt;4. ECS环境&lt;/h1&gt;

&lt;p&gt;裸机中要使用上 GPU 需要安装以下组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GPU Driver&lt;/li&gt;
&lt;li&gt;CUDA Toolkit
二者的关系如 NVIDIA 官网上的这个图所示：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250120-152256220.png&#34; alt=&#34;picture 0&#34; /&gt;&lt;/p&gt;

&lt;p&gt;GPU Driver 包括了 GPU 驱动和 CUDA 驱动，CUDA Toolkit 则包含了 CUDA Runtime。&lt;/p&gt;

&lt;p&gt;GPU 作为一个 PCIE 设备，只要安装好之后，在系统中就可以通过 lspci 命令查看到，先确认机器上是否有 GPU：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-worker1 ~]# lspci | grep -i nvidia
00:07.0 VGA compatible controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)
[root@k8s-worker1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明该worker节点有1张NVIDIA Tesla T4的GPU卡。&lt;/p&gt;

&lt;h2 id=&#34;4-1-安装grid驱动&#34;&gt;4.1. 安装GRID驱动&lt;/h2&gt;

&lt;p&gt;由于主机规格是ecs.sgn6i-vws-m2.xlarge，是虚拟化型规格，不能安装Tesla驱动的，需要安装grid驱动。
安装Tesla驱动的话将会出现&lt;a href=&#34;#61-error-unable-to-load-the-kernel-module-nvidiako&#34;&gt;ERROR: Unable to load the kernel module &amp;lsquo;nvidia.ko&amp;rsquo;.&lt;/a&gt;错误安装失败。&lt;/p&gt;

&lt;p&gt;安装步骤参考：&lt;a href=&#34;https://help.aliyun.com/zh/egs/user-guide/use-cloud-assistant-to-automatically-install-and-upgrade-grid-drivers?spm=a2c4g.11186623.help-menu-155040.d_1_5_2_1.65bd2ef7tthoW5&#34;&gt;在GPU虚拟化型实例中安装GRID驱动（Linux）&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;4-1-1-准备安装脚本&#34;&gt;4.1.1. 准备安装脚本&lt;/h3&gt;

&lt;p&gt;install-grid.sh内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
if acs-plugin-manager --list --local | grep grid_driver_install &amp;gt; /dev/null 2&amp;gt;&amp;amp;1
then
            acs-plugin-manager --remove --plugin grid_driver_install
fi

acs-plugin-manager --exec --plugin grid_driver_install
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-1-2-安装grid&#34;&gt;4.1.2. 安装grid&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-worker1 ~]# sh install-grid.sh 
RemovePlugin success, plugin[grid_driver_install]
INFO[0000] Starting environment pre-check               
INFO[0000] environment pre-check done                   
INFO[0000] Check gpu device present                     
INFO[0000] Check gpu device present done                
INFO[0000] current installed gird driver is, 470.239.06 
INFO[0000] current installed gird driver is already SWL driver 
[root@k8s-worker1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-1-3-验证&#34;&gt;4.1.3. 验证&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-worker1 ~]# nvidia-smi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行成功，可看到有一张T4-2Q的GPU卡。
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-192347251.png&#34; alt=&#34;picture 6&#34; /&gt;&lt;br /&gt;
至此，我们就安装好 GPU 驱动了，系统也能正常识别到 GPU。&lt;/p&gt;

&lt;p&gt;这里显示的 CUDA 版本表示当前驱动最大支持的 CUDA 版本。&lt;/p&gt;

&lt;h2 id=&#34;4-2-安装cuda&#34;&gt;4.2. 安装CUDA&lt;/h2&gt;

&lt;h3 id=&#34;4-2-1-安装软件&#34;&gt;4.2.1. 安装软件&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://developer.download.nvidia.com/compute/cuda/11.4.0/local_installers/cuda_11.4.0_470.42.01_linux.run
sudo sh cuda_11.4.0_470.42.01_linux.run
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于已安装了GRID驱动，需取消cuda自带的驱动安装：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-200338239.png&#34; alt=&#34;picture 7&#34; /&gt;&lt;br /&gt;
01) 安装全部组件：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-200453794.png&#34; alt=&#34;picture 8&#34; /&gt;&lt;br /&gt;
02) 输出：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-201735441.png&#34; alt=&#34;picture 9&#34; /&gt;&lt;/p&gt;

&lt;p&gt;03) 执行以下命令，重启GPU实例&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;reboot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;04) 依次执行以下命令，配置CUDA环境变量。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo &#39;export PATH=/usr/local/cuda/bin:$PATH&#39; | sudo tee /etc/profile.d/cuda.sh
source /etc/profile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;05) 检查CUDA是否成功安装。&lt;/p&gt;

&lt;p&gt;a) 执行nvcc -V命令，检查CUDA安装版本是否正确。
   &lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-202721754.png&#34; alt=&#34;picture 10&#34; /&gt;&lt;/p&gt;

&lt;p&gt;b) 依次执行以下命令，测试CUDA Samples，验证CUDA是否安装成功。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    [root@k8s-worker1 ~]# cd /usr/local/cuda-11.4/extras/demo_suite/
    [root@k8s-worker1 demo_suite]# ./deviceQuery  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果返回结果显示Result=PASS，则表示CUDA安装成功。
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-203410971.png&#34; alt=&#34;picture 11&#34; /&gt;&lt;/p&gt;

&lt;p&gt;06) 测试
我们使用一个简单的 Pytorch 程序来检测 GPU 和 CUDA 是否正常。&lt;/p&gt;

&lt;p&gt;整个调用链大概是这样的：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-204049273.png&#34; alt=&#34;picture 12&#34; /&gt;&lt;br /&gt;
使用下面代码来测试能够正常使用， check_cuda_pytorch.py 内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch

def check_cuda_with_pytorch():
    &amp;quot;&amp;quot;&amp;quot;检查 PyTorch CUDA 环境是否正常工作&amp;quot;&amp;quot;&amp;quot;
    try:
        print(&amp;quot;检查 PyTorch CUDA 环境:&amp;quot;)
        if torch.cuda.is_available():
            print(f&amp;quot;CUDA 设备可用，当前 CUDA 版本是: {torch.version.cuda}&amp;quot;)
            print(f&amp;quot;PyTorch 版本是: {torch.__version__}&amp;quot;)
            print(f&amp;quot;检测到 {torch.cuda.device_count()} 个 CUDA 设备。&amp;quot;)
            for i in range(torch.cuda.device_count()):
                print(f&amp;quot;设备 {i}: {torch.cuda.get_device_name(i)}&amp;quot;)
                print(f&amp;quot;设备 {i} 的显存总量: {torch.cuda.get_device_properties(i).total_memory / (1024 ** 3):.2f} GB&amp;quot;)
                print(f&amp;quot;设备 {i} 的显存当前使用量: {torch.cuda.memory_allocated(i) / (1024 ** 3):.2f} GB&amp;quot;)
                print(f&amp;quot;设备 {i} 的显存最大使用量: {torch.cuda.memory_reserved(i) / (1024 ** 3):.2f} GB&amp;quot;)
        else:
            print(&amp;quot;CUDA 设备不可用。&amp;quot;)
    except Exception as e:
        print(f&amp;quot;检查 PyTorch CUDA 环境时出现错误: {e}&amp;quot;)

if __name__ == &amp;quot;__main__&amp;quot;:
    check_cuda_with_pytorch()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;先安装下 torch&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install torch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行一下&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python check_cuda_pytorch.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正常输出应该是这样的：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250121-210940440.png&#34; alt=&#34;picture 13&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;4-3-更新cuda&#34;&gt;4.3. 更新CUDA&lt;/h2&gt;

&lt;p&gt;由于有些应用需要更高级的版本的CUDA，需升级，如下升级为12.6版本。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo
sudo dnf clean all
sudo dnf -y install cuda-toolkit-12-6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-203151909.png&#34; alt=&#34;picture 18&#34; /&gt;&lt;br /&gt;
验证:
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-203406637.png&#34; alt=&#34;picture 19&#34; /&gt;&lt;br /&gt;
说明12.6版本安装成功。
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-213858800.png&#34; alt=&#34;picture 20&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;5-docker环境&#34;&gt;5. Docker环境&lt;/h1&gt;

&lt;p&gt;上一步中我们已经在裸机上安装了 GPU Driver，CUDA Toolkit 等工具，实现了在宿主机上使用 GPU。&lt;/p&gt;

&lt;p&gt;现在希望在 Docker 容器中使用 GPU，需要怎么处理呢?&lt;/p&gt;

&lt;p&gt;为了让 Docker 容器中也能使用 GPU，大致步骤如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;安装docker，已有则跳过这步骤。&lt;/li&gt;
&lt;li&gt;安装 nvidia-container-toolkit 组件&lt;/li&gt;
&lt;li&gt;docker 配置使用 nvidia-runtime&lt;/li&gt;
&lt;li&gt;启动容器时增加 &amp;ndash;gpu 参数&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;5-1-docker安装&#34;&gt;5.1. Docker安装&lt;/h2&gt;

&lt;h3 id=&#34;5-1-1-安装必要的一些系统工具&#34;&gt;5.1.1. 安装必要的一些系统工具&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo yum install -y yum-utils
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-1-2-添加软件源信息&#34;&gt;5.1.2. 添加软件源信息&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-1-3-安装docker&#34;&gt;5.1.3. 安装Docker&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-1-4-开启docker服务&#34;&gt;5.1.4. 开启Docker服务&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo service docker start
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-1-5-验证&#34;&gt;5.1.5. 验证&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-worker1 pytorch]# docker version
Client: Docker Engine - Community
 Version:           26.1.3
 API version:       1.45
 Go version:        go1.21.10
 Git commit:        b72abbb
 Built:             Thu May 16 08:34:39 2024
 OS/Arch:           linux/amd64
 Context:           default

Server: Docker Engine - Community
 Engine:
  Version:          26.1.3
  API version:      1.45 (minimum version 1.24)
  Go version:       go1.21.10
  Git commit:       8e96db1
  Built:            Thu May 16 08:33:34 2024
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.6.32
  GitCommit:        8b3b7ca2e5ce38e8f31a34f35b2b68ceb8470d89
 runc:
  Version:          1.1.12
  GitCommit:        v1.1.12-0-g51d5e94
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0
[root@k8s-worker1 pytorch]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;5-2-安装-nvidia-container-toolkit&#34;&gt;5.2. 安装 nvidia-container-toolkit&lt;/h2&gt;

&lt;p&gt;NVIDIA Container Toolkit 的主要作用是将 NVIDIA GPU 设备挂载到容器中。
&amp;gt;兼容生态系统中的任意容器运行时，docker、containerd、cri-o 等。&lt;/p&gt;

&lt;p&gt;NVIDIA 官方安装文档：&lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html&#34;&gt;nvidia-container-toolkit-install-guide&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ALIOS或centos安装命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Configure the production repository:
curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | \
sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo

# Optionally, configure the repository to use experimental packages:
sudo yum-config-manager --enable nvidia-container-toolkit-experimental

#Install the NVIDIA Container Toolkit packages:
sudo yum install -y nvidia-container-toolkit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装成功如下：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-185247937.png&#34; alt=&#34;picture 14&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;5-3-配置使用该-runtime&#34;&gt;5.3. 配置使用该 runtime&lt;/h2&gt;

&lt;p&gt;支持 Docker, Containerd, CRI-O, Podman 等 CRI。
&amp;gt;具体见官方文档 &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuration&#34;&gt;container-toolkit#install-guide&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;这里以 Docker 为例进行配置：
旧版本需要手动在 /etc/docker/daemon.json 中增加配置，指定使用 nvidia 的 runtime。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;    &amp;quot;runtimes&amp;quot;: {
        &amp;quot;nvidia&amp;quot;: {
            &amp;quot;args&amp;quot;: [],
            &amp;quot;path&amp;quot;: &amp;quot;nvidia-container-runtime&amp;quot;
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;新版 toolkit 带了一个nvidia-ctk 工具，执行以下命令即可一键配置然后重启docker：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-191008933.png&#34; alt=&#34;picture 15&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;5-4-验证&#34;&gt;5.4. 验证&lt;/h2&gt;

&lt;p&gt;安装nvidia-container-toolkit 后，整个调用链如下：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-191407685.png&#34; alt=&#34;picture 16&#34; /&gt;&lt;/p&gt;

&lt;p&gt;调用链从 containerd –&amp;gt; runC 变成 containerd –&amp;gt; nvidia-container-runtime –&amp;gt; runC 。&lt;/p&gt;

&lt;p&gt;然后 nvidia-container-runtime 在中间拦截了容器 spec，就可以把 gpu 相关配置添加进去，再传给 runC 的 spec 里面就包含 gpu 信息了。&lt;/p&gt;

&lt;p&gt;Docker 环境中的 CUDA 调用大概是这样的：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-191542185.png&#34; alt=&#34;picture 17&#34; /&gt;&lt;/p&gt;

&lt;p&gt;从图中可以看到，CUDA Toolkit 跑到容器里了，因此宿主机上不需要再安装 CUDA Toolkit。&lt;/p&gt;

&lt;p&gt;使用一个带 CUDA Toolkit 的镜像即可。&lt;/p&gt;

&lt;p&gt;最后我们启动一个 Docker 容器进行测试，其中命令中增加 &amp;ndash;gpu参数来指定要分配给容器的 GPU。&lt;/p&gt;

&lt;p&gt;gpu 参数可选值：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;gpus all：表示将所有 GPU 都分配给该容器&lt;/li&gt;
&lt;li&gt;gpus &amp;ldquo;device=&lt;id&gt;[,&lt;id&gt;&amp;hellip;]&amp;ldquo;：对于多 GPU 场景，可以通过 id 指定分配给容器的 GPU，例如 –gpu “device=0” 表示只分配 0 号 GPU 给该容器
GPU 编号则是通过nvidia-smi 命令进行查看&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里我们直接使用一个带 cuda 的镜像来测试，启动该容器并执行nvidia-smi 命令&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-worker1 ~]# docker run --rm --gpus all  nvcr.io/nvidia/cuda:11.0.3-runtime-ubuntu20.04 nvidia-smi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-214303643.png&#34; alt=&#34;picture 22&#34; /&gt;&lt;br /&gt;
正常情况下应该是可以打印出容器中的 GPU 信息的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;： 镜像需与grid驱动版本兼容，否则报错。&lt;/p&gt;

&lt;h1 id=&#34;6-k8s环境&#34;&gt;6. K8S环境&lt;/h1&gt;

&lt;p&gt;更进一步，在 k8s 环境中使用 GPU，则需要在集群中部署以下组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;gpu-device-plugin 用于管理 GPU，device-plugin 以 DaemonSet 方式运行到集群各个节点，以感知节点上的 GPU 设备，从而让 k8s 能够对节点上的 GPU 设备进行管理。&lt;/li&gt;
&lt;li&gt;gpu-exporter：用于监控 GPU&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;各组件关系如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-215913188.png&#34; alt=&#34;picture 23&#34; /&gt;&lt;/p&gt;

&lt;p&gt;左图为手动安装的场景，只需要在集群中安装 device-plugin 和 监控即可使用。&lt;/p&gt;

&lt;p&gt;右图为使用 gpu-operotar 安装场景，本篇暂时忽略&lt;/p&gt;

&lt;p&gt;大致工作流程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每个节点的 kubelet 组件维护该节点的 GPU 设备状态（哪些已用，哪些未用）并定时报告给调度器，调度器知道每一个节点有多少张 GPU 卡可用。&lt;/li&gt;
&lt;li&gt;调度器为 pod 选择节点时，从符合条件的节点中选择一个节点。&lt;/li&gt;
&lt;li&gt;当 pod 调度到节点上后，kubelet 组件为 pod 分配 GPU 设备 ID，并将这些 ID 作为参数传递给 NVIDIA Device Plugin&lt;/li&gt;
&lt;li&gt;NVIDIA Device Plugin 将分配给该 pod 的容器的 GPU 设备 ID 写入到容器的环境变量 NVIDIA_VISIBLE_DEVICES中，然后将信息返回给 kubelet。&lt;/li&gt;
&lt;li&gt;kubelet 启动容器。&lt;/li&gt;
&lt;li&gt;NVIDIA Container Toolkit 检测容器的 spec 中存在环境变量 NVIDIA_VISIBLE_DEVICES，然后根据环境变量的值将 GPU 设备挂载到容器中。
在 Docker 环境我们在启动容器时通过 &amp;ndash;gpu 参数手动指定分配给容器的 GPU，k8s 环境则由 device-plugin 自行管理。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;6-1-集群环境&#34;&gt;6.1. 集群环境&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-220923122.png&#34; alt=&#34;picture 24&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;安装-device-plugin&#34;&gt;安装 device-plugin&lt;/h2&gt;

&lt;p&gt;device-plugin 一般由对应的 GPU 厂家提供，比如 NVIDIA 的 &lt;a href=&#34;https://github.com/NVIDIA/k8s-device-plugin&#34;&gt;k8s-device-plugin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;安装其实很简单，将对应的 yaml apply 到集群即可。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.15.0/deployments/static/nvidia-device-plugin.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就像这样
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-221359266.png&#34; alt=&#34;picture 25&#34; /&gt;&lt;/p&gt;

&lt;p&gt;device-plugin 启动之后，会感知节点上的 GPU 设备并上报给 kubelet，最终由 kubelet 提交到 kube-apiserver。&lt;/p&gt;

&lt;p&gt;因此我们可以在 Node 可分配资源中看到 GPU，就像这样：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250122-221933864.png&#34; alt=&#34;picture 26&#34; /&gt;&lt;br /&gt;
可以看到，除了常见的 cpu、memory 之外，还有nvidia.com/gpu, 这个就是 GPU 资源，数量为 0，应该为1，错误原因待查。
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250123-095336709.png&#34; alt=&#34;picture 27&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;安装-gpu-监控&#34;&gt;安装 GPU 监控&lt;/h2&gt;

&lt;p&gt;除此之外，如果你需要监控集群 GPU 资源使用情况，你可能还需要安装 DCCM exporter 结合 Prometheus 输出 GPU 资源监控信息。&lt;/p&gt;

&lt;p&gt;安装略。&lt;/p&gt;

&lt;h1 id=&#34;小结&#34;&gt;小结&lt;/h1&gt;

&lt;p&gt;本文主要分享了在ECS、Docker 环境、k8s 环境中如何使用 GPU。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;对于ECS环境，只需要安装对应的 GPU Driver 即可。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对应 Docker 环境，需要额外安装 nvidia-container-toolkit 并配置 docker 使用 nvidia runtime。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对应 k8s 环境，需要额外安装对应的 device-plugin 使得 kubelet 能够感知到节点上的 GPU 设备，以便 k8s 能够进行 GPU 管理。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;现在一般都是在 k8s 环境中使用，为了简化安装步骤， NVIDIA 也提供了 gpu-operator来简化安装部署，后续分享一下如何使用 gpu-operator来快速安装。&lt;/p&gt;

&lt;h1 id=&#34;7-参考资料&#34;&gt;7. 参考资料&lt;/h1&gt;

&lt;p&gt;【01】&lt;a href=&#34;https://blog.csdn.net/Doudou_Mylove/article/details/114388633&#34;&gt;阿里云GPU服务器安装驱动（完整版）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;【02】&lt;a href=&#34;https://help.aliyun.com/zh/egs/user-guide/install-a-gpu-driver-on-a-gpu-accelerated-compute-optimized-linux-instance?spm=a2c4g.11186623.help-menu-155040.d_1_5_1_1.6b70fb7cljEZnN&amp;amp;scm=20140722.H_163824._.OR_help-T_cn~zh-V_1&#34;&gt;在GPU计算型实例中手动安装Tesla驱动（Linux）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;【03】&lt;a href=&#34;https://help.aliyun.com/zh/egs/user-guide/use-cloud-assistant-to-automatically-install-and-upgrade-grid-drivers?spm=a2c4g.11186623.help-menu-155040.d_1_5_2_1.660551bd3LBP63&#34;&gt;在GPU虚拟化型实例中安装GRID驱动（Linux）&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;8-faq&#34;&gt;8. FAQ&lt;/h1&gt;

&lt;h2 id=&#34;8-1-error-unable-to-load-the-kernel-module-nvidia-ko&#34;&gt;8.1. ERROR: Unable to load the kernel module &amp;lsquo;nvidia.ko&amp;rsquo;.&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250120-155358481.png&#34; alt=&#34;picture 4&#34; /&gt;&lt;br /&gt;
错误详情提示：
&lt;img src=&#34;post/2024/images/2024-01-16-gpu-on-k8s/IMG_20250120-155501141.png&#34; alt=&#34;picture 5&#34; /&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(07): kubeadm安装k8s集群(containerd版)</title>
            <link>http://mospany.github.io/2024/01/15/kubeadm-install-k8s/</link>
            <pubDate>Mon, 15 Jan 2024 15:42:11 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2024/01/15/kubeadm-install-k8s/</guid>
            <description>

&lt;h1 id=&#34;1-规划&#34;&gt;1. 规划&lt;/h1&gt;

&lt;p&gt;使用 kubeadm 安装 Kubernetes 集群并使用 containerd 作为容器运行时（container runtime）是一种常见的安装方法。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;OS&lt;/th&gt;
&lt;th&gt;配置&lt;/th&gt;
&lt;th&gt;用途&lt;/th&gt;
&lt;th&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;aliOS(172.17.197.69)&lt;/td&gt;
&lt;td&gt;2核(vCPU) 4GiB 5 Mbps&lt;/td&gt;
&lt;td&gt;k8s-master1&lt;/td&gt;
&lt;td&gt;乌兰察布 可用区 C&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;aliOS(172.17.197.68)&lt;/td&gt;
&lt;td&gt;4核(vCPU) 10 GiB 5 Mbps GPU：NVIDIA T4/8&lt;/td&gt;
&lt;td&gt;k8s-worker1&lt;/td&gt;
&lt;td&gt;乌兰察布 可用区 C&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：这是演示 k8s 集群安装的实验环境，配置较低，生产环境中我们的服务器配置至少都是 8C/16G 的基础配置。&lt;/p&gt;

&lt;h1 id=&#34;2-版本选择&#34;&gt;2. 版本选择&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Alibaba Cloud Linux 3.2104 LTS 64位
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2-1-配置主机名&#34;&gt;2.1. 配置主机名&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# hostnamectl set-hostname k8s-master1
[root@k8s-worker1 ~]# hostnamectl set-hostname k8s-worker1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-2-关闭防火墙&#34;&gt;2.2. 关闭防火墙&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; # 关闭firewalld
2  [root@k8s-master1 ~]# systemctl stop firewalld
3  
4  # 关闭selinux
5  [root@k8s-master1 ~]# sed -i &#39;s/enforcing/disabled/&#39; /etc/selinux/config
6  [root@k8s-master1 ~]# setenforce 0
7   setenforce: SELinux is disabled
8  [root@k8s-master1 ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-3-互做本地解析&#34;&gt;2.3. 互做本地解析&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master1 ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
8.130.117.184 k8s-master1
8.130.92.114  k8s-worker1
[root@k8s-master1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-4-ssh-免密通信-可选&#34;&gt;2.4. SSH 免密通信（可选）&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;master节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master1 ~]# ssh-keygen
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250115-192630256.png&#34; alt=&#34;picture 0&#34; /&gt;&lt;br /&gt;
互发公钥&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# ssh-copy-id root@k8s-worker1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250115-193004163.png&#34; alt=&#34;picture 1&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-5-加载-br-netfilter-模块&#34;&gt;2.5. 加载 br_netfilter 模块&lt;/h2&gt;

&lt;p&gt;确保 br_netfilter 模块被加载
&amp;gt; 所有节点执行&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 加载模块
root@k8s-master1 ~]# modprobe br_netfilter
## 查看加载请看
[root@k8s-master1 ~]# lsmod | grep br_netfilter
br_netfilter           32768  0
bridge                270336  1 br_netfilter

# 永久生效
[root@k8s-master1 ~]# cat &amp;lt;&amp;lt;EOF | tee /etc/modules-load.d/k8s.conf
&amp;gt; br_netfilter
&amp;gt; EOF
br_netfilter
[root@k8s-master1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-6-允许-iptables-检查桥接流量&#34;&gt;2.6. 允许 iptables 检查桥接流量&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
&amp;gt; net.bridge.bridge-nf-call-ip6tables = 1
&amp;gt; net.bridge.bridge-nf-call-iptables = 1
&amp;gt; EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
[root@k8s-master1 ~]# sysctl --system
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-7-关闭-swap&#34;&gt;2.7. 关闭 swap&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 临时关闭
[root@k8s-master1 ~]# swapoff -a

# 永久关闭
[root@k8s-master1 ~]# sed -ri &#39;s/.*swap.*/#&amp;amp;/&#39; /etc/fstab
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-8-安装ipset及ipvsadm&#34;&gt;2.8. 安装ipset及ipvsadm&lt;/h2&gt;

&lt;p&gt;安装ipset及ipvsadm&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# dnf install -y ipset ipvsadm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置ipvsadm模块加载方式，添加需要加载的模块&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat &amp;gt; /etc/sysconfig/modules/ipvs.modules &amp;lt;&amp;lt;EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
EOF

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;授权、运行、检查是否加载&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; bash /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; lsmod | grep -e ip_vs -e nf_conntrack
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-135626938.png&#34; alt=&#34;picture 2&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;3-安装containerd&#34;&gt;3. 安装containerd&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;3-1-安装-containerd&#34;&gt;3.1. 安装 containerd&lt;/h2&gt;

&lt;p&gt;通过 yum 安装 containerd：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master1 ~]# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
Adding repo from: http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
[root@k8s-master1 ~]# yum install containerd -y

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-2-配置-containerd&#34;&gt;3.2. 配置 containerd&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ /usr/bin/containerd config default &amp;gt; /etc/containerd/config.toml
$ sed -i &#39;s/SystemdCgroup = false/SystemdCgroup = true/g&#39; /etc/containerd/config.toml
$ sed -i &#39;s/sandbox_image = &amp;quot;registry.k8s.io\/pause:3.6&amp;quot;/sandbox_image = &amp;quot;registry.aliyuncs.com\/google_containers\/pause:3.9&amp;quot;/g&#39; /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;： 记得确保配置修改成功，尤其是pause版本。&lt;/p&gt;

&lt;h2 id=&#34;3-3-启动containerd&#34;&gt;3.3. 启动containerd&lt;/h2&gt;

&lt;p&gt;启动并使 containerd 随系统启动：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master1 ~]# sudo systemctl start containerd
root@k8s-master1 ~]# sudo systemctl enable containerd
root@k8s-master1 ~]# ps -ef | grep containerd
root       37760       1  0 07:35 ?        00:00:00 /usr/bin/containerd
root       37807   36202  0 07:35 pts/0    00:00:00 grep --color=auto containerd
[root@k8s-master1 ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明containerd已启动成功。
&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-160630369.png&#34; alt=&#34;picture 3&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;4-安装-kubeadm-kubelet&#34;&gt;4. 安装 kubeadm、kubelet&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;4-1-添加-k8s-镜像源&#34;&gt;4.1. 添加 k8s 镜像源&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;地址： &lt;a href=&#34;https://developer.aliyun.com/mirror/kubernetes?spm=a2c6h.13651102.0.0.1cd01b116JYQIn&#34;&gt;https://developer.aliyun.com/mirror/kubernetes?spm=a2c6h.13651102.0.0.1cd01b116JYQIn&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat &amp;lt;&amp;lt;EOF | tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.28/rpm/
enabled=1
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.28/rpm/repodata/repomd.xml.key
EOF
setenforce 0
yum install -y --nogpgcheck kubelet kubeadm kubectl
systemctl enable kubelet &amp;amp;&amp;amp; systemctl start kubelet
echo 1 &amp;gt; /proc/sys/net/ipv4/ip_forward
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;4-2-建立-k8s-yum-缓存&#34;&gt;4.2. 建立 k8s YUM 缓存&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# yum makecache
alinux3-os                                                                                       1.3 MB/s | 3.8 kB     00:00    
alinux3-updates                                                                                  1.4 MB/s | 4.1 kB     00:00    
alinux3-module                                                                                   354 kB/s | 4.2 kB     00:00    
alinux3-plus                                                                                     645 kB/s | 3.0 kB     00:00    
alinux3-powertools                                                                               598 kB/s | 3.0 kB     00:00    
Extra Packages for Enterprise Linux 8 - x86_64                                                   1.8 MB/s | 4.4 kB     00:00    
Extra Packages for Enterprise Linux Modular 8 - x86_64                                           620 kB/s | 3.0 kB     00:00    
Kubernetes                                                                                        17 kB/s | 1.7 kB     00:00    
Metadata cache created.
[root@k8s-master1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;4-3-安装-k8s-相关工具&#34;&gt;4.3. 安装 k8s 相关工具&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# yum list kubelet --showduplicates
Last metadata expiration check: 0:01:46 ago on Wed 15 Jan 2025 10:55:44 PM CST.
Installed Packages
kubelet.x86_64                                           1.28.15-150500.1.1                                           @kubernetes
Available Packages
kubelet.aarch64                                          1.28.0-150500.1.1                                            kubernetes 
kubelet.ppc64le                                          1.28.0-150500.1.1                                            kubernetes 
kubelet.s390x                                            1.28.0-150500.1.1                                            kubernetes 
kubelet.src                                              1.28.0-150500.1.1                                            kubernetes 
kubelet.x86_64                                           1.28.0-150500.1.1                                            kubernetes 
kubelet.aarch64                                          1.28.1-150500.1.1                                            kubernetes 
kubelet.ppc64le                                          1.28.1-150500.1.1                                            kubernetes 
kubelet.s390x                                            1.28.1-150500.1.1                                            kubernetes 
kubelet.src                                              1.28.1-150500.1.1                                            kubernetes 
.....
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置crictl连接 containerd&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ crictl config --set runtime-endpoint=unix:///run/containerd/containerd.sock
$ crictl images ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置kubectl命令自动补全：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo &#39;source &amp;lt;(kubectl completion bash)&#39; &amp;gt;&amp;gt; ~/.bashrc
source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;5-master节点&#34;&gt;5. master节点&lt;/h1&gt;

&lt;h2 id=&#34;5-1-k8s-初始化&#34;&gt;5.1. k8s 初始化&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：记得修改&amp;ndash;apiserver-advertise-address与&amp;ndash;kubernetes-version修改正确。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master ~]# kubeadm init \
  --apiserver-advertise-address=172.17.197.69 \
  --image-repository registry.aliyuncs.com/google_containers \
  --kubernetes-version v1.28.15 \
  --service-cidr=10.96.0.0/12 \
  --pod-network-cidr=10.244.0.0/16 \
  --ignore-preflight-errors=all \
  --cri-socket /var/run/containerd/containerd.sock
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数说明：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;   --apiserver-advertise-address  # 集群master地址
   --image-repository             # 指定k8s镜像仓库地址
   --kubernetes-version           # 指定K8s版本（与kubeadm、kubelet版本保持一致）
   --service-cidr                 # Pod统一访问入口
   --pod-network-cidr             # Pod网络（与CNI网络保持一致）
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初始化后输出如下内容，说明成功。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.....
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.17.197.69:6443 --token ipqfom.r9ltq4t3in53cd6w \
        --discovery-token-ca-cert-hash sha256:0f01242802eae4b69408c34070a3ad8d017229faba9f8b30623ed1e9dd69d66c 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;5-2-根据输出提示创建相关文件&#34;&gt;5.2. 根据输出提示创建相关文件&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;master节点上执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master ~]# mkdir -p $HOME/.kube
[root@k8s-master ~]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[root@k8s-master ~]# chown $(id -u):$(id -g) $HOME/.kube/config
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;5-3-查看-k8s-运行的容器&#34;&gt;5.3. 查看 k8s 运行的容器&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;master节点上执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 ~]# kubectl  get pod -A -o wide
NAMESPACE     NAME                                  READY   STATUS    RESTARTS   AGE   IP              NODE          NOMINATED NODE   READINESS GATES
kube-system   coredns-66f779496c-lq5rj              0/1     Pending   0          26m   &amp;lt;none&amp;gt;          &amp;lt;none&amp;gt;        &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-66f779496c-q5pxk              0/1     Pending   0          26m   &amp;lt;none&amp;gt;          &amp;lt;none&amp;gt;        &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   etcd-k8s-master1                      1/1     Running   1          26m   172.17.197.69   k8s-master1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-apiserver-k8s-master1            1/1     Running   1          26m   172.17.197.69   k8s-master1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-controller-manager-k8s-master1   1/1     Running   1          26m   172.17.197.69   k8s-master1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-88trd                      1/1     Running   0          26m   172.17.197.69   k8s-master1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-scheduler-k8s-master1            1/1     Running   1          26m   172.17.197.69   k8s-master1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
[root@k8s-master1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;5-4-查看-k8s-节点&#34;&gt;5.4. 查看 k8s 节点&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;master节点上执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master1 ~]# kubectl  get node -o wide
NAME          STATUS     ROLES           AGE   VERSION    INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                                              KERNEL-VERSION           CONTAINER-RUNTIME
k8s-master1   NotReady   control-plane   36m   v1.28.15   172.17.197.69   &amp;lt;none&amp;gt;        Alibaba Cloud Linux 3.2104 U11 (OpenAnolis Edition)   5.10.134-18.al8.x86_64   containerd://1.6.32
[root@k8s-master1 ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可看到当前只有 k8s-master 节点，而且状态是 NotReady（未就绪），因为我们还没有部署网络插件（kubectl apply -f [podnetwork].yaml），于是接着部署容器网络（CNI）。&lt;/p&gt;

&lt;h2 id=&#34;5-5-容器网络-cni-部署&#34;&gt;5.5. 容器网络（CNI）部署&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;插件地址：&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/addons/&#34;&gt;https://kubernetes.io/docs/concepts/cluster-administration/addons/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;该地址在 k8s-master 初始化成功时打印出来。&lt;/p&gt;

&lt;h3 id=&#34;5-5-1-选择一个主流的容器网络插件部署-calico&#34;&gt;5.5.1. 选择一个主流的容器网络插件部署（Calico）&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-164005909.png&#34; alt=&#34;picture 4&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;5-5-2-下载yml文件&#34;&gt;5.5.2. 下载yml文件&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://docs.projectcalico.org/manifests/calico.yaml --no-check-certificate 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-5-3-看看该yaml文件所需要启动的容器&#34;&gt;5.5.3. 看看该yaml文件所需要启动的容器&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 install-k8s-cluster]# cat calico.yaml |grep image
          image: docker.io/calico/cni:v3.25.0
          imagePullPolicy: IfNotPresent
          image: docker.io/calico/cni:v3.25.0
          imagePullPolicy: IfNotPresent
          image: docker.io/calico/node:v3.25.0
          imagePullPolicy: IfNotPresent
          image: docker.io/calico/node:v3.25.0
          imagePullPolicy: IfNotPresent
          image: docker.io/calico/kube-controllers:v3.25.0
          imagePullPolicy: IfNotPresent
[root@k8s-master1 install-k8s-cluster]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-5-4-修改为国内可下载镜像&#34;&gt;5.5.4. 修改为国内可下载镜像&lt;/h3&gt;

&lt;p&gt;使用docker.m.daocloud.io代替docker.io&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sed -i s/docker.io/docker.m.daocloud.io/g calico.yaml 
$ cat calico.yaml | grep image

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-205457826.png&#34; alt=&#34;picture 5&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;5-5-5-安装calico&#34;&gt;5.5.5. 安装calico&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 calicodir]# kubectl  apply -f calico.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-205731350.png&#34; alt=&#34;picture 6&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;5-5-6-查看pod和node状态正常&#34;&gt;5.5.6. 查看pod和node状态正常&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 calicodir]# kubectl  get pod -A
[root@k8s-master1 calicodir]# kubectl  get node
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-210017047.png&#34; alt=&#34;picture 7&#34; /&gt;&lt;br /&gt;
可以看出pod已全部Running， node已变成Ready状态，说明master节点已安装成功完成。&lt;/p&gt;

&lt;h1 id=&#34;6-worker-节点&#34;&gt;6. worker 节点&lt;/h1&gt;

&lt;h2 id=&#34;6-1-worker-节点加入-k8s-集群&#34;&gt;6.1. worker 节点加入 k8s 集群&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有 work 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;复制k8s-master初始化屏幕输出的语句并在work节点执行&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-worker1 ~]# kubeadm join 172.17.197.69:6443 --token ipqfom.r9ltq4t3in53cd6w         --discovery-token-ca-cert-hash sha256:0f01242802eae4b69408c34070a3ad8d017229faba9f8b30623ed1e9dd69d66c
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with &#39;kubectl -n kube-system get cm kubeadm-config -o yaml&#39;
[kubelet-start] Writing kubelet configuration to file &amp;quot;/var/lib/kubelet/config.yaml&amp;quot;
[kubelet-start] Writing kubelet environment file with flags to file &amp;quot;/var/lib/kubelet/kubeadm-flags.env&amp;quot;
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run &#39;kubectl get nodes&#39; on the control-plane to see this node join the cluster.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;6-2-查询集群信息&#34;&gt;6.2. 查询集群信息&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master节点上执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 calicodir]# kubectl  get pod -A -o wide
[root@k8s-master1 calicodir]# kubectl  get node -o wide
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-213420388.png&#34; alt=&#34;picture 8&#34; /&gt;&lt;/p&gt;

&lt;p&gt;查看crictl和ctr是否安装成功。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 calicodir]# crictl version
[root@k8s-master1 calicodir]# ctr version
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-213647229.png&#34; alt=&#34;picture 9&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;7-验证&#34;&gt;7. 验证&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;Master节点上执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;k8s 集群部署 nginx 服务，并通过浏览器进行访问验证。&lt;/p&gt;

&lt;h2 id=&#34;7-1-创建pod&#34;&gt;7.1. 创建Pod&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 calicodir]# kubectl create deployment nginx --image=docker.m.daocloud.io/library/nginx:latest
deployment.apps/nginx created
[root@k8s-master1 calicodir]# kubectl expose deployment nginx --port=80 --type=NodePort
service/nginx exposed
[root@k8s-master1 calicodir]# kubectl  get all
NAME                        READY   STATUS    RESTARTS   AGE
pod/nginx-fbf584587-pp6ks   1/1     Running   0          2m45s

NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/kubernetes   ClusterIP   10.96.0.1       &amp;lt;none&amp;gt;        443/TCP        5h45m
service/nginx        NodePort    10.110.34.104   &amp;lt;none&amp;gt;        80:30884/TCP   28s

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx   1/1     1            1           2m45s

NAME                              DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-fbf584587   1         1         1       2m45s
[root@k8s-master1 calicodir]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;7-2-访问nginx&#34;&gt;7.2. 访问nginx&lt;/h2&gt;

&lt;h3 id=&#34;7-2-1-curl访问&#34;&gt;7.2.1. curl访问&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[root@k8s-master1 calicodir]# curl 10.110.34.104:80
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;For online documentation and support please refer to
&amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
Commercial support is available at
&amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
[root@k8s-master1 calicodir]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用nodePort方式访问现象同上
&lt;img src=&#34;post/2024/images/2024-01-15-kubeadm-install-k8s/IMG_20250116-220403508.png&#34; alt=&#34;picture 10&#34; /&gt;&lt;/p&gt;

&lt;p&gt;至此：kubeadm方式的k8s集群已经部署完成。&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>如何安装使用newbing和chatgpt</title>
            <link>http://mospany.github.io/2023/06/23/how-about-install-newbing-and-chatgpt/</link>
            <pubDate>Fri, 23 Jun 2023 17:36:37 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2023/06/23/how-about-install-newbing-and-chatgpt/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;org5c16ff5&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;chatgpt在2022年开始爆发，随着越来越多的人在使用，于是也打算尝试安装使用。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org53977af&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;chatgpt&#34;&gt;chatgpt&lt;/h2&gt;

&lt;p&gt;ChatGPT是一种基于人工智能技术的聊天机器人，它可以与用户进行自然语言交互，回答用户的问题，提供有用的信息和建议，甚至玩一些有趣的游戏。ChatGPT是由OpenAI开发，使用了GPT（Generative Pre-trained Transformer）技术，这是一种具有强大生成能力的神经网络模型。ChatGPT通过学习海量的语言数据集来不断提高其对自然语言的理解能力，从而能够更好地理解和回答用户的问题。与其他聊天机器人相比，ChatGPT的对话流畅度和回答准确度都很高，因此备受用户欢迎。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org292f1ea&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;newbing&#34;&gt;newbing&lt;/h2&gt;

&lt;p&gt;必应chat是微软旗下的聊天机器人服务，它是基于必应AI开发的一款智能对话系统。必应chat可以理解自然语言，能够回答用户的问题、提供服务、开玩笑等，使得用户可以与机器人进行自然的对话。必应chat的功能包括天气查询、地图导航、音乐播放、闲聊聊天、计算数学问题等，可帮助用户解决生活中的各种疑问和问题。必应chat还支持多种语言，包括英语、中文、西班牙语、法语、德语等。使用必应chat，您可以轻松地与机器人进行互动，获得便捷的服务和娱乐体验。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org79da4ca&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;安装&#34;&gt;安装&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org8197e6e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;科学上网&#34;&gt;科学上网&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;org9014292&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;自由鲸&#34;&gt;自由鲸&lt;/h3&gt;

&lt;p&gt;自由鲸 FreeWhale 是一家长期走中高端路线的 ShadowsocksR(SSR) 机场，也提供部分 V2Ray 线路，已经稳定运行多年。实际对比下来，它的性价比还是相当高的，线路又多，提供的流量也十分充足，主要推荐的套餐充分考虑了当前主流用户能够接受的价位，可以说是无可挑剔。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;特点&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;封闭邀请机制&lt;/p&gt;

&lt;p&gt;自由鲸 FreeWhale 需要邀请码才能注册成功，这样的注册机制一定程度确保了服务的稳定性。&lt;/p&gt;

&lt;p&gt;如果需要邀请码，可以使用我的邀请码:&lt;/p&gt;

&lt;p&gt;也可以直接点击我的邀请链接，进入注册。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;套餐便宜&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/chatgpt/freewhale.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;使用&lt;/p&gt;

&lt;p&gt;登录地址：&lt;a href=&#34;https://www.freewhale.world/auth/register?code=XXXX&#34;&gt;https://www.freewhale.world/auth/register?code=XXXX&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;选的是512G/360 天的套餐124元，相当于每月10元还算便宜。&lt;/p&gt;

&lt;p&gt;购买成功后将出现一堆节点列表，在用户中心里将出现订阅地址用做vpn客户端的链接信息。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a id=&#34;org9d5fbb8&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;clash&#34;&gt;clash&lt;/h3&gt;

&lt;p&gt;Clash是一款开源的多平台代理软件，支持 Windows、macOS、Linux、Android 等平台。Clash可以运行在本地并代理 HTTP、HTTPS、SOCKS5等协议，同时还支持 SS、V2Ray、Trojan 等协议，可以实现对于许多网站和应用的科学上网。Clash 的特点是支持多种协议，使用方便，功能强大，且更新频繁。Clash的开源代码托管在GitHub上，用户可以参与其开发和改进。&lt;/p&gt;

&lt;p&gt;把freeWhale上面的订阅地址粘贴到clash的配置-&amp;gt;托管配置-&amp;gt;管理里， 配置名称写为xinjieCloud。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/chatgpt/clash-face.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;使用科学上网时，查看 ip 地址可能显示在国内，以下是解决办法:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一定挂全局模式 + 支持 OpenAI 的国家（目前中俄等国不支持，推荐北美、日本等国家）；&lt;/li&gt;
&lt;li&gt;一定设置为系统代理 + 手动选择节点(DIRECT), 否则上不了外网。&lt;/li&gt;
&lt;li&gt;上面步骤完成后依然不行的话，就清空浏览器缓存然后重启浏览器或者电脑；或者直接新开一个无痕模式的窗口。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org208d97c&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;pchat&#34;&gt;Pchat&lt;/h2&gt;

&lt;p&gt;Pchat 是一款可以免费、简单易用的聊天机器人，翻墙后可以使用，不用做任何配置。它包括Pchat free版、Pchat Pro高级版本。&lt;br /&gt;
访问地址为：&lt;a href=&#34;https://www.promptboom.com&#34;&gt;https://www.promptboom.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/chatgpt/pchat.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pchat Free，一个由PromptBoom和OpenAI合作开发的AI助手。&lt;/li&gt;
&lt;li&gt;Pchat Pro，是Pchat免费版的高级版本。相比之下，Pchat免费版提供了基本的聊天功能，而Pchat Pro则包括更多的高级功能，例如语音和视频通话，自定义表情符号等等。此外，Pchat Pro还具有更高的智能和更好的性能，因为它采用了最先进的技术和算法。&lt;br /&gt;
试用期为14天。在试用期结束之前，您可以决定是否购买Pchat Pro的许可证。如果您决定购买，您将获得无限制的访问权限，并可以享受Pchat Pro提供的所有功能。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org8b69e3f&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;newbing-1&#34;&gt;newBing&lt;/h2&gt;

&lt;p&gt;微软 New Bing 是微软推出的全新搜索引擎，它使用了最新的人工智能技术和自然语言处理技术来提供更好的搜索结果和更智能的搜索体验。它可以帮助用户更快、更准确地找到他们需要的信息，并且能够根据用户的搜索历史和兴趣爱好提供更加个性化的搜索结果。&lt;/p&gt;

&lt;p&gt;1、下载最新/已支持newbing的edge浏览器&lt;/p&gt;

&lt;p&gt;2、设置bing4为默认搜索引擎&lt;br /&gt;
   URL地址写为：&lt;a href=&#34;https://www4.bing.com/search?q=%s&amp;amp;PC=U316&amp;amp;FROM=CHROMN&#34;&gt;https://www4.bing.com/search?q=%s&amp;amp;PC=U316&amp;amp;FROM=CHROMN&lt;/a&gt;&lt;br /&gt;
   &lt;img src=&#34;http://blog.mospan.cn/post/img/chatgpt/bing4-engine.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;3、设置外网DNS&lt;br /&gt;
   选择openDNS&lt;br /&gt;
   &lt;img src=&#34;http://blog.mospan.cn/post/img/chatgpt/select-dns.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;4、禁止和清理cookies&lt;br /&gt;
   禁止保存一些cn.bing.com或c.bing.com的cookies，免得老跳转到国内bing上。&lt;br /&gt;
   &lt;img src=&#34;http://blog.mospan.cn/post/img/chatgpt/forbiden-cookies.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;5、登录&lt;br /&gt;
   新建标签页，把语言设置为非国内地区&lt;br /&gt;
  &lt;img src=&#34;http://blog.mospan.cn/post/img/chatgpt/newbing-index.png&#34; alt=&#34;img&#34; /&gt;&lt;br /&gt;
   登录账号，如我个人账号为: moshengping210@gmail.com&lt;/p&gt;

&lt;p&gt;点击AI，就可以用bingChat了(不太正常，如跳转到国内bing后需清理cookies后重启浏览器)&lt;br /&gt;
&lt;img src=&#34;http://blog.mospan.cn/post/img/chatgpt/newbing.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org554ec02&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;chatgpt-1&#34;&gt;chatgpt&lt;/h2&gt;

&lt;p&gt;1、申请账号&lt;br /&gt;
   某宝上购买账号&lt;/p&gt;

&lt;p&gt;2、登录官网(需提前科学上网)&lt;br /&gt;
   ChatGPT的官方网址： &lt;a href=&#34;https://chat.openai.com/auth/login&#34;&gt;https://chat.openai.com/auth/login&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3、查询&lt;br /&gt;
  &lt;img src=&#34;http://blog.mospan.cn/post/img/chatgpt/chatgpt.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org415b7ac&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;p&gt;【01】 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/618495330&#34;&gt;当前最新稳定访问NewBing方法（操作简单）&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(06): 动手实现webhook</title>
            <link>http://mospany.github.io/2023/03/05/k8s-webhook-example/</link>
            <pubDate>Sun, 05 Mar 2023 11:32:18 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2023/03/05/k8s-webhook-example/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;orgabac5c2&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;Webhook就是一种HTTP回调，用于在某种情况下执行某些动作，Webhook不是K8S独有的，很多场景下都可以进行Webhook，比如在提交完代码后调用一个Webhook自动构建docker镜像&lt;/p&gt;

&lt;p&gt;K8S中提供了自定义资源类型和自定义控制器来扩展功能，还提供了动态准入控制，其实就是通过Webhook来实现准入控制，分为两种：验证性质的准入 Webhook （Validating Admission Webhook） 和 修改性质的准入 Webhook （Mutating Admission Webhook）&lt;/p&gt;

&lt;p&gt;Admission Webhook有哪些使用场景？如下&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在资源持久化到ETCD之前进行修改（Mutating Webhook），比如增加init Container或者sidecar Container&lt;/li&gt;
&lt;li&gt;在资源持久化到ETCD之前进行校验（Validating Webhook），不满足条件的资源直接拒绝并给出相应信息&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.mospan.cn/post/img/webhook/webhook.webp&#34;&gt;http://blog.mospan.cn/post/img/webhook/webhook.webp&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Webhook可以理解成Java Web开发中的Filter，每个请求都会经过Filter处理，从图中可以看到，先执行的是Mutating Webhook，它可以对资源进行修改，然后执行的是Validating Webhook，它可以拒绝或者接受请求，但是它不能修改请求。&lt;/p&gt;

&lt;p&gt;K8S中有已经实现了的Admission Webhook列表，详情参考每个准入控制器的作用是什么？&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org5a64fe0&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;示例原理&#34;&gt;示例原理&lt;/h1&gt;

&lt;p&gt;我们以一个简单的Webhook作为例子，该Webhook会在创建Deployment资源的时候检查它是否有相应的标签，如果没有的话，则加上（Mutating Webhook），然后在检验它是否有相应的标签（Validating Webhook），有则创建该Deployment，否则拒绝并给出相应错误提示。&lt;/p&gt;

&lt;p&gt;所有代码都在：&lt;br /&gt;
git@github.com:mospany/admission-webhook-example.git&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org02043ed&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;验证&#34;&gt;验证&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org122233e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;编译打包&#34;&gt;编译打包&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;cd admission-webhook-example/v1
DOCKER_USER=mospany bash ./build
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它将生成镜像并上传到docker.io/mospany/admission-webhook-example:v1。&lt;br /&gt;
&lt;img src=&#34;http://blog.mospan.cn/post/img/webhook/build.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org0aa6c11&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;检查是否开启了动态准入控制&#34;&gt;检查是否开启了动态准入控制&lt;/h2&gt;

&lt;p&gt;查看APIServer是否开启了MutatingAdmissionWebhook和ValidatingAdmissionWebhook&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 获取apiserver pod名字
apiserver_pod_name=`kubectl get --no-headers=true po -n kube-system | grep kube-apiserver | awk &#39;{ print $1 }&#39;`
# 查看api server的启动参数plugin
kubectl get po $apiserver_pod_name -n kube-system -o yaml | grep plugin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果输出如下，说明已经开启&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- --enable-admission-plugins=NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;否则，需要修改启动参数，请不然直接修改Pod的参数，这样修改不会成功，请修改配置文件/etc/kubernetes/manifests/kube-apiserver.yaml，加上相应的插件参数后保存，APIServer的Pod会监控该文件的变化，然后重新启动。&lt;br /&gt;
&lt;img src=&#34;http://blog.mospan.cn/post/img/webhook/check-apiserver.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orga56c5e4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;创建rbac&#34;&gt;创建RBAC&lt;/h2&gt;

&lt;p&gt;由于我们的webhook会对资源进行修改，所以需要单独给一个ServiceAccount，在K8S集群中直接创建即可&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f deployment/rbac.yaml 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;效果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deployment]# kubectl apply -f rbac.yaml
serviceaccount/admission-webhook-example-sa unchanged
clusterrole.rbac.authorization.k8s.io/admission-webhook-example-cr unchanged
clusterrolebinding.rbac.authorization.k8s.io/admission-webhook-example-crb unchanged
[root@k8s-master deployment]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org57c61b3&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;证书认证&#34;&gt;证书认证&lt;/h2&gt;

&lt;p&gt;K8S集群默认是HTTPS通信的，所以APiserver调用webhook的过程也是HTTPS的，所以需要进行证书认证，证书认证相当于是给Service的域名进行认证（Service后面会创建），将Service域名放到认证请求server.csr文件中，然后创建一个K8S证书签署请求资源CertificateSigningRequest，APIServer签署该证书后生成server-cert.pem，再将最初创建的私钥server-key.pem和签署好的证书server-cert.pem放到Secret中供Deployment调用，详细过程看脚本&lt;br /&gt;
webhook-create-signed-cert.sh&lt;/p&gt;

&lt;p&gt;认证很简单，执行该脚本即可，会创建一个名为admission-webhook-example-certs的Secret&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./deployment/webhook-create-signed-cert.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这一步顺便把Service创建了，因为证书是给该Service的域名颁发的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f deployment/service.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org9750af4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;部署deployment&#34;&gt;部署Deployment&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;kubectl  apply -f deployment.yaml 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;稍等片刻如果有类似如下输出说明Pod已经运行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deployment]# kubectl  apply -f deployment.yaml
deployment.apps/admission-webhook-example-deployment unchanged
[root@k8s-master deployment]# kubectl  get pod -A -o wide | grep web
default       admission-webhook-example-deployment-7dd75cffb6-24bhg   1/1     Running   2 (13d ago)    57d    10.244.235.217   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
[root@k8s-master deployment]# kubectl  get pod -A  | grep web
default       admission-webhook-example-deployment-7dd75cffb6-24bhg   1/1     Running   2 (13d ago)    57d
[root@k8s-master deployment]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org7111e46&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;部署validatingwebhook&#34;&gt;部署ValidatingWebhook&lt;/h2&gt;

&lt;p&gt;首先包含一个namespaceSelector，表示此webhook只针对有admission-webhook-example标签的namespace生效，当然也可以去掉&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;namespaceSelector:
   matchLabels:
     admission-webhook-example: enabled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看编排文件deployment/validatingwebhook.yaml，里面有一个占位符${CA_BUNDLE}&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;clientConfig:
 service:
 name: admission-webhook-example-svc
 namespace: default
 path: &amp;quot;/validate&amp;quot;  # Path是我们自己定义的
 caBundle: ${CA_BUNDLE}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是什么呢？webhook是APIServer调用的，此时APIServer相当于是一个客服端，webhook是一个服务端，可以对比下平时上网，打开https网站时是谁在验证域名的证书？是内置在浏览器里面的根证书在做验证，所以这里的CA_BUNDLE就类似于APIServer调用webhook的根证书，它去验证webhook证书。&lt;/p&gt;

&lt;p&gt;所以先填充这个CA_BUNDLE后再执行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 填充占位符
cat deployment/validatingwebhook.yaml | ./deployment/webhook-patch-ca-bundle.sh &amp;gt; /tmp/validatingwebhook.yaml

# 部署
kubectl apply -f /tmp/validatingwebhook.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orge4b820e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;验证-1&#34;&gt;验证&lt;/h2&gt;

&lt;p&gt;1、给default namespace添加label&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl label namespace default admission-webhook-example=enabled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、部署sleep.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f deployment/sleep.yaml 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org3ee59df&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;faq&#34;&gt;FAQ&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org896c547&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;error-unable-to-recognize-stdin-no-matches-for-kind-certificatesigningrequest-in-version-certificates-k8s-io-v1beta1&#34;&gt;error: unable to recognize &amp;ldquo;STDIN&amp;rdquo;: no matches for kind &amp;ldquo;CertificateSigningRequest&amp;rdquo; in version &amp;ldquo;certificates.k8s.io/v1beta1&amp;rdquo;&lt;/h2&gt;

&lt;p&gt;当执行`bash webhook-create-signed-cert.sh`时出现如下错误：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;error: unable to recognize &amp;ldquo;STDIN&amp;rdquo;: no matches for kind &amp;ldquo;CertificateSigningRequest&amp;rdquo; in version &amp;ldquo;certificates.k8s.io/v1beta1&amp;rdquo;`&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;需修改webhook-create-signed-cert.sh相关内容为:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# create  server cert/key CSR and  send to k8s API
cat &amp;lt;&amp;lt;EOF | kubectl create -f -
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: ${csrName}
spec:
  groups:
  - system:authenticated
  request: $(cat ${tmpdir}/server.csr | base64 | tr -d &#39;\n&#39;)
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - digital signature
  - key encipherment
  - client auth
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;详见：&lt;a href=&#34;https://bytemeta.vip/repo/morvencao/kube-sidecar-injector/issues/29&#34;&gt;missing required field &amp;ldquo;signerName&amp;rdquo; #29&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orga1e1728&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;no-matches-for-kind-validatingwebhookconfiguration-in-version-admissionregistration-k8s-io-v1beta1&#34;&gt;no matches for kind &amp;ldquo;ValidatingWebhookConfiguration&amp;rdquo; in version &amp;ldquo;admissionregistration.k8s.io/v1beta1&amp;rdquo;&lt;/h2&gt;

&lt;p&gt;当执行`kubectl apply -f validatingwebhook.yaml`时出现错误：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;no matches for kind &amp;ldquo;ValidatingWebhookConfiguration&amp;rdquo; in version &amp;ldquo;admissionregistration.k8s.io/v1beta1&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;需改为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;admissionReviewVersions: [&amp;quot;v1&amp;quot;,&amp;quot;v1beta1&amp;quot;]
sideEffects: None
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;详见： &lt;a href=&#34;https://kodekloud.com/community/t/pls-help-me-in-configuring-using-opa-in-minikbe/28428/5&#34;&gt;Pls help me in configuring/using OPA in minikbe&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgc3c8eba&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;x509-certificate-specifies-an-incompatible-key-usage&#34;&gt;x509: certificate specifies an incompatible key usage&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;orga94e650&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;p&gt;【01】&lt;a href=&#34;https://zhuanlan.zhihu.com/p/404764407&#34;&gt;从0到1开发K8S_Webhook最佳实践&lt;/a&gt;]&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>CentOS7 下安装和配置 NFS</title>
            <link>http://mospany.github.io/2023/01/15/aliyun-ecs-install-nfs/</link>
            <pubDate>Sun, 15 Jan 2023 10:44:29 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2023/01/15/aliyun-ecs-install-nfs/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;org629d3e2&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;NFS（Network File System，网络文件系统）是当前主流异构平台共享文件系统之一。主要应用在UNIX环境下。最早是由Sun Microsystems开发，现在能够支持在不同类型的系统之间通过网络进行文件共享，广泛应用在FreeBSD、SCO、Solaris等异构操作系统平台，允许一个系统在网络上与他人共享目录和文件。通过使用NFS，用户和程序可以像访问本地文件一样访问远端系统上的文件，使得每个计算机的节点能够像使用本地资源一样方便地使用网上资源。换言之，NFS可用于不同类型计算机、操作系统、网络架构和传输协议运行环境中的网络文件远程访问和共享。&lt;/p&gt;

&lt;p&gt;NFS的工作原理是使用客户端/服务器架构，由一个客户端程序和服务器程序组成。服务器程序向其他计算机提供对文件系统的访问，其过程称为输出。NFS客户端程序对共享文件系统进行访问时，把它们从NFS服务器中“输送”出来。文件通常以块为单位进行传输。其大小是8KB（虽然它可能会将操作分成更小尺寸的分片）。NFS传输协议用于服务器和客户机之间文件访问和共享的通信，从而使客户机远程地访问保存在存储设备上的数据。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org043eb1b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;服务端搭建&#34;&gt;服务端搭建&lt;/h1&gt;

&lt;p&gt;由于实现CSI需要一个后端存储，Linux提供NFS功能可以免费搭建一个NSC存储功能用来验证。&lt;br /&gt;
搭建办法详见: &lt;a href=&#34;https://www.codeleading.com/article/35162638950/&#34;&gt;阿里云服务器 CentOS7 下安装和配置 NFS&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org31fb10d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;服务端安装&#34;&gt;服务端安装&lt;/h2&gt;

&lt;p&gt;使用 yum 安装 NFS 安装包。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install nfs-utils -y 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org768e380&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;服务端配置&#34;&gt;服务端配置&lt;/h2&gt;

&lt;p&gt;设置 NFS 服务开机启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# systemctl enable rpcbind
[root@k8s-master ~]# systemctl enable nfs
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org79e8e0b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;配置共享目录&#34;&gt;配置共享目录&lt;/h2&gt;

&lt;p&gt;服务启动之后，我们在服务端配置一个共享目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# mkdir /data
[root@k8s-master ~]# chmod 755 /data
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据这个目录，相应配置导出目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim /etc/exports
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加如下配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/data/     *(rw,sync,no_root_squash,no_all_squash)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;data: 共享目录位置。&lt;br /&gt;
192.168.0.0/24: 客户端 IP 范围，* 代表所有，即没有限制（我在实验中会设置为*）。&lt;br /&gt;
rw: 权限设置，可读可写。&lt;br /&gt;
sync: 同步共享目录。&lt;br /&gt;
no_root_squash: 可以使用 root 授权。&lt;br /&gt;
no_all_squash: 可以使用普通用户授权。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orge7422da&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;启动-nfs-服务&#34;&gt;启动 NFS 服务&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# systemctl start rpcbind
[root@k8s-master ~]# systemctl start nfs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org2db5bbe&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;防火墙需要打开-rpc-bind-和-nfs-的服务&#34;&gt;防火墙需要打开 rpc-bind 和 nfs 的服务&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# firewall-cmd --zone=public --permanent --add-service={rpc-bind,mountd,nfs}
FirewallD is not running
[root@k8s-master ~]# firewall-cmd --reload
FirewallD is not running
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org9febc6e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;检查&#34;&gt;检查&lt;/h2&gt;

&lt;p&gt;可以检查一下本地的共享目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# showmount -e localhost
Export list for localhost:
/data *
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样，服务端就配置好了。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgbfea31c&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;查看端口&#34;&gt;查看端口&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# rpcinfo -p localhost
program vers proto   port  service
 100000    4   tcp    111  portmapper
 100000    3   tcp    111  portmapper
 100000    2   tcp    111  portmapper
 100000    4   udp    111  portmapper
 100000    3   udp    111  portmapper
 100000    2   udp    111  portmapper
 100005    1   udp  20048  mountd
 100005    1   tcp  20048  mountd
 100005    2   udp  20048  mountd
 100024    1   udp  54689  status
 100005    2   tcp  20048  mountd
 100024    1   tcp  46564  status
 100005    3   udp  20048  mountd
 100005    3   tcp  20048  mountd
 100003    3   tcp   2049  nfs
 100003    4   tcp   2049  nfs
 100227    3   tcp   2049  nfs_acl
 100003    3   udp   2049  nfs
 100003    4   udp   2049  nfs
 100227    3   udp   2049  nfs_acl
 100021    1   udp  40110  nlockmgr
 100021    3   udp  40110  nlockmgr
 100021    4   udp  40110  nlockmgr
 100021    1   tcp  33742  nlockmgr
 100021    3   tcp  33742  nlockmgr
 100021    4   tcp  33742  nlockmgr
 [root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orga16c2a2&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;客户端连接-nfs&#34;&gt;客户端连接 NFS&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orge9dc5a1&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;在客户端创建目录&#34;&gt;在客户端创建目录&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# mkdir -p /mnt/nfs-data/
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgce10511&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;挂载&#34;&gt;挂载&lt;/h2&gt;

&lt;p&gt;为了测试方便， 服务端和客户端均在同一台服务器上。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# mount -t nfs 127.0.0.1:/data /mnt/nfs-data
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;挂载之后，可以使用 mount 命令查看一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# mount | grep nfs
nfsd on /proc/fs/nfsd type nfsd (rw,relatime)
sunrpc on /var/lib/nfs/rpc_pipefs type rpc_pipefs (rw,relatime)
127.0.0.1:/data on /mnt/nfs-data type nfs4 (rw,relatime,vers=4.1,rsize=524288,wsize=524288,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=127.0.0.1,local_lock=none,addr=127.0.0.1)
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这说明已经挂载成功了。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgc315b27&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;测试nfs&#34;&gt;测试NFS&lt;/h1&gt;

&lt;p&gt;测试一下，在客户端向共享目录创建一个文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# echo test &amp;gt; /mnt/nfs-data/bbb.txt
[root@k8s-master ~]#
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后取 NFS 服务端查看一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# echo test &amp;gt; /mnt/nfs-data/bbb.txt
[root@k8s-master ~]#
[root@k8s-master ~]# cat /data/bbb.txt
test
[root@k8s-master ~]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，共享目录已经写入了。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org6f32303&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;客户端自动挂载&#34;&gt;客户端自动挂载&lt;/h1&gt;

&lt;p&gt;自动挂载很常用，客户端设置一下即可。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vim /etc/fstab
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在结尾添加类似如下配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# cat /etc/fstab

 #
 # /etc/fstab
 # Created by anaconda on Thu Jul 11 02:52:01 2019
 #
 # Accessible filesystems, by reference, are maintained under &#39;/dev/disk&#39;
 # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info
 #
 UUID=1114fe9e-2309-4580-b183-d778e6d97397 /                       ext4    defaults        1 1
 127.0.0.1:/data     /mnt/nfs-data                   nfs     defaults        0 0
 [root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于修改了 /etc/fstab，需要重新加载 systemctl。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# systemctl daemon-reload
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org284ecce&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考&#34;&gt;参考&lt;/h1&gt;

&lt;p&gt;【01】&lt;a href=&#34;https://www.codeleading.com/article/35162638950/&#34;&gt;阿里云服务器 CentOS7 下安装和配置 NFS&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(05): 动手实现CSI驱动</title>
            <link>http://mospany.github.io/2022/12/22/k8s-sci-driver/</link>
            <pubDate>Thu, 22 Dec 2022 22:21:36 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/12/22/k8s-sci-driver/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;org3f75c1c&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;外部存储接入 Kubernetes 的方式主要有两种：In-Tree 和 Out-of-Tree。其中 In-Tree 是指存储驱动的源码都在 Kubernetes 代码库中，与 Kubernetes 一起发布、迭代、管理，这种方式灵活性较差，且门槛较高。Out-of-Tree 是指存储插件由第三方编写、发布、管理，作为一种扩展与 Kubernetes 配合使用。Out-of-Tree 主要有 FlexVolume 和 CSI 两种实现方式，其中，FlexVolume 因为其命令式的特点，不易维护和管理，从 Kubernetes v1.23 版本开始已被弃用。因此 CSI 已经成为 Kubernetes 存储扩展（ Out-of-Tree ）的唯一方式。&lt;/p&gt;

&lt;p&gt;在介绍CSI之前，先梳理一下kubernetes中使用存储有哪些步骤：&lt;br /&gt;
1、创建pv对象&lt;br /&gt;
2、存储服务器上创建一个volume&lt;br /&gt;
3、把创建的volume挂载到宿主机上&lt;br /&gt;
4、格式化volume&lt;br /&gt;
5、把格式化的volume mount到pod的volume目录&lt;br /&gt;
6、创建pvc对象并和pv对象绑定&lt;br /&gt;
7、pod使用pvc&lt;/p&gt;

&lt;p&gt;代码工程：&lt;a href=&#34;https://github.com/mospany/nfscsi.git&#34;&gt;https://github.com/mospany/nfscsi.git&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orga5011fc&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;术语&#34;&gt;术语&lt;/h1&gt;

&lt;table border=&#34;2&#34; cellspacing=&#34;0&#34; cellpadding=&#34;6&#34; rules=&#34;groups&#34; frame=&#34;hsides&#34;&gt;


&lt;colgroup&gt;
&lt;col  class=&#34;org-left&#34; /&gt;

&lt;col  class=&#34;org-left&#34; /&gt;

&lt;col  class=&#34;org-left&#34; /&gt;

&lt;col  class=&#34;org-left&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;缩写&lt;/th&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;英文全称&lt;/th&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;中文全称&lt;/th&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;pvc&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;PersistentVolumeClaim&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;持久卷声明&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;描述的是pod希望使用的持久化存储的属性&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;pv&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;PersistentVolume&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;持久卷&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;描述的是具体的持久化存储数据卷信息&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;storageClass&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;storageClass&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;存储类&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;创建pv的模板&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a id=&#34;org20530b7&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;csi组成&#34;&gt;CSI组成&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/csi/csi-component.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;通常情况下：CSI Driver = DaemonSet + Deployment(StatefuleSet) 。&lt;br /&gt;
其中:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;绿色部分：Identity、Node、Controller 是需要开发者自己实现的，被称为 Custom Components。&lt;/li&gt;
&lt;li&gt;粉色部分：node-driver-registrar、external-attacher、external-provisioner 组件是 Kubernetes 团队开发和维护的，被称为 External Components，它们都是以 sidecar 的形式与 Custom Components 配合使用的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orga3ae62a&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;custom-components&#34;&gt;Custom Components&lt;/h2&gt;

&lt;p&gt;Custom Components 本质是 3 个 gRPC Services：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Identity Service 顾名思义，主要用于对外暴露这个插件本身的信息，比如驱动的名称、驱动的能力等：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//github.com/container-storage-interface/spec/lib/go/csi/csi.pb.go
type IdentityServer interface {
    GetPluginInfo(context.Context, *GetPluginInfoRequest) (*GetPluginInfoResponse, error)
    GetPluginCapabilities(context.Context, *GetPluginCapabilitiesRequest) (*GetPluginCapabilitiesResponse, error)
    Probe(context.Context, *ProbeRequest) (*ProbeResponse, error)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Controller Service 主要定义一些 无需在宿主机上执行的操作，这也是与下文的 Node Service 最根本的区别。以 CreateVolume 为例，k8s 通过调用该方法创建底层存储。比如底层使用了某云供应商的云硬盘服务，开发者在 CreateVolume 方法实现中应该调用云硬盘服务的创建 / 订购云硬盘的 API，调用 API 这个操作是不需要在特定宿主机上执行的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// github.com/container-storage-interface/spec/lib/go/csi/csi.pb.go
type ControllerServer interface {
    CreateVolume(context.Context, *CreateVolumeRequest) (*CreateVolumeResponse, error)
    DeleteVolume(context.Context, *DeleteVolumeRequest) (*DeleteVolumeResponse, error)
    ControllerPublishVolume(context.Context, *ControllerPublishVolumeRequest) (*ControllerPublishVolumeResponse, error)
    ControllerUnpublishVolume(context.Context, *ControllerUnpublishVolumeRequest) (*ControllerUnpublishVolumeResponse, error)
    ValidateVolumeCapabilities(context.Context, *ValidateVolumeCapabilitiesRequest) (*ValidateVolumeCapabilitiesResponse, error)
    ListVolumes(context.Context, *ListVolumesRequest) (*ListVolumesResponse, error)
    GetCapacity(context.Context, *GetCapacityRequest) (*GetCapacityResponse, error)
    ControllerGetCapabilities(context.Context, *ControllerGetCapabilitiesRequest) (*ControllerGetCapabilitiesResponse, error)
    CreateSnapshot(context.Context, *CreateSnapshotRequest) (*CreateSnapshotResponse, error)
    DeleteSnapshot(context.Context, *DeleteSnapshotRequest) (*DeleteSnapshotResponse, error)
    ListSnapshots(context.Context, *ListSnapshotsRequest) (*ListSnapshotsResponse, error)
    ControllerExpandVolume(context.Context, *ControllerExpandVolumeRequest) (*ControllerExpandVolumeResponse, error)
    ControllerGetVolume(context.Context, *ControllerGetVolumeRequest) (*ControllerGetVolumeResponse, error)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Node Service 定义了 需要在宿主机上执行的操作，比如：mount、unmount。在前面的部署架构图中，Node Service 使用 Daemonset 的方式部署，也是为了确保 Node Service 会被运行在每个节点，以便执行诸如 mount 之类的指令。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// github.com/container-storage-interface/spec/lib/go/csi/csi.pb.go
type NodeServer interface {
    NodeStageVolume(context.Context, *NodeStageVolumeRequest) (*NodeStageVolumeResponse, error)
    NodeUnstageVolume(context.Context, *NodeUnstageVolumeRequest) (*NodeUnstageVolumeResponse, error)
    NodePublishVolume(context.Context, *NodePublishVolumeRequest) (*NodePublishVolumeResponse, error)
    NodeUnpublishVolume(context.Context, *NodeUnpublishVolumeRequest) (*NodeUnpublishVolumeResponse, error)
    NodeGetVolumeStats(context.Context, *NodeGetVolumeStatsRequest) (*NodeGetVolumeStatsResponse, error)
    NodeExpandVolume(context.Context, *NodeExpandVolumeRequest) (*NodeExpandVolumeResponse, error)
    NodeGetCapabilities(context.Context, *NodeGetCapabilitiesRequest) (*NodeGetCapabilitiesResponse, error)
    NodeGetInfo(context.Context, *NodeGetInfoRequest) (*NodeGetInfoResponse, error)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org7bc93bd&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/csi/nfs-csi.png&#34; alt=&#34;img&#34; /&gt;&lt;br /&gt;
1、首先肯定是要在一个进程里编程实现前面提到的三个gRPC服务对应的方法，例如IdentityServer下的三个方法、ControllerServer下的CreateVolume/DeleteVolume方法以及NodeServer下的NodePublishVolume/NodeUnpublishVolume方法等，并以unix sock的方式提供gRPC服务（后文会用到），并打包成镜像；&lt;/p&gt;

&lt;p&gt;2、把node-driver-registrar和CSI服务作为两个容器放到一个pod里，这样node-driver-registrar服务就可以用unix sock的方式访问CSI进程里的gRPC服务并且向kubelet注册；&lt;/p&gt;

&lt;p&gt;3、node-driver-registrar完成注册后，后续的Mount/Unmount等操作kubelet会直接通过unix sock访问CSI。这里有两层含义：第一层含义是kubelet会直接通过unix sock访问CSI，因此CSI需要用hostPath的方式把自己unix sock文件暴露；第二层含义是kubelet直接调用CSI服务，这意味着node-driver-registrar和CSI的这个pod应该是daemonSet形式部署的；&lt;/p&gt;

&lt;p&gt;4、把external-provisioner和CSI服务作为两个容器放到一个pod里，去实现Dynamic Provisioning功能。因为Dynamic Provisioning设计创建卷和删除卷，因此这个pod应该看做是有状态的，在部署上通常是带有选举的deployment部署或者副本数为1的statefulSet部署（如果需要Attach/Detach功能，也可以再加个容器把external-attacher放到这个pod中）。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org953691d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;csi注册流程&#34;&gt;CSI注册流程&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/csi/csi-reg.png&#34; alt=&#34;img&#34; /&gt;&lt;br /&gt;
相关的步骤释义如下：&lt;/p&gt;

&lt;p&gt;1、kubelet启动后基于fsnotify监听/var/lib/kubelet/plugins_registry目录；&lt;br /&gt;
2、node-driver-registrar启动通过启动参数中配置的CSI进程的sock文件，调CSI进程的GetPluginInfo方法获取CSI插件名称；&lt;br /&gt;
3、node-driver-registrar启动后在/var/lib/kubelet/plugins_registry目录下创建自己的sock文件{csiName}-reg.sock；&lt;br /&gt;
4、kubelet的watcher监听到/var/lib/kubelet/plugins_registry目录下有sock文件创建，把该sock文件信息存入内存中的desiredStateOfWorld对象中；&lt;br /&gt;
5、kubelet中有个reconciler协程周期性的检查desiredStateOfWorld对象和actualStateOfWorld对象中的数据差异，发现有新的CSI插件需要执行注册过程；&lt;br /&gt;
6、reconciler通过/var/lib/kubelet/plugins_registry/{csiName}-reg.sock，调用node-driver-registrar下的GetInfo方法获取CSI插件的名称和CSI进程的sock文件路径等信息；&lt;br /&gt;
7、reconciler通过上一步拿到的CSI进程sock文件，调用CSI进程下NodeGetInfo方法获取一些数据用于后续的Node和CSINode对象；&lt;br /&gt;
8、组装数据调apiServer接口更新本节点对应的Node对象的annotation；&lt;br /&gt;
9、组装数据调apiServer接口创建/更新对应的CSINode对象；&lt;br /&gt;
10、reconciler通过/var/lib/kubelet/plugins_registry/{csiName}-reg.sock，调用node-driver-registrar的NotifyRegistrationStatus方法，告知其注册结果。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org34e354f&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;nfs搭建&#34;&gt;NFS搭建&lt;/h1&gt;

&lt;p&gt;由于实现CSI需要一个后端存储，Linux提供NFS功能可以免费搭建一个NSC存储功能用来验证。&lt;br /&gt;
搭建办法详见: &lt;a href=&#34;http://blog.mospan.cn/2023/01/15/aliyun-ecs-install-nfs/&#34;&gt;CentOS7 下安装和配置 NFS&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgaf35c61&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;部署&#34;&gt;部署&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org82b326f&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;部署node&#34;&gt;部署node&lt;/h2&gt;

&lt;p&gt;进入代码工程中的deploy下运行命令`kubectl apply -f node.yaml`&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master nfscsi]# pwd
/root/nfscsi
[root@k8s-master nfscsi]# cd deploy/
[root@k8s-master deploy]# ls
node.yaml  provisioner.yaml
[root@k8s-master deploy]# ll
总用量 12
-rw-r--r-- 1 root root 3720 1月   8 22:18 node.yaml
-rw-r--r-- 1 root root 4413 1月   8 22:16 provisioner.yaml

[root@k8s-master deploy]# k apply -f node.yaml
serviceaccount/nfs-csi-node created
clusterrole.rbac.authorization.k8s.io/nfs-csi-node created
clusterrolebinding.rbac.authorization.k8s.io/nfs-csi-node created
daemonset.apps/nfs-csi-node created
[root@k8s-master deploy]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看POD的运行状态：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deploy]# k get pod -A -o wide | grep nfs
kube-system   nfs-csi-node-x28zj                         2/2     Running   0                10m   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
[root@k8s-master deploy]#
[root@k8s-master deploy]# k get ds -A
NAMESPACE     NAME           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                              AGE
kube-system   calico-node    3         3         3       3            3           kubernetes.io/os=linux                                     19d
kube-system   kube-proxy     3         3         3       3            3           kubernetes.io/os=linux                                     52d
kube-system   nfs-csi-node   1         1         1       1            1           kubernetes.io/hostname=k8s-master,kubernetes.io/os=linux   10m
[root@k8s-master deploy]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pod正常启动后，先查看node-driver-registrar的日志，注册正常：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master nfscsi]# kubectl logs -n kube-system nfs-csi-node-x28zj node-driver-registrar
I0115 07:45:31.451826       1 main.go:166] Version: v2.5.0
I0115 07:45:31.451864       1 main.go:167] Running node-driver-registrar in mode=registration
I0115 07:45:31.452324       1 main.go:191] Attempting to open a gRPC connection with: &amp;quot;/csi/csi.sock&amp;quot;
I0115 07:45:32.455714       1 main.go:198] Calling CSI driver to discover driver name
I0115 07:45:32.457464       1 main.go:208] CSI driver name: &amp;quot;nfscsi&amp;quot;
I0115 07:45:32.457493       1 node_register.go:53] Starting Registration Server at: /registration/nfscsi-reg.sock
I0115 07:45:32.457582       1 node_register.go:62] Registration Server started at: /registration/nfscsi-reg.sock
I0115 07:45:32.458992       1 node_register.go:92] Skipping HTTP server because endpoint is set to: &amp;quot;&amp;quot;
I0115 07:45:33.018081       1 main.go:102] Received GetInfo call: &amp;amp;InfoRequest{}
I0115 07:45:33.018292       1 main.go:109] &amp;quot;Kubelet registration probe created&amp;quot; path=&amp;quot;/var/lib/kubelet/plugins/csi-nfsplugin/registration&amp;quot;
I0115 07:45:33.623838       1 main.go:120] Received NotifyRegistrationStatus call: &amp;amp;RegistrationStatus{PluginRegistered:true,Error:,}
[root@k8s-master nfscsi]# ll /var/lib/kubelet/plugins/csi-nfsplugin/csi.sock
srwxr-xr-x 1 root root 0 1月  15 15:45 /var/lib/kubelet/plugins/csi-nfsplugin/csi.sock
[root@k8s-master nfscsi]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再看看自己编码nfs-csi容器日志，注册过程只调用了GetPluginInfo和NodeGetInfo&lt;br /&gt;
方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master nfscsi]# kubectl logs -n kube-system nfs-csi-node-x28zj nfs-csi
2023/01/15 07:45:31 driverName: nfscsi, version: N/A, nodeID: k8s-master
2023/01/15 07:45:31 grpc server start
2023/01/15 07:45:32 GetPluginInfo request
2023/01/15 07:45:33 NodeGetInfo request
[root@k8s-master nfscsi]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;node-driver-registrar和CSI进程的sock文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master nfscsi]# ll /var/lib/kubelet/plugins_registry/
总用量 0
srwx------ 1 root root 0 1月  15 15:45 nfscsi-reg.sock
[root@k8s-master nfscsi]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;node对象的annotation：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master nfscsi]# kubectl get node k8s-master  -oyaml| grep annotations -A 9
annotations:
  csi.volume.kubernetes.io/nodeid: &#39;{&amp;quot;nfscsi&amp;quot;:&amp;quot;k8s-master&amp;quot;}&#39;
  kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
  node.alpha.kubernetes.io/ttl: &amp;quot;0&amp;quot;
  projectcalico.org/IPv4Address: 172.25.140.216/20
  projectcalico.org/IPv4IPIPTunnelAddr: 10.244.235.192
  volumes.kubernetes.io/controller-managed-attach-detach: &amp;quot;true&amp;quot;
creationTimestamp: &amp;quot;2022-11-23T15:34:38Z&amp;quot;
labels:
  app: hdls-csi-controller
[root@k8s-master nfscsi]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后验证CSINode对象：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master nfscsi]# k get csinode -o wide
NAME         DRIVERS   AGE
k8s-master   1         52d
k8s-work1    0         52d
k8s-work2    0         52d
[root@k8s-master nfscsi]# kubectl get csinode k8s-master -o yaml
apiVersion: storage.k8s.io/v1
kind: CSINode
metadata:
  annotations:
    storage.alpha.kubernetes.io/migrated-plugins: kubernetes.io/aws-ebs,kubernetes.io/azure-disk,kubernetes.io/cinder,kubernetes.io/gce-pd
  creationTimestamp: &amp;quot;2022-11-23T15:34:38Z&amp;quot;
  name: k8s-master
  ownerReferences:
  - apiVersion: v1
    kind: Node
    name: k8s-master
    uid: cb4c3e59-66e5-40a0-a385-aa2072719381
  resourceVersion: &amp;quot;278990&amp;quot;
  uid: cf0f93f0-495e-49f2-88b4-6f7126ca6176
spec:
  drivers:
  - name: nfscsi
    nodeID: k8s-master
    topologyKeys: null
[root@k8s-master nfscsi]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这里我们成功完成并验证了CSI的注册。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deploy]# k logs -n kube-system nfs-csi-node-vxkc8  nfs-csi
2023/01/15 08:33:22 driverName: nfscsi, version: N/A, nodeID: k8s-master
2023/01/15 08:33:22 grpc server start
2023/01/15 08:33:23 GetPluginInfo request
2023/01/15 08:33:23 NodeGetInfo request
2023/01/15 09:42:25 NodeGetCapabilities request
2023/01/15 09:42:25 NodeGetCapabilities request
2023/01/15 09:42:25 NodeGetCapabilities request
2023/01/15 09:42:25 NodeGetCapabilities request
2023/01/15 09:42:25 NodePublishVolume request
2023/01/15 09:42:25 source: 127.0.0.1:/data/pvc-910384b5-b5eb-4196-b6d3-876f9679ed05, targetPath: /var/lib/kubelet/pods/e86fa053-7d1b-4330-a6f2-555d350b45e9/volumes/kubernetes.io~csi/pvc-910384b5-b5eb-4196-b6d3-876f9679ed05/mount, options: []
2023/01/15 09:43:20 NodeGetCapabilities request
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当POD执行CreateVolume是将调用nodeService里的NodePublishVolume方法进行mount操作，即把nfs里的pvc-子目录mount到宿主机pod目录卷下。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgccab5fd&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;部署provisioner&#34;&gt;部署provisioner&lt;/h2&gt;

&lt;p&gt;Dynamic Provisioning原理：所谓的Dynamic Provisioning，其实就是创建pvc后会自动创建卷和pv，并把pv和pvc绑定&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/csi/provisioner-follow.jpg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;部署运行:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deploy]# kubectl apply -f provisioner.yaml
storageclass.storage.k8s.io/nfscsi created
csidriver.storage.k8s.io/nfscsi created
serviceaccount/nfs-csi-provisioner created
clusterrole.rbac.authorization.k8s.io/nfs-csi-provisioner created
clusterrolebinding.rbac.authorization.k8s.io/nfs-csi-provisioner created
deployment.apps/nfs-csi-provisioner created
[root@k8s-master deploy]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;观察对应的provisioner是否起来：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deploy]# k get pod -A -o wide | grep nfs
kube-system   nfs-csi-node-vxkc8                         2/2     Running   0               10m     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   nfs-csi-provisioner-6f7db46646-77lqv       2/2     Running   0               2m20s   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
[root@k8s-master deploy]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgbd2779f&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;创建pvc&#34;&gt;创建PVC&lt;/h3&gt;

&lt;p&gt;准备一个如下的pvc yaml，apply该yaml：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master test]# pwd
/root/nfscsi/test
[root@k8s-master test]# cat pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: nfscsi
  resources:
    requests:
      storage: 1Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看是否会自动创建卷、pv，并和pvc绑定：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master test]# kubectl apply -f pvc.yaml
persistentvolumeclaim/test-pvc created
[root@k8s-master test]# k get pvc
NAME       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
test-pvc   Bound    pvc-910384b5-b5eb-4196-b6d3-876f9679ed05   1Gi        RWO            nfscsi         20s
[root@k8s-master test]# k get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM              STORAGECLASS   REASON   AGE
pvc-910384b5-b5eb-4196-b6d3-876f9679ed05   1Gi        RWO            Delete           Bound      default/test-pvc   nfscsi                  35s
[root@k8s-master test]# ll /mnt/nfs-data/
总用量 16
drwxr-xr-x 2 root root 4096 1月  15 17:06 pvc-910384b5-b5eb-4196-b6d3-876f9679ed05
[root@k8s-master test]# ll /data/
总用量 16
drwxr-xr-x 2 root root 4096 1月   8 22:22 pvc-7115a52d-ba4f-4571-adbc-25e71941ca55
[root@k8s-master test]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看provisioner pod日志：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master test]# k logs -n kube-system  nfs-csi-provisioner-6f7db46646-77lqv nfs-csi
2023/01/15 08:41:03 driverName: nfscsi, version: N/A, nodeID: k8s-master
2023/01/15 08:41:03 grpc server start
2023/01/15 08:41:04 Probe request
2023/01/15 08:41:04 GetPluginInfo request
2023/01/15 08:41:04 GetPluginCapabilities request
2023/01/15 08:41:04 ControllerGetCapabilities request
...
2023/01/15 09:06:26 CreateVolume request
2023/01/15 09:06:26 req name:  pvc-910384b5-b5eb-4196-b6d3-876f9679ed05
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出创建pvc时自动绑定pv成功。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org252c6f3&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;删除pvc&#34;&gt;删除PVC&lt;/h3&gt;

&lt;p&gt;同理。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org20891ab&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;测试&#34;&gt;测试&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orgaaf51bd&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;验证pod使用csi&#34;&gt;验证POD使用CSI&lt;/h2&gt;

&lt;p&gt;先准备POD的yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master test]# cat pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-nginx-sci-pod
spec:
  nodeName: k8s-master  # 运行在安装了csi插件的node上
  containers:
  - name: nginx
    image: nginx:latest
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - name: nfs-pvc
      mountPath: /var/log/nginx
  volumes:
  - name: nfs-pvc
    persistentVolumeClaim:
      claimName: test-pvc
   [root@k8s-master test]#  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master test]# kubectl apply -f pod.yaml
pod/test-nginx-sci-pod created
[root@k8s-master test]# kubectl get pod -A | grep test-nginx
default       test-nginx-sci-pod                         1/1     Running   0                18s
[root@k8s-master test]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看nginx日志&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master test]# ll /mnt/nfs-data/pvc-910384b5-b5eb-4196-b6d3-876f9679ed05/
总用量 4
-rw-r--r-- 1 root root    0 1月  15 17:48 access.log
-rw-r--r-- 1 root root 1641 1月  15 17:52 error.log
[root@k8s-master test]#
[root@k8s-master test]# tail /mnt/nfs-data/pvc-910384b5-b5eb-4196-b6d3-876f9679ed05/access.log
[root@k8s-master test]# tail /mnt/nfs-data/pvc-910384b5-b5eb-4196-b6d3-876f9679ed05/error.log
2023/01/15 09:49:20 [notice] 1#1: worker process 33 exited with code 0
2023/01/15 09:49:20 [notice] 1#1: exit
2023/01/15 09:51:39 [notice] 1#1: using the &amp;quot;epoll&amp;quot; event method
2023/01/15 09:51:39 [notice] 1#1: nginx/1.21.5
2023/01/15 09:51:39 [notice] 1#1: built by gcc 10.2.1 20210110 (Debian 10.2.1-6)
2023/01/15 09:51:39 [notice] 1#1: OS: Linux 3.10.0-957.21.3.el7.x86_64
2023/01/15 09:51:39 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 65536:65536
2023/01/15 09:51:39 [notice] 1#1: start worker processes
2023/01/15 09:51:39 [notice] 1#1: start worker process 32
2023/01/15 09:51:39 [notice] 1#1: start worker process 33
[root@k8s-master test]#
[root@k8s-master test]# ll /data/pvc-910384b5-b5eb-4196-b6d3-876f9679ed05/
总用量 4
-rw-r--r-- 1 root root    0 1月  15 17:48 access.log
-rw-r--r-- 1 root root 1641 1月  15 17:52 error.log
[root@k8s-master test]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出宿主机和nfs服务器上该pvc下已经有nginx日志生成达到了目的。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orga9b7cb5&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;csc功能测试命令&#34;&gt;csc功能测试命令&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go install -v  github.com/rexray/gocsi/csc@latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用go env查看GOPATH, go install 的程序一般就放在第一个路径下的bin&lt;/p&gt;

&lt;p&gt;把它拷贝到目标机上并加可执行权限。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master csi-hdls]# ./csc
NAME
    csc -- a command line container storage interface (CSI) client

SYNOPSIS
    csc [flags] CMD

AVAILABLE COMMANDS
    controller
    identity
    node

Use &amp;quot;csc -h,--help&amp;quot; for more information
[root@k8s-master csi-hdls]# ./csc controller help
NAME
    controller -- the csi controller service rpcs

SYNOPSIS
    csc controller [flags] CMD

AVAILABLE COMMANDS
    create-snapshot
    create-volume
    delete-snapshot
    delete-volume
    expand-volume
    get-capabilities
    get-capacity
    list-snapshots
    list-volumes
    publish
    unpublish
    validate-volume-capabilities

Use &amp;quot;csc controller -h,--help&amp;quot; for more information
[root@k8s-master csi-hdls]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;emptyDir的位置应该位于运行pod的给定节点上的/var/lib/kubelet/pods/{podid}/volumes/kubernetes.io~empty-dir/中&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orgd2a986a&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考&#34;&gt;参考&lt;/h1&gt;

&lt;p&gt;【01】&lt;a href=&#34;https://zhuanlan.zhihu.com/p/583032625&#34;&gt;如何实现一个 Kubernetes CSI Driver&lt;/a&gt;&lt;br /&gt;
【02】&lt;a href=&#34;https://zhuanlan.zhihu.com/p/539307741&#34;&gt;Kubernetes CSI 驱动开发指南&lt;/a&gt;&lt;br /&gt;
【03】&lt;a href=&#34;https://www.cnblogs.com/cxt618/p/15487359.html&#34;&gt;gRPC详细入门介绍&lt;/a&gt;&lt;br /&gt;
【04】&lt;a href=&#34;https://jishuin.proginn.com/p/763bfbd3890f&#34;&gt;如何编写一个 CSI 插件&lt;/a&gt;&lt;br /&gt;
【05】&lt;a href=&#34;http://www.noobyard.com/article/p-qgxfxmfi-nv.html&#34;&gt;Kubernetes K8S之固定节点nodeName和nodeSelector调度详解&lt;/a&gt;&lt;br /&gt;
【06】&lt;a href=&#34;https://www.modb.pro/db/523598&#34;&gt;kubernetes CSI（下）&lt;/a&gt;&lt;br /&gt;
【07】&lt;a href=&#34;https://www.codeleading.com/article/35162638950/&#34;&gt;阿里云服务器 CentOS7 下安装和配置 NFS&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(04): 基于kubebuilder编写operator</title>
            <link>http://mospany.github.io/2022/12/14/operator-on-kubebuilder/</link>
            <pubDate>Wed, 14 Dec 2022 18:24:22 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/12/14/operator-on-kubebuilder/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;orgcfa7382&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;介绍&#34;&gt;介绍&lt;/h1&gt;

&lt;p&gt;Kubebuilder是一个由controller-runtime支持的出色SDK，它可以帮助您轻松快速地在 Go 中编写Kubernetes operator，方法是处理多种忙碌的事情，例如以组织良好的方式引导大量样板代码，设置有用的 Makefile make，目标是构建、运行和部署operator、构建 CRD、设置相关的 Dockefile、RBAC、涉及部署operator的多个 YAML 等等。&lt;br /&gt;
Kubebuilder 是一个使用 CRDs 构建 K8s API 的 SDK，主要是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;提供脚手架工具初始化 CRDs 工程，自动生成 boilerplate 代码和配置；&lt;/li&gt;
&lt;li&gt;提供代码库封装底层的 K8s go-client；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;方便用户从零开始开发 CRDs，Controllers 和 Admission Webhooks 来扩展 K8s。&lt;/p&gt;

&lt;p&gt;为了编写自定义的operatro，需要进行如下:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;安装kubebuilder&lt;/li&gt;
&lt;li&gt;使用kubebuilder进行项目创建&lt;/li&gt;
&lt;li&gt;编写operator代码&lt;/li&gt;
&lt;li&gt;编译打包镜像上传到镜像仓库。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a id=&#34;org0fa6862&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/kubebuilder-arch.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org78ac50d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;基础概念&#34;&gt;基础概念&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;org52cd124&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;gvks-gvrs&#34;&gt;GVKs&amp;amp;GVRs&lt;/h3&gt;

&lt;p&gt;GVK = GroupVersionKind，GVR = GroupVersionResource。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;API Group &amp;amp; Versions（GV）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;API Group 是相关 API 功能的集合，每个 Group 拥有一或多个 Versions，用于接口的演进。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kinds &amp;amp; Resources（GVR）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每个 GV 都包含多个 API 类型，称为 Kinds，在不同的 Versions 之间同一个 Kind 定义可能不同， Resource 是 Kind 的对象标识（resource type），一般来说 Kinds 和 Resources 是 1:1 的，比如 pods Resource 对应 Pod Kind，但是有时候相同的 Kind 可能对应多个 Resources，比如 Scale Kind 可能对应很多 Resources：deployments/scale，replicasets/scale，对于 CRD 来说，只会是 1:1 的关系。&lt;/p&gt;

&lt;p&gt;每一个 GVK 都关联着一个 package 中给定的 root Go type，比如 apps/v1/Deployment 就关联着 K8s 源码里面 k8s.io/api/apps/v1 package 中的 Deployment struct，我们提交的各类资源定义 YAML 文件都需要写：&lt;/p&gt;

&lt;p&gt;apiVersion：这个就是 GV 。kind：这个就是 K。&lt;/p&gt;

&lt;p&gt;根据 GVK K8s 就能找到你到底要创建什么类型的资源，根据你定义的 Spec 创建好资源之后就成为了 Resource，也就是 GVR。GVK/GVR 就是 K8s 资源的坐标，是我们创建/删除/修改/读取资源的基础。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org691e4a3&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;scheme&#34;&gt;Scheme&lt;/h3&gt;

&lt;p&gt;每一组 Controllers 都需要一个 Scheme，提供了 Kinds 与对应 Go types 的映射，也就是说给定 Go type 就知道他的 GVK，给定 GVK 就知道他的 Go type，比如说我们给定一个 Scheme: &amp;ldquo;tutotial.kubebuilder.io/api/v1&amp;rdquo;.CronJob{} 这个 Go type 映射到 batch.tutotial.kubebuilder.io/v1 的 CronJob GVK，那么从 Api Server 获取到下面的 JSON:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &amp;quot;kind&amp;quot;: &amp;quot;CronJob&amp;quot;,
    &amp;quot;apiVersion&amp;quot;: &amp;quot;batch.tutorial.kubebuilder.io/v1&amp;quot;,
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就能构造出对应的 Go type了，通过这个 Go type 也能正确地获取 GVR 的一些信息，控制器可以通过该 Go type 获取到期望状态以及其他辅助信息进行调谐逻辑。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgc45a76f&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;manager&#34;&gt;Manager&lt;/h3&gt;

&lt;p&gt;Kubebuilder 的核心组件，具有 3 个职责：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;负责运行所有的 Controllers；&lt;/li&gt;
&lt;li&gt;初始化共享 caches，包含 listAndWatch 功能；&lt;/li&gt;
&lt;li&gt;初始化 clients 用于与 Api Server 通信。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org50d3209&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;cache&#34;&gt;Cache&lt;/h3&gt;

&lt;p&gt;Kubebuilder 的核心组件，负责在 Controller 进程里面根据 Scheme 同步 Api Server 中所有该 Controller 关心 GVKs 的 GVRs，其核心是 GVK -&amp;gt; Informer 的映射，Informer 会负责监听对应 GVK 的 GVRs 的创建/删除/更新操作，以触发 Controller 的 Reconcile 逻辑。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org7fb1547&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;controller&#34;&gt;Controller&lt;/h3&gt;

&lt;p&gt;Kubebuidler 为我们生成的脚手架文件，我们只需要实现 Reconcile 方法即可。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org528499e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;client&#34;&gt;Client&lt;/h3&gt;

&lt;p&gt;在实现 Controller 的时候不可避免地需要对某些资源类型进行创建/删除/更新，就是通过该 Clients 实现的，其中查询功能实际查询是本地的 Cache，写操作直接访问 Api Server。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org16de8d6&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;index&#34;&gt;Index&lt;/h3&gt;

&lt;p&gt;由于 Controller 经常要对 Cache 进行查询，Kubebuilder 提供 Index utility 给 Cache 加索引提升查询效率。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org607691c&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;finalizer&#34;&gt;Finalizer&lt;/h3&gt;

&lt;p&gt;在一般情况下，如果资源被删除之后，我们虽然能够被触发删除事件，但是这个时候从 Cache 里面无法读取任何被删除对象的信息，这样一来，导致很多垃圾清理工作因为信息不足无法进行，K8s 的 Finalizer 字段用于处理这种情况。在 K8s 中，只要对象 ObjectMeta 里面的 Finalizers 不为空，对该对象的 delete 操作就会转变为 update 操作，具体说就是 update deletionTimestamp 字段，其意义就是告诉 K8s 的 GC“在deletionTimestamp 这个时刻之后，只要 Finalizers 为空，就立马删除掉该对象”。&lt;/p&gt;

&lt;p&gt;所以一般的使用姿势就是在创建对象时把 Finalizers 设置好（任意 string），然后处理 DeletionTimestamp 不为空的 update 操作（实际是 delete），根据 Finalizers 的值执行完所有的 pre-delete hook（此时可以在 Cache 里面读取到被删除对象的任何信息）之后将 Finalizers 置为空即可。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org0c0efe9&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;ownerreference&#34;&gt;OwnerReference&lt;/h3&gt;

&lt;p&gt;K8s GC 在删除一个对象时，任何 ownerReference 是该对象的对象都会被清除，与此同时，Kubebuidler 支持所有对象的变更都会触发 Owner 对象 controller 的 Reconcile 方法。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgb1ddf8b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;实现&#34;&gt;实现&lt;/h1&gt;

&lt;p&gt;1、构造新的scheme,&lt;br /&gt;
2、解析命令行参数，获取或默认metric和健康检查端口。&lt;br /&gt;
3、实例化 manager，参数 config&lt;br /&gt;
   3.1） 向 manager 添加 scheme&lt;br /&gt;
   3.2） 向 manager 添加 controller，该 controller 包含一个 reconciler 结构体，我们需要在 reconciler 结构体实现逻辑处理&lt;br /&gt;
4、向manager添加healthz和readyz探测。&lt;br /&gt;
5、启动 manager.start()&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org40fbcb7&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;镜像仓库&#34;&gt;镜像仓库&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org246c0bb&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;注册仓库&#34;&gt;注册仓库&lt;/h2&gt;

&lt;p&gt;登录&lt;a href=&#34;https://registry.hub.docker.com/进行注册，如用户名为mospany&#34;&gt;https://registry.hub.docker.com/进行注册，如用户名为mospany&lt;/a&gt;, 密码为自定义。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org43ccc0d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;使用镜像&#34;&gt;使用镜像&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;org829ca45&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;登录仓库&#34;&gt;登录仓库&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ docker login index.docker.io
Username: mospany
Password:
Login Succeeded

或直接 docker login默认登录docker hub。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org2fc914e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;上传镜像&#34;&gt;上传镜像&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ docker tag loggen:latest mospany/loggen:latest
$ docker push mospany/loggen:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/hub-docker.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;参见：&lt;a href=&#34;https://hub.docker.com/repository/docker/mospany/loggen&#34;&gt;https://hub.docker.com/repository/docker/mospany/loggen&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org3d0e3a5&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;下载镜像&#34;&gt;下载镜像&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ docker pull mospany/loggen:latest
latest: Pulling from mospany/loggen
Digest: sha256:0cdeece36f8a003dd6b9c463cc73dad93479deabec08c1def033e72ec9818539
Status: Image is up to date for mospany/loggen:latest
docker.io/mospany/loggen:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgc8a2f44&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;项目&#34;&gt;项目&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org3a9cf00&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;创建项目&#34;&gt;创建项目&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;mkdir guestbook
cd guestbook
go mod init guestbook
kubebuilder init --domain xiaohongshu.org --owner &amp;quot;luxiu&amp;quot;
kubebuilder create api --group redis  --version v1 --kind RedisCluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关键截图如下：&lt;br /&gt;
   &lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/kubebuilder-operator.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org1ddad49&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;修改文件&#34;&gt;修改文件&lt;/h2&gt;

&lt;p&gt;1）修改Dockerfile的gcr.io镜像为其他可访问镜像(如golang:1.18)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;为了防止出现“failed to solve with frontend dockerfile.v0: failed to create LLB definition: failed to do request: Head &amp;ldquo;&lt;a href=&#34;https://gcr.io/v2/distroless/static/manifests/nonroot&#34;&gt;https://gcr.io/v2/distroless/static/manifests/nonroot&lt;/a&gt;&amp;ldquo;: Service Unavailable”错误，需修改Dockerfile的gcr.io镜像为其他可访问镜像(如golang:1.18)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;2）修改Dockerfile添加代理&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;为了防止go mod download时不至于超时连不上，需在Run go mod download行上面添加&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;ENV GOPROXY=&amp;quot;https://goproxy.cn&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;否则会出现如下错误：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;3.469 go: cloud.google.com/go@v0.81.0: Get &amp;ldquo;&lt;a href=&#34;https://proxy.golang.org/cloud.google.com/go/@v/v0.81.0.mod&#34;&gt;https://proxy.golang.org/cloud.google.com/go/@v/v0.81.0.mod&lt;/a&gt;&amp;ldquo;: malformed HTTP response &amp;ldquo;\x00\x00\x12\x04&amp;#x2026;\x00\x00\x01&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;**切记切记**： 先修改好再编译，否则一直出现上面错误(当时找了半天)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Build the manager binary
  FROM golang:1.18 as builder
  ENV GOPROXY=&amp;quot;https://goproxy.cn&amp;quot;

  WORKDIR /workspace
  # Copy the Go Modules manifests
  COPY go.mod go.mod
  COPY go.sum go.sum
  # cache deps before building and copying source so that we don&#39;t need to re-download as much
  # and so that source changes don&#39;t invalidate our downloaded layer
  RUN go mod download

  # Copy the go source
  COPY main.go main.go
  COPY api/ api/
  COPY controllers/ controllers/

  # Build
  RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -o manager main.go

  # Use distroless as minimal base image to package the manager binary
  # Refer to https://github.com/GoogleContainerTools/distroless for more details
  #FROM gcr.io/distroless/static:nonroot
  FROM centos:latest
  WORKDIR /
  COPY --from=builder /workspace/manager .
  USER 65532:65532

  ENTRYPOINT [&amp;quot;/manager&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;必须在Dockerfile里面设置代理”ENV GOPROXY=&amp;rdquo;&lt;https://goproxy.cn&#34;“才行，如下设置也不行&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/docker-preferences.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;3）修改Makefile中的crd中的配置&lt;br /&gt;
  给kubectl加上所要连接的集群， 如本机为&amp;#x2013;context docker-desktop。可通过如下命令获得：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config get-contexts
kubectl cluster-info
kubectl config view
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/kubectl-info.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;对应的Makefile修改如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.PHONY: install
install: manifests kustomize ## Install CRDs into the K8s cluster specified in ~/.kube/config.
  $(KUSTOMIZE) build config/crd | kubectl --context docker-desktop  apply -f -

.PHONY: uninstall
uninstall: manifests kustomize ## Uninstall CRDs from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.
  $(KUSTOMIZE) build config/crd | kubectl --context docker-desktop  delete --ignore-not-found=$(ignore-not-found) -f -

.PHONY: deploy
deploy: manifests kustomize ## Deploy controller to the K8s cluster specified in ~/.kube/config.
  cd config/manager &amp;amp;&amp;amp; $(KUSTOMIZE) edit set image controller=${IMG}
  $(KUSTOMIZE) build config/default | kubectl --context docker-desktop  apply -f -

.PHONY: undeploy
undeploy: ## Undeploy controller from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.
  $(KUSTOMIZE) build config/default | kubectl --context docker-desktop  delete --ignore-not-found=$(ignore-not-found) -f -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgf288f34&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;编译&#34;&gt;编译&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;org99eec51&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;make-help&#34;&gt;make help&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;make help
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/make-help.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org54938e4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;make-build&#34;&gt;make build&lt;/h3&gt;

&lt;p&gt;编译并在bin/下生成目标可执行程序。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/make-build.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orge8e0f0a&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;make-install&#34;&gt;make install&lt;/h3&gt;

&lt;p&gt;安装crd到目标集群，这一步可能受github网络影响自动下载kustomize慢需要多试几次或隔天再试。&lt;br /&gt;
   &lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/make-install.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orge3f35bb&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;make-docker-build&#34;&gt;make docker-build&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/make-docker-build.png&#34; alt=&#34;img&#34; /&gt;&lt;br /&gt;
可以在刚生成的镜像列表中生成镜像。&lt;br /&gt;
&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/make-docker-images.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org54b15f4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;make-docker-push&#34;&gt;make docker-push&lt;/h3&gt;

&lt;p&gt;1）先增加要上传镜像的tag&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker tag controller:latest docker.io/mospany/controller:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2）make docker-push&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ make docker-push IMG=docker.io/mospany/controller:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/make-docker-push.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;查看docker hub上传效果&lt;br /&gt;
&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/make-docker-hub.png&#34; alt=&#34;img&#34; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a id=&#34;org5f3af46&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;运行&#34;&gt;运行&lt;/h2&gt;

&lt;p&gt;先在mac上安装k8s集群，详见：&lt;a href=&#34;https://blog.51cto.com/zlyang/4838042&#34;&gt;Mac系统安装k8s集群&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org861d0fa&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;本地运行&#34;&gt;本地运行&lt;/h3&gt;

&lt;p&gt;要想在本地运行 controller，只需要执行下面的命令，你将看到 controller 启动和运行时输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make run
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/make-run.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org5dc2c10&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;部署到k8s集群中运行&#34;&gt;部署到k8s集群中运行&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;make deploy IMG=docker.io/mospany/controller:v1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/make-deploy.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;查看日志&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl logs -n guestbook-system guestbook-controller-manager-7c67b5bd6c-gm5qs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/make-logs.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgf9f8f6e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;创建cr&#34;&gt;创建CR&lt;/h2&gt;

&lt;p&gt;该创建自定义资源对象CR了，如原生中的rc/deployment等对象&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# mosp @ mospdeMacBook-Pro in ~/work/pingan/arch/mysrc/guestbook [21:14:17]
$ kubectl get RedisCluster
NAME                  AGE
rediscluster-sample   48m

# mosp @ mospdeMacBook-Pro in ~/work/pingan/arch/mysrc/guestbook [21:22:49]
$ kubectl get RedisCluster -o yaml
apiVersion: v1
items:
- apiVersion: redis.xiaohongshu.org/v1
  kind: RedisCluster
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {&amp;quot;apiVersion&amp;quot;:&amp;quot;redis.xiaohongshu.org/v1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;RedisCluster&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;rediscluster-sample&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;default&amp;quot;},&amp;quot;spec&amp;quot;:null}
    creationTimestamp: &amp;quot;2022-07-29T12:34:11Z&amp;quot;
    generation: 1
    name: rediscluster-sample
    namespace: default
    resourceVersion: &amp;quot;200052&amp;quot;
    uid: 18eaf75f-9597-46af-bd88-abf7153c1377
  status: {}
kind: List
metadata:
  resourceVersion: &amp;quot;&amp;quot;
  selfLink: &amp;quot;&amp;quot;

# mosp @ mospdeMacBook-Pro in ~/work/pingan/arch/mysrc/guestbook [21:23:07]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org7b598e7&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;开发业务逻辑&#34;&gt;开发业务逻辑&lt;/h2&gt;

&lt;p&gt;下面我们将修改 CRD 的数据结构并在 controller 中增加一些日志输出。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org869a73b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;修改-crd&#34;&gt;修改 CRD&lt;/h3&gt;

&lt;p&gt;我们将修改api/v1/rediscluster_types.go 文件的内容，在 CRD 中增加 FirstName、LastName 和 Status 字段。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// RedisClusterSpec defines the desired state of RedisCluster
type RedisClusterSpec struct {
    // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster

    // Important: Run &amp;quot;make&amp;quot; to regenerate code after modifying this file

    // Foo is an example field of RedisCluster. Edit rediscluster_types.go to remove/update
    FirstName string `json:&amp;quot;firstname&amp;quot;`
    LastName  string `json:&amp;quot;lastname&amp;quot;`
}

// RedisClusterStatus defines the observed state of RedisCluster
type RedisClusterStatus struct {
    // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster

    // Important: Run &amp;quot;make&amp;quot; to regenerate code after modifying this file
    Status string `json:&amp;quot;Status&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orga981c1e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;修改-reconcile-函数&#34;&gt;修改 Reconcile 函数&lt;/h3&gt;

&lt;p&gt;Reconcile 函数是 Operator 的核心逻辑，Operator 的业务逻辑都位于 controllers/rediscluster_controller.go 文件的 Reconcile 函数中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *RedisClusterReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
    _ = log.FromContext(ctx)

    // TODO(user): your logic here

    // 获取当前的 CR，并打印
    logger := log.FromContext(ctx)
    obj := &amp;amp;redisv1.RedisCluster{}
    if err := r.Get(ctx, req.NamespacedName, obj); err != nil {
        logger.Error(err, &amp;quot;Unable to fetch object&amp;quot;)
        return ctrl.Result{}, nil
    } else {
        logger.Info(&amp;quot;Greeting from Kubebuilder to&amp;quot;, obj.Spec.FirstName, obj.Spec.LastName)
    }

    // 初始化 CR 的 Status 为 Running
    obj.Status.Status = &amp;quot;Running&amp;quot;
    if err := r.Status().Update(ctx, obj); err != nil {
        logger.Error(err, &amp;quot;unable to update status&amp;quot;)
    }

    return ctrl.Result{}, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org1cfb9cb&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;运行测试&#34;&gt;运行测试&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;安装CRD（同上）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;部署controller（同上）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建CR&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;修改 config/samples/redis_v1_rediscluster.yaml 文件中的配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: redis.xiaohongshu.org/v1
kind: RedisCluster
metadata:
  name: rediscluster-sample
spec:
  # TODO(user): Add fields here
  firstname: Jimmy
  lastname: Song
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行下面命令，创建CR：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k8sdev apply -f  config/samples/redis_v1_rediscluster.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看controller里的运行日志：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operator/controller-logs.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org1683514&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;p&gt;【01】&lt;a href=&#34;https://blog.csdn.net/weixin_43568070/article/details/89892620&#34;&gt;使用shell命令行登陆Docker Hub出现的404Not found的问题&lt;/a&gt;&lt;br /&gt;
【02】&lt;a href=&#34;https://blog.csdn.net/HaHa_Sir/article/details/119412754&#34;&gt;Docker镜像推送Dockerhub&lt;/a&gt;&lt;br /&gt;
【03】&lt;a href=&#34;https://www.cnblogs.com/mysql-dba/p/15982341.html&#34;&gt;使用 kubebuilder 创建并部署 k8s-operator&lt;/a&gt;&lt;br /&gt;
【04】&lt;a href=&#34;http://tnblog.net/hb/article/details/7516&#34;&gt;Kustomize的基本使用&lt;/a&gt;&lt;br /&gt;
【05】&lt;a href=&#34;https://www.cnblogs.com/lizhewei/p/13214785.html&#34;&gt;【kubebuilder2.0】安装、源码分析 &lt;/a&gt;&lt;br /&gt;
【06】&lt;a href=&#34;https://www.cnblogs.com/alisystemsoftware/p/11580202.html&#34;&gt;深入解析 Kubebuilder：让编写 CRD 变得更简单&lt;/a&gt;&lt;br /&gt;
【07】&lt;a href=&#34;https://os.51cto.com/article/661378.html&#34;&gt;一篇带给你KubeBuilder 简明教程&lt;/a&gt;&lt;br /&gt;
【08】&lt;a href=&#34;https://blog.csdn.net/chenxy02/article/details/125554680&#34;&gt;深入解析Kubebuilder&lt;/a&gt;&lt;br /&gt;
【09】&lt;a href=&#34;https://blog.csdn.net/qq_45874107/article/details/119839187&#34;&gt;什么是RBAC&lt;/a&gt;&lt;br /&gt;
【10】&lt;a href=&#34;https://blog.ihypo.net/15763910382218.html&#34;&gt;Kubernetes Controller Manager 工作原理&lt;/a&gt;&lt;br /&gt;
【11】&lt;a href=&#34;https://jishuin.proginn.com/p/763bfbd3012b&#34;&gt;controller-runtime 之 manager 实现&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(03): 随机调度器</title>
            <link>http://mospany.github.io/2022/12/11/k8s-random-scheduler/</link>
            <pubDate>Sun, 11 Dec 2022 11:51:52 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/12/11/k8s-random-scheduler/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;org3ccc143&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org6748da5&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;pod创建过程&#34;&gt;POD创建过程&lt;/h2&gt;

&lt;p&gt;1、 由kubectl解析创建pod的yaml，发送创建pod请求到APIServer。&lt;br /&gt;
2、 APIServer首先做权限认证，然后检查信息并把数据存储到ETCD里，创建deployment资源初始化。&lt;br /&gt;
3、 kube-controller通过list-watch机制，检查发现新的deployment，将资源加入到内部工作队列，检查到资源没有关联pod和replicaset,然后创建rs资源，rs controller监听到rs创建事件后再创建pod资源。&lt;br /&gt;
4、 scheduler 监听到pod创建事件，执行调度算法，将pod绑定到合适节点，然后告知APIServer更新pod的spec.nodeName&lt;br /&gt;
5、 kubelet 每隔一段时间通过其所在节点的NodeName向APIServer拉取绑定到它的pod清单，并更新本地缓存。&lt;br /&gt;
6、 kubelet发现新的pod属于自己，调用容器API来创建容器，并向APIService上报pod状态。&lt;br /&gt;
7、 Kub-proxy为新创建的pod注册动态DNS到CoreOS。为Service添加iptables/ipvs规则，用于服务发现和负载均衡。&lt;br /&gt;
8、 deploy controller对比pod的当前状态和期望来修正状态。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/scheduler/pod-life-cycle.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org44af008&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;调度器介绍&#34;&gt;调度器介绍&lt;/h2&gt;

&lt;p&gt;从上述流程中，我们能大概清楚kube-scheduler的主要工作，负责整个k8s中pod选择和绑定node的工作，这个选择的过程就是应用调度策略，包括NodeAffinity、PodAffinity、节点资源筛选、调度优先级、公平调度等等，而绑定便就是将pod资源定义里的nodeName进行更新。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org39ad97e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;设计&#34;&gt;设计&lt;/h1&gt;

&lt;p&gt;kube-scheduler的设计有两个历史阶段版本：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;基于谓词（predicate）和优先级（priority）的筛选。&lt;/li&gt;
&lt;li&gt;基于调度框架的调度器，新版本已经把所有的旧的设计都改造成扩展点插件形式(1.19+)。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所谓的谓词和优先级都是对调度算法的分类，在scheduler里，谓词调度算法是来选择出一组能够绑定pod的node，而优先级算法则是在这群node中进行打分，得出一个最高分的node。&lt;/p&gt;

&lt;p&gt;而调度框架的设计相比之前则更复杂一点，但确更加灵活和便于扩展，关于调度框架的设计细节可以查看官方文档——624-scheduling-framework，当然我也有一遍文章对其做了翻译还加了一些便于理解的补充——KEP: 624-scheduling-framework。总结来说调度框架的出现是为了解决以前webhooks扩展器的局限性，一个是扩展点只有：筛选、打分、抢占、绑定，而调度框架则在这之上又细分了11个扩展点；另一个则是通过http调用扩展进程的方式其实效率不高，调度框架的设计用的是静态编译的方式将扩展的程序代码和scheduler源码一起编译成新的scheduler，然后通过scheduler配置文件启用需要的插件，在进程内就能通过函数调用的方式执行插件。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/scheduler/scheduler-startup.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上面是一个简略版的调度器处理pod流程：&lt;/p&gt;

&lt;p&gt;首先scheduler会启动一个client-go的Informer来监听Pod事件（不只Pod其实还有Node等资源变更事件），这时候注册的Informer回调事件会区分Pod是否已经被调度（spec.nodeName），已经调度过的Pod则只是更新调度器缓存，而未被调度的Pod会加入到调度队列，然后经过调度框架执行注册的插件，在绑定周期前会进行Pod的假定动作，从而更新调度器缓存中该Pod状态，最后在绑定周期执行完向ApiServer发起BindAPI，从而完成了一次调度过程。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org8ebc078&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;实现&#34;&gt;实现&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org98f2089&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;创建调度器&#34;&gt;创建调度器&lt;/h2&gt;

&lt;p&gt;1、获取集群kubeconfig配置。&lt;br /&gt;
2、调用client-go生成clientset。&lt;br /&gt;
3、填充调度器相关参数, 包含获取node列表和关注的POD。&lt;br /&gt;
4、返回调度器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; func NewScheduler(podQueue chan *v1.Pod, quit chan struct{}) Scheduler {
  config, err := rest.InClusterConfig()
  if err != nil {
      log.Fatal(err)
  }

  clientset, err := kubernetes.NewForConfig(config)
  if err != nil {
      log.Fatal(err)
  }

  return Scheduler{
      clientset:  clientset,
      podQueue:   podQueue,
      nodeLister: initInformers(clientset, podQueue, quit),
      predicates: []predicateFunc{
          randomPredicate,
      },
      priorities: []priorityFunc{
          randomPriority,
      },
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org7e3d0d6&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;运行调度器&#34;&gt;运行调度器&lt;/h2&gt;

&lt;p&gt;不间断的从关注的POD列表中选出进行调度。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org9082797&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;找合适节点&#34;&gt;找合适节点&lt;/h3&gt;

&lt;p&gt;1、找到可用节点列表&lt;br /&gt;
2、给节点随机打100以内的分数&lt;br /&gt;
3、选择分数最高的节点&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (s *Scheduler) findFit(pod *v1.Pod) (string, error) {
    nodes, err := s.nodeLister.List(labels.Everything())
    if err != nil {
        return &amp;quot;&amp;quot;, err
    }

    filteredNodes := s.runPredicates(nodes, pod)
    if len(filteredNodes) == 0 {
        return &amp;quot;&amp;quot;, errors.New(&amp;quot;failed to find node that fits pod&amp;quot;)
    }
    priorities := s.prioritize(filteredNodes, pod)
    return s.findBestNode(priorities), nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org0354198&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;绑定pod&#34;&gt;绑定POD&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;func (s *Scheduler) bindPod(ctx context.Context, p *v1.Pod, node string) error {
    opts := metav1.CreateOptions{}
    return s.clientset.CoreV1().Pods(p.Namespace).Bind(ctx, &amp;amp;v1.Binding{
        ObjectMeta: metav1.ObjectMeta{
            Name:      p.Name,
            Namespace: p.Namespace,
        },
        Target: v1.ObjectReference{
            APIVersion: &amp;quot;v1&amp;quot;,
            Kind:       &amp;quot;Node&amp;quot;,
            Name:       node,
        },
    }, opts)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgcdd0603&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;发送event事件&#34;&gt;发送event事件&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;func (s *Scheduler) emitEvent(ctx context.Context, p *v1.Pod, message string) error {
    timestamp := time.Now().UTC()
    opts := metav1.CreateOptions{}
    _, err := s.clientset.CoreV1().Events(p.Namespace).Create(ctx, &amp;amp;v1.Event{
        Count:          1,
        Message:        message,
        Reason:         &amp;quot;Scheduled&amp;quot;,
        LastTimestamp:  metav1.NewTime(timestamp),
        FirstTimestamp: metav1.NewTime(timestamp),
        Type:           &amp;quot;Normal&amp;quot;,
        Source: v1.EventSource{
            Component: schedulerName,
        },
        InvolvedObject: v1.ObjectReference{
            Kind:      &amp;quot;Pod&amp;quot;,
            Name:      p.Name,
            Namespace: p.Namespace,
            UID:       p.UID,
        },
        ObjectMeta: metav1.ObjectMeta{
            GenerateName: p.Name + &amp;quot;-&amp;quot;,
        },
    }, opts)
    if err != nil {
        return err
    }
    return nil
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看events信息可以看出random-scheduler打出的“laced pod [default/sleep-5b6fd9944c-5scxv] on k8s-master&amp;rdquo;等信息。&amp;rdquo;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;27s         Normal    Scheduled           pod/sleep-5b6fd9944c-5scxv    Placed pod [default/sleep-5b6fd9944c-5scxv] on k8s-master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org22e94bd&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;验证&#34;&gt;验证&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org7cc5355&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;编译&#34;&gt;编译&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ make docker-image
$ make docker-push
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgede963d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;部署&#34;&gt;部署&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f rbac.yaml
$ kubectl  apply -f deployment.yaml
$ kubectl apply -f sleep.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把sleep.yaml里改为schedulerName: random-scheduler就可以使用该调度器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deployment]# kubectl  get pod -A -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS       AGE     IP               NODE         NOMINATED NODE   READINESS GATES
default       httpbin-master                       1/1     Running   2 (11h ago)    3d22h   10.244.0.36      k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       httpbin-worker                       1/1     Running   2 (11h ago)    3d22h   10.244.2.15      k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       netshoot-master                      1/1     Running   2 (11h ago)    3d22h   10.244.0.35      k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       netshoot-worker                      1/1     Running   2 (11h ago)    3d22h   10.244.2.14      k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       random-scheduler-6dc78999cc-vnxzg    1/1     Running   0              9m      10.244.0.37      k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       sleep-5b6fd9944c-7rn5m               1/1     Running   0              11h     10.244.1.6       k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       sleep-5b6fd9944c-bwb9t               1/1     Running   0              11h     10.244.0.38      k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-6d8c4cb4d-ck2x5              1/1     Running   20 (11h ago)   19d     10.244.0.34      k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-6d8c4cb4d-mbctj              1/1     Running   20 (11h ago)   19d     10.244.0.33      k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   etcd-k8s-master                      1/1     Running   22 (11h ago)   19d     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-apiserver-k8s-master            1/1     Running   24 (11h ago)   19d     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-controller-manager-k8s-master   1/1     Running   22 (11h ago)   19d     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-dnsjg                     1/1     Running   21 (11h ago)   19d     172.25.140.215   k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-r84lg                     1/1     Running   22 (11h ago)   19d     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-tbkx2                     1/1     Running   20 (11h ago)   19d     172.25.140.214   k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-scheduler-k8s-master            1/1     Running   22 (11h ago)   19d     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   minicni-node-2xq2d                   1/1     Running   2 (11h ago)    3d23h   172.25.140.214   k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   minicni-node-dsq8c                   1/1     Running   2 (11h ago)    3d23h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   minicni-node-h8hm8                   1/1     Running   2 (11h ago)    3d23h   172.25.140.215   k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出, random-scheduler-6dc78999cc-vnxzg 和 sleep pod已正常变成Running状态。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgdc135cc&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;验证-1&#34;&gt;验证&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deployment]# kubectl  logs random-scheduler-6dc78999cc-vnxzg
 I&#39;m a scheduler!
 2022/12/13 12:12:02 New Node Added to Store: k8s-master
 2022/12/13 12:12:02 New Node Added to Store: k8s-work1
 2022/12/13 12:12:02 New Node Added to Store: k8s-work2
 found a pod to schedule: default / sleep-5b6fd9944c-bwb9t
 2022/12/13 12:12:02 nodes that fit:
 2022/12/13 12:12:02 k8s-master
 2022/12/13 12:12:02 k8s-work1
 2022/12/13 12:12:02 k8s-work2
 2022/12/13 12:12:02 calculated priorities: map[k8s-master:79 k8s-work1:68 k8s-work2:15]
 Placed pod [default/sleep-5b6fd9944c-bwb9t] on k8s-master

 found a pod to schedule: default / sleep-5b6fd9944c-7rn5m
 2022/12/13 12:12:02 nodes that fit:
 2022/12/13 12:12:02 k8s-master
 2022/12/13 12:12:02 k8s-work1
 2022/12/13 12:12:02 calculated priorities: map[k8s-master:26 k8s-work1:50]
 Placed pod [default/sleep-5b6fd9944c-7rn5m] on k8s-work1

 [root@k8s-master deployment]# kubectl logs sleep-5b6fd9944c-7rn5m
 [root@k8s-master deployment]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从random-scheduler日志看， sleep容器经过random-scheduler进行调度的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deployment]# k get events
 - LAST SEEN   TYPE      REASON              OBJECT                        MESSAGE
 39m         Normal    Pulled              pod/netshoot-master           Container image &amp;quot;nicolaka/netshoot:latest&amp;quot; already present on machine
 39m         Normal    Created             pod/netshoot-master           Created container centos
 39m         Normal    Started             pod/netshoot-master           Started container centos
 39m         Normal    Pulled              pod/netshoot-worker           Container image &amp;quot;nicolaka/netshoot:latest&amp;quot; already present on machine
 39m         Normal    Created             pod/netshoot-worker           Created container centos
 39m         Normal    Started             pod/netshoot-worker           Started container centos
 27s         Normal    Scheduled           pod/sleep-5b6fd9944c-5scxv    Placed pod [default/sleep-5b6fd9944c-5scxv] on k8s-master
 26s         Normal    Pulled              pod/sleep-5b6fd9944c-5scxv    Container image &amp;quot;tutum/curl&amp;quot; already present on machine
 26s         Normal    Created             pod/sleep-5b6fd9944c-5scxv    Created container sleep
 26s         Normal    Started             pod/sleep-5b6fd9944c-5scxv    Started container sleep
 50s         Normal    Killing             pod/sleep-5b6fd9944c-7rn5m    Stopping container sleep
 50s         Normal    Killing             pod/sleep-5b6fd9944c-bwb9t    Stopping container sleep
 27s         Normal    Scheduled           pod/sleep-5b6fd9944c-rccmj    Placed pod [default/sleep-5b6fd9944c-rccmj] on k8s-work2
 26s         Normal    Pulling             pod/sleep-5b6fd9944c-rccmj    Pulling image &amp;quot;tutum/curl&amp;quot;
 27s         Normal    SuccessfulCreate    replicaset/sleep-5b6fd9944c   Created pod: sleep-5b6fd9944c-rccmj
 27s         Normal    SuccessfulCreate    replicaset/sleep-5b6fd9944c   Created pod: sleep-5b6fd9944c-5scxv
 27s         Normal    ScalingReplicaSet   deployment/sleep              Scaled up replica set sleep-5b6fd9944c to 2
 [root@k8s-master deployment]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看events信息可以看出random-scheduler打出的“laced pod [default/sleep-5b6fd9944c-5scxv] on k8s-master&amp;rdquo;等信息。&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org008f2a0&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;p&gt;[01] &lt;a href=&#34;https://www.cnblogs.com/z-gh/p/15409763.html&#34;&gt;k8s调度器介绍（调度框架版本）&lt;/a&gt;&lt;br /&gt;
[02] &lt;a href=&#34;https://zhuanlan.zhihu.com/p/400351590&#34;&gt;client-go功能详解&lt;/a&gt;&lt;br /&gt;
[03] &lt;a href=&#34;https://www.jb51.net/article/253965.htm&#34;&gt;一篇文章搞懂Go语言中的Context&lt;/a&gt;]&lt;br /&gt;
[04] &lt;a href=&#34;https://www.cnblogs.com/yangyuliufeng/p/13611126.html&#34;&gt;深入理解k8s中的informer机制&lt;/a&gt;]&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(02): 动手实现minicni</title>
            <link>http://mospany.github.io/2022/11/22/k8s-practice-minicni/</link>
            <pubDate>Tue, 22 Nov 2022 22:42:55 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/11/22/k8s-practice-minicni/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;orgfaa973a&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;不管是容器网络还是 Kubernetes 网络都需要解决以下两个核心问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;容器/Pod IP 地址的管理&lt;/li&gt;
&lt;li&gt;容器/Pod 之间的相互通信&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;容器/Pod IP 地址的管理包括容器 IP 地址的分配与回收，而容器/Pod 之间的相互通信包括同一主机的容器/Pod 之间和跨主机的容器/Pod 之间通信两种场景。这两个问题也不能完全分开来看，因为不同的解决方案往往要同时考虑以上两点。对于同一主机的容器/Pod 之间的通信来说实现相对容易，实际的挑战在于，不同容器/Pod 完全可能分布在不同的集群节点上，如何实现跨主机节点的通信不是一件容易的事情。&lt;/p&gt;

&lt;p&gt;如果不采用 SDN(Software define networking) 方式来修改底层网络设备的配置，主流方案是在主机节点的 underlay 网络平面构建新的 overlay 网络负责传输容器/Pod 之间通信数据。这种网络方案在如何复用原有的 underlay 网络平面也有不同的实现方式：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;将容器的数据包封装到原主机网络（underlay 网络平面）的三层或四层数据包中，然后使用主机网络的三层或者四层协议传输到目标主机，目标主机拆包后再转发给目标容器；&lt;/li&gt;
&lt;li&gt;把容器网络加到主机路由表中，把主机网络（underlay 网络平面）设备当作容器网关，通过路由规则转发到指定的主机，实现容器的三层互通；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org3709b23&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;cni原理&#34;&gt;CNI原理&lt;/h1&gt;

&lt;p&gt;CNI 规范相对于 CNM(Container Network Model) 对开发者的约束更少、更开放，不依赖于容器运行时，因此也更简单。关于 CNI 规范的详情请查看&lt;a href=&#34;https://github.com/containernetworking/cni/blob/master/SPEC.md&#34;&gt;官方文档&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/minicni/cni-standard.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;详见: &lt;a href=&#34;https://blog.csdn.net/elihe2011/article/details/122926399&#34;&gt;K8S 网络CNI&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;实现一个 CNI 网络插件只需要一个配置文件和一个可执行文件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;配置文件描述插件的版本、名称、描述等基本信息；&lt;/li&gt;
&lt;li&gt;可执行文件会被上层的容器管理平台调用，一个 CNI 可执行文件需要实现将容器加入到网络的 ADD 操作以及将容器从网络中删除的 DEL 操作等；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubernetes 使用 CNI 网络插件的基本工作流程是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kubelet 先创建 pause 容器创建对应的网络命名空间；&lt;/li&gt;
&lt;li&gt;根据配置调用具体的 CNI 插件，可以配置成 CNI 插件链来进行链式调用；&lt;/li&gt;
&lt;li&gt;当 CNI 插件被调用时，它根据环境变量以及命令行参数来获得网络命名空间、容器的网络设备等必要信息，然后执行 ADD 或者其他操作；&lt;/li&gt;
&lt;li&gt;CNI 插件给 pause 容器配置正确的网络，pod 中其他的容器都是复用 pause 容器的网络；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orgec132b9&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;cni配置文件&#34;&gt;CNI配置文件&lt;/h1&gt;

&lt;p&gt;现在我们来到关键的部分。一般来说，CNI 插件需要在集群的每个节点上运行，在 CNI 的规范里面，实现一个 CNI 插件首先需要一个 JSON 格式的配置文件，配置文件需要放到每个节点的 &lt;em&gt;etc/cni/net.d&lt;/em&gt; 目录，一般命名为 &amp;lt;数字&amp;gt;-&lt;CNI-plugin&gt;.conf，而且配置文件至少需要以下几个必须的字段：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;cniVersion: CNI 插件的字符串版本号，要求符合 Semantic Version 2.0 规范；&lt;/li&gt;
&lt;li&gt;name: 字符串形式的网络名；&lt;/li&gt;

&lt;li&gt;&lt;p&gt;type: 字符串表示的 CNI 插件的可运行文件；&lt;br /&gt;
除此之外，我们也可以增加一些自定义的配置字段，用于传递参数给 CNI 插件，这些配置会在运行时传递给 CNI 插件。在我们的例子里面，需要配置每个宿主机网桥的设备名、网络设备的最大传输单元(MTU)以及每个节点分配的24位子网地址，因此，我们的 CNI 插件的配置看起来会像下面这样：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  {
    &amp;quot;cniVersion&amp;quot;: &amp;quot;0.1.0&amp;quot;,
    &amp;quot;name&amp;quot;: &amp;quot;minicni&amp;quot;,
    &amp;quot;type&amp;quot;: &amp;quot;minicni&amp;quot;,
    &amp;quot;bridge&amp;quot;: &amp;quot;minicni0&amp;quot;,
    &amp;quot;mtu&amp;quot;: 1500,
    &amp;quot;subnet&amp;quot;: __NODE_SUBNET__
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: 确保配置文件放到 &lt;em&gt;etc/cni/net.d&lt;/em&gt; 目录，kubelet 默认此目录寻找 CNI 插件配置；并且，插件的配置可以分为多个插件链的形式来运行，但是为了简单起见，在我们的例子中，只配置一个独立的 CNI 插件，因为配置文件的后缀名为 .conf。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a id=&#34;orgeb0c3b0&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;cni核心实现&#34;&gt;CNI核心实现&lt;/h1&gt;

&lt;p&gt;接下来就开始看怎么实现 CNI 插件来管理 pod IP 地址以及配置容器网络设备。在此之前，我们需要明确的是，CNI 介入的时机是 kubelet 创建 pause 容器创建对应的网络命名空间之后，同时当 CNI 插件被调用的时候，kubelet 会将相关操作命令以及参数通过环境变量的形式传递给它。这些环境变量包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CNI_COMMAND: CNI 操作命令，包括 ADD, DEL, CHECK 以及 VERSION&lt;/li&gt;
&lt;li&gt;CNI_CONTAINERID: 容器 ID&lt;/li&gt;
&lt;li&gt;CNI_NETNS: pod 网络命名空间&lt;/li&gt;
&lt;li&gt;CNI_IFNAME: pod 网络设备名称&lt;/li&gt;
&lt;li&gt;CNI_PATH: CNI 插件可执行文件的搜索路径&lt;/li&gt;
&lt;li&gt;CNI_ARGS: 可选的其他参数，形式类似于 key1=value1,key2=value2&amp;#x2026;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在运行时，kubelet 通过 CNI 配置文件寻找 CNI 可执行文件，然后基于上述几个环境变量来执行相关的操作。CNI 插件必须支持的操作包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ADD: 将 pod 加入到 pod 网络中&lt;/li&gt;
&lt;li&gt;DEL: 将 pod 从 pod 网络中删除&lt;/li&gt;
&lt;li&gt;CHECK: 检查 pod 网络配置正常&lt;/li&gt;
&lt;li&gt;VERSION: 返回可选 CNI 插件的版本信息&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;让我们直接跳到 CNI 插件的入口函数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    setupLogger()

    cmd, cmdArgs, err := args.GetArgsFromEnv()
    if err != nil {
        log.Fatalf(&amp;quot;getting cmd arguments with error: %v&amp;quot;, err)
        os.Exit(1)
    }

    fh := handler.NewFileHandler(IPStore)

    switch cmd {
    case &amp;quot;ADD&amp;quot;:
        err = fh.HandleAdd(cmdArgs)
    case &amp;quot;DEL&amp;quot;:
        err = fh.HandleDel(cmdArgs)
    case &amp;quot;CHECK&amp;quot;:
        err = fh.HandleCheck(cmdArgs)
    case &amp;quot;VERSION&amp;quot;:
        err = fh.HandleVersion(cmdArgs)
    default:
        err = fmt.Errorf(&amp;quot;unknown CNI_COMMAND: %s&amp;quot;, cmd)
    }
    if err != nil {
        fmt.Fprintf(os.Stderr, &amp;quot;Failed to handle CNI_COMMAND %q: %v&amp;quot;, cmd, err)
        os.Exit(1)
    }
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，我们首先调用 GetArgsFromEnv() 函数将 CNI 插件的操作命令以及相关参数通过环境变量读入，同时从标准输入获取 CNI 插件的 JSON 配置，然后基于不同的 CNI 操作命令执行不同的处理函数。&lt;/p&gt;

&lt;p&gt;需要注意的是，我们将处理函数的集合实现为一个接口，这样就可以很容易的扩展不同的接口实现。在最基础的版本实现中，我们基本文件存储分配的 IP 信息。但是，这种实现方式存在很多问题，例如，文件存储不可靠，读写可能会发生冲突等，在后续的版本中，我们会实现基于 kubernetes 存储的接口实现，将子网信息以及 IP 信息存储到 apiserver 中，从而实现可靠存储。&lt;/p&gt;

&lt;p&gt;接下来，我们就看看基于文件的接口实现是怎么处理这些 CNI 操作命令的。&lt;br /&gt;
对于 ADD 命令：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;从标准输入获取 CNI 插件的配置信息，最重要的是当前宿主机网桥的设备名、网络设备的最大传输单元(MTU)以及当前节点分配的24位子网地址；&lt;/li&gt;
&lt;li&gt;然后从环境变量中找到对应的 CNI 操作参数，包括 pod 容器网络命名空间以及 pod 网络设备名等；&lt;/li&gt;
&lt;li&gt;接下来创建或者更新节点宿主机网桥，从当前节点分配的24位子网地址中抽取子网的网关地址，准备分配给节点宿主机网桥；&lt;/li&gt;
&lt;li&gt;接着将从文件读取已经分配的 IP 地址列表，遍历24位子网地址并从中取出第一个没有被分配的 IP 地址信息，准备分配给 pod 网络设备；pod 网络设备是 veth 设备对，一端在 pod 网络命名空间中，另外一端连接着宿主机上的网桥设备，同时所有的 pod 网络设备将宿主机上的网桥设备当作默认网关；&lt;/li&gt;
&lt;li&gt;最终成功后需要将新的 pod IP 写入到文件中。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;看起来很简单对吧？其实作为最简单的方式，这种方案可以实现最基础的 ADD 功能, kubelet调用参数如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  2022/12/09 20:17:14 cmd: ADD, ContainerID: 504032948df53a9f7bc7c35d01fb89f32b8db7a1d87514f546b78b91d2d8150a, Netns: /proc/3444/ns/net, ifName: eth0, Path: /opt/cni/bin, Args: IgnoreUnknown=1;K8S_POD_NAMESPACE=kube-system;K8S_POD_NAME=coredns-6d8c4cb4d-mbctj;K8S_POD_INFRA_CONTAINER_ID=504032948df53a9f7bc7c35d01fb89f32b8db7a1d87514f546b78b91d2d8150a, StdinData: {&amp;quot;bridge&amp;quot;:&amp;quot;minicni0&amp;quot;,&amp;quot;cniVersion&amp;quot;:&amp;quot;0.1.0&amp;quot;,&amp;quot;mtu&amp;quot;:1500,&amp;quot;name&amp;quot;:&amp;quot;minicni&amp;quot;,&amp;quot;subnet&amp;quot;:&amp;quot;10.244.0.0/24&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;minicni&amp;quot;}.

2022/12/09 20:17:14 cmd: ADD, ContainerID: 84546b1f14b339f69528d96b60e3b50a983e024daf00a6ef990819d1717aaff7, Netns: /proc/3491/ns/net, ifName: eth0, Path: /opt/cni/bin, Args: IgnoreUnknown=1;K8S_POD_NAMESPACE=kube-system;K8S_POD_NAME=coredns-6d8c4cb4d-ck2x5;K8S_POD_INFRA_CONTAINER_ID=84546b1f14b339f69528d96b60e3b50a983e024daf00a6ef990819d1717aaff7, StdinData: {&amp;quot;bridge&amp;quot;:&amp;quot;minicni0&amp;quot;,&amp;quot;cniVersion&amp;quot;:&amp;quot;0.1.0&amp;quot;,&amp;quot;mtu&amp;quot;:1500,&amp;quot;name&amp;quot;:&amp;quot;minicni&amp;quot;,&amp;quot;subnet&amp;quot;:&amp;quot;10.244.0.0/24&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;minicni&amp;quot;}.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现代码如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  func (fh *FileHandler) HandleAdd(cmdArgs *args.CmdArgs) error {
 2    cniConfig := args.CNIConfiguration{}
 3    if err := json.Unmarshal(cmdArgs.StdinData, &amp;amp;cniConfig); err != nil {
 4        return err
 5    }
 6    allIPs, err := nettool.GetAllIPs(cniConfig.Subnet)
 7    if err != nil {
 8        return err
 9    }
10    gwIP := allIPs[0]
11  
12    // open or create the file that stores all the reserved IPs
13    f, err := os.OpenFile(fh.IPStore, os.O_RDWR|os.O_CREATE, 0600)
14    if err != nil {
15        return fmt.Errorf(&amp;quot;failed to open file that stores reserved IPs %v&amp;quot;, err)
16    }
17    defer f.Close()
18  
19    // get all the reserved IPs from file
20    content, err := ioutil.ReadAll(f)
21    if err != nil {
22        return err
23    }
24    reservedIPs := strings.Split(strings.TrimSpace(string(content)), &amp;quot;\n&amp;quot;)
25  
26    podIP := &amp;quot;&amp;quot;
27    for _, ip := range allIPs[1:] {
28        reserved := false
29        for _, rip := range reservedIPs {
30            if ip == rip {
31                reserved = true
32                break
33            }
34        }
35        if !reserved {
36            podIP = ip
37            reservedIPs = append(reservedIPs, podIP)
38            break
39        }
40    }
41    if podIP == &amp;quot;&amp;quot; {
42        return fmt.Errorf(&amp;quot;no IP available&amp;quot;)
43    }
44  
45    // Create or update bridge
46    brName := cniConfig.Bridge
47    if brName != &amp;quot;&amp;quot; {
48        // fall back to default bridge name: minicni0
49        brName = &amp;quot;minicni0&amp;quot;
50    }
51    mtu := cniConfig.MTU
52    if mtu == 0 {
53        // fall back to default MTU: 1500
54        mtu = 1500
55    }
56    br, err := nettool.CreateOrUpdateBridge(brName, gwIP, mtu)
57    if err != nil {
58        return err
59    }
60  
61    netns, err := ns.GetNS(cmdArgs.Netns)
62    if err != nil {
63        return err
64    }
65  
66    if err := nettool.SetupVeth(netns, br, cmdArgs.IfName, podIP, gwIP, mtu); err != nil {
67        return err
68    }
69  
70    // write reserved IPs back into file
71    if err := ioutil.WriteFile(fh.IPStore, []byte(strings.Join(reservedIPs, &amp;quot;\n&amp;quot;)), 0600); err != nil {
72        return fmt.Errorf(&amp;quot;failed to write reserved IPs into file: %v&amp;quot;, err)
73    }
74  
75    addCmdResult := &amp;amp;AddCmdResult{
76        CniVersion: cniConfig.CniVersion,
77        IPs: &amp;amp;nettool.AllocatedIP{
78            Version: &amp;quot;IPv4&amp;quot;,
79            Address: podIP,
80            Gateway: gwIP,
81        },
82    }
83    addCmdResultBytes, err := json.Marshal(addCmdResult)
84    if err != nil {
85        return err
86    }
87  
88    // kubelet expects json format from stdout if success
89    fmt.Print(string(addCmdResultBytes))
90  
91    return nil
92  }
93  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个关键的问题是如何选择合适的 Go 语言库函数来操作 Linux 网络设备，如创建网桥设备、网络命名空间以及连接 veth 设备对。在我们的例子中，选择了比较成熟的 netlink，实际上，所有基于 iproute2 工具包的命令在 netlink 库中都有对应的 API，例如 ip link add 可以通过调用 AddLink() 函数来实现。&lt;/p&gt;

&lt;p&gt;还有一个问题需要格外小心，那就是处理网络命名空间切换、Go 协程与线程调度问题。在 Linux 中，不同的操作系统线程可能会设置不同的网络命名空间，而 Go 语言的协程会基于操作系统线程的负载以及其他信息动态地在不同的操作系统线程之间切换，这样可能会导致 Go 协程在意想不到的情况下切换到不同的网络命名空间中。&lt;/p&gt;

&lt;p&gt;比较稳妥的做法是，利用 Go 语言提供的 runtime.LockOSThread() 函数保证特定的 Go 协程绑定到当前的操作系统线程中。&lt;/p&gt;

&lt;p&gt;对于 ADD 操作的返回，确保操作成功之后向标准输出中写入 ADD 操作的返回信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    addCmdResult := &amp;amp;AddCmdResult{
    CniVersion: cniConfig.CniVersion,
    IPs: &amp;amp;nettool.AllocatedIP{
        Version: &amp;quot;IPv4&amp;quot;,
        Address: podIP,
        Gateway: gwIP,
    },
}
addCmdResultBytes, err := json.Marshal(addCmdResult)
if err != nil {
    return err
}

// kubelet expects json format from stdout if success
fmt.Print(string(addCmdResultBytes))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;保留IP文件内容如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master ~]# cat /tmp/reserved_ips
 2  10.244.0.2/24
 3  10.244.0.3/24
 4  10.244.0.4/24
 5  10.244.0.5/24
 6  10.244.0.6/24
 7  10.244.0.7/24
 8  10.244.0.8/24
 9  10.244.0.9/24
10  10.244.0.10/24
11  10.244.0.11/24
12  10.244.0.12/24
13  10.244.0.13/24
14  10.244.0.14/24
15  10.244.0.15/24
16  10.244.0.16/24
17  10.244.0.17/24
18  10.244.0.18/24
19  10.244.0.19/24
20  10.244.0.20/24
21  10.244.0.21/24
22  10.244.0.22/24
23  10.244.0.23/24
24  10.244.0.24/24
25  10.244.0.25/24
26  10.244.0.26/24
27  10.244.0.27/24  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其他三个 CNI 操作命令的处理就更简单了。DEL 操作只需要回收分配的 IP 地址，从文件中删除对应的条目，我们不需要处理 pod 网络设备的删除，原因是 kubelet 在删除 pod 网络命名空间之后这些 pod 网络设备也会自动被删除；CHECK 命令检查之前创建的网络设备与配置，暂时是可选的；VERSION 命令以 JSON 形式输出 CNI 版本信息到标准输出。&lt;br /&gt;
CNI_DEL命令参数如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2022/12/09 20:16:55 cmd: DEL, ContainerID: 88722baed9e508ab6cc4525a6b0b05da12c65efa1bbf7c1e27f0fee0c4b4f7e7, Netns: , ifName: eth0, Path: /opt/cni/bin, Args: IgnoreUnknown=1;K8S_POD_NAMESPACE=kube-system;K8S_POD_NAME=coredns-6d8c4cb4d-ck2x5;K8S_POD_INFRA_CONTAINER_ID=88722baed9e508ab6cc4525a6b0b05da12c65efa1bbf7c1e27f0fee0c4b4f7e7, StdinData: {&amp;quot;bridge&amp;quot;:&amp;quot;minicni0&amp;quot;,&amp;quot;cniVersion&amp;quot;:&amp;quot;0.1.0&amp;quot;,&amp;quot;mtu&amp;quot;:1500,&amp;quot;name&amp;quot;:&amp;quot;minicni&amp;quot;,&amp;quot;subnet&amp;quot;:&amp;quot;10.244.0.0/24&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;minicni&amp;quot;}. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;删除操作实现代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1   func (fh *FileHandler) HandleDel(cmdArgs *args.CmdArgs) error {
 2      netns, err := ns.GetNS(cmdArgs.Netns)
 3      if err != nil {
 4          return err
 5      }
 6      ip, err := nettool.GetVethIPInNS(netns, cmdArgs.IfName)
 7      if err != nil {
 8          return err
 9      }
10  
11      // open or create the file that stores all the reserved IPs
12      f, err := os.OpenFile(fh.IPStore, os.O_RDWR|os.O_CREATE, 0600)
13      if err != nil {
14          return fmt.Errorf(&amp;quot;failed to open file that stores reserved IPs %v&amp;quot;, err)
15      }
16      defer f.Close()
17  
18      // get all the reserved IPs from file
19      content, err := ioutil.ReadAll(f)
20      if err != nil {
21          return err
22      }
23      reservedIPs := strings.Split(strings.TrimSpace(string(content)), &amp;quot;\n&amp;quot;)
24  
25      for i, rip := range reservedIPs {
26          if rip == ip {
27              reservedIPs = append(reservedIPs[:i], reservedIPs[i+1:]...)
28              break
29          }
30      }
31  
32      // write reserved IPs back into file
33      if err := ioutil.WriteFile(fh.IPStore, []byte(strings.Join(reservedIPs, &amp;quot;\n&amp;quot;)), 0600); err != nil {
34          return fmt.Errorf(&amp;quot;failed to write reserved IPs into file: %v&amp;quot;, err)
35      }
36  
37      return nil
38  }
39  
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;CNI_VERSION参数如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2022/12/09 20:18:24 cmd: VERSION, ContainerID: , Netns: dummy, ifName: dummy, Path: dummy, Args: , StdinData: {&amp;quot;cniVersion&amp;quot;:&amp;quot;0.4.0&amp;quot;}. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现代码如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1  func (fh *FileHandler) HandleVersion(cmdArgs *args.CmdArgs) error {
2     versionInfo, err := json.Marshal(fh.VersionInfo)
3     if err != nil {
4         return err
5     }
6     fmt.Print(string(versionInfo))
7     return nil
8  }
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orgb4b32d9&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;cni-安装工具&#34;&gt;CNI 安装工具&lt;/h1&gt;

&lt;p&gt;CNI 插件需要运行在集群中的每个节点上，而且 CNI 插件配置信息与可运行文件必须在每个节点特殊的目录中，因此，安装 CNI 插件非常适合使用 DaemonSet 并挂载 CNI 插件目录，为了避免安装 CNI 的工具不能被正常调度，我们需要使用 hostNetwork 来使用宿主机的网络。同时，将 CNI 插件配置以 ConfigMap 的形式挂载，这样方便终端用户配置 CNI 插件。更详细的信息请查看安装工具部署文件。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# k describe node k8s-master  | grep CIDR
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外需要注意的是，我们在安装 CNI 插件的脚本中获取每个节点划分得到的24子网信息、检查是否合法然后写入到 CNI 配置信息中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  # The environment variables used to connect to the kube-apiserver
 2   SERVICE_ACCOUNT_PATH=/var/run/secrets/kubernetes.io/serviceaccount
 3   SERVICEACCOUNT_TOKEN=$(cat $SERVICE_ACCOUNT_PATH/token)
 4   KUBE_CACERT=${KUBE_CACERT:-$SERVICE_ACCOUNT_PATH/ca.crt}
 5   KUBERNETES_SERVICE_PROTOCOL=${KUBERNETES_SERVICE_PROTOCOL-https}
 6  
 7   # Check if we&#39;re running as a k8s pod.
 8   if [ -f &amp;quot;$SERVICE_ACCOUNT_PATH/token&amp;quot; ];
 9   then
10       # some variables should be automatically set inside a pod
11       if [ -z &amp;quot;${KUBERNETES_SERVICE_HOST}&amp;quot; ]; then
12           exit_with_message &amp;quot;KUBERNETES_SERVICE_HOST not set&amp;quot;
13       fi
14       if [ -z &amp;quot;${KUBERNETES_SERVICE_PORT}&amp;quot; ]; then
15           exit_with_message &amp;quot;KUBERNETES_SERVICE_PORT not set&amp;quot;
16       fi
17   fi
18  
19   # exit if the NODE_NAME environment variable is not set.
20   if [[ -z &amp;quot;${NODE_NAME}&amp;quot; ]];
21   then
22       exit_with_message &amp;quot;NODE_NAME not set.&amp;quot;
23   fi
24  
25  
26   NODE_RESOURCE_PATH=&amp;quot;${KUBERNETES_SERVICE_PROTOCOL}://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}/api/v1/nodes/${NODE_NAME}&amp;quot;
27   NODE_SUBNET=$(curl --cacert &amp;quot;${KUBE_CACERT}&amp;quot; --header &amp;quot;Authorization: Bearer ${SERVICEACCOUNT_TOKEN}&amp;quot; -X GET &amp;quot;${NODE_RESOURCE_PATH}&amp;quot; | jq &amp;quot;.spec.podCIDR&amp;quot;)
28  
29   # Check if the node subnet is valid IPv4 CIDR address
30   IPV4_CIDR_REGEX=&amp;quot;(((25[0-5]|2[0-4][0-9]|1?[0-9][0-9]?)\.){3}(25[0-5]|2[0-4][0-9]|1?[0-9][0-9]?))(\/([8-9]|[1-2][0-9]|3[0-2]))([^0-9.]|$)&amp;quot;
31   if [[ ${NODE_SUBNET} =~ ${IPV4_CIDR_REGEX} ]]
32   then
33       echo &amp;quot;${NODE_SUBNET} is a valid IPv4 CIDR address.&amp;quot;
34   else
35       exit_with_message &amp;quot;${NODE_SUBNET} is not a valid IPv4 CIDR address!&amp;quot;
36   fi
37  
38   # exit if the NODE_NAME environment variable is not set.
39   if [[ -z &amp;quot;${CNI_NETWORK_CONFIG}&amp;quot; ]];
40   then
41       exit_with_message &amp;quot;CNI_NETWORK_CONFIG not set.&amp;quot;
42   fi
43  
44   TMP_CONF=&#39;/minicni.conf.tmp&#39;
45   cat &amp;gt;&amp;quot;${TMP_CONF}&amp;quot; &amp;lt;&amp;lt;EOF
46   ${CNI_NETWORK_CONFIG}
47   EOF
48  
49   # Replace the __NODE_SUBNET__
50   grep &amp;quot;__NODE_SUBNET__&amp;quot; &amp;quot;${TMP_CONF}&amp;quot; &amp;amp;&amp;amp; sed -i s~__NODE_SUBNET__~&amp;quot;${NODE_SUBNET}&amp;quot;~g &amp;quot;${TMP_CONF}&amp;quot;
51  
52   # Log the config file
53   echo &amp;quot;CNI config: $(cat &amp;quot;${TMP_CONF}&amp;quot;)&amp;quot;
54  
55   # Move the temporary CNI config into the CNI configuration directory.
56   mv &amp;quot;${TMP_CONF}&amp;quot; &amp;quot;${CNI_NET_DIR}/${CNI_CONF_NAME}&amp;quot; || \
57     exit_with_error &amp;quot;Failed to move ${TMP_CONF} to ${CNI_CONF_NAME}.&amp;quot;
58  
59   echo &amp;quot;Created CNI config ${CNI_CONF_NAME}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org9970dc4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;编译部署&#34;&gt;编译部署&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orgd1c3c68&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-编译&#34;&gt;1、编译&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;make build
make image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/minicni/make-image-minicni.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在docker hub里已看到了minicni镜像：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/minicni/docker-hub-minicni.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org4e72f27&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;部署&#34;&gt;部署&lt;/h2&gt;

&lt;p&gt;1、登录已安装好的k8s集群，把之前已存在的cni如(calico)卸载掉再安装minici:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# kubectl  apply -f minicni.yaml
clusterrole.rbac.authorization.k8s.io/minicni created
serviceaccount/minicni created
clusterrolebinding.rbac.authorization.k8s.io/minicni created
configmap/minicni-config created
Warning: spec.template.spec.nodeSelector[beta.kubernetes.io/os]: deprecated since v1.14; use &amp;quot;kubernetes.io/os&amp;quot; instead
Warning: spec.template.metadata.annotations[scheduler.alpha.kubernetes.io/critical-pod]: non-functional in v1.16+; use the &amp;quot;priorityClassName&amp;quot; field instead
daemonset.apps/minicni-node created
[root@k8s-master minicni]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、查看minicni部署状态：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# kubectl get pod -A -o wide
 NAMESPACE     NAME                                       READY   STATUS        RESTARTS        AGE     IP               NODE         NOMINATED NODE   READINESS GATES
 default       nginx-85b98978db-qkd6h                     0/1     Completed     5               4d21h   &amp;lt;none&amp;gt;           k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   calico-kube-controllers-7b8458594b-p2fqj   0/1     Terminating   2               4d12h   &amp;lt;none&amp;gt;           k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   coredns-6d8c4cb4d-ck2x5                    0/1     Completed     7               4d22h   &amp;lt;none&amp;gt;           k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   coredns-6d8c4cb4d-mbctj                    0/1     Completed     7               4d22h   &amp;lt;none&amp;gt;           k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   etcd-k8s-master                            1/1     Running       8 (8m14s ago)   4d22h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   kube-apiserver-k8s-master                  1/1     Running       8 (8m4s ago)    4d22h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   kube-controller-manager-k8s-master         1/1     Running       8 (8m14s ago)   4d22h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   kube-proxy-dnsjg                           1/1     Running       8 (8m14s ago)   4d21h   172.25.140.215   k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   kube-proxy-r84lg                           1/1     Running       8 (8m14s ago)   4d22h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   kube-proxy-tbkx2                           1/1     Running       7 (8m14s ago)   4d21h   172.25.140.214   k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   kube-scheduler-k8s-master                  1/1     Running       8 (8m14s ago)   4d22h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   minicni-node-8lmxc                         1/1     Running       0               5m17s   172.25.140.214   k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   minicni-node-sgjmg                         1/1     Running       0               5m17s   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   minicni-node-xslgx                         1/1     Running       0               5m17s   172.25.140.215   k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出minicni处于Running状态。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org370284e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;测试&#34;&gt;测试&lt;/h1&gt;

&lt;p&gt;1、环境准备, 分别给node打上相应的标签&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# k label nodes k8s-master role=master
node/k8s-master labeled
[root@k8s-master minicni]# k label nodes k8s-work1 role=worker
node/k8s-work1 labeled
[root@k8s-master minicni]# k label nodes k8s-work2 role=worker
node/k8s-work2 labeled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、分别在 master 与 worker 节点部署 netshoot 与 httpbin：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# k apply -f test-pods.yaml
pod/httpbin-master created
pod/netshoot-master created
pod/httpbin-worker created
pod/netshoot-worker created
[root@k8s-master minicni]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、确保所有 pod 都启动并开始运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# k get pod
NAME                     READY   STATUS              RESTARTS   AGE
httpbin-master           0/1     ContainerCreating   0          8s
httpbin-worker           0/1     ContainerCreating   0          8s
netshoot-master          0/1     ContainerCreating   0          8s
netshoot-worker          0/1     ContainerCreating   0          8s
[root@k8s-master minicni]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;发现状态为ContainerCreating, 查看pod描述报错为:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Warning  FailedCreatePodSandBox  36s                kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container &amp;quot;7cb17eace85729539db7d7af1a4955a353a14eb7ffa96f84c117ee6633b3e2b5&amp;quot; network for pod &amp;quot;httpbin-master&amp;quot;: networkPlugin cni failed to set up pod &amp;quot;httpbin-master_default&amp;quot; network: error getting ClusterInformation: connection is unauthorized: Unauthorized, failed to clean up sandbox container &amp;quot;7cb17eace85729539db7d7af1a4955a353a14eb7ffa96f84c117ee6633b3e2b5&amp;quot; network for pod &amp;quot;httpbin-master&amp;quot;: networkPlugin cni failed to teardown pod &amp;quot;httpbin-master_default&amp;quot; network: error getting ClusterInformation: connection is unauthorized: Unauthorized]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果安装了calico网络插件，需要删除calico:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl delete -f  &amp;lt;yaml&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还要去所有节点etc/cni/net.d/目录下 删掉与calico相关的所有配置文件, 然后重启机器。 不然pod起不来，会报错 network: error getting ClusterInformation: connection is unauthorized: Unauthorized .&lt;/p&gt;

&lt;p&gt;4、最后再查看POD都变成Running状态了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# k get pod -A -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS       AGE     IP               NODE         NOMINATED NODE   READINESS GATES
default       httpbin-master                       1/1     Running   0              20m     10.244.0.4       k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       httpbin-worker                       1/1     Running   0              20m     10.244.2.2       k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       netshoot-master                      1/1     Running   0              20m     10.244.0.5       k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       netshoot-worker                      1/1     Running   0              20m     10.244.2.3       k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       nginx-85b98978db-2hzc9               1/1     Running   0              50m     10.244.1.2       k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-6d8c4cb4d-ck2x5              1/1     Running   9 (24h ago)    6d23h   10.244.0.2       k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-6d8c4cb4d-mbctj              1/1     Running   9 (24h ago)    6d23h   10.244.0.3       k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   etcd-k8s-master                      1/1     Running   10 (24h ago)   6d23h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-apiserver-k8s-master            1/1     Running   12 (24h ago)   6d23h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-controller-manager-k8s-master   1/1     Running   10 (24h ago)   6d23h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-dnsjg                     1/1     Running   10 (24h ago)   6d22h   172.25.140.215   k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-r84lg                     1/1     Running   10 (24h ago)   6d23h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-tbkx2                     1/1     Running   9 (24h ago)    6d22h   172.25.140.214   k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-scheduler-k8s-master            1/1     Running   10 (24h ago)   6d23h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   minicni-node-5w9fn                   1/1     Running   0              55m     172.25.140.215   k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   minicni-node-jb5cj                   1/1     Running   0              55m     172.25.140.214   k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   minicni-node-kp25h                   1/1     Running   0              55m     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
[root@k8s-master minicni]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、之后测试以下四种网络通信是否正常：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;pod 到宿主机的通信&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# kubectl exec -ti netshoot-master -- /bin/bash
bash-5.1# ping 172.25.140.216
PING 172.25.140.216 (172.25.140.216) 56(84) bytes of data.
64 bytes from 172.25.140.216: icmp_seq=1 ttl=64 time=0.074 ms
64 bytes from 172.25.140.216: icmp_seq=2 ttl=64 time=0.035 ms
64 bytes from 172.25.140.216: icmp_seq=3 ttl=64 time=0.033 ms
64 bytes from 172.25.140.216: icmp_seq=4 ttl=64 time=0.043 ms
64 bytes from 172.25.140.216: icmp_seq=5 ttl=64 time=0.048 ms
^C
--- 172.25.140.216 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 3999ms
rtt min/avg/max/mdev = 0.033/0.046/0.074/0.014 ms
bash-5.1#
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;同一个节点 pod-to-pod 通信&lt;br /&gt;
默认情况下，同一台主机上的 pod-to-pod 网络包默认会被 Linux 内核丢弃，原因是 Linux 默认会把非 default 网络命名空间的网络包看作是外部数据包，关于这个问题的具体细节，请查看 stackoverflow 上的讨论。目前，我们需要在每个集群结点上使用以下命令手动添加以下 iptables 规则来让 pod-to-pod 网络数据包顺利转发：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# iptables -t filter -A FORWARD -s 10.244.0.0/24  -j ACCEPT
[root@k8s-master ~]# iptables -t filter -A FORWARD -d 10.244.0.0/24  -j ACCEPT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再进行测试:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# kubectl  exec -ti netshoot-master -- /bin/bash
bash-5.1# ping 10.244.0.25
PING 10.244.0.25 (10.244.0.25) 56(84) bytes of data.
64 bytes from 10.244.0.25: icmp_seq=1 ttl=64 time=0.079 ms
64 bytes from 10.244.0.25: icmp_seq=2 ttl=64 time=0.061 ms 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试通信正常。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;pod 到其他主机的通信&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# kubectl  exec -ti netshoot-master -- /bin/bash
bash-5.1# ping 172.25.140.214
PING 172.25.140.214 (172.25.140.214) 56(84) bytes of data.

^C
--- 172.25.140.214 ping statistics ---
4 packets transmitted, 0 received, 100% packet loss, time 2999ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ping不通，原因是阿里云底层网络对非法IP限制，不是随便配个IP就可以通，如需通可以考虑overlay如calico或本地虚拟机搭建方式。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;跨一个节点 pod-to-pod 通信&lt;br /&gt;
对于跨节点的 pod-to-pod 网络包，需要像 Calico 那样添加宿主机的路由表，保证发往各个节点上的 pod 流量经过节点的转发。目前这些路由表需要手动添加：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ip route add 10.244.2.0/24  via 172.25.140.214 dev eth0 #run on master 
ip route add 10.244.0.0/24  via 172.25.140.216 dev eth0 #run on worker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再测试：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# kubectl  exec -ti netshoot-master -- /bin/bash
bash-5.1# ping 10.244.2.11
PING 10.244.2.11 (10.244.2.11) 56(84) bytes of data.

^C
--- 10.244.2.11 ping statistics ---
108 packets transmitted, 0 received, 100% packet loss, time 106996ms    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现象与原理同上。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orgb51c532&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;faq&#34;&gt;FAQ&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orgdc842c7&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;cannot-find-package-编译不过&#34;&gt;cannot find package 编译不过&lt;/h2&gt;

&lt;p&gt;当出现如下错误时：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ make build
Building the minicni on amd64...
cmd/main.go:8:2: cannot find package &amp;quot;github.com/morvencao/minicni/pkg/args&amp;quot; in any of:
 /usr/local/go/src/github.com/morvencao/minicni/pkg/args (from $GOROOT)
 /Users/mosp/goget/src/github.com/morvencao/minicni/pkg/args (from $GOPATH)
cmd/main.go:9:2: cannot find package &amp;quot;github.com/morvencao/minicni/pkg/handler&amp;quot; in any of:
 /usr/local/go/src/github.com/morvencao/minicni/pkg/handler (from $GOROOT)
 /Users/mosp/goget/src/github.com/morvencao/minicni/pkg/handler (from $GOPATH)
make: *** [build] Error 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要把GO111MODULE=on或auto，才能使用Go module功能，可在.bashrc或.zshrc里加上：export GO111MODULE=auto&lt;br /&gt;
详见&lt;a href=&#34;http://www.ay1.cc/article/18635.html&#34;&gt;go自动下载所有的依赖包go module使用详解_Golang&lt;/a&gt;：&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orga2808f1&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;build-constraints-exclude-all-go-files-in&#34;&gt;build constraints exclude all Go files in&lt;/h2&gt;

&lt;p&gt;当出现如下错误时:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ make build
Building the minicni on amd64...
go build github.com/containernetworking/plugins/pkg/ns: build constraints exclude all Go files in /Users/mosp/goget/pkg/mod/github.com/containernetworking/plugins@v1.1.1/pkg/ns
make: *** [build] Error 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要设置如下两个变量：&lt;br /&gt;
export GOOS=“linux”。即：不能为darwin&lt;br /&gt;
export CGO_ENABLED=“1”。&lt;br /&gt;
详见：&lt;a href=&#34;https://blog.csdn.net/weixin_42845682/article/details/124568715&#34;&gt;build constraints exclude all Go files in xxx/xxx/xxx&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orga4ae153&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;p&gt;【01】 &lt;a href=&#34;https://blog.csdn.net/u012772803/article/details/113703029&#34;&gt;find -print0和xargs -0原理及用法&lt;/a&gt;]&lt;br /&gt;
【02】 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/411181637&#34;&gt;Go语言import分组管理利器: goimports-reviser&lt;/a&gt;&lt;br /&gt;
【03】 &lt;a href=&#34;https://morven.life/posts/create-your-own-cni-with-golang/&#34;&gt;使用 Go 从零开始实现 CNI&lt;/a&gt;&lt;br /&gt;
【04】 &lt;a href=&#34;https://blog.csdn.net/a5534789/article/details/112848404&#34;&gt;centOS内网安装kubernetes集群&lt;/a&gt;&lt;br /&gt;
【05】 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/415032187&#34;&gt;Linux 路由表(RIB表、FIB表)、ARP表、MAC表整理&lt;/a&gt;]&lt;br /&gt;
【06】 &lt;a href=&#34;https://www.jianshu.com/p/75704eb30eff&#34;&gt;查看CNI中的veth pair&lt;/a&gt;&lt;br /&gt;
【07】 &lt;a href=&#34;https://mp.weixin.qq.com/s/7t_MoZ0quJF50VoIwqvKyQ&#34;&gt;一文吃透 K8S 网络模型&lt;/a&gt;&lt;br /&gt;
【08】 &lt;a href=&#34;https://blog.csdn.net/elihe2011/article/details/122926399&#34;&gt;K8S 网络CNI&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(01): kubeadm安装K8S集群</title>
            <link>http://mospany.github.io/2022/11/23/kubeadm-install-k8s/</link>
            <pubDate>Tue, 22 Nov 2022 20:58:52 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/11/23/kubeadm-install-k8s/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;org4f3c086&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;规划&#34;&gt;规划&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orgfdfecef&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;服务配置&#34;&gt;服务配置&lt;/h2&gt;

&lt;table border=&#34;2&#34; cellspacing=&#34;0&#34; cellpadding=&#34;6&#34; rules=&#34;groups&#34; frame=&#34;hsides&#34;&gt;


&lt;colgroup&gt;
&lt;col  class=&#34;org-left&#34; /&gt;

&lt;col  class=&#34;org-left&#34; /&gt;

&lt;col  class=&#34;org-left&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;OS&lt;/th&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;配置&lt;/th&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;用途&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;CentOS 7.6 (172.25.140.216)&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;2C4G&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;k8s-master&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;CentOS 7.6 (172.25.140.215)&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;2C4G&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;k8s-work1&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;CentOS 7.6 (172.25.140.214)&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;2C4G&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;k8s-work2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;**注**：这是演示 k8s 集群安装的实验环境，配置较低，生产环境中我们的服务器配置至少都是 8C/16G 的基础配置。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgbfb1037&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;版本选择&#34;&gt;版本选择&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;CentOS：7.6&lt;/li&gt;
&lt;li&gt;k8s组件版本：1.23.6&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orge0cd109&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;一-服务器基础配置&#34;&gt;一、服务器基础配置&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orgfbbc6fc&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-配置主机名&#34;&gt;1、 配置主机名&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;[root@iZ8vbfafp7g52u8976flc0Z ~]# hostnamectl set-hostname k8s-master
[root@iZ8vbfafp7g52u8976flc1Z ~]# hostnamectl set-hostname k8s-work1
[root@iZ8vbfafp7g52u8976flc2Z ~]# hostnamectl set-hostname k8s-work2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgec07355&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-关闭防火墙&#34;&gt;2、关闭防火墙&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1  # 关闭firewalld
2  [root@k8s-master ~]# systemctl stop firewalld
3  
4  # 关闭selinux
5  [root@k8s-master ~]# sed -i &#39;s/enforcing/disabled/&#39; /etc/selinux/config
6  [root@k8s-master ~]# setenforce 0
7   setenforce: SELinux is disabled
8  [root@k8s-master ~]#
9  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org1b9a204&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;3-互做本地解析&#34;&gt;3、互做本地解析&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# cat /etc/hosts
172.25.140.216 k8s-master
172.25.140.215 k8s-work1
172.25.140.214 k8s-work2

[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgf3b9e1a&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;4-ssh-免密通信-可选&#34;&gt;4、SSH 免密通信（可选）&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# ssh-keygen
  Generating public/private rsa key pair.
  Enter file in which to save the key (/root/.ssh/id_rsa):
  Enter passphrase (empty for no passphrase):
  Enter same passphrase again:
  Your identification has been saved in /root/.ssh/id_rsa.
  Your public key has been saved in /root/.ssh/id_rsa.pub.
  The key fingerprint is:
  SHA256:s+JcU9ctsItBgDC+UwxgmFhnsQGpy9SELkEOx4Lmk/0 root@k8s-master
  The key&#39;s randomart image is:
  +---[RSA 2048]----+
  |=*B+Oo ..        |
  |X=o= *.  .       |
  |+== o o   . .    |
  |o* o o   .   + . |
  |+.. +   S o o o .|
  |..   E   + + . . |
  |      . + . .    |
  |     o o .       |
  |      o          |
  +----[SHA256]-----+
  [root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行（互发公钥）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# ssh-copy-id root@k8s-work1
  /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &amp;quot;/root/.ssh/id_rsa.pub&amp;quot;
  The authenticity of host &#39;k8s-work1 (172.25.140.215)&#39; can&#39;t be established.
  ECDSA key fingerprint is SHA256:BMN7TIKDbFKdG3v1TVHmy3i6BYm7TGS8Hsnu1F9+UkI.
  ECDSA key fingerprint is MD5:71:60:e2:6c:38:e2:20:d8:9c:94:77:54:cb:10:33:32.
  Are you sure you want to continue connecting (yes/no)? yes
  /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
  /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
  root@k8s-work1&#39;s password:

  Number of key(s) added: 1

  Now try logging into the machine, with:   &amp;quot;ssh &#39;root@k8s-work1&#39;&amp;quot;
  and check to make sure that only the key(s) you wanted were added.

  [root@k8s-master ~]# ssh-copy-id root@k8s-work2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org97fcb39&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;5-加载-br-netfilter-模块&#34;&gt;5、加载 br_netfilter 模块&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;确保 br_netfilter 模块被加载&lt;br /&gt;
所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt; 1  # 加载模块
 2  [root@k8s-master ~]# modprobe br_netfilter
 3  ## 查看加载请看
 4  [root@k8s-master ~]# lsmod | grep br_netfilter
 5  br_netfilter           22256  0
 6  bridge                151336  1 br_netfilter
 7  
 8  # 永久生效
 9  [root@k8s-master ~]# cat &amp;lt;&amp;lt;EOF | tee /etc/modules-load.d/k8s.conf
10  &amp;gt; br_netfilter
11  &amp;gt; EOF
12  br_netfilter
13  [root@k8s-master ~]#
14  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org6cf490e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;6-允许-iptables-检查桥接流量&#34;&gt;6、允许 iptables 检查桥接流量&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt; 1   [root@k8s-master ~]# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
 2   &amp;gt; br_netfilter
 3   &amp;gt; EOF
 4   br_netfilter
 5   [root@k8s-master ~]# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
 6   &amp;gt; net.bridge.bridge-nf-call-ip6tables = 1
 7   &amp;gt; net.bridge.bridge-nf-call-iptables = 1
 8   &amp;gt; EOF
 9   net.bridge.bridge-nf-call-ip6tables = 1
10   net.bridge.bridge-nf-call-iptables = 1
11  [root@k8s-master ~]# sudo sysctl --system
12  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org716427b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;7-关闭-swap&#34;&gt;7、关闭 swap&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1  # 临时关闭
2  [root@k8s-master ~]# swapoff -a
3  
4  # 永久关闭
5  [root@k8s-master ~]# sed -ri &#39;s/.*swap.*/#&amp;amp;/&#39; /etc/fstab
6  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orga15d3fb&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;8-时间同步&#34;&gt;8、时间同步&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1  # 同步网络时间
2  [root@k8s-master ~]# ntpdate time.nist.gov
3  23 Nov 22:36:07 ntpdate[12307]: adjust time server 132.163.96.6 offset -0.009024 sec
4  
5  [root@k8s-master ~]#
6  # 将网络时间写入硬件时间
7  [root@k8s-master ~]# hwclock --systohc
8  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org7ff8672&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;9-安装-docker&#34;&gt;9、安装 Docker&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、使用 sudo 或 root 权限登录 Centos。&lt;/p&gt;

&lt;p&gt;2、确保 yum 包更新到最新。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum update
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、执行 Docker 安装脚本。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -fsSL https://get.docker.com/ | sh 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行这个脚本会添加 docker.repo 源并安装 Docker。&lt;/p&gt;

&lt;p&gt;4、启动 Docker 进程。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo service docker start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、验证 docker 是否安装成功并在容器中执行一个测试的镜像。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo docker run hello-world

[root@k8s-master ~]# sudo docker run hello-world

  Hello from Docker!
  This message shows that your installation appears to be working correctly.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到此，docker 在 CentOS 系统的安装完成。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orge392402&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;10-安装-kubeadm-kubelet&#34;&gt;10、安装 kubeadm、kubelet&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、添加 k8s 镜像源&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;地址：&lt;a href=&#34;https://developer.aliyun.com/mirror/kubernetes?spm=a2c6h.13651102.0.0.1cd01b116JYQIn&#34;&gt;https://developer.aliyun.com/mirror/kubernetes?spm=a2c6h.13651102.0.0.1cd01b116JYQIn&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master ~]# cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo
 2  &amp;gt; [kubernetes]
 3  &amp;gt; name=Kubernetes
 4  &amp;gt; baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
 5  &amp;gt; enabled=1
 6  &amp;gt; gpgcheck=0
 7  &amp;gt; repo_gpgcheck=0
 8  &amp;gt; gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
 9  &amp;gt; EOF
10  [root@k8s-master ~]#
11  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、建立 k8s YUM 缓存&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master ~]# yum makecache
 2  已加载插件：fastestmirror
 3  Loading mirror speeds from cached hostfile
 4  base                                                                                                                                                                                 | 3.6 kB  00:00:00
 5  docker-ce-stable                                                                                                                                                                     | 3.5 kB  00:00:00
 6  epel                                                                                                                                                                                 | 4.7 kB  00:00:00
 7  extras                                                                                                                                                                               | 2.9 kB  00:00:00
 8  updates                                                                                                                                                                              | 2.9 kB  00:00:00
 9  元数据缓存已建立
10  [root@k8s-master ~]#
11  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、安装 k8s 相关工具&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1   # 查看可安装版本
 2   [root@k8s-master ~]# yum list kubelet --showduplicates
 3  
 4   ...
 5   ...
 6   kubelet.x86_64                                           1.23.0-0                                             kubernetes
 7   kubelet.x86_64                                           1.23.1-0                                             kubernetes
 8   kubelet.x86_64                                           1.23.2-0                                             kubernetes
 9   kubelet.x86_64                                           1.23.3-0                                             kubernetes
10   kubelet.x86_64                                           1.23.4-0                                             kubernetes
11   kubelet.x86_64                                           1.23.5-0                                             kubernetes
12   kubelet.x86_64                                           1.23.6-0                                             kubernetes
13  
14   # 开始安装（指定你要安装的版本）
15   [root@k8s-master ~]# yum install -y kubelet-1.23.6 kubeadm-1.23.6 kubectl-1.23.6
16  
17  # 设置开机自启动并启动kubelet（kubelet由systemd管理）
18  [root@k8s-master ~]# systemctl enable kubelet &amp;amp;&amp;amp; systemctl start kubelet
19  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org02e7601&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;二-master-节点&#34;&gt;二、Master 节点&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org88d2a45&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-k8s-初始化&#34;&gt;1、k8s 初始化&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1   [root@k8s-master ~]# kubeadm init \
2  --apiserver-advertise-address=172.25.140.216 \
3  --image-repository registry.aliyuncs.com/google_containers \
4  --kubernetes-version v1.23.6 \
5  --service-cidr=10.96.0.0/12 \
6  --pod-network-cidr=10.244.0.0/16 \
7  --ignore-preflight-errors=all
8  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数说明：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1  --apiserver-advertise-address  # 集群master地址
2  --image-repository             # 指定k8s镜像仓库地址
3  --kubernetes-version           # 指定K8s版本（与kubeadm、kubelet版本保持一致）
4  --service-cidr                 # Pod统一访问入口
5  --pod-network-cidr             # Pod网络（与CNI网络保持一致）
6  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初始化后输出内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  ...
 2  ...
 3  [addons] Applied essential addon: CoreDNS
 4  [addons] Applied essential addon: kube-proxy
 5  
 6  Your Kubernetes control-plane has initialized successfully!
 7  
 8  To start using your cluster, you need to run the following as a regular user:
 9  
10     mkdir -p $HOME/.kube
11     sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
12     sudo chown $(id -u):$(id -g) $HOME/.kube/config
13  
14    Alternatively, if you are the root user, you can run:
15  
16    export KUBECONFIG=/etc/kubernetes/admin.conf
17  
18  You should now deploy a pod network to the cluster.
19  Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
20  https://kubernetes.io/docs/concepts/cluster-administration/addons/
21  
22  Then you can join any number of worker nodes by running the following on each as root:
23  
24  kubeadm join 172.25.140.216:6443 --token 8d9mk7.08nyz6xc2d5boiy8 \
25      --discovery-token-ca-cert-hash sha256:45542b0b380a8f959e5bc93f6dd7d1c5c78b202ff1a3eea5c97804549af9a12e
26  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org0b51700&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-根据输出提示创建相关文件&#34;&gt;2、根据输出提示创建相关文件&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1  [root@k8s-master ~]# mkdir -p $HOME/.kube
2  [root@k8s-master ~]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
3  [root@k8s-master ~]# chown $(id -u):$(id -g) $HOME/.kube/config
4  [root@k8s-master ~]#
5  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org5a02bac&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;3-查看-k8s-运行的容器&#34;&gt;3、查看 k8s 运行的容器&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt; 1  root@k8s-master ~]# kubectl get pods -n kube-system -o wide
 2  NAME                                 READY   STATUS    RESTARTS   AGE     IP               NODE         NOMINATED NODE   READINESS GATES
 3  coredns-6d8c4cb4d-ck2x5              0/1     Pending   0          8m4s    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;       &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 4  coredns-6d8c4cb4d-mbctj              0/1     Pending   0          8m4s    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;       &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 5  etcd-k8s-master                      1/1     Running   0          8m18s   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 6  kube-apiserver-k8s-master            1/1     Running   0          8m18s   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 7  kube-controller-manager-k8s-master   1/1     Running   0          8m18s   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 8  kube-proxy-r84lg                     1/1     Running   0          8m4s    172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 9  kube-scheduler-k8s-master            1/1     Running   0          8m18s   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
10  [root@k8s-master ~]#
11  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org2688836&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;4-查看-k8s-节点&#34;&gt;4、查看 k8s 节点&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1  [root@k8s-master ~]# kubectl  get node -o wide
2  NAME         STATUS     ROLES                  AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME
3  k8s-master   NotReady   control-plane,master   11m   v1.23.6   172.25.140.216   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://20.10.21
4  [root@k8s-master ~]#
5  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可看到当前只有 k8s-master 节点，而且状态是 NotReady（未就绪），因为我们还没有部署网络插件（kubectl apply -f [podnetwork].yaml），于是接着部署容器网络（CNI）。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orge6d35ae&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;5-容器网络-cni-部署&#34;&gt;5、容器网络（CNI）部署&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;br /&gt;
插件地址：&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/addons/&#34;&gt;https://kubernetes.io/docs/concepts/cluster-administration/addons/&lt;/a&gt;&lt;br /&gt;
该地址在 k8s-master 初始化成功时打印出来。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、选择一个主流的容器网络插件部署（Calico）&lt;br /&gt;
   &lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/kubeadm-install-k8s/calico-install-page.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2、下载yml文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master kubeadm-install-k8s]# wget https://docs.projectcalico.org/manifests/calico.yaml --no-check-certificate
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、根据初始化的输出提示执行启动指令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master kubeadm-install-k8s]# kubectl apply -f calico.yaml
 2  poddisruptionbudget.policy/calico-kube-controllers created
 3  serviceaccount/calico-kube-controllers created
 4  serviceaccount/calico-node created
 5  configmap/calico-config created
 6  customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
 7  customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
 8  customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
 9  customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
10  customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
11  customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
12  customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
13  customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
14  customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
15  customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
16  customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
17  customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
18  customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
19  customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
20  customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
21  customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
22  customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
23  clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
24  clusterrole.rbac.authorization.k8s.io/calico-node created
25  clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
26  clusterrolebinding.rbac.authorization.k8s.io/calico-node created
27  daemonset.apps/calico-node created
28  deployment.apps/calico-kube-controllers created
29  [root@k8s-master kubeadm-install-k8s]# 
30  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、看看该yaml文件所需要启动的容器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1   [root@k8s-master kubeadm-install-k8s]# cat calico.yaml |grep image
 2         image: docker.io/calico/cni:v3.24.5
 3         imagePullPolicy: IfNotPresent
 4         image: docker.io/calico/cni:v3.24.5
 5         imagePullPolicy: IfNotPresent
 6         image: docker.io/calico/node:v3.24.5
 7         imagePullPolicy: IfNotPresent
 8         image: docker.io/calico/node:v3.24.5
 9         imagePullPolicy: IfNotPresent
10         image: docker.io/calico/kube-controllers:v3.24.5
11         imagePullPolicy: IfNotPresent
12  [root@k8s-master kubeadm-install-k8s]   #
13  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、查看容器是否都 Running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master kubeadm-install-k8s]# kubectl get pods -n kube-system -o wide
 2  NAME                                       READY   STATUS    RESTARTS   AGE     IP               NODE         NOMINATED NODE   READINESS GATES
 3  calico-kube-controllers-7b8458594b-pdx5z   1/1     Running   0          4m37s   10.244.235.194   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 4  calico-node-t8hhf                          1/1     Running   0          4m37s   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 5  coredns-6d8c4cb4d-ck2x5                    1/1     Running   0          32m     10.244.235.195   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 6  coredns-6d8c4cb4d-mbctj                    1/1     Running   0          32m     10.244.235.193   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 7  etcd-k8s-master                            1/1     Running   0          32m     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 8  kube-apiserver-k8s-master                  1/1     Running   0          32m     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 9  kube-controller-manager-k8s-master         1/1     Running   0          32m     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
10  kube-proxy-r84lg                           1/1     Running   0          32m     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
11  kube-scheduler-k8s-master                  1/1     Running   0          32m     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
12  [root@k8s-master kubeadm-install-k8s]#
13  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org4f4a98b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;三-work-节点&#34;&gt;三、work 节点&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orgfc01846&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-work-节点加入-k8s-集群&#34;&gt;1、work 节点加入 k8s 集群&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有 work 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt; 1  # 复制k8s-master初始化屏幕输出的语句并在work节点执行
 2  [root@k8s-work1 ~]# kubeadm join 172.25.140.216:6443 --token 8d9mk7.08nyz6xc2d5boiy8 --discovery-token-ca-cert-hash sha256:45542b0b380a8f959e5bc93f6dd7d1c5c78b202ff1a3eea5c97804549af9a12e
 3  [root@k8s-work2 ~]# kubeadm join 172.25.140.216:6443 --token 8d9mk7.08nyz6xc2d5boiy8 --discovery-token-ca-cert-hash sha256:45542b0b380a8f959e5bc93f6dd7d1c5c78b202ff1a3eea5c97804549af9a12e
 4  
 5  preflight] Running pre-flight checks
 6  [preflight] Reading configuration from the cluster...
 7  [preflight] FYI: You can look at this config file with &#39;kubectl -n kube-system get cm kubeadm-config -o yaml&#39;
 8  [kubelet-start] Writing kubelet configuration to file &amp;quot;/var/lib/kubelet/config.yaml&amp;quot;
 9  [kubelet-start] Writing kubelet environment file with flags to file &amp;quot;/var/lib/kubelet/kubeadm-flags.env&amp;quot;
10  [kubelet-start] Starting the kubelet
11  [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
12  
13  This node has joined the cluster:
14  * Certificate signing request was sent to apiserver and a response was received.
15  * The Kubelet was informed of the new secure connection details.
16  
17  Run &#39;kubectl get nodes&#39; on the control-plane to see this node join the cluster.
18  
19  [root@k8s-work2 ~]#
20  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org362c07d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-查询集群节点&#34;&gt;2、查询集群节点&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1  [root@k8s-master kubeadm-install-k8s]# kubectl  get node -o wide
2  NAME         STATUS   ROLES                  AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME
3  k8s-master   Ready    control-plane,master   50m     v1.23.6   172.25.140.216   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://20.10.21
4  k8s-work1    Ready    &amp;lt;none&amp;gt;                 9m50s   v1.23.6   172.25.140.215   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://20.10.21
5  k8s-work2    Ready    &amp;lt;none&amp;gt;                 2m41s   v1.23.6   172.25.140.214   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://20.10.21
6  [root@k8s-master kubeadm-install-k8s]#
7  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;都为就绪状态了&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orge747a84&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;四-验证&#34;&gt;四、验证&lt;/h2&gt;

&lt;p&gt;k8s 集群部署 nginx 服务，并通过浏览器进行访问验证。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org5ec8220&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-创建-pod&#34;&gt;1、创建 pod&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master kubeadm-install-k8s]# kubectl create deployment nginx --image=nginx
 2  deployment.apps/nginx created
 3  [root@k8s-master kubeadm-install-k8s]# kubectl expose deployment nginx --port=80 --type=NodePort
 4  service/nginx exposed
 5  [root@k8s-master kubeadm-install-k8s]# kubectl get pod,svc
 6  NAME                         READY   STATUS    RESTARTS   AGE
 7  pod/nginx-85b98978db-qkd6h   1/1     Running   0          36s
 8  
 9  NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
10  service/kubernetes   ClusterIP   10.96.0.1       &amp;lt;none&amp;gt;        443/TCP        54m
11  service/nginx        NodePort    10.108.180.91   &amp;lt;none&amp;gt;        80:31648/TCP   15s
12  [root@k8s-master kubeadm-install-k8s]#
13  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgd537fac&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-访问-nginx&#34;&gt;2、访问 Nginx&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master kubeadm-install-k8s]# curl 10.108.180.91:80
 2  &amp;lt;!DOCTYPE html&amp;gt;
 3  &amp;lt;html&amp;gt;
 4  &amp;lt;head&amp;gt;
 5  &amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
 6  &amp;lt;style&amp;gt;
 7  html { color-scheme: light dark; }
 8  body { width: 35em; margin: 0 auto;
 9  font-family: Tahoma, Verdana, Arial, sans-serif; }
10  &amp;lt;/style&amp;gt;
11  &amp;lt;/head&amp;gt;
12  &amp;lt;body&amp;gt;
13  &amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
14  &amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
15  working. Further configuration is required.&amp;lt;/p&amp;gt;
16  
17  &amp;lt;p&amp;gt;For online documentation and support please refer to
18  &amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
19  Commercial support is available at
20  &amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;
21  
22  &amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
23  &amp;lt;/body&amp;gt;
24  &amp;lt;/html&amp;gt;
25  [root@k8s-master kubeadm-install-k8s]#
26  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此：kubeadm方式的k8s集群已经部署完成。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orga6cfda4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;faq&#34;&gt;FAQ&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org4405cb7&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-k8s编译报错&#34;&gt;1、k8s编译报错&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt; 1  ...
 2  ...
 3  [kubelet-check] It seems like the kubelet isn&#39;t running or healthy.
 4  [kubelet-check] The HTTP call equal to &#39;curl -sSL http://localhost:10248/healthz&#39; failed with error: Get &amp;quot;http://localhost:10248/healthz&amp;quot;: dial tcp [::1]:10248: connect: connection refused.
 5  
 6   Unfortunately, an error has occurred:
 7       timed out waiting for the condition
 8  
 9   This error is likely caused by:
10       - The kubelet is not running
11       - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)
12  
13   If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
14       - &#39;systemctl status kubelet&#39;
15       - &#39;journalctl -xeu kubelet&#39;
16  
17   Additionally, a control plane component may have crashed or exited when started by the container runtime.
18   To troubleshoot, list all containers using your preferred container runtimes CLI.
19  
20   Here is one example how you may list all Kubernetes containers running in docker:
21       - &#39;docker ps -a | grep kube | grep -v pause&#39;
22       Once you have found the failing container, you can inspect its logs with:
23       - &#39;docker logs CONTAINERID&#39;
24  
25  error execution phase wait-control-plane: couldn&#39;t initialize a Kubernetes cluster
26  To see the stack trace of this error execute with --v=5 or higher
27  
28  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看日志&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    Apr 26 20:33:30 test3 kubelet: I0426 20:33:30.588349   21936 docker_service.go:264] &amp;quot;Docker Info&amp;quot; dockerInfo=&amp;amp;{ID:2NSH:KJPQ:XOKI:5XHN:ULL3:L4LG:SXA4:PR6J:DITW:HHCF:2RKL:U2NJ Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:7 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:false IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:45 SystemTime:2022-04-26T20:33:30.583063427+08:00 LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion: NEventsListener:0 KernelVersion:3.10.0-1160.59.1.el7.x86_64 OperatingSystem:CentOS Linux 7 (Core) OSVersion: OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc000263340 NCPU:2 MemTotal:3873665024 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:k8s-master Labels:[] ExperimentalBuild:false ServerVersion:18.06.3-ce ClusterStore: ClusterAdvertise: Runtimes:map[runc:{Path:docker-runc Args:[] Shim:&amp;lt;nil&amp;gt;}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:&amp;lt;nil&amp;gt; Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:468a545b9edcd5932818eb9de8e72413e616e86e Expected:468a545b9edcd5932818eb9de8e72413e616e86e} RuncCommit:{ID:a592beb5bc4c4092b1b1bac971afed27687340c5 Expected:a592beb5bc4c4092b1b1bac971afed27687340c5} InitCommit:{ID:fec3683 Expected:fec3683} SecurityOptions:[name=seccomp,profile=default] ProductLicense: DefaultAddressPools:[] Warnings:[]}
Apr 26 20:33:30 test3 kubelet: E0426 20:33:30.588383   21936 server.go:302] &amp;quot;Failed to run kubelet&amp;quot; err=&amp;quot;failed to run Kubelet: misconfiguration: kubelet cgroup driver: \&amp;quot;systemd\&amp;quot; is different from docker cgroup driver: \&amp;quot;cgroupfs\&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看报错的最后解释kubelet cgroup driver: \&amp;ldquo;systemd\&amp;rdquo; is different from docker cgroup driver: \&amp;ldquo;cgroupfs\&amp;rdquo;&amp;ldquo;很明显 kubelet 与 Docker 的 cgroup 驱动程序不同，kubelet 为 systemd，而 Docker 为 cgroupfs。&lt;/p&gt;

&lt;p&gt;简单查看一下docker驱动：&lt;br /&gt;
[root@k8s-master opt]# docker info |grep Cgroup&lt;br /&gt;
Cgroup Driver: cgroupfs&lt;/p&gt;

&lt;p&gt;解决方案&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 重置初始化
[root@k8s-master ~]# kubeadm reset

# 删除相关配置文件
[root@k8s-master ~]# rm -rf $HOME/.kube/config  &amp;amp;&amp;amp; rm -rf $HOME/.kube

# 修改 Docker 驱动为 systemd（即&amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;]）
[root@k8s-master opt]# cat /etc/docker/daemon.json 
{
  &amp;quot;registry-mirrors&amp;quot;: [&amp;quot;https://q1rw9tzz.mirror.aliyuncs.com&amp;quot;],
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;]
}

# 重启 Docker
[root@k8s-master opt]# systemctl daemon-reload 
[root@k8s-master opt]# systemctl restart docker.service

# 再次初始化k8s即可
[root@k8s-master ~]# kubeadm init ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org0b33cbe&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-work-节点加入-k8s-集群报错&#34;&gt;2、work 节点加入 k8s 集群报错&lt;/h2&gt;

&lt;p&gt;报错1：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1  accepts at most 1 arg(s), received 3
2  To see the stack trace of this error execute with --v=5 or higher
3  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原因：命令不对，我是直接复制粘贴 k8s-master 初始化的终端输出结果，导致报错，所以最好先复制到 txt 文本下修改好格式再粘贴执行。&lt;/p&gt;

&lt;p&gt;报错2：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1  ...
2  [kubelet-check] The HTTP call equal to &#39;curl -sSL http://localhost:10248/healthz&#39; failed with error: Get &amp;quot;http://localhost:10248/healthz&amp;quot;: dial tcp 127.0.0.1:10248: connect: connection refused.
3  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解决方法同Master&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org9d65a10&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考&#34;&gt;参考&lt;/h1&gt;

&lt;p&gt;[01]&lt;a href=&#34;https://blog.csdn.net/IT_ZRS/article/details/124466870&#34;&gt;kubeadm 部署 k8s 集群&lt;/a&gt;&lt;br /&gt;
[02]&lt;a href=&#34;https://blog.csdn.net/oscarun/article/details/125595521?spm=1001.2014.3001.5502&#34;&gt;kubeadm系列-00-overview&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S学习笔记(07): 服务发现</title>
            <link>http://mospany.github.io/2022/10/18/k8s-service/</link>
            <pubDate>Tue, 18 Oct 2022 22:06:22 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/10/18/k8s-service/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;orga912502&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;service&#34;&gt;service&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org51c5e4e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;p&gt;01.&lt;a href=&#34;https://baijiahao.baidu.com/s?id=1681303264708121640&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;一文讲透K8s的Service概念&lt;/a&gt;&lt;br /&gt;
02.&lt;a href=&#34;https://blog.csdn.net/huahua1999/article/details/124237065&#34;&gt;k8s-service底层之 Iptables与 IPVS&lt;/a&gt;&lt;br /&gt;
03.&lt;a href=&#34;https://blog.csdn.net/Deepexi_Date/article/details/111042410&#34;&gt;K8S的ServiceIP实现原理&lt;/a&gt;&lt;br /&gt;
04.&lt;a href=&#34;http://t.zoukankan.com/fengdejiyixx-p-15568056.html&#34;&gt;http://t.zoukankan.com/fengdejiyixx-p-15568056.html&lt;/a&gt;&lt;br /&gt;
05.&lt;a href=&#34;https://blog.csdn.net/qq_37369726/article/details/121785627&#34;&gt;kubernetes pod间通信,跨namespace互访&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S学习笔记(06): 资源</title>
            <link>http://mospany.github.io/2022/09/28/resource/</link>
            <pubDate>Wed, 28 Sep 2022 20:10:36 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/09/28/resource/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;orge0656ad&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;k8s中所有的内容都抽象为资源， 资源实例化之后，叫做对象。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org32d4fb4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;资源类型介绍&#34;&gt;资源类型介绍&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;工作负载型资源对象（workload）：Pod，ReplicaSet，Deployment，StatefulSet，DaemonSet，Job，Cronjob &amp;#x2026;&lt;/li&gt;
&lt;li&gt;服务发现及均衡资源对象：Service，Ingress &amp;#x2026;&lt;/li&gt;
&lt;li&gt;配置与存储资源对象：Volume(存储卷)，CSI(容器存储接口,可以扩展各种各样的第三方存储卷)，ConfigMap，Secret，DownwardAPI&lt;/li&gt;
&lt;li&gt;集群级资源：Namespace，Node，Role，ClusterRole，RoleBinding，ClusterRoleBinding&lt;/li&gt;
&lt;li&gt;元数据型资源：HPA，PodTemplate，LimitRange&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orga7470a7&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;工作负载资源&#34;&gt;工作负载资源&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org78d773a&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;deployment&#34;&gt;Deployment&lt;/h2&gt;

&lt;p&gt;为了更好地解决服务编排的问题，k8s在V1.2版本开始，引入了deployment控制器，值得一提的是，这种控制器并不直接管理pod，&lt;br /&gt;
而是通过管理replicaset来间接管理pod，即：deployment管理replicaset，replicaset管理pod。所以deployment比replicaset的功能更强大。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/resource/deploy.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;deployment的主要功能有下面几个：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;支持replicaset的所有功能&lt;/li&gt;
&lt;li&gt;支持发布的停止、继续&lt;/li&gt;
&lt;li&gt;支持版本的滚动更新和版本回退&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org3b6bbe8&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;编写资源清单&#34;&gt;编写资源清单&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt; 1  $ cat test-deploy.yaml
 2  apiVersion: apps/v1
 3  kind: Deployment
 4  metadata:
 5    name: test-deployment
 6    namespace: dev
 7  spec:
 8    replicas: 3
 9    selector:
10      matchLabels:
11       app: nginx-pod
12    template:
13      metadata:
14        labels:
15          app: nginx-pod
16      spec:
17        containers:
18        - name: nginx
19          image: nginx:1.17.1
20  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org33be6c7&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;运行清单&#34;&gt;运行清单&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ k apply -f test-deploy.yaml
deployment.apps/test-deployment created 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org1474d63&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看效果&#34;&gt;查看效果&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ k get deploy -A
  NAMESPACE              NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
  dev                    test-deployment                3/3     3            3           94s
  guestbook-system       guestbook-controller-manager   1/1     1            1           59d
  kube-system            coredns                        2/2     2            2           63d
  kubernetes-dashboard   dashboard-metrics-scraper      1/1     1            1           14d
  kubernetes-dashboard   kubernetes-dashboard           1/1     1            1           14d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出deploy已经运行成功，3副本已处于READY状态。&lt;/p&gt;

&lt;p&gt;再查看它运行中的清单:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k get deploy -n dev test-deployment -o yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: &amp;quot;1&amp;quot;
    kubectl.kubernetes.io/last-applied-configuration: |
      {&amp;quot;apiVersion&amp;quot;:&amp;quot;apps/v1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;Deployment&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;test-deployment&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;dev&amp;quot;},&amp;quot;spec&amp;quot;:{&amp;quot;replicas&amp;quot;:3,&amp;quot;selector&amp;quot;:{&amp;quot;matchLabels&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx-pod&amp;quot;}},&amp;quot;template&amp;quot;:{&amp;quot;metadata&amp;quot;:{&amp;quot;labels&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx-pod&amp;quot;}},&amp;quot;spec&amp;quot;:{&amp;quot;containers&amp;quot;:[{&amp;quot;image&amp;quot;:&amp;quot;nginx:1.17.1&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;nginx&amp;quot;}]}}}}
  creationTimestamp: &amp;quot;2022-09-28T12:35:22Z&amp;quot;
  generation: 1
  name: test-deployment
  namespace: dev
  resourceVersion: &amp;quot;5045809&amp;quot;
  uid: 40feb568-1343-4046-8708-7e1bf5e5c384
spec:
  #spec.progressDeadlineSeconds 是可选配置项，用来指定在系统报告Deployment的failed progressing一一表现为resource的状态中 type=Progressing 、 Status=False 、 Reason=ProgressDeadlineExceeded 前可以等待的Deployment进行的秒数。Deployment controller会继续重试该Deployment。未来，在实现了自动回滚后， deployment controller在观察到这种状态时就会自动回滚。
  progressDeadlineSeconds: 600
  #.spec.replicas 是可以选字段，指定期望的pod数量，默认是1。
  replicas: 3
  revisionHistoryLimit: 10
  #.spec.selector是可选字段，用来指定 label selector ，圈定Deployment管理的pod范围。如果被指定， .spec.selector 必须匹配 .spec.template.metadata.labels，否则它将被API拒绝。如果.spec.selector 没有被指定， .spec.selector.matchLabels 默认是.spec.template.metadata.labels。在Pod的template跟.spec.template不同或者数量超过了.spec.replicas规定的数量的情况下，Deployment会杀掉label跟selector不同的Pod。
  selector:
    matchLabels:
      app: nginx-pod
  #.spec.strategy 指定新的Pod替换旧的Pod的策略。 .spec.strategy.type 可以是&amp;quot;Recreate&amp;quot;或者是&amp;quot;RollingUpdate&amp;quot;。&amp;quot;RollingUpdate&amp;quot;是默认值。
  strategy:
    rollingUpdate:
      #spec.strategy.rollingUpdate.maxSurge 是可选配置项，用来指定可以超过期望的Pod数量的最大个数。该值可以是一个绝对值（例如5）或者是期望的Pod数量的百分比（例如10%）。当 MaxUnavailable 为0时该值不可以为0。通过百分比计算的绝对值向上取整。默认值是1。
      maxSurge: 25%
      #.spec.strategy.rollingUpdate.maxUnavailable 是可选配置项，用来指定在升级过程中不可用Pod的最大数量。该值可以是一个绝对值（例如5），也可以是期望Pod数量的百分比（例如10%）。通过计算百分比的绝对值向下取整。 如 果 .spec.strategy.rollingUpdate.maxSurge 为0时，这个值不可以为0。默认值是1。例如，该值设置成30%，启动rolling update后旧的ReplicatSet将会立即缩容到期望的Pod数量的70%。新的Pod ready后，随着新的ReplicaSet的扩容，旧的ReplicaSet会进一步缩容确保在升级的所有时刻可以用的Pod数量至少是期望Pod数量的70%。
      maxUnavailable: 25%
    #滚动更新，简单定义 更新期间pod最多有几个等。可以指定 maxUnavailable 和 maxSurge 来控制 rolling update 进程。
    type: RollingUpdate
  #.spec.template 是 .spec中唯一要求的字段。.spec.template 是 pod template. 它跟 Pod有一模一样的schema，除了它是嵌套的并且不需要apiVersion 和 kind字段。另外为了划分Pod的范围，Deployment中的pod template必须指定适当的label（不要跟其他controller重复了，参考selector）和适当的重启策略。.spec.template.spec.restartPolicy 可以设置为 Always , 如果不指定的话这就是默认配置。
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx-pod
    spec:
      containers:
      - image: nginx:1.17.1
        imagePullPolicy: IfNotPresent
        name: nginx
        resources: {}
        #terminationMessagePath 表示容器的异常终止消息的路径，默认在 /dev/termination-log 下。当容器退出时，可以通过容器的状态看到退出信息。
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      #“ClusterFirst“:如果DNS查询与配置好的默认集群域名前缀不匹配，则将查询请求转发到从节点继承而来，作为查询的上游服务器。
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      #spec:schedulername参数指定调度器的名字，可以为 pod 选择某个调度器进行调度
      schedulerName: default-scheduler
      #安全上下文（Security Context）定义 Pod 或 Container 的特权与访问控制设置。
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 3
  conditions:
  - lastTransitionTime: &amp;quot;2022-09-28T12:35:25Z&amp;quot;
    lastUpdateTime: &amp;quot;2022-09-28T12:35:25Z&amp;quot;
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: &amp;quot;True&amp;quot;
    type: Available
  - lastTransitionTime: &amp;quot;2022-09-28T12:35:22Z&amp;quot;
    lastUpdateTime: &amp;quot;2022-09-28T12:35:25Z&amp;quot;
    message: ReplicaSet &amp;quot;test-deployment-5d9c9b97bb&amp;quot; has successfully progressed.
    reason: NewReplicaSetAvailable
    status: &amp;quot;True&amp;quot;
    type: Progressing
  observedGeneration: 1
  readyReplicas: 3
  replicas: 3
  updatedReplicas: 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org3fa900a&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;replicaset&#34;&gt;ReplicaSet&lt;/h2&gt;

&lt;p&gt;ReplicaSet是kubernetes中的一种副本控制器，主要作用是控制由其管理的pod，使pod副本的数量始终维持在预设的个数。&lt;/p&gt;

&lt;p&gt;kubernetes官方推荐不要直接使用ReplicaSet，用Deployments取而代之，Deployments是比ReplicaSet更高级的概念，它会管理ReplicaSet并提供很多其它有用的特性，最重要的是Deployments支持声明式更新，声明式更好相比于命令式更新的好处是不会丢失历史变更。总结起来就是：不要再直接使用ReplicaSet。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgfdcdb41&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看效果-1&#34;&gt;查看效果&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ k get rs -A
NAMESPACE              NAME                                      DESIRED   CURRENT   READY   AGE
dev                    test-deployment-5d9c9b97bb                3         3         3       23h
guestbook-system       guestbook-controller-manager-84cd65964f   1         1         1       60d
kube-system            coredns-78fcd69978                        2         2         2       64d
kubernetes-dashboard   dashboard-metrics-scraper-7c857855d9      1         1         1       15d
kubernetes-dashboard   kubernetes-dashboard-6b79449649           1         1         1       15d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看清单：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k get rs -n dev test-deployment-5d9c9b97bb -o yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  annotations:
    deployment.kubernetes.io/desired-replicas: &amp;quot;3&amp;quot;
    deployment.kubernetes.io/max-replicas: &amp;quot;4&amp;quot;
    deployment.kubernetes.io/revision: &amp;quot;1&amp;quot;
  creationTimestamp: &amp;quot;2022-09-28T12:35:22Z&amp;quot;
  generation: 1
  labels:
    app: nginx-pod
    pod-template-hash: 5d9c9b97bb
  name: test-deployment-5d9c9b97bb
  namespace: dev
  ownerReferences:
  - apiVersion: apps/v1
    #控制特定的从属对象是否可以阻止垃圾收集删除其所有者对象
    blockOwnerDeletion: true
    controller: true
    kind: Deployment
    name: test-deployment
    uid: 40feb568-1343-4046-8708-7e1bf5e5c384
  resourceVersion: &amp;quot;5045808&amp;quot;
  uid: 1dc27f38-b2db-48a1-b76a-ad044e3da2d7
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-pod
      pod-template-hash: 5d9c9b97bb
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx-pod
        pod-template-hash: 5d9c9b97bb
    spec:
      containers:
      - image: nginx:1.17.1
        imagePullPolicy: IfNotPresent
        name: nginx
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 3
  fullyLabeledReplicas: 3
  observedGeneration: 1
  readyReplicas: 3
  replicas: 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org1c8a1ca&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;pod&#34;&gt;Pod&lt;/h2&gt;

&lt;p&gt;Pod是kubernetes中最小的资源管理组件，Pod也是最小化运行容器化应用的资源对象。一个Pod代表着集群中运行的一个进程。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k get pod -A
NAMESPACE              NAME                                            READY   STATUS    RESTARTS         AGE
dev                    test-deployment-5d9c9b97bb-7hmqb                1/1     Running   0                23h
dev                    test-deployment-5d9c9b97bb-bdwhq                1/1     Running   0                23h
dev                    test-deployment-5d9c9b97bb-l9fj2                1/1     Running   0                23h
guestbook-system       guestbook-controller-manager-84cd65964f-c2z8x   2/2     Running   184 (34h ago)    60d
kube-system            coredns-78fcd69978-s52td                        1/1     Running   7 (11d ago)      64d
kube-system            coredns-78fcd69978-z5fvx                        1/1     Running   7 (11d ago)      64d
kube-system            etcd-docker-desktop                             1/1     Running   7 (11d ago)      64d
kube-system            kube-apiserver-docker-desktop                   1/1     Running   7 (11d ago)      64d
kube-system            kube-controller-manager-docker-desktop          1/1     Running   7 (11d ago)      64d
kube-system            kube-proxy-b5954                                1/1     Running   7 (11d ago)      64d
kube-system            kube-scheduler-docker-desktop                   1/1     Running   146 (34h ago)    64d
kube-system            storage-provisioner                             1/1     Running   229 (34h ago)    64d
kube-system            vpnkit-controller                               1/1     Running   4395 (17m ago)   64d
kubernetes-dashboard   dashboard-metrics-scraper-7c857855d9-g4slf      1/1     Running   1 (11d ago)      15d
kubernetes-dashboard   kubernetes-dashboard-6b79449649-lrnz8           1/1     Running   10 (3d10h ago)   15d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看清单：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k get pod -n dev test-deployment-5d9c9b97bb-7hmqb -o yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: &amp;quot;2022-09-28T12:35:22Z&amp;quot;
  generateName: test-deployment-5d9c9b97bb-
  labels:
    app: nginx-pod
    pod-template-hash: 5d9c9b97bb
  name: test-deployment-5d9c9b97bb-7hmqb
  namespace: dev
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: test-deployment-5d9c9b97bb
    uid: 1dc27f38-b2db-48a1-b76a-ad044e3da2d7
  resourceVersion: &amp;quot;5045804&amp;quot;
  uid: 06aeda67-ccaa-4715-ab1a-992ae409cd12
spec:
  containers:
  - image: nginx:1.17.1
    imagePullPolicy: IfNotPresent
    name: nginx
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-sgghr
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: docker-desktop
  #优先级在普通的 pod 执行优先级的剔除是没什么问题的，但是在 job 控制器来运行的 pod 上就是一个灾难，如果 job 控制器运行的任务计划 pod 正在执行任务，此时因为集群节点的资源不够用导致 job 的 pod 被剔除集群节点，从而导致指定运行的任务被搁置，为了解决这个问题，可以在 PriorityClass 中设置属性 preemptionPolicy ，当它的值为 preemptionLowerPriorty（默认）时，则正常执行抢占策略（表示优先级低会被剔除）。如果将值设置为 Never 时则为默认不抢占，不会被剔除而是等待调度机会。
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-sgghr
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-09-28T12:35:22Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-09-28T12:35:25Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-09-28T12:35:25Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-09-28T12:35:22Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: PodScheduled
  containerStatuses:
  - containerID: docker://1453a8ace21fbf107edd376e3357e7c9880afe6c67b2fdde535092bb8c654ddf
    image: nginx:1.17.1
    imageID: docker-pullable://nginx@sha256:b4b9b3eee194703fc2fa8afa5b7510c77ae70cfba567af1376a573a967c03dbb
    lastState: {}
    name: nginx
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: &amp;quot;2022-09-28T12:35:24Z&amp;quot;
  hostIP: 192.168.65.4
  phase: Running
  podIP: 10.1.0.57
  podIPs:
  - ip: 10.1.0.57
  qosClass: BestEffort
  startTime: &amp;quot;2022-09-28T12:35:22Z&amp;quot; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org4ffc8cc&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;statefulset&#34;&gt;Statefulset&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;orgc4ecdec&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;应用场景&#34;&gt;应用场景&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;稳定的不共享持久化存储：即每个pod的存储资源是不共享的，且pod重新调度后还是能访问到相同的持久化数据，基于pvc实现。&lt;/li&gt;
&lt;li&gt;稳定的网络标志：即pod重新调度后其PodName和HostName不变，且PodName和HostName是相同的，基于Headless Service来实现的。&lt;/li&gt;
&lt;li&gt;有序部署，有序扩展：即pod是有顺序的，在部署或者扩展的时候是根据定义的顺序依次依序部署的（即从0到N-1,在下一个Pod运行之前所有之前的pod必都是Running状态或者Ready状态），是基于init containers来实现的。&lt;/li&gt;
&lt;li&gt;有序收缩：在pod删除时是从最后一个依次往前删除，即从N-1到0.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;基于上面的特性，可以发现statefulset由以下几个部分组成：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用于定义网络标志（DNS domain）的headless service&lt;/li&gt;
&lt;li&gt;用于**创建pvc的volumeClaimTemplates**&lt;/li&gt;
&lt;li&gt;具体的statefulSet应用&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org75c4d8e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;创建yaml文件示例&#34;&gt;创建yaml文件示例&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None                                 # 要点1，可以不用指定clusterIP
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: &amp;quot;nginx&amp;quot;                           # 要点2，指定serviceName名称
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
          name: web
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org9af1bda&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看效果-2&#34;&gt;查看效果&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# mosp @ mospdeMacBook-Pro in ~/study/myblog/mospan-hugo-blog on git:master x [17:35:39]
$ k get sts -A
NAMESPACE   NAME   READY   AGE
default     web    3/3     24h

# 可以看出StatefulSet不走ReplicalSet
$ k get rs -A
NAMESPACE              NAME                                      DESIRED   CURRENT   READY   AGE
guestbook-system       guestbook-controller-manager-84cd65964f   1         1         1       63d
kube-system            coredns-78fcd69978                        2         2         2       67d
kubernetes-dashboard   dashboard-metrics-scraper-7c857855d9      1         1         1       19d
kubernetes-dashboard   kubernetes-dashboard-6b79449649           1         1         1       19d

# mosp @ mospdeMacBook-Pro in ~/study/myblog/mospan-hugo-blog on git:master x [11:11:00]
$ k get svc -A
NAMESPACE              NAME                                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
default                kubernetes                                     ClusterIP   10.96.0.1       &amp;lt;none&amp;gt;        443/TCP                  67d
default                nginx                                          ClusterIP   None            &amp;lt;none&amp;gt;        80/TCP                   24h
guestbook-system       guestbook-controller-manager-metrics-service   ClusterIP   10.107.190.68   &amp;lt;none&amp;gt;        8443/TCP                 63d
kube-system            kube-dns                                       ClusterIP   10.96.0.10      &amp;lt;none&amp;gt;        53/UDP,53/TCP,9153/TCP   67d
kubernetes-dashboard   dashboard-metrics-scraper                      ClusterIP   10.96.38.104    &amp;lt;none&amp;gt;        8000/TCP                 19d
kubernetes-dashboard   kubernetes-dashboard                           ClusterIP   10.102.50.227   &amp;lt;none&amp;gt;        443/TCP                  19d

# mosp @ mospdeMacBook-Pro in ~/study/myblog/mospan-hugo-blog on git:master x [11:11:13]
$ k get pod -A
NAMESPACE              NAME                                            READY   STATUS    RESTARTS         AGE
default                web-0                                           1/1     Running   0                24h
default                web-1                                           1/1     Running   0                24h
default                web-2                                           1/1     Running   0                24h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgad33d68&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看清单&#34;&gt;查看清单&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;查看service&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k get svc nginx -n -o yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {&amp;quot;apiVersion&amp;quot;:&amp;quot;v1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;Service&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;labels&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx&amp;quot;},&amp;quot;name&amp;quot;:&amp;quot;nginx&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;default&amp;quot;},&amp;quot;spec&amp;quot;:{&amp;quot;clusterIP&amp;quot;:&amp;quot;None&amp;quot;,&amp;quot;ports&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;web&amp;quot;,&amp;quot;port&amp;quot;:80}],&amp;quot;selector&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx&amp;quot;}}}
  creationTimestamp: &amp;quot;2022-10-02T03:06:17Z&amp;quot;
  labels:
    app: nginx
  name: nginx
  namespace: default
  resourceVersion: &amp;quot;5343708&amp;quot;
  uid: ff6a9fb8-4abd-4fda-af08-6ecdf7695b5a
spec:
  clusterIP: None
  clusterIPs:
  - None
  #kube-proxy 基于 spec.internalTrafficPolicy 的设置来过滤路由的目标服务端点。当它的值设为 Local 时，只选择节点本地的服务端点。当它的值设为 Cluster 或缺省时，则选择所有的服务端点。
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  #用spec.ipFamilyPolicy字段配置的，可设置为以下选项之一：
  #SingleStack
  #PreferDualStack
  #RequireDualStack
  #要使用双堆栈支持，你需要设置.spec.ipFamilyPolicy为PreferredDualStack或RequiredDualStack。此功能在Kubernetes中默认启用，还包括通过IPv4和IPv6地址的脱离集群出口路由。
  ipFamilyPolicy: SingleStack
  ports:
  - name: web
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  #使用svc.spec.sessionAffinity设置会话亲和性，默认是None。指定为ClientIP会使来自同一个Client IP的请求转发到同一个Pod上。
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看StatefulSet&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  $ k get sts web -o yaml
 2  apiVersion: apps/v1
 3  kind: StatefulSet
 4  metadata:
 5    annotations:
 6      kubectl.kubernetes.io/last-applied-configuration: |
 7        {&amp;quot;apiVersion&amp;quot;:&amp;quot;apps/v1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;StatefulSet&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;web&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;default&amp;quot;},&amp;quot;spec&amp;quot;:{&amp;quot;replicas&amp;quot;:3,&amp;quot;selector&amp;quot;:{&amp;quot;matchLabels&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx&amp;quot;}},&amp;quot;serviceName&amp;quot;:&amp;quot;nginx&amp;quot;,&amp;quot;template&amp;quot;:{&amp;quot;metadata&amp;quot;:{&amp;quot;labels&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx&amp;quot;}},&amp;quot;spec&amp;quot;:{&amp;quot;containers&amp;quot;:[{&amp;quot;image&amp;quot;:&amp;quot;nginx:alpine&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;nginx&amp;quot;,&amp;quot;ports&amp;quot;:[{&amp;quot;containerPort&amp;quot;:80,&amp;quot;name&amp;quot;:&amp;quot;web&amp;quot;}]}]}}}}
 8    creationTimestamp: &amp;quot;2022-10-02T03:06:17Z&amp;quot;
 9    generation: 1
10    name: web
11    namespace: default
12    resourceVersion: &amp;quot;5343757&amp;quot;
13    uid: b68fa0b0-ce5e-428c-b39e-b41956ddcc12
14  spec:
15    #pod管理策略,v1.7+可以通过.spec.podManagementPolicy设置pod的管理策略，支持以下俩中方式
16    #OrderedReady:默认方式，按照pod的次序依次创建每个pod并等待ready之后才创建后面的pod。
17    #Parallel：并行创建或删除pod，和deployment类型的pod一样。（不等待前面的pod ready就开始创建所有的pod）。
18    podManagementPolicy: OrderedReady
19    replicas: 3
20    revisionHistoryLimit: 10
21    selector:
22      matchLabels:
23        app: nginx
24    serviceName: nginx
25    template:
26      metadata:
27        creationTimestamp: null
28        labels:
29          app: nginx
30      spec:
31        containers:
32        - image: nginx:alpine
33          imagePullPolicy: IfNotPresent
34          name: nginx
35          ports:
36          - containerPort: 80
37            name: web
38            protocol: TCP
39          resources: {}
40          terminationMessagePath: /dev/termination-log
41          terminationMessagePolicy: File
42        dnsPolicy: ClusterFirst
43        restartPolicy: Always
44        schedulerName: default-scheduler
45        securityContext: {}
46        terminationGracePeriodSeconds: 30
47    updateStrategy:
48      rollingUpdate:
49        partition: 0
50      type: RollingUpdate
51  status:
52    availableReplicas: 3
53    collisionCount: 0
54    currentReplicas: 3
55    currentRevision: web-6d796b6548
56    observedGeneration: 1
57    readyReplicas: 3
58    replicas: 3
59    updateRevision: web-6d796b6548
60    updatedReplicas: 3
61  
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看Pod清单&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k get pod web-0 -o yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: &amp;quot;2022-10-02T03:06:17Z&amp;quot;
  generateName: web-
  labels:
    app: nginx
    controller-revision-hash: web-6d796b6548
    statefulset.kubernetes.io/pod-name: web-0
  name: web-0
  namespace: default
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: StatefulSet
    name: web
    uid: b68fa0b0-ce5e-428c-b39e-b41956ddcc12
  resourceVersion: &amp;quot;5343727&amp;quot;
  uid: 04b8e1f1-eed1-493e-a9b3-3b0791eca087
spec:
  containers:
  - image: nginx:alpine
    imagePullPolicy: IfNotPresent
    name: nginx
    ports:
    - containerPort: 80
      name: web
      protocol: TCP
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-v2xb5
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  hostname: web-0
  nodeName: docker-desktop
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  subdomain: nginx
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-v2xb5
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-10-02T03:06:17Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-10-02T03:06:18Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-10-02T03:06:18Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-10-02T03:06:17Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: PodScheduled
  containerStatuses:
  - containerID: docker://bd6e1b11a24457b4892fd83fb306ec2bac9adca1a280b079b9502a0672a9d521
    image: nginx:alpine
    imageID: docker-pullable://nginx@sha256:082f8c10bd47b6acc8ef15ae61ae45dd8fde0e9f389a8b5cb23c37408642bf5d
    lastState: {}
    name: nginx
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: &amp;quot;2022-10-02T03:06:18Z&amp;quot;
  hostIP: 192.168.65.4
  phase: Running
  podIP: 10.1.0.72
  podIPs:
  - ip: 10.1.0.72
  qosClass: BestEffort
  startTime: &amp;quot;2022-10-02T03:06:17Z&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a id=&#34;orga9c50ad&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;daemonset&#34;&gt;DaemonSet&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;orgface580&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;作用及概念&#34;&gt;作用及概念&lt;/h3&gt;

&lt;p&gt;DaemonSet确保集群中每个（部分）node运行一份po副本，当node加入集群时创建pod，当node离开集群时回收pod。&lt;/p&gt;

&lt;p&gt;如果删除DaemonSet，其创建的所有pod也被删除， DaemonSet中的pod覆盖整个集群。&lt;/p&gt;

&lt;p&gt;当需要在集群内每个node运行同一个pod，使用 DaemonSet是有价值的，以下是典型使用场景：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;运行集群存储守护进程，如 glusterd、ceph&lt;/li&gt;
&lt;li&gt;运行集群日志收集守护进程，如 fluent、 logstash&lt;/li&gt;
&lt;li&gt;运行节点监控守护进程，如 Prometheus Node Exporter， collectd， Datadog agent，&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org9260917&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;配置清单&#34;&gt;配置清单&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ cat daemonset-demo.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: deamonset-demo
  namespace: default
spec:
  selector:
    matchLabels:
     app: nginx-ds
  template:
    metadata:
      labels:
        app: nginx-ds
    spec:
      containers:
      - name: nginx
        image: nginx:1.17.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org89256ec&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看效果-3&#34;&gt;查看效果&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource/daemonset [18:51:51]
$ k get ds -A
NAMESPACE     NAME             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
default       deamonset-demo   1         1         1       1            1           &amp;lt;none&amp;gt;                   10m
kube-system   kube-proxy       1         1         1       1            1           kubernetes.io/os=linux   68d

# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource/daemonset [18:53:22]
$ k get rs -A
NAMESPACE              NAME                                      DESIRED   CURRENT   READY   AGE
guestbook-system       guestbook-controller-manager-84cd65964f   1         1         1       63d
kube-system            coredns-78fcd69978                        2         2         2       68d
kubernetes-dashboard   dashboard-metrics-scraper-7c857855d9      1         1         1       19d
kubernetes-dashboard   kubernetes-dashboard-6b79449649           1         1         1       19d

# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource/daemonset [18:53:28]
$ k get pod -A
NAMESPACE              NAME                                            READY   STATUS    RESTARTS          AGE
default                deamonset-demo-lbmmq                            1/1     Running   0                 10m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org7a1d726&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;job&#34;&gt;Job&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;org1def512&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;作用及概念-1&#34;&gt;作用及概念&lt;/h3&gt;

&lt;p&gt;详见：&lt;a href=&#34;https://blog.csdn.net/u012124304/article/details/107729972&#34;&gt;k8s Job详解&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org59a91b0&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;配置清单-1&#34;&gt;配置清单&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ cat job-demo.yaml

apiVersion: batch/v1
kind: Job
metadata:
  name: echo-time
spec:
  completions: 10 #任务个数
  parallelism: 5 #并行数
  backoffLimit: 2 #失败最多执行几次
  template:
    spec:
      containers:
      - name: echo-time
        image: centos:latest
        imagePullPolicy: IfNotPresent
        command:
        - /bin/bash
        - &amp;quot;-c&amp;quot;
        - &amp;quot;for i in `seq 1 100`;do echo $i: `date` &amp;amp;&amp;amp; sleep 1;done&amp;quot;
      restartPolicy: OnFailure
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org2f3a615&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看效果-4&#34;&gt;查看效果&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource/job [20:17:06]
$ k get job -A
NAMESPACE   NAME        COMPLETIONS   DURATION   AGE
default     echo-time   10/10         3m28s      5m40s

# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource/job [20:20:49]
$ k get pod -A
NAMESPACE              NAME                                            READY   STATUS      RESTARTS         AGE
default                deamonset-demo-lbmmq                            1/1     Running     1 (9h ago)       5d1h
default                echo-time--1-24lfm                              0/1     Completed   0                4m7s
default                echo-time--1-5drjv                              0/1     Completed   0                4m9s
default                echo-time--1-869rd                              0/1     Completed   0                4m7s
default                echo-time--1-8fd7m                              0/1     Completed   0                4m9s
default                echo-time--1-cqfjs                              0/1     Completed   0                5m52s
default                echo-time--1-f276r                              0/1     Completed   0                5m52s
default                echo-time--1-h4v9l                              0/1     Completed   0                5m52s
default                echo-time--1-lx4ww                              0/1     Completed   0                5m52s
default                echo-time--1-qd4kc                              0/1     Completed   0                4m8s
default                echo-time--1-zfchg                              0/1     Completed   0                5m52s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个Pod日志都能从1输出到100。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org6491cbd&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;cronjob&#34;&gt;cronJob&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;org6cd6fcb&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;作用及概念-2&#34;&gt;作用及概念&lt;/h3&gt;

&lt;p&gt;CronJobs 对于创建周期性的、反复重复的任务很有用，例如执行数据备份或者发送邮件。 CronJobs 也可以用来计划在指定时间来执行的独立任务，例如计划当集群看起来很空闲时 执行某个 Job。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org63ce1b0&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;配置清单-2&#34;&gt;配置清单&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ cat cronjob-demo.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cronjob-demo
spec:
  schedule: &amp;quot;*/1 * * * *&amp;quot;
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cronjob-demo
            image: busybox
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org2e52734&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看效果-5&#34;&gt;查看效果&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource [20:27:32]
$ k get cronjobs.batch -A
NAMESPACE   NAME           SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
default     cronjob-demo   */1 * * * *   False     0        40s             7m53s

# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource [20:27:40]
$ k get pod -A
NAMESPACE              NAME                                            READY   STATUS      RESTARTS         AGE
default                cronjob-demo-27761065--1-bjdj7                  0/1     Completed   0                2m47s
default                cronjob-demo-27761066--1-z9ttq                  0/1     Completed   0                107s
default                cronjob-demo-27761067--1-9x5fn                  0/1     Completed   0                47s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过观察发现，它只保留最新的3个pod，其他的将被删除掉免得占用资源。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org546b0d0&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;hpa&#34;&gt;HPA&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;orgba6b263&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;作用及概念-3&#34;&gt;作用及概念&lt;/h3&gt;

&lt;p&gt;&lt;a id=&#34;org278c786&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;label-annotation-selector&#34;&gt;Label&amp;amp;Annotation&amp;amp;Selector&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;orgacecd12&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;label&#34;&gt;Label&lt;/h3&gt;

&lt;p&gt;详见：&lt;a href=&#34;https://www.cnblogs.com/chuanzhang053/p/16351442.html&#34;&gt;k8s中label和label selector的基本概念及使用方法&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org1cb2230&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;annotation&#34;&gt;Annotation&lt;/h3&gt;

&lt;p&gt;详见：&lt;a href=&#34;https://www.jianshu.com/p/21275c1c701c&#34;&gt;K8s Annotation（注解）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org33a8552&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;selector&#34;&gt;Selector&lt;/h3&gt;

&lt;p&gt;Label selector的使用场景:&lt;/p&gt;

&lt;p&gt;1.kube-controller进程通过资源对象RC上定义的Label Selector来筛选要监控的Pod副本的数量，从而实现Pod副本的数量始终符合预期设定的全自动控制流程&lt;/p&gt;

&lt;p&gt;2.kupe-proxy进程通过Service的Label Selector来选择对应的Pod，自动建立器每个Service到对应Pod的请求转发路由表，从而实现Service的智能负载均衡机制&lt;/p&gt;

&lt;p&gt;3.通过对某些Node定义特定的Label,并且在Pod定义文件中使用NodeSelector这种标签调度策略，Kube-scheduler进程可以实现Pod定向调度的特性&lt;/p&gt;

&lt;p&gt;详见: &lt;a href=&#34;https://blog.csdn.net/weixin_35673021/article/details/112946553&#34;&gt;k8s selector_k8s之Label与Selector&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org14788c6&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;p&gt;1.&lt;a href=&#34;https://blog.51cto.com/u_15155091/2723613&#34;&gt;k8s之terminationMessagePath&lt;/a&gt;&lt;br /&gt;
2.&lt;a href=&#34;https://blog.csdn.net/dkfajsldfsdfsd/article/details/81209150&#34;&gt;Kubernetes之DNS&lt;/a&gt;&lt;br /&gt;
3.&lt;a href=&#34;https://www.jianshu.com/p/1f64a4694ace&#34;&gt;Kubernetes——调度器Scheduler&lt;/a&gt;&lt;br /&gt;
4.&lt;a href=&#34;https://cloud.tencent.com/developer/article/1748675&#34;&gt;k8s之securityContext&lt;/a&gt;&lt;br /&gt;
5.&lt;a href=&#34;https://www.jianshu.com/p/229ef1daf986&#34;&gt;Kubernetes的所有者和依赖&lt;/a&gt;&lt;br /&gt;
6.&lt;a href=&#34;https://blog.csdn.net/qq_44079072/article/details/121144611&#34;&gt;k8s之pod详解&lt;/a&gt;&lt;br /&gt;
7.&lt;a href=&#34;https://blog.csdn.net/weixin_45710286/article/details/125474671&#34;&gt;kubernetes 之 pod 调度策略（一）&lt;/a&gt;&lt;br /&gt;
8.&lt;a href=&#34;https://www.jianshu.com/p/ebb8f0ba67f6&#34;&gt;污点（taints）和容忍度（tolerations）&lt;/a&gt;&lt;br /&gt;
9.&lt;a href=&#34;https://www.oomspot.com//post/k8swangluomoxingfuwuneibuliuliangcelue&#34;&gt;k8s网络模型-服务内部流量策略&lt;/a&gt;&lt;br /&gt;
10.&lt;a href=&#34;https://www.jianshu.com/p/cedf8c9d18f1&#34;&gt;k8s中statefulset资源类型的深入理解&lt;/a&gt;&lt;br /&gt;
11.&lt;a href=&#34;https://copyfuture.com/blogs-details/202006012253389106o9ttg1qo5by175&#34;&gt;k8s job简介和访问&lt;/a&gt;&lt;br /&gt;
12.&lt;a href=&#34;https://blog.csdn.net/m0_47288926/article/details/122819880&#34;&gt;k8s-HPA&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
