<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>墨斯潘園 on 墨斯潘園</title>
        <link>http://mospany.github.io/</link>
        <language>zh-CN</language>
        <author>Mospan</author>
        <rights>Copyright (c) 2016, mospan; all rights reserved.</rights>
        <updated>Wed, 14 Dec 2022 18:24:22 CST</updated>
        
        <item>
            <title>K8S项目实践(04): 基于kubebuilder编写operator</title>
            <link>http://mospany.github.io/2022/12/14/operator-on-kubebuilder/</link>
            <pubDate>Wed, 14 Dec 2022 18:24:22 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/12/14/operator-on-kubebuilder/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;orgfd75324&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;介绍&#34;&gt;介绍&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orge181acf&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;镜像仓库&#34;&gt;镜像仓库&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org4dc70c9&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;注册仓库&#34;&gt;注册仓库&lt;/h2&gt;

&lt;p&gt;登录&lt;a href=&#34;https://registry.hub.docker.com/进行注册，如用户名为mospany&#34;&gt;https://registry.hub.docker.com/进行注册，如用户名为mospany&lt;/a&gt;, 密码为自定义。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgcf5186b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;使用镜像&#34;&gt;使用镜像&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;org6869380&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;登录仓库&#34;&gt;登录仓库&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ docker login index.docker.io
Username: mospany
Password:
Login Succeeded

或直接 docker login默认登录docker hub。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org710c04e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;上传镜像&#34;&gt;上传镜像&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ docker tag loggen:latest mospany/loggen:latest
$ docker push mospany/loggen:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/hub-docker.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;参见：&lt;a href=&#34;https://hub.docker.com/repository/docker/mospany/loggen&#34;&gt;https://hub.docker.com/repository/docker/mospany/loggen&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org24bdd0f&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;下载镜像&#34;&gt;下载镜像&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ docker pull mospany/loggen:latest
latest: Pulling from mospany/loggen
Digest: sha256:0cdeece36f8a003dd6b9c463cc73dad93479deabec08c1def033e72ec9818539
Status: Image is up to date for mospany/loggen:latest
docker.io/mospany/loggen:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org3dc8e5f&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;项目&#34;&gt;项目&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org8e7c80b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;创建项目&#34;&gt;创建项目&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;mkdir guestbook
cd guestbook
go mod init guestbook
kubebuilder init --domain xiaohongshu.org --owner &amp;quot;luxiu&amp;quot;
kubebuilder create api --group redis  --version v1 --kind RedisCluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关键截图如下：&lt;br /&gt;
   &lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/kubebuilder-operator.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org28a7b3e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;修改文件&#34;&gt;修改文件&lt;/h2&gt;

&lt;p&gt;1）修改Dockerfile的gcr.io镜像为其他可访问镜像(如golang:1.18)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;为了防止出现“failed to solve with frontend dockerfile.v0: failed to create LLB definition: failed to do request: Head &amp;ldquo;&lt;a href=&#34;https://gcr.io/v2/distroless/static/manifests/nonroot&#34;&gt;https://gcr.io/v2/distroless/static/manifests/nonroot&lt;/a&gt;&amp;ldquo;: Service Unavailable”错误，需修改Dockerfile的gcr.io镜像为其他可访问镜像(如golang:1.18)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;2）修改Dockerfile添加代理&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;为了防止go mod download时不至于超时连不上，需在Run go mod download行上面添加&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;ENV GOPROXY=&amp;quot;https://goproxy.cn&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;否则会出现如下错误：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;3.469 go: cloud.google.com/go@v0.81.0: Get &amp;ldquo;&lt;a href=&#34;https://proxy.golang.org/cloud.google.com/go/@v/v0.81.0.mod&#34;&gt;https://proxy.golang.org/cloud.google.com/go/@v/v0.81.0.mod&lt;/a&gt;&amp;ldquo;: malformed HTTP response &amp;ldquo;\x00\x00\x12\x04\x00\x00\x00\x00\x00\x00\x03\x00\x00\x00\x80\x00\x04\x00\x01\x00\x00\x00\x05\x00\xff\xff\xff\x00\x00\x04\b\x00\x00\x00\x00\x00\u007f\xff\x00\x00\x00\x00\b\a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;**切记切记**： 先修改好再编译，否则一直出现上面错误(当时找了半天)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Build the manager binary
  FROM golang:1.18 as builder
  ENV GOPROXY=&amp;quot;https://goproxy.cn&amp;quot;

  WORKDIR /workspace
  # Copy the Go Modules manifests
  COPY go.mod go.mod
  COPY go.sum go.sum
  # cache deps before building and copying source so that we don&#39;t need to re-download as much
  # and so that source changes don&#39;t invalidate our downloaded layer
  RUN go mod download

  # Copy the go source
  COPY main.go main.go
  COPY api/ api/
  COPY controllers/ controllers/

  # Build
  RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -o manager main.go

  # Use distroless as minimal base image to package the manager binary
  # Refer to https://github.com/GoogleContainerTools/distroless for more details
  #FROM gcr.io/distroless/static:nonroot
  FROM centos:latest
  WORKDIR /
  COPY --from=builder /workspace/manager .
  USER 65532:65532

  ENTRYPOINT [&amp;quot;/manager&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;必须在Dockerfile里面设置代理”ENV GOPROXY=&amp;rdquo;&lt;https://goproxy.cn&#34;“才行，如下设置也不行&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/docker-preferences.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;3）修改Makefile中的crd中的配置&lt;br /&gt;
  给kubectl加上所要连接的集群， 如本机为&amp;#x2013;context docker-desktop。可通过如下命令获得：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config get-contexts
kubectl cluster-info
kubectl config view
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/kubectl-info.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;对应的Makefile修改如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.PHONY: install
install: manifests kustomize ## Install CRDs into the K8s cluster specified in ~/.kube/config.
  $(KUSTOMIZE) build config/crd | kubectl --context docker-desktop  apply -f -

.PHONY: uninstall
uninstall: manifests kustomize ## Uninstall CRDs from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.
  $(KUSTOMIZE) build config/crd | kubectl --context docker-desktop  delete --ignore-not-found=$(ignore-not-found) -f -

.PHONY: deploy
deploy: manifests kustomize ## Deploy controller to the K8s cluster specified in ~/.kube/config.
  cd config/manager &amp;amp;&amp;amp; $(KUSTOMIZE) edit set image controller=${IMG}
  $(KUSTOMIZE) build config/default | kubectl --context docker-desktop  apply -f -

.PHONY: undeploy
undeploy: ## Undeploy controller from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.
  $(KUSTOMIZE) build config/default | kubectl --context docker-desktop  delete --ignore-not-found=$(ignore-not-found) -f -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org36758f2&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;编译&#34;&gt;编译&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;org6bf6172&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;make-help&#34;&gt;make help&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;make help
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/make-help.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org519553b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;make-build&#34;&gt;make build&lt;/h3&gt;

&lt;p&gt;编译并在bin/下生成目标可执行程序。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/make-build.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org2dd91bb&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;make-install&#34;&gt;make install&lt;/h3&gt;

&lt;p&gt;安装crd到目标集群，这一步可能受github网络影响自动下载kustomize慢需要多试几次或隔天再试。&lt;br /&gt;
   &lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/make-install.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org416c2a6&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;make-docker-build&#34;&gt;make docker-build&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/make-docker-build.png&#34; alt=&#34;img&#34; /&gt;&lt;br /&gt;
可以在刚生成的镜像列表中生成镜像。&lt;br /&gt;
&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/make-docker-images.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org80dc9ab&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;make-docker-push&#34;&gt;make docker-push&lt;/h3&gt;

&lt;p&gt;1）先增加要上传镜像的tag&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker tag controller:latest docker.io/mospany/controller:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2）make docker-push&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ make docker-push IMG=docker.io/mospany/controller:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/make-docker-push.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;查看docker hub上传效果&lt;br /&gt;
&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/make-docker-hub.png&#34; alt=&#34;img&#34; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a id=&#34;org7c30fe1&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;运行&#34;&gt;运行&lt;/h2&gt;

&lt;p&gt;先在mac上安装k8s集群，详见：&lt;a href=&#34;https://blog.51cto.com/zlyang/4838042&#34;&gt;Mac系统安装k8s集群&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org48206c2&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;本地运行&#34;&gt;本地运行&lt;/h3&gt;

&lt;p&gt;要想在本地运行 controller，只需要执行下面的命令，你将看到 controller 启动和运行时输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make run
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/make-run.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgaf67979&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;部署到k8s集群中运行&#34;&gt;部署到k8s集群中运行&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;make deploy IMG=docker.io/mospany/controller:v1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/make-deploy.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;查看日志&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl logs -n guestbook-system guestbook-controller-manager-7c67b5bd6c-gm5qs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/make-logs.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgd52a315&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;创建cr&#34;&gt;创建CR&lt;/h2&gt;

&lt;p&gt;该创建自定义资源对象CR了，如原生中的rc/deployment等对象&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# mosp @ mospdeMacBook-Pro in ~/work/pingan/arch/mysrc/guestbook [21:14:17]
$ kubectl get RedisCluster
NAME                  AGE
rediscluster-sample   48m

# mosp @ mospdeMacBook-Pro in ~/work/pingan/arch/mysrc/guestbook [21:22:49]
$ kubectl get RedisCluster -o yaml
apiVersion: v1
items:
- apiVersion: redis.xiaohongshu.org/v1
  kind: RedisCluster
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {&amp;quot;apiVersion&amp;quot;:&amp;quot;redis.xiaohongshu.org/v1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;RedisCluster&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;rediscluster-sample&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;default&amp;quot;},&amp;quot;spec&amp;quot;:null}
    creationTimestamp: &amp;quot;2022-07-29T12:34:11Z&amp;quot;
    generation: 1
    name: rediscluster-sample
    namespace: default
    resourceVersion: &amp;quot;200052&amp;quot;
    uid: 18eaf75f-9597-46af-bd88-abf7153c1377
  status: {}
kind: List
metadata:
  resourceVersion: &amp;quot;&amp;quot;
  selfLink: &amp;quot;&amp;quot;

# mosp @ mospdeMacBook-Pro in ~/work/pingan/arch/mysrc/guestbook [21:23:07]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org6e0a70e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;开发业务逻辑&#34;&gt;开发业务逻辑&lt;/h2&gt;

&lt;p&gt;下面我们将修改 CRD 的数据结构并在 controller 中增加一些日志输出。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org2d593f7&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;修改-crd&#34;&gt;修改 CRD&lt;/h3&gt;

&lt;p&gt;我们将修改api/v1/rediscluster_types.go 文件的内容，在 CRD 中增加 FirstName、LastName 和 Status 字段。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// RedisClusterSpec defines the desired state of RedisCluster
type RedisClusterSpec struct {
    // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster

    // Important: Run &amp;quot;make&amp;quot; to regenerate code after modifying this file

    // Foo is an example field of RedisCluster. Edit rediscluster_types.go to remove/update
    FirstName string `json:&amp;quot;firstname&amp;quot;`
    LastName  string `json:&amp;quot;lastname&amp;quot;`
}

// RedisClusterStatus defines the observed state of RedisCluster
type RedisClusterStatus struct {
    // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster

    // Important: Run &amp;quot;make&amp;quot; to regenerate code after modifying this file
    Status string `json:&amp;quot;Status&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org2e8bba4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;修改-reconcile-函数&#34;&gt;修改 Reconcile 函数&lt;/h3&gt;

&lt;p&gt;Reconcile 函数是 Operator 的核心逻辑，Operator 的业务逻辑都位于 controllers/rediscluster_controller.go 文件的 Reconcile 函数中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *RedisClusterReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
    _ = log.FromContext(ctx)

    // TODO(user): your logic here

    // 获取当前的 CR，并打印
    logger := log.FromContext(ctx)
    obj := &amp;amp;redisv1.RedisCluster{}
    if err := r.Get(ctx, req.NamespacedName, obj); err != nil {
        logger.Error(err, &amp;quot;Unable to fetch object&amp;quot;)
        return ctrl.Result{}, nil
    } else {
        logger.Info(&amp;quot;Greeting from Kubebuilder to&amp;quot;, obj.Spec.FirstName, obj.Spec.LastName)
    }

    // 初始化 CR 的 Status 为 Running
    obj.Status.Status = &amp;quot;Running&amp;quot;
    if err := r.Status().Update(ctx, obj); err != nil {
        logger.Error(err, &amp;quot;unable to update status&amp;quot;)
    }

    return ctrl.Result{}, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org77b1993&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;运行测试&#34;&gt;运行测试&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;安装CRD（同上）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;部署controller（同上）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建CR&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;修改 config/samples/redis_v1_rediscluster.yaml 文件中的配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: redis.xiaohongshu.org/v1
kind: RedisCluster
metadata:
  name: rediscluster-sample
spec:
  # TODO(user): Add fields here
  firstname: Jimmy
  lastname: Song
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行下面命令，创建CR：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k8sdev apply -f  config/samples/redis_v1_rediscluster.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看controller里的运行日志：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/operatorr/controller-logs.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;p&gt;【01】&lt;a href=&#34;https://blog.csdn.net/weixin_43568070/article/details/89892620&#34;&gt;使用shell命令行登陆Docker Hub出现的404Not found的问题&lt;/a&gt;&lt;br /&gt;
【02】&lt;a href=&#34;https://blog.csdn.net/HaHa_Sir/article/details/119412754&#34;&gt;Docker镜像推送Dockerhub&lt;/a&gt;&lt;br /&gt;
【03】&lt;a href=&#34;https://www.cnblogs.com/mysql-dba/p/15982341.html&#34;&gt;使用 kubebuilder 创建并部署 k8s-operator&lt;/a&gt;&lt;br /&gt;
【04】&lt;a href=&#34;http://tnblog.net/hb/article/details/7516&#34;&gt;Kustomize的基本使用&lt;/a&gt;&lt;br /&gt;
【05】&lt;a href=&#34;https://www.cnblogs.com/lizhewei/p/13214785.html&#34;&gt;【kubebuilder2.0】安装、源码分析 &lt;/a&gt;&lt;br /&gt;
【06】&lt;a href=&#34;https://www.cnblogs.com/alisystemsoftware/p/11580202.html&#34;&gt;深入解析 Kubebuilder：让编写 CRD 变得更简单&lt;/a&gt;&lt;br /&gt;
【07】&lt;a href=&#34;https://os.51cto.com/article/661378.html&#34;&gt;一篇带给你KubeBuilder 简明教程&lt;/a&gt;&lt;br /&gt;
【08】&lt;a href=&#34;https://blog.csdn.net/chenxy02/article/details/125554680&#34;&gt;深入解析Kubebuilder&lt;/a&gt;&lt;br /&gt;
【09】&lt;a href=&#34;https://blog.csdn.net/qq_45874107/article/details/119839187&#34;&gt;什么是RBAC&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(03): 随机调度器</title>
            <link>http://mospany.github.io/2022/12/11/k8s-random-scheduler/</link>
            <pubDate>Sun, 11 Dec 2022 11:51:52 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/12/11/k8s-random-scheduler/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;org3ccc143&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org6748da5&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;pod创建过程&#34;&gt;POD创建过程&lt;/h2&gt;

&lt;p&gt;1、 由kubectl解析创建pod的yaml，发送创建pod请求到APIServer。&lt;br /&gt;
2、 APIServer首先做权限认证，然后检查信息并把数据存储到ETCD里，创建deployment资源初始化。&lt;br /&gt;
3、 kube-controller通过list-watch机制，检查发现新的deployment，将资源加入到内部工作队列，检查到资源没有关联pod和replicaset,然后创建rs资源，rs controller监听到rs创建事件后再创建pod资源。&lt;br /&gt;
4、 scheduler 监听到pod创建事件，执行调度算法，将pod绑定到合适节点，然后告知APIServer更新pod的spec.nodeName&lt;br /&gt;
5、 kubelet 每隔一段时间通过其所在节点的NodeName向APIServer拉取绑定到它的pod清单，并更新本地缓存。&lt;br /&gt;
6、 kubelet发现新的pod属于自己，调用容器API来创建容器，并向APIService上报pod状态。&lt;br /&gt;
7、 Kub-proxy为新创建的pod注册动态DNS到CoreOS。为Service添加iptables/ipvs规则，用于服务发现和负载均衡。&lt;br /&gt;
8、 deploy controller对比pod的当前状态和期望来修正状态。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/scheduler/pod-life-cycle.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org44af008&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;调度器介绍&#34;&gt;调度器介绍&lt;/h2&gt;

&lt;p&gt;从上述流程中，我们能大概清楚kube-scheduler的主要工作，负责整个k8s中pod选择和绑定node的工作，这个选择的过程就是应用调度策略，包括NodeAffinity、PodAffinity、节点资源筛选、调度优先级、公平调度等等，而绑定便就是将pod资源定义里的nodeName进行更新。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org39ad97e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;设计&#34;&gt;设计&lt;/h1&gt;

&lt;p&gt;kube-scheduler的设计有两个历史阶段版本：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;基于谓词（predicate）和优先级（priority）的筛选。&lt;/li&gt;
&lt;li&gt;基于调度框架的调度器，新版本已经把所有的旧的设计都改造成扩展点插件形式(1.19+)。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所谓的谓词和优先级都是对调度算法的分类，在scheduler里，谓词调度算法是来选择出一组能够绑定pod的node，而优先级算法则是在这群node中进行打分，得出一个最高分的node。&lt;/p&gt;

&lt;p&gt;而调度框架的设计相比之前则更复杂一点，但确更加灵活和便于扩展，关于调度框架的设计细节可以查看官方文档——624-scheduling-framework，当然我也有一遍文章对其做了翻译还加了一些便于理解的补充——KEP: 624-scheduling-framework。总结来说调度框架的出现是为了解决以前webhooks扩展器的局限性，一个是扩展点只有：筛选、打分、抢占、绑定，而调度框架则在这之上又细分了11个扩展点；另一个则是通过http调用扩展进程的方式其实效率不高，调度框架的设计用的是静态编译的方式将扩展的程序代码和scheduler源码一起编译成新的scheduler，然后通过scheduler配置文件启用需要的插件，在进程内就能通过函数调用的方式执行插件。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/scheduler/scheduler-startup.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上面是一个简略版的调度器处理pod流程：&lt;/p&gt;

&lt;p&gt;首先scheduler会启动一个client-go的Informer来监听Pod事件（不只Pod其实还有Node等资源变更事件），这时候注册的Informer回调事件会区分Pod是否已经被调度（spec.nodeName），已经调度过的Pod则只是更新调度器缓存，而未被调度的Pod会加入到调度队列，然后经过调度框架执行注册的插件，在绑定周期前会进行Pod的假定动作，从而更新调度器缓存中该Pod状态，最后在绑定周期执行完向ApiServer发起BindAPI，从而完成了一次调度过程。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org8ebc078&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;实现&#34;&gt;实现&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org98f2089&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;创建调度器&#34;&gt;创建调度器&lt;/h2&gt;

&lt;p&gt;1、获取集群kubeconfig配置。&lt;br /&gt;
2、调用client-go生成clientset。&lt;br /&gt;
3、填充调度器相关参数, 包含获取node列表和关注的POD。&lt;br /&gt;
4、返回调度器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; func NewScheduler(podQueue chan *v1.Pod, quit chan struct{}) Scheduler {
  config, err := rest.InClusterConfig()
  if err != nil {
      log.Fatal(err)
  }

  clientset, err := kubernetes.NewForConfig(config)
  if err != nil {
      log.Fatal(err)
  }

  return Scheduler{
      clientset:  clientset,
      podQueue:   podQueue,
      nodeLister: initInformers(clientset, podQueue, quit),
      predicates: []predicateFunc{
          randomPredicate,
      },
      priorities: []priorityFunc{
          randomPriority,
      },
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org7e3d0d6&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;运行调度器&#34;&gt;运行调度器&lt;/h2&gt;

&lt;p&gt;不间断的从关注的POD列表中选出进行调度。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org9082797&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;找合适节点&#34;&gt;找合适节点&lt;/h3&gt;

&lt;p&gt;1、找到可用节点列表&lt;br /&gt;
2、给节点随机打100以内的分数&lt;br /&gt;
3、选择分数最高的节点&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (s *Scheduler) findFit(pod *v1.Pod) (string, error) {
    nodes, err := s.nodeLister.List(labels.Everything())
    if err != nil {
        return &amp;quot;&amp;quot;, err
    }

    filteredNodes := s.runPredicates(nodes, pod)
    if len(filteredNodes) == 0 {
        return &amp;quot;&amp;quot;, errors.New(&amp;quot;failed to find node that fits pod&amp;quot;)
    }
    priorities := s.prioritize(filteredNodes, pod)
    return s.findBestNode(priorities), nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org0354198&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;绑定pod&#34;&gt;绑定POD&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;func (s *Scheduler) bindPod(ctx context.Context, p *v1.Pod, node string) error {
    opts := metav1.CreateOptions{}
    return s.clientset.CoreV1().Pods(p.Namespace).Bind(ctx, &amp;amp;v1.Binding{
        ObjectMeta: metav1.ObjectMeta{
            Name:      p.Name,
            Namespace: p.Namespace,
        },
        Target: v1.ObjectReference{
            APIVersion: &amp;quot;v1&amp;quot;,
            Kind:       &amp;quot;Node&amp;quot;,
            Name:       node,
        },
    }, opts)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgcdd0603&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;发送event事件&#34;&gt;发送event事件&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;func (s *Scheduler) emitEvent(ctx context.Context, p *v1.Pod, message string) error {
    timestamp := time.Now().UTC()
    opts := metav1.CreateOptions{}
    _, err := s.clientset.CoreV1().Events(p.Namespace).Create(ctx, &amp;amp;v1.Event{
        Count:          1,
        Message:        message,
        Reason:         &amp;quot;Scheduled&amp;quot;,
        LastTimestamp:  metav1.NewTime(timestamp),
        FirstTimestamp: metav1.NewTime(timestamp),
        Type:           &amp;quot;Normal&amp;quot;,
        Source: v1.EventSource{
            Component: schedulerName,
        },
        InvolvedObject: v1.ObjectReference{
            Kind:      &amp;quot;Pod&amp;quot;,
            Name:      p.Name,
            Namespace: p.Namespace,
            UID:       p.UID,
        },
        ObjectMeta: metav1.ObjectMeta{
            GenerateName: p.Name + &amp;quot;-&amp;quot;,
        },
    }, opts)
    if err != nil {
        return err
    }
    return nil
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看events信息可以看出random-scheduler打出的“laced pod [default/sleep-5b6fd9944c-5scxv] on k8s-master&amp;rdquo;等信息。&amp;rdquo;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;27s         Normal    Scheduled           pod/sleep-5b6fd9944c-5scxv    Placed pod [default/sleep-5b6fd9944c-5scxv] on k8s-master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org22e94bd&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;验证&#34;&gt;验证&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org7cc5355&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;编译&#34;&gt;编译&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ make docker-image
$ make docker-push
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgede963d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;部署&#34;&gt;部署&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f rbac.yaml
$ kubectl  apply -f deployment.yaml
$ kubectl apply -f sleep.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把sleep.yaml里改为schedulerName: random-scheduler就可以使用该调度器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deployment]# kubectl  get pod -A -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS       AGE     IP               NODE         NOMINATED NODE   READINESS GATES
default       httpbin-master                       1/1     Running   2 (11h ago)    3d22h   10.244.0.36      k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       httpbin-worker                       1/1     Running   2 (11h ago)    3d22h   10.244.2.15      k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       netshoot-master                      1/1     Running   2 (11h ago)    3d22h   10.244.0.35      k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       netshoot-worker                      1/1     Running   2 (11h ago)    3d22h   10.244.2.14      k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       random-scheduler-6dc78999cc-vnxzg    1/1     Running   0              9m      10.244.0.37      k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       sleep-5b6fd9944c-7rn5m               1/1     Running   0              11h     10.244.1.6       k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       sleep-5b6fd9944c-bwb9t               1/1     Running   0              11h     10.244.0.38      k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-6d8c4cb4d-ck2x5              1/1     Running   20 (11h ago)   19d     10.244.0.34      k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-6d8c4cb4d-mbctj              1/1     Running   20 (11h ago)   19d     10.244.0.33      k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   etcd-k8s-master                      1/1     Running   22 (11h ago)   19d     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-apiserver-k8s-master            1/1     Running   24 (11h ago)   19d     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-controller-manager-k8s-master   1/1     Running   22 (11h ago)   19d     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-dnsjg                     1/1     Running   21 (11h ago)   19d     172.25.140.215   k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-r84lg                     1/1     Running   22 (11h ago)   19d     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-tbkx2                     1/1     Running   20 (11h ago)   19d     172.25.140.214   k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-scheduler-k8s-master            1/1     Running   22 (11h ago)   19d     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   minicni-node-2xq2d                   1/1     Running   2 (11h ago)    3d23h   172.25.140.214   k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   minicni-node-dsq8c                   1/1     Running   2 (11h ago)    3d23h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   minicni-node-h8hm8                   1/1     Running   2 (11h ago)    3d23h   172.25.140.215   k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出, random-scheduler-6dc78999cc-vnxzg 和 sleep pod已正常变成Running状态。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgdc135cc&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;验证-1&#34;&gt;验证&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deployment]# kubectl  logs random-scheduler-6dc78999cc-vnxzg
 I&#39;m a scheduler!
 2022/12/13 12:12:02 New Node Added to Store: k8s-master
 2022/12/13 12:12:02 New Node Added to Store: k8s-work1
 2022/12/13 12:12:02 New Node Added to Store: k8s-work2
 found a pod to schedule: default / sleep-5b6fd9944c-bwb9t
 2022/12/13 12:12:02 nodes that fit:
 2022/12/13 12:12:02 k8s-master
 2022/12/13 12:12:02 k8s-work1
 2022/12/13 12:12:02 k8s-work2
 2022/12/13 12:12:02 calculated priorities: map[k8s-master:79 k8s-work1:68 k8s-work2:15]
 Placed pod [default/sleep-5b6fd9944c-bwb9t] on k8s-master

 found a pod to schedule: default / sleep-5b6fd9944c-7rn5m
 2022/12/13 12:12:02 nodes that fit:
 2022/12/13 12:12:02 k8s-master
 2022/12/13 12:12:02 k8s-work1
 2022/12/13 12:12:02 calculated priorities: map[k8s-master:26 k8s-work1:50]
 Placed pod [default/sleep-5b6fd9944c-7rn5m] on k8s-work1

 [root@k8s-master deployment]# kubectl logs sleep-5b6fd9944c-7rn5m
 [root@k8s-master deployment]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从random-scheduler日志看， sleep容器经过random-scheduler进行调度的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master deployment]# k get events
 - LAST SEEN   TYPE      REASON              OBJECT                        MESSAGE
 39m         Normal    Pulled              pod/netshoot-master           Container image &amp;quot;nicolaka/netshoot:latest&amp;quot; already present on machine
 39m         Normal    Created             pod/netshoot-master           Created container centos
 39m         Normal    Started             pod/netshoot-master           Started container centos
 39m         Normal    Pulled              pod/netshoot-worker           Container image &amp;quot;nicolaka/netshoot:latest&amp;quot; already present on machine
 39m         Normal    Created             pod/netshoot-worker           Created container centos
 39m         Normal    Started             pod/netshoot-worker           Started container centos
 27s         Normal    Scheduled           pod/sleep-5b6fd9944c-5scxv    Placed pod [default/sleep-5b6fd9944c-5scxv] on k8s-master
 26s         Normal    Pulled              pod/sleep-5b6fd9944c-5scxv    Container image &amp;quot;tutum/curl&amp;quot; already present on machine
 26s         Normal    Created             pod/sleep-5b6fd9944c-5scxv    Created container sleep
 26s         Normal    Started             pod/sleep-5b6fd9944c-5scxv    Started container sleep
 50s         Normal    Killing             pod/sleep-5b6fd9944c-7rn5m    Stopping container sleep
 50s         Normal    Killing             pod/sleep-5b6fd9944c-bwb9t    Stopping container sleep
 27s         Normal    Scheduled           pod/sleep-5b6fd9944c-rccmj    Placed pod [default/sleep-5b6fd9944c-rccmj] on k8s-work2
 26s         Normal    Pulling             pod/sleep-5b6fd9944c-rccmj    Pulling image &amp;quot;tutum/curl&amp;quot;
 27s         Normal    SuccessfulCreate    replicaset/sleep-5b6fd9944c   Created pod: sleep-5b6fd9944c-rccmj
 27s         Normal    SuccessfulCreate    replicaset/sleep-5b6fd9944c   Created pod: sleep-5b6fd9944c-5scxv
 27s         Normal    ScalingReplicaSet   deployment/sleep              Scaled up replica set sleep-5b6fd9944c to 2
 [root@k8s-master deployment]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看events信息可以看出random-scheduler打出的“laced pod [default/sleep-5b6fd9944c-5scxv] on k8s-master&amp;rdquo;等信息。&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org008f2a0&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;p&gt;[01] &lt;a href=&#34;https://www.cnblogs.com/z-gh/p/15409763.html&#34;&gt;k8s调度器介绍（调度框架版本）&lt;/a&gt;&lt;br /&gt;
[02] &lt;a href=&#34;https://zhuanlan.zhihu.com/p/400351590&#34;&gt;client-go功能详解&lt;/a&gt;&lt;br /&gt;
[03] &lt;a href=&#34;https://www.jb51.net/article/253965.htm&#34;&gt;一篇文章搞懂Go语言中的Context&lt;/a&gt;]&lt;br /&gt;
[04] &lt;a href=&#34;https://www.cnblogs.com/yangyuliufeng/p/13611126.html&#34;&gt;深入理解k8s中的informer机制&lt;/a&gt;]&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(02): 动手实现minicni</title>
            <link>http://mospany.github.io/2022/11/22/k8s-practice-minicni/</link>
            <pubDate>Tue, 22 Nov 2022 22:42:55 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/11/22/k8s-practice-minicni/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;orgfaa973a&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;不管是容器网络还是 Kubernetes 网络都需要解决以下两个核心问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;容器/Pod IP 地址的管理&lt;/li&gt;
&lt;li&gt;容器/Pod 之间的相互通信&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;容器/Pod IP 地址的管理包括容器 IP 地址的分配与回收，而容器/Pod 之间的相互通信包括同一主机的容器/Pod 之间和跨主机的容器/Pod 之间通信两种场景。这两个问题也不能完全分开来看，因为不同的解决方案往往要同时考虑以上两点。对于同一主机的容器/Pod 之间的通信来说实现相对容易，实际的挑战在于，不同容器/Pod 完全可能分布在不同的集群节点上，如何实现跨主机节点的通信不是一件容易的事情。&lt;/p&gt;

&lt;p&gt;如果不采用 SDN(Software define networking) 方式来修改底层网络设备的配置，主流方案是在主机节点的 underlay 网络平面构建新的 overlay 网络负责传输容器/Pod 之间通信数据。这种网络方案在如何复用原有的 underlay 网络平面也有不同的实现方式：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;将容器的数据包封装到原主机网络（underlay 网络平面）的三层或四层数据包中，然后使用主机网络的三层或者四层协议传输到目标主机，目标主机拆包后再转发给目标容器；&lt;/li&gt;
&lt;li&gt;把容器网络加到主机路由表中，把主机网络（underlay 网络平面）设备当作容器网关，通过路由规则转发到指定的主机，实现容器的三层互通；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org3709b23&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;cni原理&#34;&gt;CNI原理&lt;/h1&gt;

&lt;p&gt;CNI 规范相对于 CNM(Container Network Model) 对开发者的约束更少、更开放，不依赖于容器运行时，因此也更简单。关于 CNI 规范的详情请查看&lt;a href=&#34;https://github.com/containernetworking/cni/blob/master/SPEC.md&#34;&gt;官方文档&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/minicni/cni-standard.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;详见: &lt;a href=&#34;https://blog.csdn.net/elihe2011/article/details/122926399&#34;&gt;K8S 网络CNI&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;实现一个 CNI 网络插件只需要一个配置文件和一个可执行文件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;配置文件描述插件的版本、名称、描述等基本信息；&lt;/li&gt;
&lt;li&gt;可执行文件会被上层的容器管理平台调用，一个 CNI 可执行文件需要实现将容器加入到网络的 ADD 操作以及将容器从网络中删除的 DEL 操作等；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubernetes 使用 CNI 网络插件的基本工作流程是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kubelet 先创建 pause 容器创建对应的网络命名空间；&lt;/li&gt;
&lt;li&gt;根据配置调用具体的 CNI 插件，可以配置成 CNI 插件链来进行链式调用；&lt;/li&gt;
&lt;li&gt;当 CNI 插件被调用时，它根据环境变量以及命令行参数来获得网络命名空间、容器的网络设备等必要信息，然后执行 ADD 或者其他操作；&lt;/li&gt;
&lt;li&gt;CNI 插件给 pause 容器配置正确的网络，pod 中其他的容器都是复用 pause 容器的网络；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orgec132b9&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;cni配置文件&#34;&gt;CNI配置文件&lt;/h1&gt;

&lt;p&gt;现在我们来到关键的部分。一般来说，CNI 插件需要在集群的每个节点上运行，在 CNI 的规范里面，实现一个 CNI 插件首先需要一个 JSON 格式的配置文件，配置文件需要放到每个节点的 &lt;em&gt;etc/cni/net.d&lt;/em&gt; 目录，一般命名为 &amp;lt;数字&amp;gt;-&lt;CNI-plugin&gt;.conf，而且配置文件至少需要以下几个必须的字段：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;cniVersion: CNI 插件的字符串版本号，要求符合 Semantic Version 2.0 规范；&lt;/li&gt;
&lt;li&gt;name: 字符串形式的网络名；&lt;/li&gt;

&lt;li&gt;&lt;p&gt;type: 字符串表示的 CNI 插件的可运行文件；&lt;br /&gt;
除此之外，我们也可以增加一些自定义的配置字段，用于传递参数给 CNI 插件，这些配置会在运行时传递给 CNI 插件。在我们的例子里面，需要配置每个宿主机网桥的设备名、网络设备的最大传输单元(MTU)以及每个节点分配的24位子网地址，因此，我们的 CNI 插件的配置看起来会像下面这样：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  {
    &amp;quot;cniVersion&amp;quot;: &amp;quot;0.1.0&amp;quot;,
    &amp;quot;name&amp;quot;: &amp;quot;minicni&amp;quot;,
    &amp;quot;type&amp;quot;: &amp;quot;minicni&amp;quot;,
    &amp;quot;bridge&amp;quot;: &amp;quot;minicni0&amp;quot;,
    &amp;quot;mtu&amp;quot;: 1500,
    &amp;quot;subnet&amp;quot;: __NODE_SUBNET__
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: 确保配置文件放到 &lt;em&gt;etc/cni/net.d&lt;/em&gt; 目录，kubelet 默认此目录寻找 CNI 插件配置；并且，插件的配置可以分为多个插件链的形式来运行，但是为了简单起见，在我们的例子中，只配置一个独立的 CNI 插件，因为配置文件的后缀名为 .conf。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a id=&#34;orgeb0c3b0&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;cni核心实现&#34;&gt;CNI核心实现&lt;/h1&gt;

&lt;p&gt;接下来就开始看怎么实现 CNI 插件来管理 pod IP 地址以及配置容器网络设备。在此之前，我们需要明确的是，CNI 介入的时机是 kubelet 创建 pause 容器创建对应的网络命名空间之后，同时当 CNI 插件被调用的时候，kubelet 会将相关操作命令以及参数通过环境变量的形式传递给它。这些环境变量包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CNI_COMMAND: CNI 操作命令，包括 ADD, DEL, CHECK 以及 VERSION&lt;/li&gt;
&lt;li&gt;CNI_CONTAINERID: 容器 ID&lt;/li&gt;
&lt;li&gt;CNI_NETNS: pod 网络命名空间&lt;/li&gt;
&lt;li&gt;CNI_IFNAME: pod 网络设备名称&lt;/li&gt;
&lt;li&gt;CNI_PATH: CNI 插件可执行文件的搜索路径&lt;/li&gt;
&lt;li&gt;CNI_ARGS: 可选的其他参数，形式类似于 key1=value1,key2=value2&amp;#x2026;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在运行时，kubelet 通过 CNI 配置文件寻找 CNI 可执行文件，然后基于上述几个环境变量来执行相关的操作。CNI 插件必须支持的操作包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ADD: 将 pod 加入到 pod 网络中&lt;/li&gt;
&lt;li&gt;DEL: 将 pod 从 pod 网络中删除&lt;/li&gt;
&lt;li&gt;CHECK: 检查 pod 网络配置正常&lt;/li&gt;
&lt;li&gt;VERSION: 返回可选 CNI 插件的版本信息&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;让我们直接跳到 CNI 插件的入口函数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    setupLogger()

    cmd, cmdArgs, err := args.GetArgsFromEnv()
    if err != nil {
        log.Fatalf(&amp;quot;getting cmd arguments with error: %v&amp;quot;, err)
        os.Exit(1)
    }

    fh := handler.NewFileHandler(IPStore)

    switch cmd {
    case &amp;quot;ADD&amp;quot;:
        err = fh.HandleAdd(cmdArgs)
    case &amp;quot;DEL&amp;quot;:
        err = fh.HandleDel(cmdArgs)
    case &amp;quot;CHECK&amp;quot;:
        err = fh.HandleCheck(cmdArgs)
    case &amp;quot;VERSION&amp;quot;:
        err = fh.HandleVersion(cmdArgs)
    default:
        err = fmt.Errorf(&amp;quot;unknown CNI_COMMAND: %s&amp;quot;, cmd)
    }
    if err != nil {
        fmt.Fprintf(os.Stderr, &amp;quot;Failed to handle CNI_COMMAND %q: %v&amp;quot;, cmd, err)
        os.Exit(1)
    }
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，我们首先调用 GetArgsFromEnv() 函数将 CNI 插件的操作命令以及相关参数通过环境变量读入，同时从标准输入获取 CNI 插件的 JSON 配置，然后基于不同的 CNI 操作命令执行不同的处理函数。&lt;/p&gt;

&lt;p&gt;需要注意的是，我们将处理函数的集合实现为一个接口，这样就可以很容易的扩展不同的接口实现。在最基础的版本实现中，我们基本文件存储分配的 IP 信息。但是，这种实现方式存在很多问题，例如，文件存储不可靠，读写可能会发生冲突等，在后续的版本中，我们会实现基于 kubernetes 存储的接口实现，将子网信息以及 IP 信息存储到 apiserver 中，从而实现可靠存储。&lt;/p&gt;

&lt;p&gt;接下来，我们就看看基于文件的接口实现是怎么处理这些 CNI 操作命令的。&lt;br /&gt;
对于 ADD 命令：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;从标准输入获取 CNI 插件的配置信息，最重要的是当前宿主机网桥的设备名、网络设备的最大传输单元(MTU)以及当前节点分配的24位子网地址；&lt;/li&gt;
&lt;li&gt;然后从环境变量中找到对应的 CNI 操作参数，包括 pod 容器网络命名空间以及 pod 网络设备名等；&lt;/li&gt;
&lt;li&gt;接下来创建或者更新节点宿主机网桥，从当前节点分配的24位子网地址中抽取子网的网关地址，准备分配给节点宿主机网桥；&lt;/li&gt;
&lt;li&gt;接着将从文件读取已经分配的 IP 地址列表，遍历24位子网地址并从中取出第一个没有被分配的 IP 地址信息，准备分配给 pod 网络设备；pod 网络设备是 veth 设备对，一端在 pod 网络命名空间中，另外一端连接着宿主机上的网桥设备，同时所有的 pod 网络设备将宿主机上的网桥设备当作默认网关；&lt;/li&gt;
&lt;li&gt;最终成功后需要将新的 pod IP 写入到文件中。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;看起来很简单对吧？其实作为最简单的方式，这种方案可以实现最基础的 ADD 功能, kubelet调用参数如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  2022/12/09 20:17:14 cmd: ADD, ContainerID: 504032948df53a9f7bc7c35d01fb89f32b8db7a1d87514f546b78b91d2d8150a, Netns: /proc/3444/ns/net, ifName: eth0, Path: /opt/cni/bin, Args: IgnoreUnknown=1;K8S_POD_NAMESPACE=kube-system;K8S_POD_NAME=coredns-6d8c4cb4d-mbctj;K8S_POD_INFRA_CONTAINER_ID=504032948df53a9f7bc7c35d01fb89f32b8db7a1d87514f546b78b91d2d8150a, StdinData: {&amp;quot;bridge&amp;quot;:&amp;quot;minicni0&amp;quot;,&amp;quot;cniVersion&amp;quot;:&amp;quot;0.1.0&amp;quot;,&amp;quot;mtu&amp;quot;:1500,&amp;quot;name&amp;quot;:&amp;quot;minicni&amp;quot;,&amp;quot;subnet&amp;quot;:&amp;quot;10.244.0.0/24&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;minicni&amp;quot;}.

2022/12/09 20:17:14 cmd: ADD, ContainerID: 84546b1f14b339f69528d96b60e3b50a983e024daf00a6ef990819d1717aaff7, Netns: /proc/3491/ns/net, ifName: eth0, Path: /opt/cni/bin, Args: IgnoreUnknown=1;K8S_POD_NAMESPACE=kube-system;K8S_POD_NAME=coredns-6d8c4cb4d-ck2x5;K8S_POD_INFRA_CONTAINER_ID=84546b1f14b339f69528d96b60e3b50a983e024daf00a6ef990819d1717aaff7, StdinData: {&amp;quot;bridge&amp;quot;:&amp;quot;minicni0&amp;quot;,&amp;quot;cniVersion&amp;quot;:&amp;quot;0.1.0&amp;quot;,&amp;quot;mtu&amp;quot;:1500,&amp;quot;name&amp;quot;:&amp;quot;minicni&amp;quot;,&amp;quot;subnet&amp;quot;:&amp;quot;10.244.0.0/24&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;minicni&amp;quot;}.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现代码如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  func (fh *FileHandler) HandleAdd(cmdArgs *args.CmdArgs) error {
 2    cniConfig := args.CNIConfiguration{}
 3    if err := json.Unmarshal(cmdArgs.StdinData, &amp;amp;cniConfig); err != nil {
 4        return err
 5    }
 6    allIPs, err := nettool.GetAllIPs(cniConfig.Subnet)
 7    if err != nil {
 8        return err
 9    }
10    gwIP := allIPs[0]
11  
12    // open or create the file that stores all the reserved IPs
13    f, err := os.OpenFile(fh.IPStore, os.O_RDWR|os.O_CREATE, 0600)
14    if err != nil {
15        return fmt.Errorf(&amp;quot;failed to open file that stores reserved IPs %v&amp;quot;, err)
16    }
17    defer f.Close()
18  
19    // get all the reserved IPs from file
20    content, err := ioutil.ReadAll(f)
21    if err != nil {
22        return err
23    }
24    reservedIPs := strings.Split(strings.TrimSpace(string(content)), &amp;quot;\n&amp;quot;)
25  
26    podIP := &amp;quot;&amp;quot;
27    for _, ip := range allIPs[1:] {
28        reserved := false
29        for _, rip := range reservedIPs {
30            if ip == rip {
31                reserved = true
32                break
33            }
34        }
35        if !reserved {
36            podIP = ip
37            reservedIPs = append(reservedIPs, podIP)
38            break
39        }
40    }
41    if podIP == &amp;quot;&amp;quot; {
42        return fmt.Errorf(&amp;quot;no IP available&amp;quot;)
43    }
44  
45    // Create or update bridge
46    brName := cniConfig.Bridge
47    if brName != &amp;quot;&amp;quot; {
48        // fall back to default bridge name: minicni0
49        brName = &amp;quot;minicni0&amp;quot;
50    }
51    mtu := cniConfig.MTU
52    if mtu == 0 {
53        // fall back to default MTU: 1500
54        mtu = 1500
55    }
56    br, err := nettool.CreateOrUpdateBridge(brName, gwIP, mtu)
57    if err != nil {
58        return err
59    }
60  
61    netns, err := ns.GetNS(cmdArgs.Netns)
62    if err != nil {
63        return err
64    }
65  
66    if err := nettool.SetupVeth(netns, br, cmdArgs.IfName, podIP, gwIP, mtu); err != nil {
67        return err
68    }
69  
70    // write reserved IPs back into file
71    if err := ioutil.WriteFile(fh.IPStore, []byte(strings.Join(reservedIPs, &amp;quot;\n&amp;quot;)), 0600); err != nil {
72        return fmt.Errorf(&amp;quot;failed to write reserved IPs into file: %v&amp;quot;, err)
73    }
74  
75    addCmdResult := &amp;amp;AddCmdResult{
76        CniVersion: cniConfig.CniVersion,
77        IPs: &amp;amp;nettool.AllocatedIP{
78            Version: &amp;quot;IPv4&amp;quot;,
79            Address: podIP,
80            Gateway: gwIP,
81        },
82    }
83    addCmdResultBytes, err := json.Marshal(addCmdResult)
84    if err != nil {
85        return err
86    }
87  
88    // kubelet expects json format from stdout if success
89    fmt.Print(string(addCmdResultBytes))
90  
91    return nil
92  }
93  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个关键的问题是如何选择合适的 Go 语言库函数来操作 Linux 网络设备，如创建网桥设备、网络命名空间以及连接 veth 设备对。在我们的例子中，选择了比较成熟的 netlink，实际上，所有基于 iproute2 工具包的命令在 netlink 库中都有对应的 API，例如 ip link add 可以通过调用 AddLink() 函数来实现。&lt;/p&gt;

&lt;p&gt;还有一个问题需要格外小心，那就是处理网络命名空间切换、Go 协程与线程调度问题。在 Linux 中，不同的操作系统线程可能会设置不同的网络命名空间，而 Go 语言的协程会基于操作系统线程的负载以及其他信息动态地在不同的操作系统线程之间切换，这样可能会导致 Go 协程在意想不到的情况下切换到不同的网络命名空间中。&lt;/p&gt;

&lt;p&gt;比较稳妥的做法是，利用 Go 语言提供的 runtime.LockOSThread() 函数保证特定的 Go 协程绑定到当前的操作系统线程中。&lt;/p&gt;

&lt;p&gt;对于 ADD 操作的返回，确保操作成功之后向标准输出中写入 ADD 操作的返回信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    addCmdResult := &amp;amp;AddCmdResult{
    CniVersion: cniConfig.CniVersion,
    IPs: &amp;amp;nettool.AllocatedIP{
        Version: &amp;quot;IPv4&amp;quot;,
        Address: podIP,
        Gateway: gwIP,
    },
}
addCmdResultBytes, err := json.Marshal(addCmdResult)
if err != nil {
    return err
}

// kubelet expects json format from stdout if success
fmt.Print(string(addCmdResultBytes))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;保留IP文件内容如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master ~]# cat /tmp/reserved_ips
 2  10.244.0.2/24
 3  10.244.0.3/24
 4  10.244.0.4/24
 5  10.244.0.5/24
 6  10.244.0.6/24
 7  10.244.0.7/24
 8  10.244.0.8/24
 9  10.244.0.9/24
10  10.244.0.10/24
11  10.244.0.11/24
12  10.244.0.12/24
13  10.244.0.13/24
14  10.244.0.14/24
15  10.244.0.15/24
16  10.244.0.16/24
17  10.244.0.17/24
18  10.244.0.18/24
19  10.244.0.19/24
20  10.244.0.20/24
21  10.244.0.21/24
22  10.244.0.22/24
23  10.244.0.23/24
24  10.244.0.24/24
25  10.244.0.25/24
26  10.244.0.26/24
27  10.244.0.27/24  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其他三个 CNI 操作命令的处理就更简单了。DEL 操作只需要回收分配的 IP 地址，从文件中删除对应的条目，我们不需要处理 pod 网络设备的删除，原因是 kubelet 在删除 pod 网络命名空间之后这些 pod 网络设备也会自动被删除；CHECK 命令检查之前创建的网络设备与配置，暂时是可选的；VERSION 命令以 JSON 形式输出 CNI 版本信息到标准输出。&lt;br /&gt;
CNI_DEL命令参数如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2022/12/09 20:16:55 cmd: DEL, ContainerID: 88722baed9e508ab6cc4525a6b0b05da12c65efa1bbf7c1e27f0fee0c4b4f7e7, Netns: , ifName: eth0, Path: /opt/cni/bin, Args: IgnoreUnknown=1;K8S_POD_NAMESPACE=kube-system;K8S_POD_NAME=coredns-6d8c4cb4d-ck2x5;K8S_POD_INFRA_CONTAINER_ID=88722baed9e508ab6cc4525a6b0b05da12c65efa1bbf7c1e27f0fee0c4b4f7e7, StdinData: {&amp;quot;bridge&amp;quot;:&amp;quot;minicni0&amp;quot;,&amp;quot;cniVersion&amp;quot;:&amp;quot;0.1.0&amp;quot;,&amp;quot;mtu&amp;quot;:1500,&amp;quot;name&amp;quot;:&amp;quot;minicni&amp;quot;,&amp;quot;subnet&amp;quot;:&amp;quot;10.244.0.0/24&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;minicni&amp;quot;}. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;删除操作实现代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1   func (fh *FileHandler) HandleDel(cmdArgs *args.CmdArgs) error {
 2      netns, err := ns.GetNS(cmdArgs.Netns)
 3      if err != nil {
 4          return err
 5      }
 6      ip, err := nettool.GetVethIPInNS(netns, cmdArgs.IfName)
 7      if err != nil {
 8          return err
 9      }
10  
11      // open or create the file that stores all the reserved IPs
12      f, err := os.OpenFile(fh.IPStore, os.O_RDWR|os.O_CREATE, 0600)
13      if err != nil {
14          return fmt.Errorf(&amp;quot;failed to open file that stores reserved IPs %v&amp;quot;, err)
15      }
16      defer f.Close()
17  
18      // get all the reserved IPs from file
19      content, err := ioutil.ReadAll(f)
20      if err != nil {
21          return err
22      }
23      reservedIPs := strings.Split(strings.TrimSpace(string(content)), &amp;quot;\n&amp;quot;)
24  
25      for i, rip := range reservedIPs {
26          if rip == ip {
27              reservedIPs = append(reservedIPs[:i], reservedIPs[i+1:]...)
28              break
29          }
30      }
31  
32      // write reserved IPs back into file
33      if err := ioutil.WriteFile(fh.IPStore, []byte(strings.Join(reservedIPs, &amp;quot;\n&amp;quot;)), 0600); err != nil {
34          return fmt.Errorf(&amp;quot;failed to write reserved IPs into file: %v&amp;quot;, err)
35      }
36  
37      return nil
38  }
39  
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;CNI_VERSION参数如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2022/12/09 20:18:24 cmd: VERSION, ContainerID: , Netns: dummy, ifName: dummy, Path: dummy, Args: , StdinData: {&amp;quot;cniVersion&amp;quot;:&amp;quot;0.4.0&amp;quot;}. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现代码如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1  func (fh *FileHandler) HandleVersion(cmdArgs *args.CmdArgs) error {
2     versionInfo, err := json.Marshal(fh.VersionInfo)
3     if err != nil {
4         return err
5     }
6     fmt.Print(string(versionInfo))
7     return nil
8  }
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orgb4b32d9&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;cni-安装工具&#34;&gt;CNI 安装工具&lt;/h1&gt;

&lt;p&gt;CNI 插件需要运行在集群中的每个节点上，而且 CNI 插件配置信息与可运行文件必须在每个节点特殊的目录中，因此，安装 CNI 插件非常适合使用 DaemonSet 并挂载 CNI 插件目录，为了避免安装 CNI 的工具不能被正常调度，我们需要使用 hostNetwork 来使用宿主机的网络。同时，将 CNI 插件配置以 ConfigMap 的形式挂载，这样方便终端用户配置 CNI 插件。更详细的信息请查看安装工具部署文件。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# k describe node k8s-master  | grep CIDR
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外需要注意的是，我们在安装 CNI 插件的脚本中获取每个节点划分得到的24子网信息、检查是否合法然后写入到 CNI 配置信息中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  # The environment variables used to connect to the kube-apiserver
 2   SERVICE_ACCOUNT_PATH=/var/run/secrets/kubernetes.io/serviceaccount
 3   SERVICEACCOUNT_TOKEN=$(cat $SERVICE_ACCOUNT_PATH/token)
 4   KUBE_CACERT=${KUBE_CACERT:-$SERVICE_ACCOUNT_PATH/ca.crt}
 5   KUBERNETES_SERVICE_PROTOCOL=${KUBERNETES_SERVICE_PROTOCOL-https}
 6  
 7   # Check if we&#39;re running as a k8s pod.
 8   if [ -f &amp;quot;$SERVICE_ACCOUNT_PATH/token&amp;quot; ];
 9   then
10       # some variables should be automatically set inside a pod
11       if [ -z &amp;quot;${KUBERNETES_SERVICE_HOST}&amp;quot; ]; then
12           exit_with_message &amp;quot;KUBERNETES_SERVICE_HOST not set&amp;quot;
13       fi
14       if [ -z &amp;quot;${KUBERNETES_SERVICE_PORT}&amp;quot; ]; then
15           exit_with_message &amp;quot;KUBERNETES_SERVICE_PORT not set&amp;quot;
16       fi
17   fi
18  
19   # exit if the NODE_NAME environment variable is not set.
20   if [[ -z &amp;quot;${NODE_NAME}&amp;quot; ]];
21   then
22       exit_with_message &amp;quot;NODE_NAME not set.&amp;quot;
23   fi
24  
25  
26   NODE_RESOURCE_PATH=&amp;quot;${KUBERNETES_SERVICE_PROTOCOL}://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}/api/v1/nodes/${NODE_NAME}&amp;quot;
27   NODE_SUBNET=$(curl --cacert &amp;quot;${KUBE_CACERT}&amp;quot; --header &amp;quot;Authorization: Bearer ${SERVICEACCOUNT_TOKEN}&amp;quot; -X GET &amp;quot;${NODE_RESOURCE_PATH}&amp;quot; | jq &amp;quot;.spec.podCIDR&amp;quot;)
28  
29   # Check if the node subnet is valid IPv4 CIDR address
30   IPV4_CIDR_REGEX=&amp;quot;(((25[0-5]|2[0-4][0-9]|1?[0-9][0-9]?)\.){3}(25[0-5]|2[0-4][0-9]|1?[0-9][0-9]?))(\/([8-9]|[1-2][0-9]|3[0-2]))([^0-9.]|$)&amp;quot;
31   if [[ ${NODE_SUBNET} =~ ${IPV4_CIDR_REGEX} ]]
32   then
33       echo &amp;quot;${NODE_SUBNET} is a valid IPv4 CIDR address.&amp;quot;
34   else
35       exit_with_message &amp;quot;${NODE_SUBNET} is not a valid IPv4 CIDR address!&amp;quot;
36   fi
37  
38   # exit if the NODE_NAME environment variable is not set.
39   if [[ -z &amp;quot;${CNI_NETWORK_CONFIG}&amp;quot; ]];
40   then
41       exit_with_message &amp;quot;CNI_NETWORK_CONFIG not set.&amp;quot;
42   fi
43  
44   TMP_CONF=&#39;/minicni.conf.tmp&#39;
45   cat &amp;gt;&amp;quot;${TMP_CONF}&amp;quot; &amp;lt;&amp;lt;EOF
46   ${CNI_NETWORK_CONFIG}
47   EOF
48  
49   # Replace the __NODE_SUBNET__
50   grep &amp;quot;__NODE_SUBNET__&amp;quot; &amp;quot;${TMP_CONF}&amp;quot; &amp;amp;&amp;amp; sed -i s~__NODE_SUBNET__~&amp;quot;${NODE_SUBNET}&amp;quot;~g &amp;quot;${TMP_CONF}&amp;quot;
51  
52   # Log the config file
53   echo &amp;quot;CNI config: $(cat &amp;quot;${TMP_CONF}&amp;quot;)&amp;quot;
54  
55   # Move the temporary CNI config into the CNI configuration directory.
56   mv &amp;quot;${TMP_CONF}&amp;quot; &amp;quot;${CNI_NET_DIR}/${CNI_CONF_NAME}&amp;quot; || \
57     exit_with_error &amp;quot;Failed to move ${TMP_CONF} to ${CNI_CONF_NAME}.&amp;quot;
58  
59   echo &amp;quot;Created CNI config ${CNI_CONF_NAME}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org9970dc4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;编译部署&#34;&gt;编译部署&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orgd1c3c68&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-编译&#34;&gt;1、编译&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;make build
make image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/minicni/make-image-minicni.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在docker hub里已看到了minicni镜像：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/minicni/docker-hub-minicni.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org4e72f27&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;部署&#34;&gt;部署&lt;/h2&gt;

&lt;p&gt;1、登录已安装好的k8s集群，把之前已存在的cni如(calico)卸载掉再安装minici:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# kubectl  apply -f minicni.yaml
clusterrole.rbac.authorization.k8s.io/minicni created
serviceaccount/minicni created
clusterrolebinding.rbac.authorization.k8s.io/minicni created
configmap/minicni-config created
Warning: spec.template.spec.nodeSelector[beta.kubernetes.io/os]: deprecated since v1.14; use &amp;quot;kubernetes.io/os&amp;quot; instead
Warning: spec.template.metadata.annotations[scheduler.alpha.kubernetes.io/critical-pod]: non-functional in v1.16+; use the &amp;quot;priorityClassName&amp;quot; field instead
daemonset.apps/minicni-node created
[root@k8s-master minicni]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、查看minicni部署状态：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# kubectl get pod -A -o wide
 NAMESPACE     NAME                                       READY   STATUS        RESTARTS        AGE     IP               NODE         NOMINATED NODE   READINESS GATES
 default       nginx-85b98978db-qkd6h                     0/1     Completed     5               4d21h   &amp;lt;none&amp;gt;           k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   calico-kube-controllers-7b8458594b-p2fqj   0/1     Terminating   2               4d12h   &amp;lt;none&amp;gt;           k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   coredns-6d8c4cb4d-ck2x5                    0/1     Completed     7               4d22h   &amp;lt;none&amp;gt;           k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   coredns-6d8c4cb4d-mbctj                    0/1     Completed     7               4d22h   &amp;lt;none&amp;gt;           k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   etcd-k8s-master                            1/1     Running       8 (8m14s ago)   4d22h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   kube-apiserver-k8s-master                  1/1     Running       8 (8m4s ago)    4d22h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   kube-controller-manager-k8s-master         1/1     Running       8 (8m14s ago)   4d22h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   kube-proxy-dnsjg                           1/1     Running       8 (8m14s ago)   4d21h   172.25.140.215   k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   kube-proxy-r84lg                           1/1     Running       8 (8m14s ago)   4d22h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   kube-proxy-tbkx2                           1/1     Running       7 (8m14s ago)   4d21h   172.25.140.214   k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   kube-scheduler-k8s-master                  1/1     Running       8 (8m14s ago)   4d22h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   minicni-node-8lmxc                         1/1     Running       0               5m17s   172.25.140.214   k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   minicni-node-sgjmg                         1/1     Running       0               5m17s   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 kube-system   minicni-node-xslgx                         1/1     Running       0               5m17s   172.25.140.215   k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出minicni处于Running状态。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org370284e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;测试&#34;&gt;测试&lt;/h1&gt;

&lt;p&gt;1、环境准备, 分别给node打上相应的标签&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# k label nodes k8s-master role=master
node/k8s-master labeled
[root@k8s-master minicni]# k label nodes k8s-work1 role=worker
node/k8s-work1 labeled
[root@k8s-master minicni]# k label nodes k8s-work2 role=worker
node/k8s-work2 labeled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、分别在 master 与 worker 节点部署 netshoot 与 httpbin：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# k apply -f test-pods.yaml
pod/httpbin-master created
pod/netshoot-master created
pod/httpbin-worker created
pod/netshoot-worker created
[root@k8s-master minicni]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、确保所有 pod 都启动并开始运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# k get pod
NAME                     READY   STATUS              RESTARTS   AGE
httpbin-master           0/1     ContainerCreating   0          8s
httpbin-worker           0/1     ContainerCreating   0          8s
netshoot-master          0/1     ContainerCreating   0          8s
netshoot-worker          0/1     ContainerCreating   0          8s
[root@k8s-master minicni]# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;发现状态为ContainerCreating, 查看pod描述报错为:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Warning  FailedCreatePodSandBox  36s                kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container &amp;quot;7cb17eace85729539db7d7af1a4955a353a14eb7ffa96f84c117ee6633b3e2b5&amp;quot; network for pod &amp;quot;httpbin-master&amp;quot;: networkPlugin cni failed to set up pod &amp;quot;httpbin-master_default&amp;quot; network: error getting ClusterInformation: connection is unauthorized: Unauthorized, failed to clean up sandbox container &amp;quot;7cb17eace85729539db7d7af1a4955a353a14eb7ffa96f84c117ee6633b3e2b5&amp;quot; network for pod &amp;quot;httpbin-master&amp;quot;: networkPlugin cni failed to teardown pod &amp;quot;httpbin-master_default&amp;quot; network: error getting ClusterInformation: connection is unauthorized: Unauthorized]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果安装了calico网络插件，需要删除calico:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl delete -f  &amp;lt;yaml&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还要去所有节点etc/cni/net.d/目录下 删掉与calico相关的所有配置文件, 然后重启机器。 不然pod起不来，会报错 network: error getting ClusterInformation: connection is unauthorized: Unauthorized .&lt;/p&gt;

&lt;p&gt;4、最后再查看POD都变成Running状态了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# k get pod -A -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS       AGE     IP               NODE         NOMINATED NODE   READINESS GATES
default       httpbin-master                       1/1     Running   0              20m     10.244.0.4       k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       httpbin-worker                       1/1     Running   0              20m     10.244.2.2       k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       netshoot-master                      1/1     Running   0              20m     10.244.0.5       k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       netshoot-worker                      1/1     Running   0              20m     10.244.2.3       k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
default       nginx-85b98978db-2hzc9               1/1     Running   0              50m     10.244.1.2       k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-6d8c4cb4d-ck2x5              1/1     Running   9 (24h ago)    6d23h   10.244.0.2       k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-6d8c4cb4d-mbctj              1/1     Running   9 (24h ago)    6d23h   10.244.0.3       k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   etcd-k8s-master                      1/1     Running   10 (24h ago)   6d23h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-apiserver-k8s-master            1/1     Running   12 (24h ago)   6d23h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-controller-manager-k8s-master   1/1     Running   10 (24h ago)   6d23h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-dnsjg                     1/1     Running   10 (24h ago)   6d22h   172.25.140.215   k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-r84lg                     1/1     Running   10 (24h ago)   6d23h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-tbkx2                     1/1     Running   9 (24h ago)    6d22h   172.25.140.214   k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-scheduler-k8s-master            1/1     Running   10 (24h ago)   6d23h   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   minicni-node-5w9fn                   1/1     Running   0              55m     172.25.140.215   k8s-work1    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   minicni-node-jb5cj                   1/1     Running   0              55m     172.25.140.214   k8s-work2    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   minicni-node-kp25h                   1/1     Running   0              55m     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
[root@k8s-master minicni]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、之后测试以下四种网络通信是否正常：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;pod 到宿主机的通信&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master minicni]# kubectl exec -ti netshoot-master -- /bin/bash
bash-5.1# ping 172.25.140.216
PING 172.25.140.216 (172.25.140.216) 56(84) bytes of data.
64 bytes from 172.25.140.216: icmp_seq=1 ttl=64 time=0.074 ms
64 bytes from 172.25.140.216: icmp_seq=2 ttl=64 time=0.035 ms
64 bytes from 172.25.140.216: icmp_seq=3 ttl=64 time=0.033 ms
64 bytes from 172.25.140.216: icmp_seq=4 ttl=64 time=0.043 ms
64 bytes from 172.25.140.216: icmp_seq=5 ttl=64 time=0.048 ms
^C
--- 172.25.140.216 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 3999ms
rtt min/avg/max/mdev = 0.033/0.046/0.074/0.014 ms
bash-5.1#
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;同一个节点 pod-to-pod 通信&lt;br /&gt;
默认情况下，同一台主机上的 pod-to-pod 网络包默认会被 Linux 内核丢弃，原因是 Linux 默认会把非 default 网络命名空间的网络包看作是外部数据包，关于这个问题的具体细节，请查看 stackoverflow 上的讨论。目前，我们需要在每个集群结点上使用以下命令手动添加以下 iptables 规则来让 pod-to-pod 网络数据包顺利转发：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# iptables -t filter -A FORWARD -s 10.244.0.0/24  -j ACCEPT
[root@k8s-master ~]# iptables -t filter -A FORWARD -d 10.244.0.0/24  -j ACCEPT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再进行测试:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# kubectl  exec -ti netshoot-master -- /bin/bash
bash-5.1# ping 10.244.0.25
PING 10.244.0.25 (10.244.0.25) 56(84) bytes of data.
64 bytes from 10.244.0.25: icmp_seq=1 ttl=64 time=0.079 ms
64 bytes from 10.244.0.25: icmp_seq=2 ttl=64 time=0.061 ms 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试通信正常。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;pod 到其他主机的通信&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# kubectl  exec -ti netshoot-master -- /bin/bash
bash-5.1# ping 172.25.140.214
PING 172.25.140.214 (172.25.140.214) 56(84) bytes of data.

^C
--- 172.25.140.214 ping statistics ---
4 packets transmitted, 0 received, 100% packet loss, time 2999ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ping不通，原因是阿里云底层网络对非法IP限制，不是随便配个IP就可以通，如需通可以考虑overlay如calico或本地虚拟机搭建方式。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;跨一个节点 pod-to-pod 通信&lt;br /&gt;
对于跨节点的 pod-to-pod 网络包，需要像 Calico 那样添加宿主机的路由表，保证发往各个节点上的 pod 流量经过节点的转发。目前这些路由表需要手动添加：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ip route add 10.244.2.0/24  via 172.25.140.214 dev eth0 #run on master 
ip route add 10.244.0.0/24  via 172.25.140.216 dev eth0 #run on worker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再测试：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# kubectl  exec -ti netshoot-master -- /bin/bash
bash-5.1# ping 10.244.2.11
PING 10.244.2.11 (10.244.2.11) 56(84) bytes of data.

^C
--- 10.244.2.11 ping statistics ---
108 packets transmitted, 0 received, 100% packet loss, time 106996ms    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现象与原理同上。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orgb51c532&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;faq&#34;&gt;FAQ&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orgdc842c7&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;cannot-find-package-编译不过&#34;&gt;cannot find package 编译不过&lt;/h2&gt;

&lt;p&gt;当出现如下错误时：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ make build
Building the minicni on amd64...
cmd/main.go:8:2: cannot find package &amp;quot;github.com/morvencao/minicni/pkg/args&amp;quot; in any of:
 /usr/local/go/src/github.com/morvencao/minicni/pkg/args (from $GOROOT)
 /Users/mosp/goget/src/github.com/morvencao/minicni/pkg/args (from $GOPATH)
cmd/main.go:9:2: cannot find package &amp;quot;github.com/morvencao/minicni/pkg/handler&amp;quot; in any of:
 /usr/local/go/src/github.com/morvencao/minicni/pkg/handler (from $GOROOT)
 /Users/mosp/goget/src/github.com/morvencao/minicni/pkg/handler (from $GOPATH)
make: *** [build] Error 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要把GO111MODULE=on或auto，才能使用Go module功能，可在.bashrc或.zshrc里加上：export GO111MODULE=auto&lt;br /&gt;
详见&lt;a href=&#34;http://www.ay1.cc/article/18635.html&#34;&gt;go自动下载所有的依赖包go module使用详解_Golang&lt;/a&gt;：&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orga2808f1&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;build-constraints-exclude-all-go-files-in&#34;&gt;build constraints exclude all Go files in&lt;/h2&gt;

&lt;p&gt;当出现如下错误时:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ make build
Building the minicni on amd64...
go build github.com/containernetworking/plugins/pkg/ns: build constraints exclude all Go files in /Users/mosp/goget/pkg/mod/github.com/containernetworking/plugins@v1.1.1/pkg/ns
make: *** [build] Error 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要设置如下两个变量：&lt;br /&gt;
export GOOS=“linux”。即：不能为darwin&lt;br /&gt;
export CGO_ENABLED=“1”。&lt;br /&gt;
详见：&lt;a href=&#34;https://blog.csdn.net/weixin_42845682/article/details/124568715&#34;&gt;build constraints exclude all Go files in xxx/xxx/xxx&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orga4ae153&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;p&gt;【01】 &lt;a href=&#34;https://blog.csdn.net/u012772803/article/details/113703029&#34;&gt;find -print0和xargs -0原理及用法&lt;/a&gt;]&lt;br /&gt;
【02】 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/411181637&#34;&gt;Go语言import分组管理利器: goimports-reviser&lt;/a&gt;&lt;br /&gt;
【03】 &lt;a href=&#34;https://morven.life/posts/create-your-own-cni-with-golang/&#34;&gt;使用 Go 从零开始实现 CNI&lt;/a&gt;&lt;br /&gt;
【04】 &lt;a href=&#34;https://blog.csdn.net/a5534789/article/details/112848404&#34;&gt;centOS内网安装kubernetes集群&lt;/a&gt;&lt;br /&gt;
【05】 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/415032187&#34;&gt;Linux 路由表(RIB表、FIB表)、ARP表、MAC表整理&lt;/a&gt;]&lt;br /&gt;
【06】 &lt;a href=&#34;https://www.jianshu.com/p/75704eb30eff&#34;&gt;查看CNI中的veth pair&lt;/a&gt;&lt;br /&gt;
【07】 &lt;a href=&#34;https://mp.weixin.qq.com/s/7t_MoZ0quJF50VoIwqvKyQ&#34;&gt;一文吃透 K8S 网络模型&lt;/a&gt;&lt;br /&gt;
【08】 &lt;a href=&#34;https://blog.csdn.net/elihe2011/article/details/122926399&#34;&gt;K8S 网络CNI&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S项目实践(01): kubeadm安装K8S集群</title>
            <link>http://mospany.github.io/2022/11/23/kubeadm-install-k8s/</link>
            <pubDate>Tue, 22 Nov 2022 20:58:52 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/11/23/kubeadm-install-k8s/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;org4f3c086&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;规划&#34;&gt;规划&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orgfdfecef&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;服务配置&#34;&gt;服务配置&lt;/h2&gt;

&lt;table border=&#34;2&#34; cellspacing=&#34;0&#34; cellpadding=&#34;6&#34; rules=&#34;groups&#34; frame=&#34;hsides&#34;&gt;


&lt;colgroup&gt;
&lt;col  class=&#34;org-left&#34; /&gt;

&lt;col  class=&#34;org-left&#34; /&gt;

&lt;col  class=&#34;org-left&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;OS&lt;/th&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;配置&lt;/th&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;用途&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;CentOS 7.6 (172.25.140.216)&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;2C4G&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;k8s-master&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;CentOS 7.6 (172.25.140.215)&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;2C4G&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;k8s-work1&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;CentOS 7.6 (172.25.140.214)&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;2C4G&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;k8s-work2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;**注**：这是演示 k8s 集群安装的实验环境，配置较低，生产环境中我们的服务器配置至少都是 8C/16G 的基础配置。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgbfb1037&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;版本选择&#34;&gt;版本选择&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;CentOS：7.6&lt;/li&gt;
&lt;li&gt;k8s组件版本：1.23.6&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orge0cd109&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;一-服务器基础配置&#34;&gt;一、服务器基础配置&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orgfbbc6fc&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-配置主机名&#34;&gt;1、 配置主机名&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;[root@iZ8vbfafp7g52u8976flc0Z ~]# hostnamectl set-hostname k8s-master
[root@iZ8vbfafp7g52u8976flc1Z ~]# hostnamectl set-hostname k8s-work1
[root@iZ8vbfafp7g52u8976flc2Z ~]# hostnamectl set-hostname k8s-work2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgec07355&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-关闭防火墙&#34;&gt;2、关闭防火墙&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1  # 关闭firewalld
2  [root@k8s-master ~]# systemctl stop firewalld
3  
4  # 关闭selinux
5  [root@k8s-master ~]# sed -i &#39;s/enforcing/disabled/&#39; /etc/selinux/config
6  [root@k8s-master ~]# setenforce 0
7   setenforce: SELinux is disabled
8  [root@k8s-master ~]#
9  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org1b9a204&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;3-互做本地解析&#34;&gt;3、互做本地解析&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# cat /etc/hosts
172.25.140.216 k8s-master
172.25.140.215 k8s-work1
172.25.140.214 k8s-work2

[root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgf3b9e1a&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;4-ssh-免密通信-可选&#34;&gt;4、SSH 免密通信（可选）&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# ssh-keygen
  Generating public/private rsa key pair.
  Enter file in which to save the key (/root/.ssh/id_rsa):
  Enter passphrase (empty for no passphrase):
  Enter same passphrase again:
  Your identification has been saved in /root/.ssh/id_rsa.
  Your public key has been saved in /root/.ssh/id_rsa.pub.
  The key fingerprint is:
  SHA256:s+JcU9ctsItBgDC+UwxgmFhnsQGpy9SELkEOx4Lmk/0 root@k8s-master
  The key&#39;s randomart image is:
  +---[RSA 2048]----+
  |=*B+Oo ..        |
  |X=o= *.  .       |
  |+== o o   . .    |
  |o* o o   .   + . |
  |+.. +   S o o o .|
  |..   E   + + . . |
  |      . + . .    |
  |     o o .       |
  |      o          |
  +----[SHA256]-----+
  [root@k8s-master ~]#
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行（互发公钥）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# ssh-copy-id root@k8s-work1
  /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &amp;quot;/root/.ssh/id_rsa.pub&amp;quot;
  The authenticity of host &#39;k8s-work1 (172.25.140.215)&#39; can&#39;t be established.
  ECDSA key fingerprint is SHA256:BMN7TIKDbFKdG3v1TVHmy3i6BYm7TGS8Hsnu1F9+UkI.
  ECDSA key fingerprint is MD5:71:60:e2:6c:38:e2:20:d8:9c:94:77:54:cb:10:33:32.
  Are you sure you want to continue connecting (yes/no)? yes
  /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
  /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
  root@k8s-work1&#39;s password:

  Number of key(s) added: 1

  Now try logging into the machine, with:   &amp;quot;ssh &#39;root@k8s-work1&#39;&amp;quot;
  and check to make sure that only the key(s) you wanted were added.

  [root@k8s-master ~]# ssh-copy-id root@k8s-work2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org97fcb39&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;5-加载-br-netfilter-模块&#34;&gt;5、加载 br_netfilter 模块&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;确保 br_netfilter 模块被加载&lt;br /&gt;
所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt; 1  # 加载模块
 2  [root@k8s-master ~]# modprobe br_netfilter
 3  ## 查看加载请看
 4  [root@k8s-master ~]# lsmod | grep br_netfilter
 5  br_netfilter           22256  0
 6  bridge                151336  1 br_netfilter
 7  
 8  # 永久生效
 9  [root@k8s-master ~]# cat &amp;lt;&amp;lt;EOF | tee /etc/modules-load.d/k8s.conf
10  &amp;gt; br_netfilter
11  &amp;gt; EOF
12  br_netfilter
13  [root@k8s-master ~]#
14  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org6cf490e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;6-允许-iptables-检查桥接流量&#34;&gt;6、允许 iptables 检查桥接流量&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt; 1   [root@k8s-master ~]# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
 2   &amp;gt; br_netfilter
 3   &amp;gt; EOF
 4   br_netfilter
 5   [root@k8s-master ~]# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
 6   &amp;gt; net.bridge.bridge-nf-call-ip6tables = 1
 7   &amp;gt; net.bridge.bridge-nf-call-iptables = 1
 8   &amp;gt; EOF
 9   net.bridge.bridge-nf-call-ip6tables = 1
10   net.bridge.bridge-nf-call-iptables = 1
11  [root@k8s-master ~]# sudo sysctl --system
12  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org716427b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;7-关闭-swap&#34;&gt;7、关闭 swap&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1  # 临时关闭
2  [root@k8s-master ~]# swapoff -a
3  
4  # 永久关闭
5  [root@k8s-master ~]# sed -ri &#39;s/.*swap.*/#&amp;amp;/&#39; /etc/fstab
6  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orga15d3fb&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;8-时间同步&#34;&gt;8、时间同步&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1  # 同步网络时间
2  [root@k8s-master ~]# ntpdate time.nist.gov
3  23 Nov 22:36:07 ntpdate[12307]: adjust time server 132.163.96.6 offset -0.009024 sec
4  
5  [root@k8s-master ~]#
6  # 将网络时间写入硬件时间
7  [root@k8s-master ~]# hwclock --systohc
8  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org7ff8672&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;9-安装-docker&#34;&gt;9、安装 Docker&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、使用 sudo 或 root 权限登录 Centos。&lt;/p&gt;

&lt;p&gt;2、确保 yum 包更新到最新。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum update
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、执行 Docker 安装脚本。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -fsSL https://get.docker.com/ | sh 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行这个脚本会添加 docker.repo 源并安装 Docker。&lt;/p&gt;

&lt;p&gt;4、启动 Docker 进程。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo service docker start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、验证 docker 是否安装成功并在容器中执行一个测试的镜像。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo docker run hello-world

[root@k8s-master ~]# sudo docker run hello-world

  Hello from Docker!
  This message shows that your installation appears to be working correctly.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到此，docker 在 CentOS 系统的安装完成。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orge392402&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;10-安装-kubeadm-kubelet&#34;&gt;10、安装 kubeadm、kubelet&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、添加 k8s 镜像源&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;地址：&lt;a href=&#34;https://developer.aliyun.com/mirror/kubernetes?spm=a2c6h.13651102.0.0.1cd01b116JYQIn&#34;&gt;https://developer.aliyun.com/mirror/kubernetes?spm=a2c6h.13651102.0.0.1cd01b116JYQIn&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master ~]# cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo
 2  &amp;gt; [kubernetes]
 3  &amp;gt; name=Kubernetes
 4  &amp;gt; baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
 5  &amp;gt; enabled=1
 6  &amp;gt; gpgcheck=0
 7  &amp;gt; repo_gpgcheck=0
 8  &amp;gt; gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
 9  &amp;gt; EOF
10  [root@k8s-master ~]#
11  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、建立 k8s YUM 缓存&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master ~]# yum makecache
 2  已加载插件：fastestmirror
 3  Loading mirror speeds from cached hostfile
 4  base                                                                                                                                                                                 | 3.6 kB  00:00:00
 5  docker-ce-stable                                                                                                                                                                     | 3.5 kB  00:00:00
 6  epel                                                                                                                                                                                 | 4.7 kB  00:00:00
 7  extras                                                                                                                                                                               | 2.9 kB  00:00:00
 8  updates                                                                                                                                                                              | 2.9 kB  00:00:00
 9  元数据缓存已建立
10  [root@k8s-master ~]#
11  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、安装 k8s 相关工具&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1   # 查看可安装版本
 2   [root@k8s-master ~]# yum list kubelet --showduplicates
 3  
 4   ...
 5   ...
 6   kubelet.x86_64                                           1.23.0-0                                             kubernetes
 7   kubelet.x86_64                                           1.23.1-0                                             kubernetes
 8   kubelet.x86_64                                           1.23.2-0                                             kubernetes
 9   kubelet.x86_64                                           1.23.3-0                                             kubernetes
10   kubelet.x86_64                                           1.23.4-0                                             kubernetes
11   kubelet.x86_64                                           1.23.5-0                                             kubernetes
12   kubelet.x86_64                                           1.23.6-0                                             kubernetes
13  
14   # 开始安装（指定你要安装的版本）
15   [root@k8s-master ~]# yum install -y kubelet-1.23.6 kubeadm-1.23.6 kubectl-1.23.6
16  
17  # 设置开机自启动并启动kubelet（kubelet由systemd管理）
18  [root@k8s-master ~]# systemctl enable kubelet &amp;amp;&amp;amp; systemctl start kubelet
19  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org02e7601&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;二-master-节点&#34;&gt;二、Master 节点&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org88d2a45&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-k8s-初始化&#34;&gt;1、k8s 初始化&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1   [root@k8s-master ~]# kubeadm init \
2  --apiserver-advertise-address=172.25.140.216 \
3  --image-repository registry.aliyuncs.com/google_containers \
4  --kubernetes-version v1.23.6 \
5  --service-cidr=10.96.0.0/12 \
6  --pod-network-cidr=10.244.0.0/16 \
7  --ignore-preflight-errors=all
8  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数说明：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1  --apiserver-advertise-address  # 集群master地址
2  --image-repository             # 指定k8s镜像仓库地址
3  --kubernetes-version           # 指定K8s版本（与kubeadm、kubelet版本保持一致）
4  --service-cidr                 # Pod统一访问入口
5  --pod-network-cidr             # Pod网络（与CNI网络保持一致）
6  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初始化后输出内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  ...
 2  ...
 3  [addons] Applied essential addon: CoreDNS
 4  [addons] Applied essential addon: kube-proxy
 5  
 6  Your Kubernetes control-plane has initialized successfully!
 7  
 8  To start using your cluster, you need to run the following as a regular user:
 9  
10     mkdir -p $HOME/.kube
11     sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
12     sudo chown $(id -u):$(id -g) $HOME/.kube/config
13  
14    Alternatively, if you are the root user, you can run:
15  
16    export KUBECONFIG=/etc/kubernetes/admin.conf
17  
18  You should now deploy a pod network to the cluster.
19  Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
20  https://kubernetes.io/docs/concepts/cluster-administration/addons/
21  
22  Then you can join any number of worker nodes by running the following on each as root:
23  
24  kubeadm join 172.25.140.216:6443 --token 8d9mk7.08nyz6xc2d5boiy8 \
25      --discovery-token-ca-cert-hash sha256:45542b0b380a8f959e5bc93f6dd7d1c5c78b202ff1a3eea5c97804549af9a12e
26  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org0b51700&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-根据输出提示创建相关文件&#34;&gt;2、根据输出提示创建相关文件&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1  [root@k8s-master ~]# mkdir -p $HOME/.kube
2  [root@k8s-master ~]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
3  [root@k8s-master ~]# chown $(id -u):$(id -g) $HOME/.kube/config
4  [root@k8s-master ~]#
5  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org5a02bac&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;3-查看-k8s-运行的容器&#34;&gt;3、查看 k8s 运行的容器&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt; 1  root@k8s-master ~]# kubectl get pods -n kube-system -o wide
 2  NAME                                 READY   STATUS    RESTARTS   AGE     IP               NODE         NOMINATED NODE   READINESS GATES
 3  coredns-6d8c4cb4d-ck2x5              0/1     Pending   0          8m4s    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;       &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 4  coredns-6d8c4cb4d-mbctj              0/1     Pending   0          8m4s    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;       &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 5  etcd-k8s-master                      1/1     Running   0          8m18s   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 6  kube-apiserver-k8s-master            1/1     Running   0          8m18s   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 7  kube-controller-manager-k8s-master   1/1     Running   0          8m18s   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 8  kube-proxy-r84lg                     1/1     Running   0          8m4s    172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 9  kube-scheduler-k8s-master            1/1     Running   0          8m18s   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
10  [root@k8s-master ~]#
11  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org2688836&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;4-查看-k8s-节点&#34;&gt;4、查看 k8s 节点&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1  [root@k8s-master ~]# kubectl  get node -o wide
2  NAME         STATUS     ROLES                  AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME
3  k8s-master   NotReady   control-plane,master   11m   v1.23.6   172.25.140.216   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://20.10.21
4  [root@k8s-master ~]#
5  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可看到当前只有 k8s-master 节点，而且状态是 NotReady（未就绪），因为我们还没有部署网络插件（kubectl apply -f [podnetwork].yaml），于是接着部署容器网络（CNI）。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orge6d35ae&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;5-容器网络-cni-部署&#34;&gt;5、容器网络（CNI）部署&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;br /&gt;
插件地址：&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/addons/&#34;&gt;https://kubernetes.io/docs/concepts/cluster-administration/addons/&lt;/a&gt;&lt;br /&gt;
该地址在 k8s-master 初始化成功时打印出来。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、选择一个主流的容器网络插件部署（Calico）&lt;br /&gt;
   &lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/kubeadm-install-k8s/calico-install-page.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2、下载yml文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master kubeadm-install-k8s]# wget https://docs.projectcalico.org/manifests/calico.yaml --no-check-certificate
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、根据初始化的输出提示执行启动指令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master kubeadm-install-k8s]# kubectl apply -f calico.yaml
 2  poddisruptionbudget.policy/calico-kube-controllers created
 3  serviceaccount/calico-kube-controllers created
 4  serviceaccount/calico-node created
 5  configmap/calico-config created
 6  customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
 7  customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
 8  customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
 9  customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
10  customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
11  customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
12  customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
13  customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
14  customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
15  customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
16  customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
17  customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
18  customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
19  customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
20  customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
21  customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
22  customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
23  clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
24  clusterrole.rbac.authorization.k8s.io/calico-node created
25  clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
26  clusterrolebinding.rbac.authorization.k8s.io/calico-node created
27  daemonset.apps/calico-node created
28  deployment.apps/calico-kube-controllers created
29  [root@k8s-master kubeadm-install-k8s]# 
30  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、看看该yaml文件所需要启动的容器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1   [root@k8s-master kubeadm-install-k8s]# cat calico.yaml |grep image
 2         image: docker.io/calico/cni:v3.24.5
 3         imagePullPolicy: IfNotPresent
 4         image: docker.io/calico/cni:v3.24.5
 5         imagePullPolicy: IfNotPresent
 6         image: docker.io/calico/node:v3.24.5
 7         imagePullPolicy: IfNotPresent
 8         image: docker.io/calico/node:v3.24.5
 9         imagePullPolicy: IfNotPresent
10         image: docker.io/calico/kube-controllers:v3.24.5
11         imagePullPolicy: IfNotPresent
12  [root@k8s-master kubeadm-install-k8s]   #
13  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、查看容器是否都 Running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master kubeadm-install-k8s]# kubectl get pods -n kube-system -o wide
 2  NAME                                       READY   STATUS    RESTARTS   AGE     IP               NODE         NOMINATED NODE   READINESS GATES
 3  calico-kube-controllers-7b8458594b-pdx5z   1/1     Running   0          4m37s   10.244.235.194   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 4  calico-node-t8hhf                          1/1     Running   0          4m37s   172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 5  coredns-6d8c4cb4d-ck2x5                    1/1     Running   0          32m     10.244.235.195   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 6  coredns-6d8c4cb4d-mbctj                    1/1     Running   0          32m     10.244.235.193   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 7  etcd-k8s-master                            1/1     Running   0          32m     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 8  kube-apiserver-k8s-master                  1/1     Running   0          32m     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
 9  kube-controller-manager-k8s-master         1/1     Running   0          32m     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
10  kube-proxy-r84lg                           1/1     Running   0          32m     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
11  kube-scheduler-k8s-master                  1/1     Running   0          32m     172.25.140.216   k8s-master   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
12  [root@k8s-master kubeadm-install-k8s]#
13  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org4f4a98b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;三-work-节点&#34;&gt;三、work 节点&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orgfc01846&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-work-节点加入-k8s-集群&#34;&gt;1、work 节点加入 k8s 集群&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;所有 work 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt; 1  # 复制k8s-master初始化屏幕输出的语句并在work节点执行
 2  [root@k8s-work1 ~]# kubeadm join 172.25.140.216:6443 --token 8d9mk7.08nyz6xc2d5boiy8 --discovery-token-ca-cert-hash sha256:45542b0b380a8f959e5bc93f6dd7d1c5c78b202ff1a3eea5c97804549af9a12e
 3  [root@k8s-work2 ~]# kubeadm join 172.25.140.216:6443 --token 8d9mk7.08nyz6xc2d5boiy8 --discovery-token-ca-cert-hash sha256:45542b0b380a8f959e5bc93f6dd7d1c5c78b202ff1a3eea5c97804549af9a12e
 4  
 5  preflight] Running pre-flight checks
 6  [preflight] Reading configuration from the cluster...
 7  [preflight] FYI: You can look at this config file with &#39;kubectl -n kube-system get cm kubeadm-config -o yaml&#39;
 8  [kubelet-start] Writing kubelet configuration to file &amp;quot;/var/lib/kubelet/config.yaml&amp;quot;
 9  [kubelet-start] Writing kubelet environment file with flags to file &amp;quot;/var/lib/kubelet/kubeadm-flags.env&amp;quot;
10  [kubelet-start] Starting the kubelet
11  [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
12  
13  This node has joined the cluster:
14  * Certificate signing request was sent to apiserver and a response was received.
15  * The Kubelet was informed of the new secure connection details.
16  
17  Run &#39;kubectl get nodes&#39; on the control-plane to see this node join the cluster.
18  
19  [root@k8s-work2 ~]#
20  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org362c07d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-查询集群节点&#34;&gt;2、查询集群节点&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Master 节点执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1  [root@k8s-master kubeadm-install-k8s]# kubectl  get node -o wide
2  NAME         STATUS   ROLES                  AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME
3  k8s-master   Ready    control-plane,master   50m     v1.23.6   172.25.140.216   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://20.10.21
4  k8s-work1    Ready    &amp;lt;none&amp;gt;                 9m50s   v1.23.6   172.25.140.215   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://20.10.21
5  k8s-work2    Ready    &amp;lt;none&amp;gt;                 2m41s   v1.23.6   172.25.140.214   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://20.10.21
6  [root@k8s-master kubeadm-install-k8s]#
7  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;都为就绪状态了&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orge747a84&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;四-验证&#34;&gt;四、验证&lt;/h2&gt;

&lt;p&gt;k8s 集群部署 nginx 服务，并通过浏览器进行访问验证。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org5ec8220&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-创建-pod&#34;&gt;1、创建 pod&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master kubeadm-install-k8s]# kubectl create deployment nginx --image=nginx
 2  deployment.apps/nginx created
 3  [root@k8s-master kubeadm-install-k8s]# kubectl expose deployment nginx --port=80 --type=NodePort
 4  service/nginx exposed
 5  [root@k8s-master kubeadm-install-k8s]# kubectl get pod,svc
 6  NAME                         READY   STATUS    RESTARTS   AGE
 7  pod/nginx-85b98978db-qkd6h   1/1     Running   0          36s
 8  
 9  NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
10  service/kubernetes   ClusterIP   10.96.0.1       &amp;lt;none&amp;gt;        443/TCP        54m
11  service/nginx        NodePort    10.108.180.91   &amp;lt;none&amp;gt;        80:31648/TCP   15s
12  [root@k8s-master kubeadm-install-k8s]#
13  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgd537fac&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-访问-nginx&#34;&gt;2、访问 Nginx&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt; 1  [root@k8s-master kubeadm-install-k8s]# curl 10.108.180.91:80
 2  &amp;lt;!DOCTYPE html&amp;gt;
 3  &amp;lt;html&amp;gt;
 4  &amp;lt;head&amp;gt;
 5  &amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
 6  &amp;lt;style&amp;gt;
 7  html { color-scheme: light dark; }
 8  body { width: 35em; margin: 0 auto;
 9  font-family: Tahoma, Verdana, Arial, sans-serif; }
10  &amp;lt;/style&amp;gt;
11  &amp;lt;/head&amp;gt;
12  &amp;lt;body&amp;gt;
13  &amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
14  &amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
15  working. Further configuration is required.&amp;lt;/p&amp;gt;
16  
17  &amp;lt;p&amp;gt;For online documentation and support please refer to
18  &amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
19  Commercial support is available at
20  &amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;
21  
22  &amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
23  &amp;lt;/body&amp;gt;
24  &amp;lt;/html&amp;gt;
25  [root@k8s-master kubeadm-install-k8s]#
26  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此：kubeadm方式的k8s集群已经部署完成。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orga6cfda4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;faq&#34;&gt;FAQ&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org4405cb7&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-k8s编译报错&#34;&gt;1、k8s编译报错&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt; 1  ...
 2  ...
 3  [kubelet-check] It seems like the kubelet isn&#39;t running or healthy.
 4  [kubelet-check] The HTTP call equal to &#39;curl -sSL http://localhost:10248/healthz&#39; failed with error: Get &amp;quot;http://localhost:10248/healthz&amp;quot;: dial tcp [::1]:10248: connect: connection refused.
 5  
 6   Unfortunately, an error has occurred:
 7       timed out waiting for the condition
 8  
 9   This error is likely caused by:
10       - The kubelet is not running
11       - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)
12  
13   If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
14       - &#39;systemctl status kubelet&#39;
15       - &#39;journalctl -xeu kubelet&#39;
16  
17   Additionally, a control plane component may have crashed or exited when started by the container runtime.
18   To troubleshoot, list all containers using your preferred container runtimes CLI.
19  
20   Here is one example how you may list all Kubernetes containers running in docker:
21       - &#39;docker ps -a | grep kube | grep -v pause&#39;
22       Once you have found the failing container, you can inspect its logs with:
23       - &#39;docker logs CONTAINERID&#39;
24  
25  error execution phase wait-control-plane: couldn&#39;t initialize a Kubernetes cluster
26  To see the stack trace of this error execute with --v=5 or higher
27  
28  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看日志&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    Apr 26 20:33:30 test3 kubelet: I0426 20:33:30.588349   21936 docker_service.go:264] &amp;quot;Docker Info&amp;quot; dockerInfo=&amp;amp;{ID:2NSH:KJPQ:XOKI:5XHN:ULL3:L4LG:SXA4:PR6J:DITW:HHCF:2RKL:U2NJ Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:7 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:false IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:45 SystemTime:2022-04-26T20:33:30.583063427+08:00 LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion: NEventsListener:0 KernelVersion:3.10.0-1160.59.1.el7.x86_64 OperatingSystem:CentOS Linux 7 (Core) OSVersion: OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc000263340 NCPU:2 MemTotal:3873665024 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:k8s-master Labels:[] ExperimentalBuild:false ServerVersion:18.06.3-ce ClusterStore: ClusterAdvertise: Runtimes:map[runc:{Path:docker-runc Args:[] Shim:&amp;lt;nil&amp;gt;}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:&amp;lt;nil&amp;gt; Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:468a545b9edcd5932818eb9de8e72413e616e86e Expected:468a545b9edcd5932818eb9de8e72413e616e86e} RuncCommit:{ID:a592beb5bc4c4092b1b1bac971afed27687340c5 Expected:a592beb5bc4c4092b1b1bac971afed27687340c5} InitCommit:{ID:fec3683 Expected:fec3683} SecurityOptions:[name=seccomp,profile=default] ProductLicense: DefaultAddressPools:[] Warnings:[]}
Apr 26 20:33:30 test3 kubelet: E0426 20:33:30.588383   21936 server.go:302] &amp;quot;Failed to run kubelet&amp;quot; err=&amp;quot;failed to run Kubelet: misconfiguration: kubelet cgroup driver: \&amp;quot;systemd\&amp;quot; is different from docker cgroup driver: \&amp;quot;cgroupfs\&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看报错的最后解释kubelet cgroup driver: \&amp;ldquo;systemd\&amp;rdquo; is different from docker cgroup driver: \&amp;ldquo;cgroupfs\&amp;rdquo;&amp;ldquo;很明显 kubelet 与 Docker 的 cgroup 驱动程序不同，kubelet 为 systemd，而 Docker 为 cgroupfs。&lt;/p&gt;

&lt;p&gt;简单查看一下docker驱动：&lt;br /&gt;
[root@k8s-master opt]# docker info |grep Cgroup&lt;br /&gt;
Cgroup Driver: cgroupfs&lt;/p&gt;

&lt;p&gt;解决方案&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 重置初始化
[root@k8s-master ~]# kubeadm reset

# 删除相关配置文件
[root@k8s-master ~]# rm -rf $HOME/.kube/config  &amp;amp;&amp;amp; rm -rf $HOME/.kube

# 修改 Docker 驱动为 systemd（即&amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;]）
[root@k8s-master opt]# cat /etc/docker/daemon.json 
{
  &amp;quot;registry-mirrors&amp;quot;: [&amp;quot;https://q1rw9tzz.mirror.aliyuncs.com&amp;quot;],
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;]
}

# 重启 Docker
[root@k8s-master opt]# systemctl daemon-reload 
[root@k8s-master opt]# systemctl restart docker.service

# 再次初始化k8s即可
[root@k8s-master ~]# kubeadm init ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org0b33cbe&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-work-节点加入-k8s-集群报错&#34;&gt;2、work 节点加入 k8s 集群报错&lt;/h2&gt;

&lt;p&gt;报错1：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1  accepts at most 1 arg(s), received 3
2  To see the stack trace of this error execute with --v=5 or higher
3  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原因：命令不对，我是直接复制粘贴 k8s-master 初始化的终端输出结果，导致报错，所以最好先复制到 txt 文本下修改好格式再粘贴执行。&lt;/p&gt;

&lt;p&gt;报错2：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1  ...
2  [kubelet-check] The HTTP call equal to &#39;curl -sSL http://localhost:10248/healthz&#39; failed with error: Get &amp;quot;http://localhost:10248/healthz&amp;quot;: dial tcp 127.0.0.1:10248: connect: connection refused.
3  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解决方法同Master&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org9d65a10&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考&#34;&gt;参考&lt;/h1&gt;

&lt;p&gt;[01]&lt;a href=&#34;https://blog.csdn.net/IT_ZRS/article/details/124466870&#34;&gt;kubeadm 部署 k8s 集群&lt;/a&gt;&lt;br /&gt;
[02]&lt;a href=&#34;https://blog.csdn.net/oscarun/article/details/125595521?spm=1001.2014.3001.5502&#34;&gt;kubeadm系列-00-overview&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S学习笔记(07): 服务发现</title>
            <link>http://mospany.github.io/2022/10/18/k8s-service/</link>
            <pubDate>Tue, 18 Oct 2022 22:06:22 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/10/18/k8s-service/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;orga912502&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;service&#34;&gt;service&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org51c5e4e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;p&gt;01.&lt;a href=&#34;https://baijiahao.baidu.com/s?id=1681303264708121640&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;一文讲透K8s的Service概念&lt;/a&gt;&lt;br /&gt;
02.&lt;a href=&#34;https://blog.csdn.net/huahua1999/article/details/124237065&#34;&gt;k8s-service底层之 Iptables与 IPVS&lt;/a&gt;&lt;br /&gt;
03.&lt;a href=&#34;https://blog.csdn.net/Deepexi_Date/article/details/111042410&#34;&gt;K8S的ServiceIP实现原理&lt;/a&gt;&lt;br /&gt;
04.&lt;a href=&#34;http://t.zoukankan.com/fengdejiyixx-p-15568056.html&#34;&gt;http://t.zoukankan.com/fengdejiyixx-p-15568056.html&lt;/a&gt;&lt;br /&gt;
05.&lt;a href=&#34;https://blog.csdn.net/qq_37369726/article/details/121785627&#34;&gt;kubernetes pod间通信,跨namespace互访&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S学习笔记(06): 资源</title>
            <link>http://mospany.github.io/2022/09/28/resource/</link>
            <pubDate>Wed, 28 Sep 2022 20:10:36 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/09/28/resource/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;orge0656ad&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;k8s中所有的内容都抽象为资源， 资源实例化之后，叫做对象。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org32d4fb4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;资源类型介绍&#34;&gt;资源类型介绍&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;工作负载型资源对象（workload）：Pod，ReplicaSet，Deployment，StatefulSet，DaemonSet，Job，Cronjob &amp;#x2026;&lt;/li&gt;
&lt;li&gt;服务发现及均衡资源对象：Service，Ingress &amp;#x2026;&lt;/li&gt;
&lt;li&gt;配置与存储资源对象：Volume(存储卷)，CSI(容器存储接口,可以扩展各种各样的第三方存储卷)，ConfigMap，Secret，DownwardAPI&lt;/li&gt;
&lt;li&gt;集群级资源：Namespace，Node，Role，ClusterRole，RoleBinding，ClusterRoleBinding&lt;/li&gt;
&lt;li&gt;元数据型资源：HPA，PodTemplate，LimitRange&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orga7470a7&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;工作负载资源&#34;&gt;工作负载资源&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org78d773a&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;deployment&#34;&gt;Deployment&lt;/h2&gt;

&lt;p&gt;为了更好地解决服务编排的问题，k8s在V1.2版本开始，引入了deployment控制器，值得一提的是，这种控制器并不直接管理pod，&lt;br /&gt;
而是通过管理replicaset来间接管理pod，即：deployment管理replicaset，replicaset管理pod。所以deployment比replicaset的功能更强大。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/resource/deploy.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;deployment的主要功能有下面几个：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;支持replicaset的所有功能&lt;/li&gt;
&lt;li&gt;支持发布的停止、继续&lt;/li&gt;
&lt;li&gt;支持版本的滚动更新和版本回退&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org3b6bbe8&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;编写资源清单&#34;&gt;编写资源清单&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt; 1  $ cat test-deploy.yaml
 2  apiVersion: apps/v1
 3  kind: Deployment
 4  metadata:
 5    name: test-deployment
 6    namespace: dev
 7  spec:
 8    replicas: 3
 9    selector:
10      matchLabels:
11       app: nginx-pod
12    template:
13      metadata:
14        labels:
15          app: nginx-pod
16      spec:
17        containers:
18        - name: nginx
19          image: nginx:1.17.1
20  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org33be6c7&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;运行清单&#34;&gt;运行清单&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ k apply -f test-deploy.yaml
deployment.apps/test-deployment created 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org1474d63&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看效果&#34;&gt;查看效果&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ k get deploy -A
  NAMESPACE              NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
  dev                    test-deployment                3/3     3            3           94s
  guestbook-system       guestbook-controller-manager   1/1     1            1           59d
  kube-system            coredns                        2/2     2            2           63d
  kubernetes-dashboard   dashboard-metrics-scraper      1/1     1            1           14d
  kubernetes-dashboard   kubernetes-dashboard           1/1     1            1           14d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出deploy已经运行成功，3副本已处于READY状态。&lt;/p&gt;

&lt;p&gt;再查看它运行中的清单:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k get deploy -n dev test-deployment -o yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: &amp;quot;1&amp;quot;
    kubectl.kubernetes.io/last-applied-configuration: |
      {&amp;quot;apiVersion&amp;quot;:&amp;quot;apps/v1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;Deployment&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;test-deployment&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;dev&amp;quot;},&amp;quot;spec&amp;quot;:{&amp;quot;replicas&amp;quot;:3,&amp;quot;selector&amp;quot;:{&amp;quot;matchLabels&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx-pod&amp;quot;}},&amp;quot;template&amp;quot;:{&amp;quot;metadata&amp;quot;:{&amp;quot;labels&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx-pod&amp;quot;}},&amp;quot;spec&amp;quot;:{&amp;quot;containers&amp;quot;:[{&amp;quot;image&amp;quot;:&amp;quot;nginx:1.17.1&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;nginx&amp;quot;}]}}}}
  creationTimestamp: &amp;quot;2022-09-28T12:35:22Z&amp;quot;
  generation: 1
  name: test-deployment
  namespace: dev
  resourceVersion: &amp;quot;5045809&amp;quot;
  uid: 40feb568-1343-4046-8708-7e1bf5e5c384
spec:
  #spec.progressDeadlineSeconds 是可选配置项，用来指定在系统报告Deployment的failed progressing一一表现为resource的状态中 type=Progressing 、 Status=False 、 Reason=ProgressDeadlineExceeded 前可以等待的Deployment进行的秒数。Deployment controller会继续重试该Deployment。未来，在实现了自动回滚后， deployment controller在观察到这种状态时就会自动回滚。
  progressDeadlineSeconds: 600
  #.spec.replicas 是可以选字段，指定期望的pod数量，默认是1。
  replicas: 3
  revisionHistoryLimit: 10
  #.spec.selector是可选字段，用来指定 label selector ，圈定Deployment管理的pod范围。如果被指定， .spec.selector 必须匹配 .spec.template.metadata.labels，否则它将被API拒绝。如果.spec.selector 没有被指定， .spec.selector.matchLabels 默认是.spec.template.metadata.labels。在Pod的template跟.spec.template不同或者数量超过了.spec.replicas规定的数量的情况下，Deployment会杀掉label跟selector不同的Pod。
  selector:
    matchLabels:
      app: nginx-pod
  #.spec.strategy 指定新的Pod替换旧的Pod的策略。 .spec.strategy.type 可以是&amp;quot;Recreate&amp;quot;或者是&amp;quot;RollingUpdate&amp;quot;。&amp;quot;RollingUpdate&amp;quot;是默认值。
  strategy:
    rollingUpdate:
      #spec.strategy.rollingUpdate.maxSurge 是可选配置项，用来指定可以超过期望的Pod数量的最大个数。该值可以是一个绝对值（例如5）或者是期望的Pod数量的百分比（例如10%）。当 MaxUnavailable 为0时该值不可以为0。通过百分比计算的绝对值向上取整。默认值是1。
      maxSurge: 25%
      #.spec.strategy.rollingUpdate.maxUnavailable 是可选配置项，用来指定在升级过程中不可用Pod的最大数量。该值可以是一个绝对值（例如5），也可以是期望Pod数量的百分比（例如10%）。通过计算百分比的绝对值向下取整。 如 果 .spec.strategy.rollingUpdate.maxSurge 为0时，这个值不可以为0。默认值是1。例如，该值设置成30%，启动rolling update后旧的ReplicatSet将会立即缩容到期望的Pod数量的70%。新的Pod ready后，随着新的ReplicaSet的扩容，旧的ReplicaSet会进一步缩容确保在升级的所有时刻可以用的Pod数量至少是期望Pod数量的70%。
      maxUnavailable: 25%
    #滚动更新，简单定义 更新期间pod最多有几个等。可以指定 maxUnavailable 和 maxSurge 来控制 rolling update 进程。
    type: RollingUpdate
  #.spec.template 是 .spec中唯一要求的字段。.spec.template 是 pod template. 它跟 Pod有一模一样的schema，除了它是嵌套的并且不需要apiVersion 和 kind字段。另外为了划分Pod的范围，Deployment中的pod template必须指定适当的label（不要跟其他controller重复了，参考selector）和适当的重启策略。.spec.template.spec.restartPolicy 可以设置为 Always , 如果不指定的话这就是默认配置。
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx-pod
    spec:
      containers:
      - image: nginx:1.17.1
        imagePullPolicy: IfNotPresent
        name: nginx
        resources: {}
        #terminationMessagePath 表示容器的异常终止消息的路径，默认在 /dev/termination-log 下。当容器退出时，可以通过容器的状态看到退出信息。
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      #“ClusterFirst“:如果DNS查询与配置好的默认集群域名前缀不匹配，则将查询请求转发到从节点继承而来，作为查询的上游服务器。
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      #spec:schedulername参数指定调度器的名字，可以为 pod 选择某个调度器进行调度
      schedulerName: default-scheduler
      #安全上下文（Security Context）定义 Pod 或 Container 的特权与访问控制设置。
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 3
  conditions:
  - lastTransitionTime: &amp;quot;2022-09-28T12:35:25Z&amp;quot;
    lastUpdateTime: &amp;quot;2022-09-28T12:35:25Z&amp;quot;
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: &amp;quot;True&amp;quot;
    type: Available
  - lastTransitionTime: &amp;quot;2022-09-28T12:35:22Z&amp;quot;
    lastUpdateTime: &amp;quot;2022-09-28T12:35:25Z&amp;quot;
    message: ReplicaSet &amp;quot;test-deployment-5d9c9b97bb&amp;quot; has successfully progressed.
    reason: NewReplicaSetAvailable
    status: &amp;quot;True&amp;quot;
    type: Progressing
  observedGeneration: 1
  readyReplicas: 3
  replicas: 3
  updatedReplicas: 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org3fa900a&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;replicaset&#34;&gt;ReplicaSet&lt;/h2&gt;

&lt;p&gt;ReplicaSet是kubernetes中的一种副本控制器，主要作用是控制由其管理的pod，使pod副本的数量始终维持在预设的个数。&lt;/p&gt;

&lt;p&gt;kubernetes官方推荐不要直接使用ReplicaSet，用Deployments取而代之，Deployments是比ReplicaSet更高级的概念，它会管理ReplicaSet并提供很多其它有用的特性，最重要的是Deployments支持声明式更新，声明式更好相比于命令式更新的好处是不会丢失历史变更。总结起来就是：不要再直接使用ReplicaSet。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgfdcdb41&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看效果-1&#34;&gt;查看效果&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ k get rs -A
NAMESPACE              NAME                                      DESIRED   CURRENT   READY   AGE
dev                    test-deployment-5d9c9b97bb                3         3         3       23h
guestbook-system       guestbook-controller-manager-84cd65964f   1         1         1       60d
kube-system            coredns-78fcd69978                        2         2         2       64d
kubernetes-dashboard   dashboard-metrics-scraper-7c857855d9      1         1         1       15d
kubernetes-dashboard   kubernetes-dashboard-6b79449649           1         1         1       15d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看清单：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k get rs -n dev test-deployment-5d9c9b97bb -o yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  annotations:
    deployment.kubernetes.io/desired-replicas: &amp;quot;3&amp;quot;
    deployment.kubernetes.io/max-replicas: &amp;quot;4&amp;quot;
    deployment.kubernetes.io/revision: &amp;quot;1&amp;quot;
  creationTimestamp: &amp;quot;2022-09-28T12:35:22Z&amp;quot;
  generation: 1
  labels:
    app: nginx-pod
    pod-template-hash: 5d9c9b97bb
  name: test-deployment-5d9c9b97bb
  namespace: dev
  ownerReferences:
  - apiVersion: apps/v1
    #控制特定的从属对象是否可以阻止垃圾收集删除其所有者对象
    blockOwnerDeletion: true
    controller: true
    kind: Deployment
    name: test-deployment
    uid: 40feb568-1343-4046-8708-7e1bf5e5c384
  resourceVersion: &amp;quot;5045808&amp;quot;
  uid: 1dc27f38-b2db-48a1-b76a-ad044e3da2d7
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-pod
      pod-template-hash: 5d9c9b97bb
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx-pod
        pod-template-hash: 5d9c9b97bb
    spec:
      containers:
      - image: nginx:1.17.1
        imagePullPolicy: IfNotPresent
        name: nginx
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 3
  fullyLabeledReplicas: 3
  observedGeneration: 1
  readyReplicas: 3
  replicas: 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org1c8a1ca&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;pod&#34;&gt;Pod&lt;/h2&gt;

&lt;p&gt;Pod是kubernetes中最小的资源管理组件，Pod也是最小化运行容器化应用的资源对象。一个Pod代表着集群中运行的一个进程。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k get pod -A
NAMESPACE              NAME                                            READY   STATUS    RESTARTS         AGE
dev                    test-deployment-5d9c9b97bb-7hmqb                1/1     Running   0                23h
dev                    test-deployment-5d9c9b97bb-bdwhq                1/1     Running   0                23h
dev                    test-deployment-5d9c9b97bb-l9fj2                1/1     Running   0                23h
guestbook-system       guestbook-controller-manager-84cd65964f-c2z8x   2/2     Running   184 (34h ago)    60d
kube-system            coredns-78fcd69978-s52td                        1/1     Running   7 (11d ago)      64d
kube-system            coredns-78fcd69978-z5fvx                        1/1     Running   7 (11d ago)      64d
kube-system            etcd-docker-desktop                             1/1     Running   7 (11d ago)      64d
kube-system            kube-apiserver-docker-desktop                   1/1     Running   7 (11d ago)      64d
kube-system            kube-controller-manager-docker-desktop          1/1     Running   7 (11d ago)      64d
kube-system            kube-proxy-b5954                                1/1     Running   7 (11d ago)      64d
kube-system            kube-scheduler-docker-desktop                   1/1     Running   146 (34h ago)    64d
kube-system            storage-provisioner                             1/1     Running   229 (34h ago)    64d
kube-system            vpnkit-controller                               1/1     Running   4395 (17m ago)   64d
kubernetes-dashboard   dashboard-metrics-scraper-7c857855d9-g4slf      1/1     Running   1 (11d ago)      15d
kubernetes-dashboard   kubernetes-dashboard-6b79449649-lrnz8           1/1     Running   10 (3d10h ago)   15d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看清单：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k get pod -n dev test-deployment-5d9c9b97bb-7hmqb -o yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: &amp;quot;2022-09-28T12:35:22Z&amp;quot;
  generateName: test-deployment-5d9c9b97bb-
  labels:
    app: nginx-pod
    pod-template-hash: 5d9c9b97bb
  name: test-deployment-5d9c9b97bb-7hmqb
  namespace: dev
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: test-deployment-5d9c9b97bb
    uid: 1dc27f38-b2db-48a1-b76a-ad044e3da2d7
  resourceVersion: &amp;quot;5045804&amp;quot;
  uid: 06aeda67-ccaa-4715-ab1a-992ae409cd12
spec:
  containers:
  - image: nginx:1.17.1
    imagePullPolicy: IfNotPresent
    name: nginx
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-sgghr
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: docker-desktop
  #优先级在普通的 pod 执行优先级的剔除是没什么问题的，但是在 job 控制器来运行的 pod 上就是一个灾难，如果 job 控制器运行的任务计划 pod 正在执行任务，此时因为集群节点的资源不够用导致 job 的 pod 被剔除集群节点，从而导致指定运行的任务被搁置，为了解决这个问题，可以在 PriorityClass 中设置属性 preemptionPolicy ，当它的值为 preemptionLowerPriorty（默认）时，则正常执行抢占策略（表示优先级低会被剔除）。如果将值设置为 Never 时则为默认不抢占，不会被剔除而是等待调度机会。
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-sgghr
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-09-28T12:35:22Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-09-28T12:35:25Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-09-28T12:35:25Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-09-28T12:35:22Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: PodScheduled
  containerStatuses:
  - containerID: docker://1453a8ace21fbf107edd376e3357e7c9880afe6c67b2fdde535092bb8c654ddf
    image: nginx:1.17.1
    imageID: docker-pullable://nginx@sha256:b4b9b3eee194703fc2fa8afa5b7510c77ae70cfba567af1376a573a967c03dbb
    lastState: {}
    name: nginx
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: &amp;quot;2022-09-28T12:35:24Z&amp;quot;
  hostIP: 192.168.65.4
  phase: Running
  podIP: 10.1.0.57
  podIPs:
  - ip: 10.1.0.57
  qosClass: BestEffort
  startTime: &amp;quot;2022-09-28T12:35:22Z&amp;quot; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org4ffc8cc&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;statefulset&#34;&gt;Statefulset&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;orgc4ecdec&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;应用场景&#34;&gt;应用场景&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;稳定的不共享持久化存储：即每个pod的存储资源是不共享的，且pod重新调度后还是能访问到相同的持久化数据，基于pvc实现。&lt;/li&gt;
&lt;li&gt;稳定的网络标志：即pod重新调度后其PodName和HostName不变，且PodName和HostName是相同的，基于Headless Service来实现的。&lt;/li&gt;
&lt;li&gt;有序部署，有序扩展：即pod是有顺序的，在部署或者扩展的时候是根据定义的顺序依次依序部署的（即从0到N-1,在下一个Pod运行之前所有之前的pod必都是Running状态或者Ready状态），是基于init containers来实现的。&lt;/li&gt;
&lt;li&gt;有序收缩：在pod删除时是从最后一个依次往前删除，即从N-1到0.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;基于上面的特性，可以发现statefulset由以下几个部分组成：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用于定义网络标志（DNS domain）的headless service&lt;/li&gt;
&lt;li&gt;用于**创建pvc的volumeClaimTemplates**&lt;/li&gt;
&lt;li&gt;具体的statefulSet应用&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org75c4d8e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;创建yaml文件示例&#34;&gt;创建yaml文件示例&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None                                 # 要点1，可以不用指定clusterIP
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: &amp;quot;nginx&amp;quot;                           # 要点2，指定serviceName名称
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
          name: web
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org9af1bda&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看效果-2&#34;&gt;查看效果&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# mosp @ mospdeMacBook-Pro in ~/study/myblog/mospan-hugo-blog on git:master x [17:35:39]
$ k get sts -A
NAMESPACE   NAME   READY   AGE
default     web    3/3     24h

# 可以看出StatefulSet不走ReplicalSet
$ k get rs -A
NAMESPACE              NAME                                      DESIRED   CURRENT   READY   AGE
guestbook-system       guestbook-controller-manager-84cd65964f   1         1         1       63d
kube-system            coredns-78fcd69978                        2         2         2       67d
kubernetes-dashboard   dashboard-metrics-scraper-7c857855d9      1         1         1       19d
kubernetes-dashboard   kubernetes-dashboard-6b79449649           1         1         1       19d

# mosp @ mospdeMacBook-Pro in ~/study/myblog/mospan-hugo-blog on git:master x [11:11:00]
$ k get svc -A
NAMESPACE              NAME                                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
default                kubernetes                                     ClusterIP   10.96.0.1       &amp;lt;none&amp;gt;        443/TCP                  67d
default                nginx                                          ClusterIP   None            &amp;lt;none&amp;gt;        80/TCP                   24h
guestbook-system       guestbook-controller-manager-metrics-service   ClusterIP   10.107.190.68   &amp;lt;none&amp;gt;        8443/TCP                 63d
kube-system            kube-dns                                       ClusterIP   10.96.0.10      &amp;lt;none&amp;gt;        53/UDP,53/TCP,9153/TCP   67d
kubernetes-dashboard   dashboard-metrics-scraper                      ClusterIP   10.96.38.104    &amp;lt;none&amp;gt;        8000/TCP                 19d
kubernetes-dashboard   kubernetes-dashboard                           ClusterIP   10.102.50.227   &amp;lt;none&amp;gt;        443/TCP                  19d

# mosp @ mospdeMacBook-Pro in ~/study/myblog/mospan-hugo-blog on git:master x [11:11:13]
$ k get pod -A
NAMESPACE              NAME                                            READY   STATUS    RESTARTS         AGE
default                web-0                                           1/1     Running   0                24h
default                web-1                                           1/1     Running   0                24h
default                web-2                                           1/1     Running   0                24h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgad33d68&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看清单&#34;&gt;查看清单&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;查看service&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k get svc nginx -n -o yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {&amp;quot;apiVersion&amp;quot;:&amp;quot;v1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;Service&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;labels&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx&amp;quot;},&amp;quot;name&amp;quot;:&amp;quot;nginx&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;default&amp;quot;},&amp;quot;spec&amp;quot;:{&amp;quot;clusterIP&amp;quot;:&amp;quot;None&amp;quot;,&amp;quot;ports&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;web&amp;quot;,&amp;quot;port&amp;quot;:80}],&amp;quot;selector&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx&amp;quot;}}}
  creationTimestamp: &amp;quot;2022-10-02T03:06:17Z&amp;quot;
  labels:
    app: nginx
  name: nginx
  namespace: default
  resourceVersion: &amp;quot;5343708&amp;quot;
  uid: ff6a9fb8-4abd-4fda-af08-6ecdf7695b5a
spec:
  clusterIP: None
  clusterIPs:
  - None
  #kube-proxy 基于 spec.internalTrafficPolicy 的设置来过滤路由的目标服务端点。当它的值设为 Local 时，只选择节点本地的服务端点。当它的值设为 Cluster 或缺省时，则选择所有的服务端点。
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  #用spec.ipFamilyPolicy字段配置的，可设置为以下选项之一：
  #SingleStack
  #PreferDualStack
  #RequireDualStack
  #要使用双堆栈支持，你需要设置.spec.ipFamilyPolicy为PreferredDualStack或RequiredDualStack。此功能在Kubernetes中默认启用，还包括通过IPv4和IPv6地址的脱离集群出口路由。
  ipFamilyPolicy: SingleStack
  ports:
  - name: web
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  #使用svc.spec.sessionAffinity设置会话亲和性，默认是None。指定为ClientIP会使来自同一个Client IP的请求转发到同一个Pod上。
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看StatefulSet&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1  $ k get sts web -o yaml
 2  apiVersion: apps/v1
 3  kind: StatefulSet
 4  metadata:
 5    annotations:
 6      kubectl.kubernetes.io/last-applied-configuration: |
 7        {&amp;quot;apiVersion&amp;quot;:&amp;quot;apps/v1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;StatefulSet&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;web&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;default&amp;quot;},&amp;quot;spec&amp;quot;:{&amp;quot;replicas&amp;quot;:3,&amp;quot;selector&amp;quot;:{&amp;quot;matchLabels&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx&amp;quot;}},&amp;quot;serviceName&amp;quot;:&amp;quot;nginx&amp;quot;,&amp;quot;template&amp;quot;:{&amp;quot;metadata&amp;quot;:{&amp;quot;labels&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx&amp;quot;}},&amp;quot;spec&amp;quot;:{&amp;quot;containers&amp;quot;:[{&amp;quot;image&amp;quot;:&amp;quot;nginx:alpine&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;nginx&amp;quot;,&amp;quot;ports&amp;quot;:[{&amp;quot;containerPort&amp;quot;:80,&amp;quot;name&amp;quot;:&amp;quot;web&amp;quot;}]}]}}}}
 8    creationTimestamp: &amp;quot;2022-10-02T03:06:17Z&amp;quot;
 9    generation: 1
10    name: web
11    namespace: default
12    resourceVersion: &amp;quot;5343757&amp;quot;
13    uid: b68fa0b0-ce5e-428c-b39e-b41956ddcc12
14  spec:
15    #pod管理策略,v1.7+可以通过.spec.podManagementPolicy设置pod的管理策略，支持以下俩中方式
16    #OrderedReady:默认方式，按照pod的次序依次创建每个pod并等待ready之后才创建后面的pod。
17    #Parallel：并行创建或删除pod，和deployment类型的pod一样。（不等待前面的pod ready就开始创建所有的pod）。
18    podManagementPolicy: OrderedReady
19    replicas: 3
20    revisionHistoryLimit: 10
21    selector:
22      matchLabels:
23        app: nginx
24    serviceName: nginx
25    template:
26      metadata:
27        creationTimestamp: null
28        labels:
29          app: nginx
30      spec:
31        containers:
32        - image: nginx:alpine
33          imagePullPolicy: IfNotPresent
34          name: nginx
35          ports:
36          - containerPort: 80
37            name: web
38            protocol: TCP
39          resources: {}
40          terminationMessagePath: /dev/termination-log
41          terminationMessagePolicy: File
42        dnsPolicy: ClusterFirst
43        restartPolicy: Always
44        schedulerName: default-scheduler
45        securityContext: {}
46        terminationGracePeriodSeconds: 30
47    updateStrategy:
48      rollingUpdate:
49        partition: 0
50      type: RollingUpdate
51  status:
52    availableReplicas: 3
53    collisionCount: 0
54    currentReplicas: 3
55    currentRevision: web-6d796b6548
56    observedGeneration: 1
57    readyReplicas: 3
58    replicas: 3
59    updateRevision: web-6d796b6548
60    updatedReplicas: 3
61  
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看Pod清单&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k get pod web-0 -o yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: &amp;quot;2022-10-02T03:06:17Z&amp;quot;
  generateName: web-
  labels:
    app: nginx
    controller-revision-hash: web-6d796b6548
    statefulset.kubernetes.io/pod-name: web-0
  name: web-0
  namespace: default
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: StatefulSet
    name: web
    uid: b68fa0b0-ce5e-428c-b39e-b41956ddcc12
  resourceVersion: &amp;quot;5343727&amp;quot;
  uid: 04b8e1f1-eed1-493e-a9b3-3b0791eca087
spec:
  containers:
  - image: nginx:alpine
    imagePullPolicy: IfNotPresent
    name: nginx
    ports:
    - containerPort: 80
      name: web
      protocol: TCP
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-v2xb5
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  hostname: web-0
  nodeName: docker-desktop
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  subdomain: nginx
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-v2xb5
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-10-02T03:06:17Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-10-02T03:06:18Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-10-02T03:06:18Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: &amp;quot;2022-10-02T03:06:17Z&amp;quot;
    status: &amp;quot;True&amp;quot;
    type: PodScheduled
  containerStatuses:
  - containerID: docker://bd6e1b11a24457b4892fd83fb306ec2bac9adca1a280b079b9502a0672a9d521
    image: nginx:alpine
    imageID: docker-pullable://nginx@sha256:082f8c10bd47b6acc8ef15ae61ae45dd8fde0e9f389a8b5cb23c37408642bf5d
    lastState: {}
    name: nginx
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: &amp;quot;2022-10-02T03:06:18Z&amp;quot;
  hostIP: 192.168.65.4
  phase: Running
  podIP: 10.1.0.72
  podIPs:
  - ip: 10.1.0.72
  qosClass: BestEffort
  startTime: &amp;quot;2022-10-02T03:06:17Z&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a id=&#34;orga9c50ad&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;daemonset&#34;&gt;DaemonSet&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;orgface580&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;作用及概念&#34;&gt;作用及概念&lt;/h3&gt;

&lt;p&gt;DaemonSet确保集群中每个（部分）node运行一份po副本，当node加入集群时创建pod，当node离开集群时回收pod。&lt;/p&gt;

&lt;p&gt;如果删除DaemonSet，其创建的所有pod也被删除， DaemonSet中的pod覆盖整个集群。&lt;/p&gt;

&lt;p&gt;当需要在集群内每个node运行同一个pod，使用 DaemonSet是有价值的，以下是典型使用场景：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;运行集群存储守护进程，如 glusterd、ceph&lt;/li&gt;
&lt;li&gt;运行集群日志收集守护进程，如 fluent、 logstash&lt;/li&gt;
&lt;li&gt;运行节点监控守护进程，如 Prometheus Node Exporter， collectd， Datadog agent，&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org9260917&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;配置清单&#34;&gt;配置清单&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ cat daemonset-demo.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: deamonset-demo
  namespace: default
spec:
  selector:
    matchLabels:
     app: nginx-ds
  template:
    metadata:
      labels:
        app: nginx-ds
    spec:
      containers:
      - name: nginx
        image: nginx:1.17.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org89256ec&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看效果-3&#34;&gt;查看效果&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource/daemonset [18:51:51]
$ k get ds -A
NAMESPACE     NAME             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
default       deamonset-demo   1         1         1       1            1           &amp;lt;none&amp;gt;                   10m
kube-system   kube-proxy       1         1         1       1            1           kubernetes.io/os=linux   68d

# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource/daemonset [18:53:22]
$ k get rs -A
NAMESPACE              NAME                                      DESIRED   CURRENT   READY   AGE
guestbook-system       guestbook-controller-manager-84cd65964f   1         1         1       63d
kube-system            coredns-78fcd69978                        2         2         2       68d
kubernetes-dashboard   dashboard-metrics-scraper-7c857855d9      1         1         1       19d
kubernetes-dashboard   kubernetes-dashboard-6b79449649           1         1         1       19d

# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource/daemonset [18:53:28]
$ k get pod -A
NAMESPACE              NAME                                            READY   STATUS    RESTARTS          AGE
default                deamonset-demo-lbmmq                            1/1     Running   0                 10m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org7a1d726&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;job&#34;&gt;Job&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;org1def512&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;作用及概念-1&#34;&gt;作用及概念&lt;/h3&gt;

&lt;p&gt;详见：&lt;a href=&#34;https://blog.csdn.net/u012124304/article/details/107729972&#34;&gt;k8s Job详解&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org59a91b0&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;配置清单-1&#34;&gt;配置清单&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ cat job-demo.yaml

apiVersion: batch/v1
kind: Job
metadata:
  name: echo-time
spec:
  completions: 10 #任务个数
  parallelism: 5 #并行数
  backoffLimit: 2 #失败最多执行几次
  template:
    spec:
      containers:
      - name: echo-time
        image: centos:latest
        imagePullPolicy: IfNotPresent
        command:
        - /bin/bash
        - &amp;quot;-c&amp;quot;
        - &amp;quot;for i in `seq 1 100`;do echo $i: `date` &amp;amp;&amp;amp; sleep 1;done&amp;quot;
      restartPolicy: OnFailure
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org2f3a615&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看效果-4&#34;&gt;查看效果&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource/job [20:17:06]
$ k get job -A
NAMESPACE   NAME        COMPLETIONS   DURATION   AGE
default     echo-time   10/10         3m28s      5m40s

# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource/job [20:20:49]
$ k get pod -A
NAMESPACE              NAME                                            READY   STATUS      RESTARTS         AGE
default                deamonset-demo-lbmmq                            1/1     Running     1 (9h ago)       5d1h
default                echo-time--1-24lfm                              0/1     Completed   0                4m7s
default                echo-time--1-5drjv                              0/1     Completed   0                4m9s
default                echo-time--1-869rd                              0/1     Completed   0                4m7s
default                echo-time--1-8fd7m                              0/1     Completed   0                4m9s
default                echo-time--1-cqfjs                              0/1     Completed   0                5m52s
default                echo-time--1-f276r                              0/1     Completed   0                5m52s
default                echo-time--1-h4v9l                              0/1     Completed   0                5m52s
default                echo-time--1-lx4ww                              0/1     Completed   0                5m52s
default                echo-time--1-qd4kc                              0/1     Completed   0                4m8s
default                echo-time--1-zfchg                              0/1     Completed   0                5m52s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个Pod日志都能从1输出到100。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org6491cbd&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;cronjob&#34;&gt;cronJob&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;org6cd6fcb&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;作用及概念-2&#34;&gt;作用及概念&lt;/h3&gt;

&lt;p&gt;CronJobs 对于创建周期性的、反复重复的任务很有用，例如执行数据备份或者发送邮件。 CronJobs 也可以用来计划在指定时间来执行的独立任务，例如计划当集群看起来很空闲时 执行某个 Job。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org63ce1b0&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;配置清单-2&#34;&gt;配置清单&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ cat cronjob-demo.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cronjob-demo
spec:
  schedule: &amp;quot;*/1 * * * *&amp;quot;
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cronjob-demo
            image: busybox
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org2e52734&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查看效果-5&#34;&gt;查看效果&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource [20:27:32]
$ k get cronjobs.batch -A
NAMESPACE   NAME           SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
default     cronjob-demo   */1 * * * *   False     0        40s             7m53s

# mosp @ mospdeMacBook-Pro in ~/study/k8s/resource [20:27:40]
$ k get pod -A
NAMESPACE              NAME                                            READY   STATUS      RESTARTS         AGE
default                cronjob-demo-27761065--1-bjdj7                  0/1     Completed   0                2m47s
default                cronjob-demo-27761066--1-z9ttq                  0/1     Completed   0                107s
default                cronjob-demo-27761067--1-9x5fn                  0/1     Completed   0                47s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过观察发现，它只保留最新的3个pod，其他的将被删除掉免得占用资源。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org546b0d0&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;hpa&#34;&gt;HPA&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;orgba6b263&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;作用及概念-3&#34;&gt;作用及概念&lt;/h3&gt;

&lt;p&gt;&lt;a id=&#34;org278c786&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;label-annotation-selector&#34;&gt;Label&amp;amp;Annotation&amp;amp;Selector&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;orgacecd12&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;label&#34;&gt;Label&lt;/h3&gt;

&lt;p&gt;详见：&lt;a href=&#34;https://www.cnblogs.com/chuanzhang053/p/16351442.html&#34;&gt;k8s中label和label selector的基本概念及使用方法&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org1cb2230&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;annotation&#34;&gt;Annotation&lt;/h3&gt;

&lt;p&gt;详见：&lt;a href=&#34;https://www.jianshu.com/p/21275c1c701c&#34;&gt;K8s Annotation（注解）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org33a8552&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;selector&#34;&gt;Selector&lt;/h3&gt;

&lt;p&gt;Label selector的使用场景:&lt;/p&gt;

&lt;p&gt;1.kube-controller进程通过资源对象RC上定义的Label Selector来筛选要监控的Pod副本的数量，从而实现Pod副本的数量始终符合预期设定的全自动控制流程&lt;/p&gt;

&lt;p&gt;2.kupe-proxy进程通过Service的Label Selector来选择对应的Pod，自动建立器每个Service到对应Pod的请求转发路由表，从而实现Service的智能负载均衡机制&lt;/p&gt;

&lt;p&gt;3.通过对某些Node定义特定的Label,并且在Pod定义文件中使用NodeSelector这种标签调度策略，Kube-scheduler进程可以实现Pod定向调度的特性&lt;/p&gt;

&lt;p&gt;详见: &lt;a href=&#34;https://blog.csdn.net/weixin_35673021/article/details/112946553&#34;&gt;k8s selector_k8s之Label与Selector&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org14788c6&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;p&gt;1.&lt;a href=&#34;https://blog.51cto.com/u_15155091/2723613&#34;&gt;k8s之terminationMessagePath&lt;/a&gt;&lt;br /&gt;
2.&lt;a href=&#34;https://blog.csdn.net/dkfajsldfsdfsd/article/details/81209150&#34;&gt;Kubernetes之DNS&lt;/a&gt;&lt;br /&gt;
3.&lt;a href=&#34;https://www.jianshu.com/p/1f64a4694ace&#34;&gt;Kubernetes——调度器Scheduler&lt;/a&gt;&lt;br /&gt;
4.&lt;a href=&#34;https://cloud.tencent.com/developer/article/1748675&#34;&gt;k8s之securityContext&lt;/a&gt;&lt;br /&gt;
5.&lt;a href=&#34;https://www.jianshu.com/p/229ef1daf986&#34;&gt;Kubernetes的所有者和依赖&lt;/a&gt;&lt;br /&gt;
6.&lt;a href=&#34;https://blog.csdn.net/qq_44079072/article/details/121144611&#34;&gt;k8s之pod详解&lt;/a&gt;&lt;br /&gt;
7.&lt;a href=&#34;https://blog.csdn.net/weixin_45710286/article/details/125474671&#34;&gt;kubernetes 之 pod 调度策略（一）&lt;/a&gt;&lt;br /&gt;
8.&lt;a href=&#34;https://www.jianshu.com/p/ebb8f0ba67f6&#34;&gt;污点（taints）和容忍度（tolerations）&lt;/a&gt;&lt;br /&gt;
9.&lt;a href=&#34;https://www.oomspot.com//post/k8swangluomoxingfuwuneibuliuliangcelue&#34;&gt;k8s网络模型-服务内部流量策略&lt;/a&gt;&lt;br /&gt;
10.&lt;a href=&#34;https://www.jianshu.com/p/cedf8c9d18f1&#34;&gt;k8s中statefulset资源类型的深入理解&lt;/a&gt;&lt;br /&gt;
11.&lt;a href=&#34;https://copyfuture.com/blogs-details/202006012253389106o9ttg1qo5by175&#34;&gt;k8s job简介和访问&lt;/a&gt;&lt;br /&gt;
12.&lt;a href=&#34;https://blog.csdn.net/m0_47288926/article/details/122819880&#34;&gt;k8s-HPA&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S学习笔记(05): yaml语法</title>
            <link>http://mospany.github.io/2022/09/27/k8s-yaml/</link>
            <pubDate>Tue, 27 Sep 2022 19:58:47 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/09/27/k8s-yaml/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;orgfba5ff2&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;k8s支持的文件格式&#34;&gt;k8s支持的文件格式&lt;/h1&gt;

&lt;p&gt;Kubernetes支持YAML和JSON格式管理资源对象&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;JSON格式：主要用于api接口之间消息的传递&lt;/li&gt;
&lt;li&gt;YAML格式：用于配置和管理，YAML是一种简洁的非标记性语言，内容格式人性化，较易读&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org3c21164&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;yaml语言格式&#34;&gt;YAML语言格式&lt;/h1&gt;

&lt;p&gt;● 大小写敏感&lt;br /&gt;
● 使用缩进表示层级关系&lt;br /&gt;
● 不支持Tab键制表符缩进，只使用空格缩进&lt;br /&gt;
● 缩进的空格数目不重要，只要相同层级的元素左侧对齐即可，通常开头缩进两个空格&lt;br /&gt;
● 符号字符后缩进一个空格，如冒号，逗号，短横杠（-）等&lt;br /&gt;
● “&amp;#x2014;”表示YAML格式，一个文件的开始，用于分隔文件&lt;br /&gt;
● “#”表示注释&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgd843414&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;编写资源配置清单&#34;&gt;编写资源配置清单&lt;/h1&gt;

&lt;p&gt;[root@master test]# vim nginx-test.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#指定api版本标签
apiVersion: apps/v1
#定义资源的类型/角色，deployment为副本控制器
#此处资源类型可以是Deployment、Job、Ingress、Service等
kind: Deployment
#定义资源的元数据信息，比如资源的名称、namespace、标签等信息
metadata:
  #定义资源的名称，在同一个namespace空间中必须是唯一的
  name: nginx-test
  lables:
    app: nginx
#定义deployment资源需要的参数属性，诸如是否在容器失败时重新启动容器的属性
spec:
  #定义副本数量
  replicas: 3
  #定义标签选择器
  selector:
    #定义匹配标签
    matchLabels:
      #需与后面的.spec.template.metadata.labels定义的标签保持一致
      app: nginx
  #定义业务模板，如果有多个副本，所有副本的属性会按照模板的相关配置进行匹配
  template:
    metadata:
      #定义Pod副本将使用的标签，需与前面的.spec.selector.matchLabels定义的标签保持一致
      labels:
        app: nginx
    spec:
      #定义容器属性
      containers:
      #定义一个容器名，一个-name:定义一个容器
      - name: nginx
        #定义容器使用的镜像以及版本
        image: nginx:1.15.4
        ports:
        #定义容器对外的端口
        - containerPort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org32111ee&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;创建service服务对外提供访问并测试&#34;&gt;创建service服务对外提供访问并测试&lt;/h1&gt;

&lt;p&gt;[root@master test]# vim nginx-svc-test.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  labels:
    app: nginx
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
  selector:
    #此处定义的selector要与deployment所定义的selector相同
    #service依靠标签选择器来检索提供服务的nodes
    app: nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org7f24d32&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;详解k8s中的port&#34;&gt;详解k8s中的port&lt;/h1&gt;

&lt;p&gt;port是k8s集群内部访问service的端口，即通过clusterIP:port可以从Pod所在的Node上访问到service&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nodePort&lt;br /&gt;
nodePort是外部访问k8s集群中service的端口，通过nodeIP:nodePort可以从外部访问到某个service&lt;/li&gt;
&lt;li&gt;targetPort&lt;br /&gt;
targetPort是Pod的端口，从port或nodePort来的流量经过kube-proxy反向代理负载均衡转发到后端Pod的targetPort上，最后进入容器。&lt;/li&gt;
&lt;li&gt;containerPort&lt;br /&gt;
containerPort是Pod内部容器的端口，targetPort映射到containerPort。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orgbae1b9d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;获取资源配置清单的总结&#34;&gt;获取资源配置清单的总结&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;没有相关资源，使用run命令&amp;#x2013;dry-run选项&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run dryrun-test --image=nginx --port=80 --replicas=3 --dry-run -o yaml &amp;gt; dryrun-test.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;已有相关资源，使用get命令&amp;#x2013;export选项&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get deploy dryrun-test --export -o yaml &amp;gt; export-test.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt; ：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;现在不推荐使用“ export”（从1.14版本开始，通常应该在1.18版本中消失（在changelog中找不到））&lt;/li&gt;
&lt;li&gt;&amp;#x2013;dry-run is deprecated and can be replaced with &amp;#x2013;dry-run=client.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org86e8206&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.escapelife.site/posts/8032061c.html&#34;&gt;Kubernetes之YAML语法&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        
        <item>
            <title>K8S学习笔记(04): kubectl命令技巧大全</title>
            <link>http://mospany.github.io/2022/09/21/k8s-kubectl-all-cmd/</link>
            <pubDate>Wed, 21 Sep 2022 21:01:59 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/09/21/k8s-kubectl-all-cmd/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;org8c37926&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;kubectl-命令技巧大全&#34;&gt;kubectl 命令技巧大全&lt;/h1&gt;

&lt;p&gt;kubectl 命令是操作 Kubernetes 集群的最直接和最高效的途径，这个60多 MB 大小的二进制文件，到底有啥能耐呢？&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org42fef24&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;kubectl-自动补全&#34;&gt;Kubectl 自动补全&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;$ source &amp;lt;(kubectl completion bash) # setup autocomplete in bash, bash-completion package should be installed first.
$ source &amp;lt;(kubectl completion zsh)  # setup autocomplete in zsh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org27c704f&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;kubectl-上下文和配置&#34;&gt;Kubectl 上下文和配置&lt;/h1&gt;

&lt;p&gt;设置 kubectl 命令交互的 kubernetes 集群并修改配置信息。参阅 使用 kubeconfig 文件进行跨集群验证 获取关于配置文件的详细信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl config view # 显示合并后的 kubeconfig 配置

# 同时使用多个 kubeconfig 文件并查看合并后的配置
$ KUBECONFIG=~/.kube/config:~/.kube/kubconfig2 kubectl config view

# 获取 e2e 用户的密码
$ kubectl config view -o jsonpath=&#39;{.users[?(@.name == &amp;quot;e2e&amp;quot;)].user.password}&#39;

$ kubectl config current-context              # 显示当前的上下文
$ kubectl config use-context my-cluster-name  # 设置默认上下文为 my-cluster-name

# 向 kubeconf 中增加支持基本认证的新集群
$ kubectl config set-credentials kubeuser/foo.kubernetes.com --username=kubeuser --password=kubepassword

# 使用指定的用户名和 namespace 设置上下文
$ kubectl config set-context gce --user=cluster-admin --namespace=foo \
  &amp;amp;&amp;amp; kubectl config use-context gce
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgaa430dd&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;创建对象&#34;&gt;创建对象&lt;/h1&gt;

&lt;p&gt;Kubernetes 的清单文件可以使用 json 或 yaml 格式定义。可以以 .yaml、.yml、或者 .json 为扩展名。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f ./my-manifest.yaml           # 创建资源
$ kubectl create -f ./my1.yaml -f ./my2.yaml     # 使用多个文件创建资源
$ kubectl create -f ./dir                        # 使用目录下的所有清单文件来创建资源
$ kubectl create -f https://git.io/vPieo         # 使用 url 来创建资源
$ kubectl run nginx --image=nginx                # 启动一个 nginx 实例
$ kubectl explain pods,svc                       # 获取 pod 和 svc 的文档

# 从 stdin 输入中创建多个 YAML 对象
$ cat &amp;lt;&amp;lt;EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: busybox-sleep
spec:
  containers:
  - name: busybox
    image: busybox
    args:
    - sleep
    - &amp;quot;1000000&amp;quot;
---
apiVersion: v1
kind: Pod
metadata:
  name: busybox-sleep-less
spec:
  containers:
  - name: busybox
    image: busybox
    args:
    - sleep
    - &amp;quot;1000&amp;quot;
EOF

# 创建包含几个 key 的 Secret
$ cat &amp;lt;&amp;lt;EOF | kubectl create -f -
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  password: $(echo &amp;quot;s33msi4&amp;quot; | base64)
  username: $(echo &amp;quot;jane&amp;quot; | base64)
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org62782f5&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;显示和查找资源&#34;&gt;显示和查找资源&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;# Get commands with basic output
$ kubectl get services                          # 列出所有 namespace 中的所有 service
$ kubectl get pods --all-namespaces             # 列出所有 namespace 中的所有 pod
$ kubectl get pods -o wide                      # 列出所有 pod 并显示详细信息
$ kubectl get deployment my-dep                 # 列出指定 deployment

# 使用详细输出来描述命令
$ kubectl describe nodes my-node
$ kubectl describe pods my-pod

$ kubectl get services --sort-by=.metadata.name # List Services Sorted by Name

# 根据重启次数排序列出 pod
$ kubectl get pods --sort-by=&#39;.status.containerStatuses[0].restartCount&#39;

# 获取所有具有 app=cassandra 的 pod 中的 version 标签
$ kubectl get pods --selector=app=cassandra rc -o \
  jsonpath=&#39;{.items[*].metadata.labels.version}&#39;

# 获取所有节点的 ExternalIP
$ kubectl get nodes -o jsonpath=&#39;{.items[*].status.addresses[?(@.type==&amp;quot;ExternalIP&amp;quot;)].address}&#39;

# 列出属于某个 PC 的 Pod 的名字
# “jq”命令用于转换复杂的 jsonpath，参考 https://stedolan.github.io/jq/
$ sel=${$(kubectl get rc my-rc --output=json | jq -j &#39;.spec.selector | to_entries | .[] | &amp;quot;\(.key)=\(.value),&amp;quot;&#39;)%?}
$ echo $(kubectl get pods --selector=$sel --output=jsonpath={.items..metadata.name})

# 查看哪些节点已就绪
$ JSONPATH=&#39;{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}&#39; \
 &amp;amp;&amp;amp; kubectl get nodes -o jsonpath=&amp;quot;$JSONPATH&amp;quot; | grep &amp;quot;Ready=True&amp;quot;

# 列出当前 Pod 中使用的 Secret
$ kubectl get pods -o json | jq &#39;.items[].spec.containers[].env[]?.valueFrom.secretKeyRef.name&#39; | grep -v null | sort | uniq
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgc7a7c88&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;更新资源&#34;&gt;更新资源&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl rolling-update frontend-v1 -f frontend-v2.json           # 滚动更新 pod frontend-v1
$ kubectl rolling-update frontend-v1 frontend-v2 --image=image:v2  # 更新资源名称并更新镜像
$ kubectl rolling-update frontend --image=image:v2                 # 更新 frontend pod 中的镜像
$ kubectl rolling-update frontend-v1 frontend-v2 --rollback        # 退出已存在的进行中的滚动更新
$ cat pod.json | kubectl replace -f -                              # 基于 stdin 输入的 JSON 替换 pod

# 强制替换，删除后重新创建资源。会导致服务中断。
$ kubectl replace --force -f ./pod.json

# 为 nginx RC 创建服务，启用本地 80 端口连接到容器上的 8000 端口
$ kubectl expose rc nginx --port=80 --target-port=8000

# 更新单容器 pod 的镜像版本（tag）到 v4
$ kubectl get pod mypod -o yaml | sed &#39;s/\(image: myimage\):.*$/\1:v4/&#39; | kubectl replace -f -

$ kubectl label pods my-pod new-label=awesome                      # 添加标签
$ kubectl annotate pods my-pod icon-url=http://goo.gl/XXBTWq       # 添加注解
$ kubectl autoscale deployment foo --min=2 --max=10                # 自动扩展 deployment “foo”
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orga04178c&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;修补资源&#34;&gt;修补资源&lt;/h1&gt;

&lt;p&gt;使用策略合并补丁并修补资源。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl patch node k8s-node-1 -p &#39;{&amp;quot;spec&amp;quot;:{&amp;quot;unschedulable&amp;quot;:true}}&#39; # 部分更新节点

# 更新容器镜像； spec.containers[*].name 是必须的，因为这是合并的关键字
$ kubectl patch pod valid-pod -p &#39;{&amp;quot;spec&amp;quot;:{&amp;quot;containers&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;kubernetes-serve-hostname&amp;quot;,&amp;quot;image&amp;quot;:&amp;quot;new image&amp;quot;}]}}&#39;

# 使用具有位置数组的 json 补丁更新容器镜像
$ kubectl patch pod valid-pod --type=&#39;json&#39; -p=&#39;[{&amp;quot;op&amp;quot;: &amp;quot;replace&amp;quot;, &amp;quot;path&amp;quot;: &amp;quot;/spec/containers/0/image&amp;quot;, &amp;quot;value&amp;quot;:&amp;quot;new image&amp;quot;}]&#39;

# 使用具有位置数组的 json 补丁禁用 deployment 的 livenessProbe
$ kubectl patch deployment valid-deployment  --type json   -p=&#39;[{&amp;quot;op&amp;quot;: &amp;quot;remove&amp;quot;, &amp;quot;path&amp;quot;: &amp;quot;/spec/template/spec/containers/0/livenessProbe&amp;quot;}]&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org4b6db63&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;编辑资源&#34;&gt;编辑资源&lt;/h1&gt;

&lt;p&gt;在编辑器中编辑任何 API 资源。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl edit svc/docker-registry                      # 编辑名为 docker-registry 的 service
$ KUBE_EDITOR=&amp;quot;nano&amp;quot; kubectl edit svc/docker-registry   # 使用其它编辑器
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org43c4654&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;scale-资源&#34;&gt;Scale 资源&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl scale --replicas=3 rs/foo                                 # Scale a replicaset named &#39;foo&#39; to 3
$ kubectl scale --replicas=3 -f foo.yaml                            # Scale a resource specified in &amp;quot;foo.yaml&amp;quot; to 3
$ kubectl scale --current-replicas=2 --replicas=3 deployment/mysql  # If the deployment named mysql&#39;s current size is 2, scale mysql to 3
$ kubectl scale --replicas=5 rc/foo rc/bar rc/baz                   # Scale multiple replication controllers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgc4b94c3&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;删除资源&#34;&gt;删除资源&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl delete -f ./pod.json                                              # 删除 pod.json 文件中定义的类型和名称的 pod
$ kubectl delete pod,service baz foo                                        # 删除名为“baz”的 pod 和名为“foo”的 service
$ kubectl delete pods,services -l name=myLabel                              # 删除具有 name=myLabel 标签的 pod 和 serivce
$ kubectl delete pods,services -l name=myLabel --include-uninitialized      # 删除具有 name=myLabel 标签的 pod 和 service，包括尚未初始化的
$ kubectl -n my-ns delete po,svc --all                                      # 删除 my-ns namespace 下的所有 pod 和 serivce，包括尚未初始化的
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org2dfd797&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;与运行中的-pod-交互&#34;&gt;与运行中的 Pod 交互&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl logs my-pod                                 # dump 输出 pod 的日志（stdout）
$ kubectl logs my-pod -c my-container                 # dump 输出 pod 中容器的日志（stdout，pod 中有多个容器的情况下使用）
$ kubectl logs -f my-pod                              # 流式输出 pod 的日志（stdout）
$ kubectl logs -f my-pod -c my-container              # 流式输出 pod 中容器的日志（stdout，pod 中有多个容器的情况下使用）
$ kubectl run -i --tty busybox --image=busybox -- sh  # 交互式 shell 的方式运行 pod
$ kubectl attach my-pod -i                            # 连接到运行中的容器
$ kubectl port-forward my-pod 5000:6000               # 转发 pod 中的 6000 端口到本地的 5000 端口
$ kubectl exec my-pod -- ls /                         # 在已存在的容器中执行命令（只有一个容器的情况下）
$ kubectl exec my-pod -c my-container -- ls /         # 在已存在的容器中执行命令（pod 中有多个容器的情况下）
$ kubectl top pod POD_NAME --containers               # 显示指定 pod 和容器的指标度量
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org293bbe3&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;与节点和集群交互&#34;&gt;与节点和集群交互&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl cordon my-node                                                # 标记 my-node 不可调度
$ kubectl drain my-node                                                 # 清空 my-node 以待维护
$ kubectl uncordon my-node                                              # 标记 my-node 可调度
$ kubectl top node my-node                                              # 显示 my-node 的指标度量
$ kubectl cluster-info                                                  # 显示 master 和服务的地址
$ kubectl cluster-info dump                                             # 将当前集群状态输出到 stdout                                    
$ kubectl cluster-info dump --output-directory=/path/to/cluster-state   # 将当前集群状态输出到 /path/to/cluster-state

# 如果该键和影响的污点（taint）已存在，则使用指定的值替换
$ kubectl taint nodes foo dedicated=special-user:NoSchedule
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgef08c6d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;资源类型&#34;&gt;资源类型&lt;/h1&gt;

&lt;p&gt;下表列出的是 kubernetes 中所有支持的类型和缩写的别名。&lt;/p&gt;

&lt;table border=&#34;2&#34; cellspacing=&#34;0&#34; cellpadding=&#34;6&#34; rules=&#34;groups&#34; frame=&#34;hsides&#34;&gt;


&lt;colgroup&gt;
&lt;col  class=&#34;org-left&#34; /&gt;

&lt;col  class=&#34;org-left&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;资源类型&lt;/th&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;缩写别名&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;clusters&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;componentstatuses&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;cs&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;configmaps&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;cm&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;daemonsets&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;ds&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;deployments&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;deploy&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;endpoints&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;ep&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;event&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;ev&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;horizontalpodautoscalers&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;hpa&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;ingresses&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;ing&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;jobs&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;limitranges&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;limits&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;namespaces&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;ns&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;networkpolicies&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;nodes&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;no&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;statefulsets&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;persistentvolumeclaims&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;pvc&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;persistentvolumes&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;pv&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;pods&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;po&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;podsecuritypolicies&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;psp&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;podtemplates&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;replicasets&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;rs&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;replicationcontrollers&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;rc&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;resourcequotas&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;quota&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;cronjob&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;secrets&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;serviceaccount&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;sa&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;services&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;svc&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;storageclasses&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;thirdpartyresources&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a id=&#34;orgb32a4c5&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;格式化输出&#34;&gt;格式化输出&lt;/h1&gt;

&lt;p&gt;要以特定的格式向终端窗口输出详细信息，可以在 kubectl 命令中添加 -o 或者 -output 标志。&lt;/p&gt;

&lt;table border=&#34;2&#34; cellspacing=&#34;0&#34; cellpadding=&#34;6&#34; rules=&#34;groups&#34; frame=&#34;hsides&#34;&gt;


&lt;colgroup&gt;
&lt;col  class=&#34;org-left&#34; /&gt;

&lt;col  class=&#34;org-left&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;输出格式&lt;/th&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;-o=json&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;输出 JSON 格式的 API 对象&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;-o=jsonpath=template&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;打印 jsonpath 表达式中定义的字段&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;-o=jsonpath-file=filename&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;打印由 文件中的 jsonpath 表达式定义的字段&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;-o=name&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;仅打印资源名称&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;-o=wide&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;以纯文本格式输出任何附加信息，对于 Pod ，包含节点名称&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;-o=yaml&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;输出 YAML 格式的 API 对象&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a id=&#34;org8e6aac4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;kubectl-详细输出和调试&#34;&gt;Kubectl 详细输出和调试&lt;/h1&gt;

&lt;p&gt;使用 -v 或 &amp;#x2013;v 标志跟着一个整数来指定日志级别。&lt;/p&gt;

&lt;table border=&#34;2&#34; cellspacing=&#34;0&#34; cellpadding=&#34;6&#34; rules=&#34;groups&#34; frame=&#34;hsides&#34;&gt;


&lt;colgroup&gt;
&lt;col  class=&#34;org-left&#34; /&gt;

&lt;col  class=&#34;org-left&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;详细等级&lt;/th&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#x2013;v=0&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;总是对操作人员可见。&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#x2013;v=1&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;合理的默认日志级别，如果您不需要详细输出。&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#x2013;v=2&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;可能与系统的重大变化相关的，有关稳定状态的信息和重要的日志信息。这是对大多数系统推荐的日志级别。&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#x2013;v=3&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;有关更改的扩展信息。&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#x2013;v=4&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;调试级别详细输出。&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#x2013;v=6&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;显示请求的资源。&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#x2013;v=7&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;显示HTTP请求的header。&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#x2013;v=8&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;显示HTTP请求的内容。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a id=&#34;org0f9fe33&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考&#34;&gt;参考&lt;/h1&gt;

&lt;p&gt;【01】&lt;a href=&#34;https://kubernetes.io/docs/reference/kubectl/&#34;&gt;Kubectl 概览&lt;/a&gt;&lt;br /&gt;
【02】&lt;a href=&#34;https://kubernetes.io/docs/reference/kubectl/jsonpath/&#34;&gt;JsonPath 手册&lt;/a&gt;&lt;br /&gt;
【03】&lt;a href=&#34;https://kubernetes.io/docs/reference/kubectl/cheatsheet/&#34;&gt;Cheatsheet&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>K8S学习笔记(03): Mac系统安装k8s集群</title>
            <link>http://mospany.github.io/2022/09/13/k8s-study-install-for-mac/</link>
            <pubDate>Tue, 13 Sep 2022 19:48:30 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/09/13/k8s-study-install-for-mac/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;orgba0078b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;版本介绍&#34;&gt;版本介绍&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org21778c4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;mac系统版本&#34;&gt;Mac系统版本&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/k8s-install-for-mac/macos-version.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgfd3ff14&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;docker-desktop-版本&#34;&gt;Docker Desktop 版本&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/k8s-install-for-mac/docker-version.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org75f453a&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;k8s-版本&#34;&gt;K8S 版本&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/k8s-install-for-mac/k8s-version.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org5708814&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;安装&#34;&gt;安装&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orge5e2699&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;docker安装&#34;&gt;Docker安装&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;brew install -cask docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;org42bc861&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;开机自启动k8s&#34;&gt;开机自启动K8S&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/k8s-install-for-mac/docker-startup-k8s.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;重启成功后查看K8S集群。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgd262c8d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;安装kubernetes-dashboard&#34;&gt;安装Kubernetes Dashboard&lt;/h2&gt;

&lt;p&gt;命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.1/aio/deploy/recommended.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/k8s-install-for-mac/k8s-dashboard-install.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;最新版本参考：&lt;a href=&#34;https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/&#34;&gt;https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/&lt;/a&gt;​&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgb628b73&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;创建sa登录token&#34;&gt;创建SA登录Token&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# vim dashboard-adminuser.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard

# kubectl create -f dashboard-adminuser.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;orgca914fe&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;绑定资源并获取登录token&#34;&gt;绑定资源并获取登录Token&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;kubectl create sa dashboard-admin -n kube-system
kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin
ADMIN_SECRET=$(kubectl get secrets -n kube-system | grep dashboard-admin | awk &#39;{print $1}&#39;)
DASHBOARD_LOGIN_TOKEN=$(kubectl describe secret -n kube-system ${ADMIN_SECRET} | grep -E &#39;^token&#39; | awk &#39;{print $2}&#39;)
echo $DASHBOARD_LOGIN_TOKEN
kubectl proxy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/k8s-install-for-mac/k8s-dashboard-rbac.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1、先创建服务账号dashboard-admin&lt;br /&gt;
2、再把账号dashboard-admin绑定到集群角色cluster-admin上使其拥有集群的管理权限，cluster-admin 超级管理员，对集群所有权限,和linux下面root一样（在部署dashboard的时候，先创建sa，然后将sa绑定到角色cluster-admin，最后获取到token，这就使用了内置的cluster-admin ）。&lt;br /&gt;
3、最后再获取该账号的secrets里的token做为登录。观察serviceAccount的创建，并创建一个相应的Secret 来允许API访问。&lt;br /&gt;
4、使用kubectl proxy命令就可以使API server监听在本地的8001端口上，作用是建立一条通往API服务器的隧道，可以方便查看API服务器上的资源。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org498919d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;登录dashbaord&#34;&gt;登录dashbaord&lt;/h3&gt;

&lt;p&gt;登录地址：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/configmap?namespace=default
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注：上面URL的#号后面configmap为锚点，&lt;a href=&#34;https:kubernetes-dashboard&#34;&gt;https:kubernetes-dashboard&lt;/a&gt;: 为普通字符串&lt;/p&gt;

&lt;p&gt;输入上面生成的token:&lt;br /&gt;
&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/k8s-install-for-mac/k8s-dashboard-login.png&#34; alt=&#34;img&#34; /&gt;&lt;br /&gt;
登录成功界面如下：&lt;br /&gt;
&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/k8s-install-for-mac/k8s-dashboard-main.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;参考资料&lt;br /&gt;
【01】 &lt;a href=&#34;https://blog.csdn.net/a772304419/article/details/125482351&#34;&gt;k8s 内置cluster role（集群角色） cluster-admin、admin、 edit、 view的作用范围及区别&lt;/a&gt;&lt;br /&gt;
【02】 &lt;a href=&#34;https://blog.csdn.net/m0_45406092/article/details/119890156&#34;&gt;kubectl proxy&lt;/a&gt;&lt;br /&gt;
【03】 &lt;a href=&#34;https://blog.csdn.net/marvel__dead/article/details/78833921&#34;&gt;URL中“#”号的作用&lt;/a&gt;&lt;br /&gt;
【04】 &lt;a href=&#34;https://blog.csdn.net/afz8572/article/details/101745100/&#34;&gt;mac上k8s学习踩坑&lt;/a&gt;&lt;br /&gt;
【05】 &lt;a href=&#34;https://javajgs.com/archives/8386&#34;&gt;mac下安装kubeneters及zsh下配置自动补全&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
        <item>
            <title>K8S学习笔记(02): 架构</title>
            <link>http://mospany.github.io/2022/09/12/k8s-arch/</link>
            <pubDate>Mon, 12 Sep 2022 19:34:12 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/09/12/k8s-arch/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;org2f48c43&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;Kubernetes 是一个可移植的、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。 Kubernetes 拥有一个庞大且快速增长的生态系统。Kubernetes 的服务、支持和工具广泛可用。&lt;/p&gt;

&lt;p&gt;Kubernetes 这个名字源于希腊语，意为“舵手”或“飞行员”。k8s 这个缩写是因为 k 和 s 之间有八个字符的关系。 Google 在 2014 年开源了 Kubernetes 项目。Kubernetes 建立在 Google 在大规模运行生产工作负载方面拥有十几年的经验 的基础上，结合了社区中最好的想法和实践。&lt;/p&gt;

&lt;p&gt;Kubernetes 最初源于谷歌内部的 Borg，提供了面向应用的容器集群部署和管理系统。Kubernetes 的目标旨在消除编排物理 / 虚拟计算，网络和存储基础设施的负担，并使应用程序运营商和开发人员完全将重点放在以容器为中心的原语上进行自助运营。Kubernetes 也提供稳定、兼容的基础（平台），用于构建定制化的 workflows 和更高级的自动化任务。 Kubernetes 具备完善的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建负载均衡器、故障发现和自我修复能力、服务滚动升级和在线扩容、可扩展的资源自动调度机制、多粒度的资源配额管理能力。Kubernetes 还提供完善的管理工具，涵盖开发、部署测试、运维监控等各个环节。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org895d7b5&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;时光回溯&#34;&gt;时光回溯&lt;/h1&gt;

&lt;p&gt;让我们回顾一下为什么 Kubernetes 如此有用。&lt;br /&gt;
&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/k8s-arch/k8s-develop.svg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org54684e8&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;传统部署时代&#34;&gt;传统部署时代&lt;/h2&gt;

&lt;p&gt;早期，各个组织机构在物理服务器上运行应用程序。无法为物理服务器中的应用程序定义资源边界，这会导致资源分配问题。 例如，如果在物理服务器上运行多个应用程序，则可能会出现一个应用程序占用大部分资源的情况， 结果可能导致其他应用程序的性能下降。 一种解决方案是在不同的物理服务器上运行每个应用程序，但是由于资源利用不足而无法扩展， 并且维护许多物理服务器的成本很高。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orga38329f&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;虚拟化部署时代&#34;&gt;虚拟化部署时代&lt;/h2&gt;

&lt;p&gt;作为解决方案，引入了虚拟化。虚拟化技术允许你在单个物理服务器的 CPU 上运行多个虚拟机（VM）。 虚拟化允许应用程序在 VM 之间隔离，并提供一定程度的安全，因为一个应用程序的信息 不能被另一应用程序随意访问。&lt;/p&gt;

&lt;p&gt;虚拟化技术能够更好地利用物理服务器上的资源，并且因为可轻松地添加或更新应用程序 而可以实现更好的可伸缩性，降低硬件成本等等。&lt;/p&gt;

&lt;p&gt;每个 VM 是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括其自己的操作系统。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgb581bb2&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;容器部署时代&#34;&gt;容器部署时代&lt;/h2&gt;

&lt;p&gt;容器类似于 VM，但是它们具有被放宽的隔离属性，可以在应用程序之间共享操作系统（OS）。 因此，容器被认为是轻量级的。容器与 VM 类似，具有自己的文件系统、CPU、内存、进程空间等。 由于它们与基础架构分离，因此可以跨云和 OS 发行版本进行移植。&lt;/p&gt;

&lt;p&gt;容器因具有许多优势而变得流行起来。下面列出的是容器的一些好处：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;敏捷应用程序的创建和部署：与使用 VM 镜像相比，提高了容器镜像创建的简便性和效率。&lt;/li&gt;
&lt;li&gt;持续开发、集成和部署：通过快速简单的回滚（由于镜像不可变性），支持可靠且频繁的 容器镜像构建和部署。&lt;/li&gt;
&lt;li&gt;关注开发与运维的分离：在构建/发布时而不是在部署时创建应用程序容器镜像， 从而将应用程序与基础架构分离。&lt;/li&gt;
&lt;li&gt;可观察性：不仅可以显示操作系统级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。&lt;/li&gt;
&lt;li&gt;跨开发、测试和生产的环境一致性：在便携式计算机上与在云中相同地运行。&lt;/li&gt;
&lt;li&gt;跨云和操作系统发行版本的可移植性：可在 Ubuntu、RHEL、CoreOS、本地、 Google Kubernetes Engine 和其他任何地方运行。&lt;/li&gt;
&lt;li&gt;以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行 OS 到使用逻辑资源在 OS 上运行应用程序。&lt;/li&gt;
&lt;li&gt;松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分， 并且可以动态部署和管理 - 而不是在一台大型单机上整体运行。&lt;/li&gt;
&lt;li&gt;资源隔离：可预测的应用程序性能。&lt;/li&gt;
&lt;li&gt;资源利用：高效率和高密度。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org8a82c78&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;为什么需要-kubernetes-它能做什么&#34;&gt;为什么需要 Kubernetes，它能做什么?&lt;/h1&gt;

&lt;p&gt;容器是打包和运行应用程序的好方式。在生产环境中，你需要管理运行应用程序的容器，并确保不会停机。 例如，如果一个容器发生故障，则需要启动另一个容器。如果系统处理此行为，会不会更容易？&lt;/p&gt;

&lt;p&gt;这就是 Kubernetes 来解决这些问题的方法！ Kubernetes 为你提供了一个可弹性运行分布式系统的框架。 Kubernetes 会满足你的扩展要求、故障转移、部署模式等。 例如，Kubernetes 可以轻松管理系统的 Canary 部署。&lt;/p&gt;

&lt;p&gt;Kubernetes 为你提供：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;服务发现和负载均衡&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubernetes 可以使用 DNS 名称或自己的 IP 地址公开容器，如果进入容器的流量很大， Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;存储编排&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;自动部署和回滚&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;你可以使用 Kubernetes 描述已部署容器的所需状态，它可以以受控的速率将实际状态 更改为期望状态。例如，你可以自动化 Kubernetes 来为你的部署创建新容器， 删除现有容器并将它们的所有资源用于新容器。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;自动完成装箱计算&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubernetes 允许你指定每个容器所需 CPU 和内存（RAM）。 当容器指定了资源请求时，Kubernetes 可以做出更好的决策来管理容器的资源。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;自我修复&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubernetes 重新启动失败的容器、替换容器、杀死不响应用户定义的 运行状况检查的容器，并且在准备好服务之前不将其通告给客户端。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;密钥与配置管理&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 ssh 密钥。 你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org5423f7d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;kubernetes-不是什么&#34;&gt;Kubernetes 不是什么&lt;/h1&gt;

&lt;p&gt;Kubernetes 不是传统的、包罗万象的 PaaS（平台即服务）系统。 由于 Kubernetes 在容器级别而不是在硬件级别运行，它提供了 PaaS 产品共有的一些普遍适用的功能， 例如部署、扩展、负载均衡、日志记录和监视。 但是，Kubernetes 不是单体系统，默认解决方案都是可选和可插拔的。 Kubernetes 提供了构建开发人员平台的基础，但是在重要的地方保留了用户的选择和灵活性。&lt;/p&gt;

&lt;p&gt;Kubernetes：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;不限制支持的应用程序类型。 Kubernetes 旨在支持极其多种多样的工作负载，包括无状态、有状态和数据处理工作负载。 如果应用程序可以在容器中运行，那么它应该可以在 Kubernetes 上很好地运行。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;不部署源代码，也不构建你的应用程序。 持续集成(CI)、交付和部署（CI/CD）工作流取决于组织的文化和偏好以及技术要求。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;不提供应用程序级别的服务作为内置服务，例如中间件（例如，消息中间件）、 数据处理框架（例如，Spark）、数据库（例如，mysql）、缓存、集群存储系统 （例如，Ceph）。这样的组件可以在 Kubernetes 上运行，并且/或者可以由运行在 Kubernetes 上的应用程序通过可移植机制（例如， 开放服务代理）来访问。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;不要求日志记录、监视或警报解决方案。 它提供了一些集成作为概念证明，并提供了收集和导出指标的机制。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;不提供或不要求配置语言/系统（例如 jsonnet），它提供了声明性 API， 该声明性 API 可以由任意形式的声明性规范所构成。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;不提供也不采用任何全面的机器配置、维护、管理或自我修复系统。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此外，Kubernetes 不仅仅是一个编排系统，实际上它消除了编排的需要。 编排的技术定义是执行已定义的工作流程：首先执行 A，然后执行 B，再执行 C。 相比之下，Kubernetes 包含一组独立的、可组合的控制过程， 这些过程连续地将当前状态驱动到所提供的所需状态。 如何从 A 到 C 的方式无关紧要，也不需要集中控制，这使得系统更易于使用 且功能更强大、系统更健壮、更为弹性和可扩展。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org76d4217&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;k8s架构原理&#34;&gt;K8S架构原理&lt;/h1&gt;

&lt;p&gt;概括来说 K8s 架构就是一个 Master 对应一群 Node 节点。 高可用时Master一般为3个。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/k8s-arch/k8s-arch.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org33f9855&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;master节点&#34;&gt;Master节点&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;org817c289&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;kube-apiserver&#34;&gt;kube-apiserver&lt;/h3&gt;

&lt;p&gt;K8s 网关，所有的指令请求都必须要经过 apiserver, 对外暴露K8S的api接口，是外界进行资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org51afc21&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;kube-scheduler&#34;&gt;kube-scheduler&lt;/h3&gt;

&lt;p&gt;调度器，使用调度算法，把请求资源调度到某一个 node 节点；负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上；就是监视新创建的 Pod，如果没有分配节点，就选择一个节点供他们运行，这就是pod的调度。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgdf97f22&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;kube-controller-manager&#34;&gt;kube-controller-manager&lt;/h3&gt;

&lt;p&gt;维护 K8s 资源对象, 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等，它们是处理集群中常规任务的后台线程。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgf648933&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;etcd&#34;&gt;etcd&lt;/h3&gt;

&lt;p&gt;存储资源对象, 一个可信赖的分布式键值存储服务，能够为整个分布式集群存储一些关键数据，协助分布式集群运转。储存K8S集群所有重要信息（持久化）。v2版本时基于内存的存储，v3开始才是序列化到介质。新版本K8S（v1.11以上）已经改用v3版本etcd。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org7931198&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;worker节点&#34;&gt;Worker节点&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;org72f8096&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;kubelet&#34;&gt;kubelet&lt;/h3&gt;

&lt;p&gt;在每一个 node 节点都存在一份，在 node 节点上的资源操作指令由 kubelet 来执行, 直接跟容器引擎交互实现容器的生命周期管理。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org2c78346&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;kube-proxy&#34;&gt;kube-proxy&lt;/h3&gt;

&lt;p&gt;代理服务，处理服务间负载均衡；负责写入规则至 IPTABLES、IPVS 实现服务映射访问的。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org1a12d32&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;组件通信&#34;&gt;组件通信&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/k8s/k8s-arch/k8s-component-arch.jpg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org028c0e9&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/concepts/&#34;&gt;Kubernetes 架构&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/0voice/k8s_awesome_document/blob/main/%25E6%2596%2587%25E7%25AB%25A0%25E7%25B2%25BE%25E9%2580%2589/%25E4%25B8%2580%25E6%2596%2587%25E4%25BA%2586%25E8%25A7%25A3%2520Kubernetes.md&#34;&gt;一文了解 Kubernetes.md&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        
        <item>
            <title>K8S学习笔记(01): 目录</title>
            <link>http://mospany.github.io/2022/09/10/k8s-study-toc/</link>
            <pubDate>Sat, 10 Sep 2022 16:13:51 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2022/09/10/k8s-study-toc/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;org184128e&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;目录&#34;&gt;目录&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;org8d20da4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;k8s使用与原理&#34;&gt;K8S使用与原理&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.mospan.cn/2022/09/10/k8s-study-toc/&#34;&gt;K8S学习笔记(01): 目录&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.mospan.cn/2022/09/13/k8s-study-install-for-mac/&#34;&gt;K8S学习笔记(03): Mac系统安装k8s集群&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.mospan.cn/2022/09/21/k8s-kubectl-all-cmd/&#34;&gt;K8S学习笔记(04): kubectl命令技巧大全&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a id=&#34;orgd60bd20&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;ppt内容&#34;&gt;PPT内容&lt;/h1&gt;

&lt;p&gt;&lt;center&gt;&lt;embed src=&#34;http://blog.mospan.cn/post/img/k8s/k8s-study-toc.pdf&#34; width=100% height=800&gt;&lt;/center&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>互联网最重要入口DNS系列(1): 基本常识</title>
            <link>http://mospany.github.io/2019/11/14/dns-base/</link>
            <pubDate>Thu, 14 Nov 2019 22:30:22 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2019/11/14/dns-base/</guid>
            <description>

&lt;p&gt;&lt;a id=&#34;orga57a6db&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;dns是什么&#34;&gt;DNS是什么？&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;因特尔域名系统&lt;br /&gt;
DNS是Domain Name System(域名系统)的缩写，互联网的入口服务，监听53端口。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;提供域名到IP地址的映射，或反之&lt;br /&gt;
将域名和IP地址相互映射的一个分布式数据库，能够更方便地访问互联网。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;是分布式、C/S结构的服务&lt;br /&gt;
DNS是一个倒树形结构的分布式服务。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;主要定义在RFC1034/1035上&lt;br /&gt;
其中RFC1034为域名的概念和设施, RFC1035为域名的概念和设施。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orgf21c435&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;为什么需要dns&#34;&gt;为什么需要DNS？&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;IP地址难以记忆与理解&lt;br /&gt;
IPv4地址形如192.168.1.1，而IPv6地址形如x:x:x:x:x:x:x:x的128位地址，非常难以记忆和理解，平时我们上网只要在浏览器上输入域名地址如wwww.baidu.com，由DNS系统翻译成所对应的IP地址更加方便一些。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;邮件投递需要寻址 (MX)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;域身份鉴定 (DomainKey, SPF)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;负载均衡(轮询、最小连接)&lt;br /&gt;
通过配置多条A记录实现访问轮询，也通过配置权重来进行负载均衡。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;CDN、GSLB&lt;br /&gt;
CDN和GSLB主要利用DNS的CNAME记录来流量的接入与调度。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orgfe8becb&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;什么是域-zone-与域名-domain&#34;&gt;什么是域(zone)与域名(domain)？&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;“.”是域，是所有其他域的起始点&lt;/li&gt;
&lt;li&gt;宇宙大爆炸：混沌初开，乾坤乃定&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/dns-base/zone.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;com.、com.cn.、cn、net.是域&lt;/li&gt;
&lt;li&gt;mospan.cn.、mospx.com.也是域&lt;/li&gt;
&lt;li&gt;blog.mospan.cn.、www.mospx.com.是域名&lt;/li&gt;
&lt;li&gt;domain = host(主机名) + zone&lt;/li&gt;
&lt;li&gt;标准的域名与主机名，只包括字母、数字、短横线和点&lt;/li&gt;
&lt;li&gt;域名的最大长度为255个字符，单个label最大为63个字符&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orga5e50c4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;dns基本概念&#34;&gt;DNS基本概念&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/dns-base/concept.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;DNS是一种倒树形的分布式系统，每一层存储它下一级的记录信息，自顶向下的查找过程。&lt;/li&gt;
&lt;li&gt;DNS主要分为授权和缓存，其他缓存DNS又分为Local（运营商本地） 和 public（公共） DNS。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org081a6e5&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;dns报文格式&#34;&gt;DNS报文格式&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/dns-base/format.png&#34; alt=&#34;img&#34; /&gt;&lt;br /&gt;
DNS报文如上图所示，其他左图为报文完整格式，后图区域数据展开形式。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgc9600cf&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;dns-rr-资源记录集&#34;&gt;DNS RR(资源记录集)&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/dns-base/rr.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;通过dig工具探测典型结果如上图所示，包含多条的域名、生存时间TTL、记录类型、记录数据rdata组成。&lt;br /&gt;
其中TTL为该记录可缓存的时间秒数。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;DNS RR(资源记录集)&lt;/li&gt;
&lt;li&gt;ttl:  缓存生存时间(单位：秒)，4个字节)&lt;/li&gt;
&lt;li&gt;type: 常用有A、CNAME、MX、TXT等&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;详见：&lt;a href=&#34;https://ephen.me/2016/dns-rr/&#34;&gt;https://ephen.me/2016/dns-rr/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgc38c77f&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;ends-client-subnet&#34;&gt;ends-client-subnet&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/dns-base/edns.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;正常情况下，DNS服务器只能获取到跟它通信的客户端(一般是local DNS)IP来进行区域解析，edns机制是允许在报文中携带用户真实IP来进行区域解析。&lt;br /&gt;
例如: 假设202.106.0.1是北京联通用户，它配置的DNS解析服务器为8.8.8.8，在不支持edns情况下，授权DNS将根据8.8.8.8来解析到国外去，如果支持了edns会根据202.106.0.1来解析到国内服务器中来，这样解析更加准确。&lt;/p&gt;

&lt;p&gt;详见：&lt;a href=&#34;http://www.cnblogs.com/cobbliu/p/3188632.html&#34;&gt;http://www.cnblogs.com/cobbliu/p/3188632.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org9e17f41&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;dns解析过程&#34;&gt;DNS解析过程&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/dns-base/resolv.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在浏览器中输入域名后DNS域名解析过程如下:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;浏览器DNS缓存中搜索&lt;/li&gt;
&lt;li&gt;在操作系统DNS缓存中搜索&lt;/li&gt;
&lt;li&gt;读取系统hosts文件，查找其中是否有对应的ip&lt;/li&gt;
&lt;li&gt;向本地配置的首选DNS服务器发起域名解析请求&lt;br /&gt;

&lt;ul&gt;
&lt;li&gt;本地DNS收到请求后，查找自己的缓存，如找到则直接响应给客户端&lt;/li&gt;
&lt;li&gt;如未找到，则授权DNS服务器进行迭代查找，最终构造完整的响应给客户端。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org6cc97d0&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;dns组织机构&#34;&gt;DNS组织机构&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;ICANN(互联网名称与数字地址分配机构)&lt;br /&gt;

&lt;ul&gt;
&lt;li&gt;IP地址分配&lt;/li&gt;
&lt;li&gt;协议标识符的指派&lt;/li&gt;
&lt;li&gt;通用顶级域名（gTLD）管理&lt;/li&gt;
&lt;li&gt;国家和地区顶级域名（ccTLD）系统的管理&lt;/li&gt;
&lt;li&gt;根服务器系统的管理&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Registry:   注册局。&lt;br /&gt;

&lt;ul&gt;
&lt;li&gt;维护区数据、委托注册机构提供注册服务。(CNNIC)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Registrar:   注册机构。&lt;br /&gt;

&lt;ul&gt;
&lt;li&gt;代表注册局提供注册服务。(万网、新网)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Registrant:   域名所有者。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;托管机构:  为域名所有者提供域名托管服务。(dnspod、XNS)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;根服务器。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;全球有13个根服务器，每个根服务器由不同的机构管理，每个服务器都有若干的镜像，使用anycast技术提供就近访问。&lt;/li&gt;
&lt;li&gt;大部分服务器在美国，北京和香港有根的镜像服务器。&lt;/li&gt;
&lt;li&gt;根区的内容由ICANN管理， 除了现有的TLD，ICANN已经开放了顶级域的注册。&lt;/li&gt;
&lt;li&gt;全球有13台根服务器(242节点)。&lt;br /&gt;

&lt;ul&gt;
&lt;li&gt;a.root-servers.net.  ~~  m.root-servers.net.。&lt;/li&gt;
&lt;li&gt;美国10台&lt;br /&gt;

&lt;ul&gt;
&lt;li&gt;A(Verisign, 6);   B(ISI, 1)&lt;/li&gt;
&lt;li&gt;C(Cogent, 6);   B(UMD, 1)&lt;/li&gt;
&lt;li&gt;E(NASA, 1);   B(ISC, 49)&lt;/li&gt;
&lt;li&gt;G(DOD, 6);   H(US Army, 2)&lt;/li&gt;
&lt;li&gt;J(Verisign, 70);   H(ICANN, 39)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;欧洲2台&lt;br /&gt;

&lt;ul&gt;
&lt;li&gt;J(RIPE NCC, 18);   H(SE, 33)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;日本1台&lt;br /&gt;

&lt;ul&gt;
&lt;li&gt;M(WIDE, 6)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/dns-base/root.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>一文读懂《区块链入门》，读完我们就是同志了</title>
            <link>http://mospany.github.io/2019/10/26/blockchain-base/</link>
            <pubDate>Sat, 26 Oct 2019 13:12:03 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2019/10/26/blockchain-base/</guid>
            <description>

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/blockchain-base/main.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在2011年就开始听到朋友说他同学某某今年挖矿赚了一大笔钱，之后期间也陆陆续续的听到或了解一些币圈链圈的事，&lt;br /&gt;
当时对于投资投机等不以为然，时过境迁, 随着对其的慢慢了解发现未来已来，作为一名初学者，偶尔也小试牛刀，觉得还是有必要通过&lt;br /&gt;
文字与声音来记录分享，以便记录自己的成长与拥抱未来。&lt;/p&gt;

&lt;p&gt;刚开始的时候可能也会有人跟我一样有疑问， 区块链是什么？数字货币又是什么？币在哪儿买？它长得怎么样？于是乎就开始在网络上搜索，大致看到了&lt;br /&gt;
一些购买方式，但看到需要梯子在交易所之间几经转换才能买下来和觉得到未来不明朗就打了退堂鼓，现在政策扶持与国家监管下，可以了解了解该技术、行业与市场。&lt;br /&gt;
以免又发生了&amp;rdquo;看不见，看不起，看不懂，来不及&amp;rdquo;的机会丢失。&lt;/p&gt;

&lt;p&gt;由于区块链具有技术、应用和金融属性，这是它区别于其他技术的独特之处，也是比较有魅力之处，毕竟大家都喜欢升官发财嘛。重要事情说三遍：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;反对炒币、投资有风险、入市须谨慎&lt;/strong&gt;&lt;br /&gt;
&lt;strong&gt;反对炒币、投资有风险、入市须谨慎&lt;/strong&gt;&lt;br /&gt;
&lt;strong&gt;反对炒币、投资有风险、入市须谨慎&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;下面咱们聊聊区块链技术、数字货币和他们之间关系及其应用。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org073cee4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;政策指引&#34;&gt;政策指引&lt;/h1&gt;

&lt;p&gt;国家为什么要发展区块链，而且还有上升到最高层，权威新闻发布的&amp;rdquo;四个要&amp;rdquo;为我们指引了方向。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/blockchain-base/policy1.png&#34; alt=&#34;img&#34; /&gt;&lt;br /&gt;
民生服务：为教育、商品防伪、食品安全、就业、养老、精准扶贫、医疗健康、公益、社会救助等提供更快捷、优质的服务。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/blockchain-base/policy2.png&#34; alt=&#34;img&#34; /&gt;&lt;br /&gt;
智慧城市：为信息基础设施、交通、能源等城市管理提高智能化、精准化水平。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/blockchain-base/policy3.png&#34; alt=&#34;img&#34; /&gt;&lt;br /&gt;
信息共享：在信息、人才、资金、征信等信息共享。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/blockchain-base/policy4.png&#34; alt=&#34;img&#34; /&gt;&lt;br /&gt;
政务服务：为跨部门、跨地区合作提高服务体验。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org52afc5c&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;俗话说区块链&#34;&gt;俗话说区块链&lt;/h1&gt;

&lt;p&gt;为了通俗的理解区块链是什么， 我打个比方举个简单但不完全恰当只为说明问题的场景例子：&lt;/p&gt;

&lt;p&gt;在某个山村里，村民A辛辛苦苦的养大一头牛打算要卖掉， 刚好村民B正缺牛耕田打算也想买牛，在咱们正常的做法是一手交钱一手交货， 于是村民B约村民A一起喝酒谈谈这桩买卖，从这家长那家短的开始聊到酒过三巡价格也基本谈拢了，村民B把钱给了村民A，村民B就把牛牵到自己家了，在正常情况下这没有问题，但在个别情况或别有用心的情况下，第二天酒醒了可能出现如下问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;村民A钱丢了，说村民B还没给钱&lt;/li&gt;
&lt;li&gt;村民A觉得卖便宜了，说村民B只给了部分钱，还有多少以后再给&lt;/li&gt;
&lt;li&gt;村民B觉得自己买贵了，说自己给了个整数村民A没有零钱，过两天找零了再退给村民B。&lt;/li&gt;
&lt;li&gt;可能还有其他极端情况。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;两人争来争去谁也不承认，于是找来村长说一通，村长由于当时没有在现场仅凭一方说辞也很难断定谁在说假话。&lt;/p&gt;

&lt;p&gt;然而在区块链的世界里是这样的：&lt;br /&gt;
村民B约村民A牵着牛来到村子上大家平时唠嗑摆龙门阵的地方，这里有很多村民在玩，大声喊了一下说我们要买牛卖牛了大家看看怎么样，大家你一言我一语的，最终把价格谈拢了，村民B把钱给村民A并说大家都看到了哦， 我已经给钱了，谁帮我记录一下给谁点辛苦费(他们村也有统计习惯以便衡量村里经济情况)，于是人群中有人回答我带笔和记事本我记录一下吧, 记录交易详情与时间双方在上面按了手印并抄了好多页份分给了大家回家自己入册， 最后村民B牵着牛回家了，大家都相安无事。&lt;/p&gt;

&lt;p&gt;它们对应区块链的术语是这样的：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;村民A卖牛给村民B =&amp;gt;  交易: 一次在区块链上完成的交易行为&lt;/li&gt;
&lt;li&gt;钱 =&amp;gt;  数字货币: 由算法生成的虚拟货币，具有易分割、易携带、总数可控的特点&lt;/li&gt;
&lt;li&gt;钱放入的口袋 =&amp;gt; 钱包: 管理数字货币地址的软件&lt;/li&gt;
&lt;li&gt;围观村民 =&amp;gt; 矿机或节点: 用来存储交易记录与赚取数字货币的计算机&lt;/li&gt;
&lt;li&gt;抢记录 =&amp;gt; 挖矿: 为了赚取数字货币而进行抢到记账优先权的计算行为，一般是计算一个随机大数，需要大量的运算保不齐哪个矿机先算出，一般分为工作量证明PoW、权益证明PoS、小蚁共识等，保证了公平和分散。&lt;/li&gt;
&lt;li&gt;按手印 =&amp;gt; 数字签名: 对交易记录进行数字签名，一串hash值仿篡改，如果交易记录改了hash值也变了。&lt;/li&gt;
&lt;li&gt;记事本 =&amp;gt; 分布账本: 所有记录交易的数据库&lt;/li&gt;
&lt;li&gt;记事页 =&amp;gt; 区块: 一定时间内的所有交易记录。&lt;/li&gt;
&lt;li&gt;辛苦费 =&amp;gt; Gas: 给矿工的辛苦费&lt;/li&gt;
&lt;li&gt;入册 =&amp;gt; 区块链:  所有区块虚拟连在一起的链。&lt;/li&gt;
&lt;li&gt;相安无事 =&amp;gt; 安全，交易有记录证明且很多份，如要篡改，按照少数服从多数原则，需得至少51%以上的记录人同意才行，想想难度可知，即51%攻击。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从上面看，一次交易是在大庭广众并有人记录中完成，具有透明交易、透明账本、可追溯、不可篡改的过程，按此类推，可适用于陌生人、生活中很多更大范围的场景。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org8f8330b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;与数字货币的关系&#34;&gt;与数字货币的关系&lt;/h1&gt;

&lt;p&gt;货币的本质是人群共识，大家都相信它能买东西别人也认可，例如：古代的贝壳，现在的纸币等，数字货币就是一种加密货币的形式所在，区块链是数字货币的背后技术，数字货币由算法生成且增量与总量可测，研发应用团队会保留一小部分，其他大部分发行到区块链上由矿工挖取，其他人在通过法币或其他数字货币进行购买或交换而产生价值，区块链网络参与人、节点越多越频繁其数字货币就越有价值。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgbc7c241&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;部分主流数字货币&#34;&gt;部分主流数字货币&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/blockchain-base/coin-1.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;BTC: 比特币，区块链上的数字货币祖师爷，总量2100万枚，最开始每10分钟生成50个比特币，每4年减半，到2140年被挖完。&lt;/li&gt;
&lt;li&gt;ETH: 一个开源的有智能合约功能的公共区块链平台币，被誉为区块链2.0代表。&lt;/li&gt;
&lt;li&gt;EOS: 为商用分布式应用设计的一款区块链操作系统，旨在实现分布式应用的性能扩展，被称为区块链3.0代表。&lt;/li&gt;
&lt;li&gt;其他：BCH、BTM等等&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/blockchain-base/coin.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org368d337&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;部分钱包&#34;&gt;部分钱包&lt;/h2&gt;

&lt;p&gt;imToken 是一款去中心化的数字资产钱包。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/blockchain-base/imtoken-2.png&#34; alt=&#34;img&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/blockchain-base/imtoken-1.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org86e757b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;交易所&#34;&gt;交易所&lt;/h2&gt;

&lt;p&gt;&lt;a id=&#34;org8a792be&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;otcbtc&#34;&gt;otcbtc&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/blockchain-base/otcbtc.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://otcbtc.com/&#34;&gt;otcbtc&lt;/a&gt; 作为中文版的简易便捷的场外交易平台，可支持支付宝、微信、银行卡直接交易，使得入门大大降低。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;优点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;支持支付宝、微信、银行卡。&lt;/li&gt;
&lt;li&gt;简单方便。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;缺点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;买卖都稍高于各个大交易所&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a id=&#34;org46681ed&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;gateio-io&#34;&gt;gateio.io&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/blockchain-base/gateio.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;优点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;支持支付宝、微信、银行卡。&lt;/li&gt;
&lt;li&gt;币种比较齐全，尤其是新币。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;缺点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;需要先兑换成usdt，再通过usdt购买所需的币种，两次交易较麻烦。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a id=&#34;org080feed&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;小结&#34;&gt;小结&lt;/h1&gt;

&lt;p&gt;区块链是一个比较新也比较复杂的技术系统，本着学习与分享的小学生心态，有些理解和认知不足在所难免，欢迎指正，大家一起学习，再次提醒&amp;rdquo;反对炒币、投资有风险、入市须谨慎&amp;rdquo;, 多多花时间研究其背后的技术或支持有价值有潜力的应用团队，让更多的区块链应用做到科技普惠。&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>从《习总：加快区块链发展》中我们如何应对</title>
            <link>http://mospany.github.io/2019/10/25/learn-blockchain-view/</link>
            <pubDate>Fri, 25 Oct 2019 14:06:08 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2019/10/25/learn-blockchain-view/</guid>
            <description>

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/learn-blockchain-view/main.jpg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org4daa5cd&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;引言&#34;&gt;引言&lt;/h1&gt;

&lt;p&gt;今天一则《习总：加快推动区块链产业创新发展》新闻在圈内炸开了锅， 让曾经炽手可热现在却处于寒冬时期的区块链行业又看到了曙光， 让从业人员可以光明正大的说自己这几年是做区块链的。&lt;br /&gt;
区块链在前两年很多人见面都谈区块链， 不谈都感觉自己快融不进去马上到来的价值互联网， 随着没有现象级应用落地和资本寒冬伴随的几次大裁员大洗礼，这一行业又慢慢淡出视野谁都觉得技术是好的但不知道春天什么时候到来， 很多公司倒闭很多人转行，随着鼓励政策一出，让我们浅析下政策悟出什么并如何准备和应对。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgc8e6388&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;区块链的诞生&#34;&gt;区块链的诞生&lt;/h1&gt;

&lt;p&gt;十年前，2018年10月31日，一个化名为中本聪的人发布了一份标题为《比特币：一个点对点电子现金系统》（Bitcoin: A Peer-to-Peer Electronic Cash System）的白皮书。随后2009年1月3日，比特币系统开始运行。转眼十年间，比特币经历了非同寻常的跌宕起伏，虽然仍在遭受质疑，但它依然还在。随后在此基础上诞生了很多币，人们开始背后的技术&amp;#x2013;区块链，发现这个技术适用于很多行业，认为是继TCP/IP之后最伟大的发明，将带来天翻地覆的变革。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org36f7a78&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;主要应用场景&#34;&gt;主要应用场景&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/learn-blockchain-view/use.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数字货币：货币数字化将杜绝了假币、容易丢失、容易损坏特性，由于数字货币是经过一定区块链算法生成将可以抵制通货膨胀导致的资本贬值和交易不可篡改而导致的抵赖行为。&lt;/li&gt;
&lt;li&gt;跨境支付: 现在跨境支付需要很多金融公司等经过多天的运算和转账才能成功，如果运用区块链技术产生的币(如：比特币)只需要几分钟就可以完成，大大加快了速度与减少成本。&lt;/li&gt;
&lt;li&gt;信息共享：保持各个节点的数据一致性的，利用区块链的不可篡改和共识机制，可构建其一条安全可靠的信息共享通道。&lt;/li&gt;
&lt;li&gt;版权保护: 可以利用于音乐、著作等的版权保护，区块链的去中心化存储，保证没有一家机构可以任意篡改数据。&lt;/li&gt;
&lt;li&gt;物流链：商品从生产商到消费者手中，需要经历多个环节，每个环节都写入区块链，具有可溯源防止假货。&lt;/li&gt;
&lt;li&gt;&amp;#x2026;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org9b18b3b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;人物与机构&#34;&gt;人物与机构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/learn-blockchain-view/people.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;两年前的区块链行业也曾经盛极一时，也诞生了很多大人物与公司机构，例如：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;中本聪：比特币&amp;amp;区块链之父， 手握100万比特币, 比特币最高单价2万美元左右。&lt;/li&gt;
&lt;li&gt;V神：以太坊的创始人V神，推出了智能合约技术，以太坊被认为是区块链2.0.&lt;/li&gt;
&lt;li&gt;李笑来：原新东方名师，自称“中国比特币首富”，《通往财富自由之路》作者。&lt;/li&gt;
&lt;li&gt;吴忌寒：比特大陆联合创始人，研发出矿机曾经占领全球70%以上的市场， 用了三四年时间估值高达300多亿美元。&lt;/li&gt;
&lt;li&gt;何一： 曾经的币圈一姐。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org3218f75&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;本质与发展&#34;&gt;本质与发展&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/learn-blockchain-view/develop.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;区块链短期看技术、中期看应用、长期改造社会，技术主要包含解决了拜占庭将军问题的共识机制，去中心化、分布式一致性和信用的建立。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/learn-blockchain-view/tech1.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org4e41416&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;数字货币&#34;&gt;数字货币&lt;/h1&gt;

&lt;p&gt;数字货币在区块链技术上发行， 具有交易成本低、高度匿名性和易分割特性，去中心化便于交易的特点，可存储在冷热钱包中。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/learn-blockchain-view/coin.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orga459b73&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;区块链技术&#34;&gt;区块链技术&lt;/h1&gt;

&lt;p&gt;区块链中大量使用了密码学保证了资产安全，利用了默克尔树支持了轻量级验证，使得节点存储量可以非常小。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/learn-blockchain-view/crypto.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org4a3bd54&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;城市生态图&#34;&gt;城市生态图&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/learn-blockchain-view/invest-bj.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;以北京为例，曾经出现了大量以区块链为中心的投资机构、公司、资讯服务、社区平台与工具等。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/learn-blockchain-view/company.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgd779c09&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;小结&#34;&gt;小结&lt;/h1&gt;

&lt;p&gt;区块链是个要技术有技术，要场景有场景，希望在国家政策推动下曾经风靡一时的现象能再回来，出现很多现象级应用改善现有社会，作为普通人的我们可以有钱出钱有力出力， 投资好可以高回报，有技术出力研发出有用好用的产品， 让互联网和资本走出寒冬， 大环境变好后至少大家好找工作赚到钱不更好么？&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>编辑器终结者: vim与emacs双剑合璧</title>
            <link>http://mospany.github.io/2019/10/17/vim-in-emacs/</link>
            <pubDate>Thu, 17 Oct 2019 13:54:40 CST</pubDate>
            <author>Mospan</author>
            <guid>http://mospany.github.io/2019/10/17/vim-in-emacs/</guid>
            <description>

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/vim-in-emacs/main.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgdbd1829&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;神器传说&#34;&gt;神器传说&lt;/h1&gt;

&lt;p&gt;在猿的世界里，流传着两大神器的传说： 一个是神的编辑器Emacs，另一个是编辑器之神Vim。&lt;br /&gt;
追求独步天下的高手为了得到它驾驭它，在江湖里宣起了几十年的血雨腥风至今任无法收拾呈蔓延趋势:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;有的把Emacs比作屠龙宝刀，Vim则是倚天剑， 有“武林至尊，宝刀屠龙，号令天下，莫敢不从，倚天不出，谁与争锋”之势。&lt;/li&gt;
&lt;li&gt;用Emacs的是海盗， 用Vim的是忍者。Emacs大而全， Vi小而快，Vi有两大模式：“命令模式”和“插入模式”，分别对应忍者的：“匿踪模式”和“战斗模式”；&lt;/li&gt;
&lt;li&gt;性子暴躁的喜欢vim，性格平缓的喜欢emacs，用vim能让你享受到敲击出噼里啪啦的肌肉快感， 而用emacs却能让心如止水、修心养性的感受。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;怎么样才能做到神在使用编辑器之神呢？&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org9bee0d2&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;追寻编辑器的心路历程&#34;&gt;追寻编辑器的心路历程&lt;/h1&gt;

&lt;p&gt;猿在江湖漂为了少挨刀，要么追求绝世武功，要么无上兵器，功夫不负有心人，最终找到它前也一路坎坷：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第一次见到师傅习武，仍是DOS时代使用的Turbo C（妈呀暴露年龄了），只见白色光标在蓝屏之间闪烁挥之不去。&lt;/li&gt;
&lt;li&gt;进入师门后先后经历了notepad、UltraEdit、visual c++ 6.0等刀、枪、剑、戟、斧、钺、钩、叉、鞭、锏、锤、挝、镋、棍、槊、棒、拐、流星锤十八种兵器，打下十八铜人阵拿到双证后下山出道。&lt;/li&gt;
&lt;li&gt;出道后先后尝试用了UltraEdit、visual c++ 6.0、sourceinsight、eclipse、sublime text、VS Code、Vim等始终不尽如意。&lt;/li&gt;
&lt;li&gt;直到后来遇见emacs， 才感受到了它的强大，作为一个伪装成的编辑器的操作系统，用着用着有时想单手操作，于是想到了把vim集成到emacs，既能享受到了emacs的强大又能用到vim的快捷，每天都有左右互博的久违感觉，目前已用多年再也纠结其他编辑器认定了它就是唯一的终极编辑器。&lt;/li&gt;
&lt;li&gt;为什么喜欢全键盘的终端模式而不用带鼠标的现在GUI IDE环境， 一个是全键盘更快更能专注其中， 而是现在已是云时代部署在公司开发时到家里也即可使用而不用同步不用每天背mbp回来，终端模式是最好的选择。&lt;/li&gt;
&lt;li&gt;都说编辑器分为三种， 一种是Vim，一种是emacs，剩下的就是Other， 你是属于哪一种呢？ 欢迎在留言下方分享您的使用心得。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;它们大致学习曲线如下：&lt;br /&gt;
&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/vim-in-emacs/learning-curves.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org3716a5b&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;安装配置&#34;&gt;安装配置&lt;/h1&gt;

&lt;p&gt;要想在emacs使用vim，其实是在emacs配置中增加Vim模拟器evil-mode即可。&lt;br /&gt;
先从官网下载git clone git://gitorious.org/evil/evil.git到.emacs.d下再安装,&lt;br /&gt;
在配置文件里.emacs或init.el里增加如下配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(add-to-list &#39;load-path &amp;quot;~/.emacs.d/evil&amp;quot;)
(require &#39;evil)
(evil-mode 1)
;(setq evil-default-state &#39;emacs) ;; emacs-mode
(setq evil-default-state &#39;normal) ;; vim-normal-mode
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;要在emacs和vim模式里面随意切换使用：C-z。&lt;br /&gt;
也可以把emacs中想用的功能映射到evil-mode下，这样就可以拥有了emacs中所有功能的快捷操作方式，它大致配置如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;;; I learn this trick from ReneFroger, need latest expand-region
;; @see https://github.com/redguardtoo/evil-matchit/issues/38
(define-key evil-visual-state-map (kbd &amp;quot;v&amp;quot;) &#39;er/expand-region)
(define-key evil-insert-state-map (kbd &amp;quot;C-e&amp;quot;) &#39;move-end-of-line)
(define-key evil-insert-state-map (kbd &amp;quot;C-k&amp;quot;) &#39;kill-line)
(define-key evil-insert-state-map (kbd &amp;quot;M-j&amp;quot;) &#39;yas-expand)
(define-key evil-emacs-state-map (kbd &amp;quot;M-j&amp;quot;) &#39;yas-expand)
(global-set-key (kbd &amp;quot;C-r&amp;quot;) &#39;undo-tree-redo)

;; My frequently used commands are listed here
(setq evil-leader/leader &amp;quot;,&amp;quot;)
(require &#39;evil-leader)
(evil-leader/set-key
 ;windowmove
 &amp;quot;wh&amp;quot; &#39;windmove-left
 &amp;quot;wl&amp;quot; &#39;windmove-right
 &amp;quot;wk&amp;quot; &#39;windmove-up
 &amp;quot;wj&amp;quot; &#39;windmove-down
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体可参考&lt;a href=&#34;https://github.com/mospany/emacs.d-for-linux/blob/master/config/init-evil.el即可&#34;&gt;https://github.com/mospany/emacs.d-for-linux/blob/master/config/init-evil.el即可&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orge046aca&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;emacs功能概览&#34;&gt;emacs功能概览&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;收发电子邮件&lt;/li&gt;
&lt;li&gt;通过FTP/TRAMP编辑远程档案&lt;/li&gt;
&lt;li&gt;通过Telnet登录主机&lt;/li&gt;
&lt;li&gt;上新闻组&lt;/li&gt;
&lt;li&gt;对多种编程语言的编辑&lt;/li&gt;
&lt;li&gt;调试程序，结合GDB，EDebug等，支持C/C++，Perl，&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Python，Lisp等等&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;玩游戏&lt;/li&gt;
&lt;li&gt;管理日程，Task，ToDo，约会等&lt;/li&gt;
&lt;li&gt;个人信息管理&lt;/li&gt;
&lt;li&gt;文件比较&lt;/li&gt;
&lt;li&gt;阅读info和man文档&lt;/li&gt;
&lt;li&gt;浏览网站&lt;/li&gt;

&lt;li&gt;&lt;p&gt;为各种程序（TeX等）提供统一的操作界面&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/vim-in-emacs/emacs-frame.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org166e498&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;文档编辑&#34;&gt;文档编辑&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;写文档(org)&lt;/li&gt;
&lt;li&gt;做幻灯片(beamer)&lt;/li&gt;
&lt;li&gt;做表格&lt;/li&gt;
&lt;li&gt;列编辑(cua)&lt;/li&gt;
&lt;li&gt;记笔记(evernote)&lt;/li&gt;
&lt;li&gt;写博客(cnblogs)&lt;/li&gt;
&lt;li&gt;时间管理(gtd)&lt;/li&gt;
&lt;li&gt;任务计划(gtd)&lt;/li&gt;
&lt;li&gt;思维导图(freemind)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orgb22db31&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;org-mode&#34;&gt;Org-mode&lt;/h2&gt;

&lt;p&gt;Org-模式（Org-mode）是文本编辑软件Emacs的一种支持内容分级显示的编辑模式。&lt;br /&gt;
这种模式支持写 to-do 列表，日志管理，做笔记，做工程计划或者写网页。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/vim-in-emacs/org-mode.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgf33b210&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;编程相关&#34;&gt;编程相关&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;适用多种语言编写&lt;/li&gt;
&lt;li&gt;代码阅读(XCscope和ecb)&lt;/li&gt;
&lt;li&gt;代码调试(gdb)&lt;/li&gt;
&lt;li&gt;代码搜索定位(grep)&lt;/li&gt;
&lt;li&gt;代码着色&lt;/li&gt;
&lt;li&gt;自动补全&lt;/li&gt;
&lt;li&gt;统一注释格式(doxymacs)&lt;/li&gt;
&lt;li&gt;代码折叠&lt;/li&gt;
&lt;li&gt;智能编译&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;org4dcec9d&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;版本控制-git&#34;&gt;版本控制(git)&lt;/h2&gt;

&lt;p&gt;在emacs用git功能，更加方便的进行git库的提交/查看等版本控制功能。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/vim-in-emacs/emacs-git.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgc0f6593&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;休闲娱乐&#34;&gt;休闲娱乐&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;看电影听音乐(EMMS)&lt;/li&gt;
&lt;li&gt;心理医生(doctor)&lt;/li&gt;
&lt;li&gt;游戏(俄罗斯方块/贪食蛇)&lt;/li&gt;
&lt;li&gt;屏幕保护(zone)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#34;orgfc36767&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;看电影听音乐-emms&#34;&gt;看电影听音乐(EMMS)&lt;/h2&gt;

&lt;p&gt;EMMS 让你可以在 Emacs 里面播放多媒体文件，他被设计的小巧干净，使用外部 播放器进行播放。是一个小巧、可扩展的完全由 Elisp写成的多媒体系统，支持 多种音频、视频格式以及流媒体播放(这事实上由你所使用的播放器决定)，可以 进行标签操作、方便的播放列表管理以及打分制度，&lt;br /&gt;
总之，这正是我想要的。&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgefe8084&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;听音乐&#34;&gt;听音乐&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/vim-in-emacs/emacs-music.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org71fcae1&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;看电影&#34;&gt;看电影&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/vim-in-emacs/emacs-movice.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;org2740f5f&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;游戏&#34;&gt;游戏&lt;/h2&gt;

&lt;p&gt;俄罗斯方块或贪食蛇等&lt;br /&gt;
&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/vim-in-emacs/emacs-game.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;orgded90a9&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;做美食&#34;&gt;做美食&lt;/h1&gt;

&lt;p&gt;&lt;a id=&#34;orgebe7bab&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;煮咖啡-炒川菜&#34;&gt;煮咖啡、 炒川菜&lt;/h2&gt;

&lt;p&gt;RFC中支持超文本咖啡壶协议（RFC2324）与茶壶协议(RFC7168),而emacs提供了完全支持RFC2323的coffee.el插件提供控制展示界面，只要你有一根网线，加一个支持此协议的咖啡壶，你确实可以用emacs煮咖啡，甚至有人已经用它来炒过川菜。&lt;br /&gt;
&lt;img src=&#34;http://blog.mospan.cn/post/img/2019/vim-in-emacs/emacs-cook.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
